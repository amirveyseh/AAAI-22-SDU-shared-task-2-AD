[{"sentence": "Dialogue fillers and acceptance words af- fect the accuracy of POS tagging.", "acronym": "POS", "label": "Part-Of-Speech", "ID": "1"}, {"sentence": "Another possible explanation is that as a set of spoken dialogue data, Loqui is inherently more difficult to process than written form, since some common tasks such POS tagging have lower accuracy for spoken data.", "acronym": "POS", "label": "Part-Of-Speech", "ID": "2"}, {"sentence": "POS tags: we chose the GENIA dependency parser for parsing the corpus for two reasons.", "acronym": "POS", "label": "Part-Of-Speech", "ID": "3"}, {"sentence": "We built up three systems based on Condi- tional Random Field for Chinese Word  Segmentation, Named Entity Recognition  and POS Tagging respectively.", "acronym": "POS", "label": "Part-Of-Speech", "ID": "4"}, {"sentence": "Our final system achieve a F-score of  94.18 at CTB, 92.86 at NCC, 94.59 at SXU  on Segmentation, 85.26 at MSRA on  Named Entity Recognition, and 90.65 at  PKU on POS Ta", "acronym": "POS", "label": "Part-Of-Speech", "ID": "5"}, {"sentence": "The corpus has been part of speech tagged and lemmatized with Stanford POS Tagger (Toutanova et al.,", "acronym": "POS", "label": "Part-Of-Speech", "ID": "6"}, {"sentence": "A Maximum Entropy Model  for POS Tagging.", "acronym": "POS", "label": "Part-Of-Speech", "ID": "7"}, {"sentence": "In fact, the only POS tags  necessary are those indicating nouns and pro-  nouns .", "acronym": "POS", "label": "part-of-speech", "ID": "8"}, {"sentence": "The training attributes  consist of lexical word attributes (surface word, stem  form, POS, semantic ode, morphological  attributes) applied to the anaphor, antecedent can-  didate, and clause predicate.", "acronym": "POS", "label": "part-of-speech", "ID": "9"}, {"sentence": "Antecedent candidates are  identified according to noun phrase POS  tags.", "acronym": "POS", "label": "part-of-speech", "ID": "10"}, {"sentence": "Our preliminary experiments did not show noticeable improvements from bi- gram or character-based features, but it is pos- sible that higher-level features such as morpho- logical, POS or syntactic features could yield further performance gains.", "acronym": "POS", "label": "part-of-speech", "ID": "11"}, {"sentence": "Including the disjunctive POS types, just under half (49%) of the types in the grammar are provided by the Matrix.", "acronym": "POS", "label": "part-of-speech", "ID": "12"}, {"sentence": "This apparent parser overkill is a control  to ensure that the POS tags assigned  to words are the same when we use the previ-  ous noun heuristic and the Hobbs algorithm, to  which we wish to compare the previous noun  method.", "acronym": "POS", "label": "part-of-speech", "ID": "13"}, {"sentence": "POS tags are not used in the n-gram system except to identify insertion points for missing prepositions and determiners.", "acronym": "POS", "label": "Part of speech", "ID": "14"}, {"sentence": "log ntypes log ntokens ) POS distribution The relative frequen- cies of different parts of speech also correlate with essay grade, although more weakly so than the re- lated measure of average word length.", "acronym": "POS", "label": "Part of speech", "ID": "15"}, {"sentence": "POS ambiguity,  3.", "acronym": "POS", "label": "Part of speech", "ID": "16"}, {"sentence": "POS tagging and chunking with hmm and crf.", "acronym": "POS", "label": "Part of speech", "ID": "17"}, {"sentence": "POS, however, is not balanced across our seed sets.", "acronym": "POS", "label": "Part of speech", "ID": "18"}, {"sentence": "POS, or \"class.\"", "acronym": "POS", "label": "Part of speech", "ID": "19"}, {"sentence": "Recall = #POS classified correctly#POS Precision = #POS classified correctly#instances classified as positive The above metrics evaluate the capability of the learned classifier in identifying posi- tive instances3.", "acronym": "POS", "label": "positive instances", "ID": "20"}, {"sentence": "These heuristics filter out redundant constituents  and raise the ratio of POS in the  dataset.", "acronym": "POS", "label": "positive instances", "ID": "21"}, {"sentence": "DDI interaction 14.47% Interaction type effect 6.07% Interaction type advise 2.97% Interaction type mechanism 4.75% Interaction type int 0.68% Table 1: POS for the different class types The data is not balanced, as shown in table 1.", "acronym": "POS", "label": "positive instances", "ID": "22"}, {"sentence": "Collection of POS As indicated be- fore, every sentence from the original documents matching a summary sentence that contains at least one SCU is considered a positive example.", "acronym": "POS", "label": "positive instances", "ID": "23"}, {"sentence": "For instance, the number of constituents labeled to  arguments (POS) is much less than  the number of the rest (negative instances).", "acronym": "POS", "label": "positive instances", "ID": "24"}, {"sentence": "Table1 shows the percentage of POS in the dataset.", "acronym": "POS", "label": "positive instances", "ID": "25"}, {"sentence": "The definite article marker is placed on  the second word of the construction:  (2) \u0001\u0002\u0003\u0004\u0005\u0006\u0007   beit sefer / house-[const] book   School  (3) \u0001\u0002\u0003\b\u0004\u0005\u0006\u0007   beit ha-sefer / house-[const] the-book   The school    The construct form can also be embedded:  (4) \b\t \u000b\u000b\b\u0004 \f\u0001\u0004 \u0001 \u000b  690 misrad ro$ ha-mem$ala   Office-[const poss] head-[const] the-government  The prime-minister?s office    POS: the smixut form can be used to indi- cate possession.", "acronym": "POS", "label": "Possessive", "ID": "26"}, {"sentence": "The content of Figure 1 can be reconstructed  straightforwardly asa category structure subject o a set  of L c constraints (for a closely related analysis of this  10  {~ Animate  Question _ _  -- Subjective  Case Objective  Reflex ve  POS POS-Determ ner  _ I First  Personal ~_ .P__~ Second _ _ I Feminine  Ingu la r - -  J Neuter  f \\[ | Plural  Demonstrative - - l~  Near  / Far  Figure 1: Systemic Network for English Pronouns  example, developed independently, see Mellish (1986).", "acronym": "POS", "label": "Possessive", "ID": "27"}, {"sentence": "Premodifiers, POS, Preposition, For- mulaic and Verbal.", "acronym": "POS", "label": "Possessive", "ID": "28"}, {"sentence": "Premodifier relations specify the proper adjective or proper noun premodifier and the following noun it modifies, e.g.: [the [Seattle] zoo] POS indicates that the first mention is in a possessive case, e.g.: [[California] ?", "acronym": "POS", "label": "Possessive", "ID": "29"}, {"sentence": "% related PPs are  allowed:   \u0012\u0013\u0004\u0001\u0005\u000e\u0001\u0006\u0014\u000b\b\u0004\u0004\u0004 5% of the sales    POS \u0011\f - '$el' /  'of' - is not consid- ered a PP  Table 2.", "acronym": "POS", "label": "Possessive", "ID": "30"}, {"sentence": "739 Supt Richard-Nixon Supt Abraham-Lincoln 50 Date of birth: 419 Government POSs Held: - Jan 9, 1913 - United States Representative Mar 4,1847-Mar 3,1849 108 Tracks Recorded: 558 Military Commands: - 23-73 Broadcast: End of the Vietnam War - American Civil War - United States of America 120 Works Written About This Topic: 810 Quotations: - Nearly all men can stand adversity, but if - Watergate you want to test a man?s character, give him power.", "acronym": "POS", "label": "Position", "ID": "31"}, {"sentence": "POS of first occurrence : Important concepts are expected to be mentioned before less relevant ones.", "acronym": "POS", "label": "Position", "ID": "32"}, {"sentence": "2 3 POS sets and binarizations Throughout this section we assume an LCFRS production p : A?", "acronym": "POS", "label": "Position", "ID": "33"}, {"sentence": "POS of first appearance: the position  where a keyword first appears in a doc- ument, normalized by number of words  in the document.", "acronym": "POS", "label": "Position", "ID": "34"}, {"sentence": "POS and scope of epis- temic phrases in planned and unplanned american english.", "acronym": "POS", "label": "Position", "ID": "35"}, {"sentence": "- Martin Feldstein Reagan?s first National Security advisor was quoted as declaring... - Chief Economic Advisor Government POSs Held: 967 1981 Jan 20, Ronald Reagan was sworn in as president as 52 American hostages - President of the United States boarded a plane in Tehran and headed toward freedom.", "acronym": "POS", "label": "Position", "ID": "36"}, {"sentence": "Helmut Schmid, 1994, Probabilistic POS Tag- ging Using Decision Trees, Intl.", "acronym": "POS", "label": "Part-of-Speech", "ID": "37"}, {"sentence": "The 529 PoS tagging Lemmatisation PoS after Lemmas error rate error rate update after update French 4.50 % 2.29 % 1.92 % 1.22 % English 5.16 % 3.13 % 2.66 % 3.03 % Table 1: POS tagging and lemmatisation error rate on the test sentences average sentence length of a database entry is 9 words.", "acronym": "POS", "label": "Part-of-Speech", "ID": "38"}, {"sentence": "Arabic Tok- enization, POS Tagging and Morphological Disambiguation in One Fell Swoop.", "acronym": "POS", "label": "Part-of-Speech", "ID": "39"}, {"sentence": "l/,obust POS Tagging  Using a lIidden Markov Model.", "acronym": "POS", "label": "Part-of-Speech", "ID": "40"}, {"sentence": "770  Learning POS Guessing Rules from Lexicon:  Extension to Non-Concatenat ive Operations*  Andre i  M ikheev   HCRC Language Techno logy  Group  Un ivers i ty  of Ed inburgh   2 Buccleuch Place  Edinburgh EH8 9LW, Scotland, UK  : Andrei.", "acronym": "POS", "label": "Part-of-Speech", "ID": "41"}, {"sentence": "On the tech- nology side, the tutorial mainly covers Chinese  word segmentation and POS tagging.", "acronym": "POS", "label": "Part-of-Speech", "ID": "42"}, {"sentence": "Transformation-Based Error-  Driven Learning and Natural Language  Processing: A Case Study in POSh  Tagging. (", "acronym": "POS", "label": "Part Of Speec", "ID": "43"}, {"sentence": "A Simple Rule-Based POSh Tagger (Workshop On Speech And Natural Language, 1992) As Figure 1 shows, probabilistic models seem to have arrived significantly before classifiers.", "acronym": "POS", "label": "Part Of Speec", "ID": "44"}, {"sentence": "7 FirstLevel Preprocessing Second Level Preprocessing Editors and Interfaces Models and Other Applications Higher Level Multilingual NLP Applications Text Language-Encoding Identification Encoding Converters Text Normalization Sentence Splitting Tokenization Morphological Analyzer Encoding Converter Generator Model of Scripts Spell Checker Model of Morphology POSh Tagger Other Specialized Interfaces Text Editor Annotation Interfaces Local Word Grouper or Chunker Figure 1: One view of the basic computational in- frastructure required for Natural Language Process- ing or Computational Linguistics.", "acronym": "POS", "label": "Part Of Speec", "ID": "45"}, {"sentence": "Linguistics features like POSh, Chunk, etc are also used.", "acronym": "POS", "label": "Part Of Speec", "ID": "46"}, {"sentence": "path through the parse  tree from the parse constituent  to  the predicate being classified  Position A binary feature identifying whether  the phrase is before or after the  predicate  Phrase Type The syntactic category of the phrase  corresponding to the argument  Phrase type of the  sibling to the left The syntactic category of the phrase  is sibling to the argument in the left Head Word and  POSh  The syntactic head of the phrase  First and last word  of the constituent  in focus  First and last word of phrase corre- sponding to the argument  Syntactic Frame The syntactic frame consists of the  NPs that surround the predicate  Table 1.", "acronym": "POS", "label": "Part Of Speec", "ID": "47"}, {"sentence": "WordNet (292) POSh (45) Precision (172) Probablistic CFG (43) Recall (167) FrameNet (38) Noun phrase (97) Conditional Random Field (29) Word sense disambiguation (60) Inverse document frequency (28) Support Vector Machine (60) PropBank (27) Hidden Markov Model (54) Context Free Grammar (25) Latent Semantic Analysis (57) Accuracy (20) Table 6: Subset of most frequently defined terms.", "acronym": "POS", "label": "Part Of Speec", "ID": "48"}, {"sentence": "MBT: A memory-based POSh tagger generator.", "acronym": "POS", "label": "part of speec", "ID": "49"}, {"sentence": "Turning this around, if a word which is ambiguous between a preposition and an- other POSh is not followed by the respective form till the end of the sentence, it is safe to discard the prepositional reading in almost all non-idiomatic, non-coordinated cases.", "acronym": "POS", "label": "part of speec", "ID": "50"}, {"sentence": "types, the powerset of 9 POSh types.", "acronym": "POS", "label": "part of speec", "ID": "51"}, {"sentence": "Since the nine proposed POSh types have varying crosslinguistic validity (e.g., not all languages have conjunctions), it might be better to provide software support for creating the disjunctive types as the need arises, rather than predefining them.", "acronym": "POS", "label": "part of speec", "ID": "52"}, {"sentence": "Setting aside the types for POSh disjunc- tions, 59% of the Matrix-provided types are invoked by the Wambaya-specific portion of the grammar.", "acronym": "POS", "label": "part of speec", "ID": "53"}, {"sentence": "Our framework opens up the possibility of efficiently adding many other con- straints that are directly applicable to word alignments, such as preferring alignments that respect dependency tree structure, POSh tags, or syntactic boundaries.", "acronym": "POS", "label": "part of speec", "ID": "54"}, {"sentence": "Chinese POS tagging: One-at-a-time or all-at-once?", "acronym": "POS", "label": "partof-speech", "ID": "55"}, {"sentence": "c?2009 Association for Computational Linguistics     Tagging Urdu Text with POS: A Tagger Comparison    Hassan Sajjad  Universit?t Stuttgart  Stuttgart.", "acronym": "POS", "label": "Parts of Speech", "ID": "56"}, {"sentence": "2.2 POS and Morphology  In traditional Sinhala grammar, several classifi- cations have been proposed for parts of speech.", "acronym": "POS", "label": "Parts of Speech", "ID": "57"}, {"sentence": "Development of a Web-scale Chinese Word N-gram Corpus  with POS Information.", "acronym": "POS", "label": "Parts of Speech", "ID": "58"}, {"sentence": "POS (PoS) extends the WoC model by appending each lexical feature with its part of speech.", "acronym": "POS", "label": "Parts of Speech", "ID": "59"}, {"sentence": "4.2 Extending the Approach to Other  POS  The two-step approach to generating verbal  morphology also presents advantages for the  inflectional morphology of nouns and  adjectives.", "acronym": "POS", "label": "Parts of Speech", "ID": "60"}, {"sentence": "POS Tagging.", "acronym": "POS", "label": "Parts of Speech", "ID": "61"}, {"sentence": "rel:POS=NN} ?{", "acronym": "POS", "label": "postag", "ID": "62"}, {"sentence": "rel:POS=VBN} ?", "acronym": "POS", "label": "postag", "ID": "63"}, {"sentence": "rel:POS=VBD} ?", "acronym": "POS", "label": "postag", "ID": "64"}, {"sentence": "slot:POS=VBN;lex ?", "acronym": "POS", "label": "postag", "ID": "65"}, {"sentence": "arg1; be {rel} of; arg2) {rel:POS=NN;type=Person} ?", "acronym": "POS", "label": "postag", "ID": "66"}, {"sentence": "rel:POS=VBN} ?{", "acronym": "POS", "label": "postag", "ID": "67"}, {"sentence": "The main bottlenecks have been sentence connectors and non-projective dependencies which could not be straightforwardly converted into projec- tive tree structures, requiring a mechanism similar to traces in the PTB.", "acronym": "PTB", "label": "Penn English Treebank", "ID": "68"}, {"sentence": "Our primary reference sets are derived from the PTB?s Wall Street Journal por- tion (Marcus et al, 1993): WSJ45 (sentences with fewer than 46 tokens) and Section 23 of WSJ? (", "acronym": "PTB", "label": "Penn English Treebank", "ID": "69"}, {"sentence": "3.2 Data Sets and Scoring We trained on the PTB?s Wall Street Journal portion (Marcus et al, 1993).", "acronym": "PTB", "label": "Penn English Treebank", "ID": "70"}, {"sentence": "The PTB (Marcus, Marcinkiewicz, and Santorini 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality.", "acronym": "PTB", "label": "Penn English Treebank", "ID": "71"}, {"sentence": "4 Experiments  The CoNLL 2000 provided the software4 to con- vert PTB II into the IOB tags  form.", "acronym": "PTB", "label": "Penn English Treebank", "ID": "72"}, {"sentence": "2.2 PropBank Like FrameNet, PropBank (Kingsbury et al, 2002) is a project aimed at semantic annotation, in this case of the PTB.4 The intent of PropBank is to provide for ?", "acronym": "PTB", "label": "Penn English Treebank", "ID": "73"}, {"sentence": "We used the English WSJ PTB corpus in our experiments.", "acronym": "PTB", "label": "PennTreebank", "ID": "74"}, {"sentence": "The POS tagset used by the POS-based features is that of the WSJ PTB (see Section 7).", "acronym": "PTB", "label": "PennTreebank", "ID": "75"}, {"sentence": "To illustrate this point, the distance between known|classic/JJ and old/JJ 2Note that, although our tagger produces the very detailed PTB labels, we consider that all nouns (NN, NNS, NNP and NNPS) belong to the same part-of-speech class, and the same for adjectives, verbs and adverbs.", "acronym": "PTB", "label": "PennTreebank", "ID": "76"}, {"sentence": "McClosky et al(2006a) use sections 2-21 of the WSJ PTB as seed data and between 50K to 2,500K unlabeled NANC corpus sentences as self-training data.", "acronym": "PTB", "label": "PennTreebank", "ID": "77"}, {"sentence": "The number of supertags is generally much larger than the number of labels used in other sequence labeling tasks; Comparing to 45 POS tags used in PTB, the HPSG grammar used in our experiments includes 2,308 supertags.", "acronym": "PTB", "label": "PennTreebank", "ID": "78"}, {"sentence": "4 Experiments and Analysis We conducted experiments on WSJ-HPSG tree- bank corpus (Miyao, 2006), which was semi- automatically converted from the WSJ portion of PTB.", "acronym": "PTB", "label": "PennTreebank", "ID": "79"}, {"sentence": "The SPE.", "acronym": "SPE", "label": "Sound Pattern of English", "ID": "80"}, {"sentence": "The  SPE.", "acronym": "SPE", "label": "Sound Pattern of English", "ID": "81"}, {"sentence": "Stedman's Medical Dic t ionary ,  Baltimore, 1961  Although some phonologis ts  would consider  both prevocal ic  t i l a n d  morph-  f i n a l  101 t o  be underlyingly s h o r t  ( l a x ) ,  and would apply a lengthening  ( te<sing)  r u l e  t o  them a f t e r  stress placement (N. Chomsky and M. Hal le ,   The SPE (New York, 1968),  p a  74; M. Ha l le  and S.J.  Keyser, Engl ish  Stress (New York, 1971)  p'.", "acronym": "SPE", "label": "Sound Pattern of English", "ID": "82"}, {"sentence": "N.. and Halle, M., The SPE.", "acronym": "SPE", "label": "Sound Pattern of English", "ID": "83"}, {"sentence": "We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  SPEally the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.", "acronym": "SPE", "label": "specific", "ID": "84"}, {"sentence": "42 3.3 Quote Attribution & Speaker Identification Here the goal is to attribute (or assign) each quote to a SPE story character from the set identified in the previous step.", "acronym": "SPE", "label": "specific", "ID": "85"}, {"sentence": "This system performs the following story analysis tasks: identification of charac- ters in each story; attribution of quotes to SPE story characters; identification of character age, gender and other salient personality attributes; and finally, affective analysis of the quoted material.", "acronym": "SPE", "label": "specific", "ID": "86"}, {"sentence": "Regarding the story-SPE information, the associations between characters and third person pronouns (identified via anaphora resolution) were counted.", "acronym": "SPE", "label": "specific", "ID": "87"}, {"sentence": "We SPEally extract the character reference CH either from the dependency relation nsubj, which links a speech verb SV with a CH that is the syntactic subject of a clause, or from the dependency relation dobj, which links a SV with a CH that is the direct object of the speech verb, across a conjunct (e.g., and).", "acronym": "SPE", "label": "specific", "ID": "88"}, {"sentence": "The system also con-  tains domain knowledge including the domain  concepts, SPE list of subjects and verbs, and  topic headings.", "acronym": "SPE", "label": "specific", "ID": "89"}, {"sentence": "As a lower bound, we evaluated the 1220 most frequent candidates in our Monolingual corpus (FC).", "acronym": "FC", "label": "Frequent Candidates", "ID": "90"}, {"sentence": "This method 641 Recall-at-1220 Dev Test FC 17.0 19.3 B as el in e WordNet 3.0 Frequent 41.6 43.7 WordNet 3.0 Filtered 49.4 48.8 Monolingual Only 30.1 30.2 B oo st ed Bilingual Only 47.1 43.9 Monolingual+Bilingual 50.8 47.9 Table 3: Our boosted ranker combining monolingual and bilingual features (bottom) compared to three base- lines (top) gives comparable performance to the human- curated upper bound.", "acronym": "FC", "label": "Frequent Candidates", "ID": "91"}, {"sentence": "-variables to be type consistent, and forcing the resulting FC to be acyclic and fully connected (we refer the reader to (Clarke et al, 2010) for more details).", "acronym": "FC", "label": "functional composition", "ID": "92"}, {"sentence": "x) and concatena- tion (+) is defined to be FC (?", "acronym": "FC", "label": "functional composition", "ID": "93"}, {"sentence": "Essentially, we now com- bine partial dependency trees using forward and backward concatenation rather than combining se- mantic representations by FC and application.", "acronym": "FC", "label": "functional composition", "ID": "94"}, {"sentence": "Figure 1 lists a core set of commonly assumed rules, derived from functional application and the B combinator, which models FC.", "acronym": "FC", "label": "functional composition", "ID": "95"}, {"sentence": "Non-ellipsis-based approach is characterized by: (a) strong proof sys- tem (Lambek, 1958), and (b) FC and type raising that allow coordination of incom- plete constituents, such as CG (Ajdukiewicz, 1935; Bar-Hillel, 1953; Moortgat, 2002), CCG (Steed- man, 2000), and multimodal CCG (Baldridge and Kruijff, 2003).", "acronym": "FC", "label": "functional composition", "ID": "96"}, {"sentence": "The rules derive state- ments about triples w ` A : f , expressing that the substring w can be assigned the category A and the semantic representation f ; an entire string counts as grammatical if it can be assigned the start cat- egory s. In parallel to the combination of substrings by the combinatory rules, their semantic represent- ations are combined by FC.", "acronym": "FC", "label": "functional composition", "ID": "97"}, {"sentence": "To solve this and other problems, a tree is now defined  to be admissible only if each non-terminal node of the  tree satisfies the FC, which is related to the  original FFP of GPSG 85.", "acronym": "FC", "label": "foot condition", "ID": "98"}, {"sentence": "The FC is defined  as follows.", "acronym": "FC", "label": "foot condition", "ID": "99"}, {"sentence": "\"LF and language  learning.\"", "acronym": "LF", "label": "Lexical functions", "ID": "100"}, {"sentence": "LF: A tool for the description of lexical relations in a lexicon.", "acronym": "LF", "label": "Lexical functions", "ID": "101"}, {"sentence": "LF and  knowledge representation.", "acronym": "LF", "label": "Lexical functions", "ID": "102"}, {"sentence": "LF: a tool for the description of lexical relations in a lexicon.", "acronym": "LF", "label": "Lexical functions", "ID": "103"}, {"sentence": "4.2 LF Except wh(N), which generates interrogatives as shown in the bottom line of Table 1, the relations we have so far implemented are lexical derivations.", "acronym": "LF", "label": "Lexical functions", "ID": "104"}, {"sentence": "LF: a tool for the descrip- tion of lexical relations in a lexicon.", "acronym": "LF", "label": "Lexical functions", "ID": "105"}, {"sentence": "A trivial adaptation also reveals which parts of an abbreviation (one or more characters) map to which parts of the LF (one token, one partial token.)", "acronym": "LF", "label": "long form", "ID": "106"}, {"sentence": "at the end of a name (for in- stance, a noun phrase) hints on a family-member or 4The original algorithm decides whether a given short form can be explained by a given LF.", "acronym": "LF", "label": "long form", "ID": "107"}, {"sentence": "has to match the first letter of the proposed LF.", "acronym": "LF", "label": "long form", "ID": "108"}, {"sentence": "As S&H allows for miss- ing tokens in the LF, we can also add the pos- sibility for (few) characters in the abbreviation not being reflected in the LF.", "acronym": "LF", "label": "long form", "ID": "109"}, {"sentence": "We changed some of the details so that, for instance, the first letter of the potential abbreviation has to match the first letter of the proposed LF.", "acronym": "LF", "label": "long form", "ID": "110"}, {"sentence": "LF) for the FASLG gene.", "acronym": "LF", "label": "long form", "ID": "111"}, {"sentence": "Working with LF Skolemization: Skolemization of an existential formula of type (some x R S), where x is a variable, R is a restrictor formula and S is the nuclear scope, is performed via the transduction (/ (some ! !", "acronym": "LF", "label": "Logical Forms", "ID": "112"}, {"sentence": "Examples are Quasi LF (Al- shawi, 1990), Dynamic Predicate Logic (Groe- nendijk and Stokhof, 1991), and Underspecified Discourse Representation Theory (Reyle, 1993).", "acronym": "LF", "label": "Logical Forms", "ID": "113"}, {"sentence": "Resolving Quasi LF,  Computational Linguistics, 6(13), pp.", "acronym": "LF", "label": "Logical Forms", "ID": "114"}, {"sentence": "Deriving Database  Queries from LF by Abductive Definition  Expansion\".", "acronym": "LF", "label": "Logical Forms", "ID": "115"}, {"sentence": "5 Building LF  Based on entries in the Generative Lexicon and  on the context given by a sentence to be inter-  preted, appropriate logical forms can be built  that represent semantic relations involved more  explicitly than this is the case with previous  approaches.", "acronym": "LF", "label": "Logical Forms", "ID": "116"}, {"sentence": "Generation of Paraphrases from  Ambiguous LF.", "acronym": "LF", "label": "Logical Forms", "ID": "117"}, {"sentence": "To compute soft truth values for LFulas, Lukasiewicz?s re- laxation of conjunctions(?),", "acronym": "LF", "label": "logical form", "ID": "118"}, {"sentence": "Inflected queries are performed by expanding an n-gram into all its morphoLFs.", "acronym": "LF", "label": "logical form", "ID": "119"}, {"sentence": "The MLN constructed to determine the probability of a given entailment includes the LFs for both T and H as well as soft inference rules that are constructed from distributional information.", "acronym": "LF", "label": "logical form", "ID": "120"}, {"sentence": "Unifying LFs to instantiate variables in this way follows the ?", "acronym": "LF", "label": "logical form", "ID": "121"}, {"sentence": "Given a set of weighted LFulas, PSL builds a graphical model defining a probability distribution over the continuous space of values of the random variables in the model.", "acronym": "LF", "label": "logical form", "ID": "122"}, {"sentence": "This guessing is based on the morphoLF of the surface form.", "acronym": "LF", "label": "logical form", "ID": "123"}, {"sentence": "Experiments in both (Shriberg et al, 2000) and (Kim et al, 2004) find no conclusive win- ner among early fusion, additive LF, and multiplicative LF.", "acronym": "LF", "label": "late fusion", "ID": "124"}, {"sentence": "6 Results and Discussion In this section we present the results of our empiri- cal evaluation designed to test the three main char- acteristics of the LMDE model: (1) integration of multiple sources of information, (2) LF ap- proach and (3) latent variable which models the hidden dynamic between experts.", "acronym": "LF", "label": "late fusion", "ID": "125"}, {"sentence": "As in LF, m", "acronym": "LF", "label": "late fusion", "ID": "126"}, {"sentence": "As in LF, modality- specific classifiers are trained independently.", "acronym": "LF", "label": "late fusion", "ID": "127"}, {"sentence": "Due to the very different nature of the models we opt to not fuse them at the feature level, using a LF scheme instead.", "acronym": "LF", "label": "late fusion", "ID": "128"}, {"sentence": "LF.?", "acronym": "LF", "label": "late fusion", "ID": "129"}, {"sentence": "0  2  4  6  8  10  12  0  5000  10000  15000  20000Solution Id LF of solutions for sentences with 5 tokens (a) Sentence length = 5 -1  0  1  2  3  4  5  6  7  8  0  50000  100000  150000  200000  250000Solution Id LF of solutions for sentences with 10 tokens (b) Sentence length = 10 -1  0  1  2  3  4  5  6  7  8  0  50000  100000  150000  200000  250000  300000  350000Solution Id LF of solutions for sentences with 15 tokens (c) Sentence length = 15 Figure 3: These plots show the log-frequencies of occurrences of part-of-speech sequences for sentences with five, ten and fifteen tokens.", "acronym": "LF", "label": "Log frequency", "ID": "130"}, {"sentence": "LF of  the head noun was not significantly correlated  with plausibility (r = .098), which suggests  that adjective-noun plausibility judgements are  not influenced by noun familiarity.", "acronym": "LF", "label": "Log frequency", "ID": "131"}, {"sentence": "0  2  4  6  8  10  12  0  5000  10000  15000  20000Solution Id LF of solutions for sentences with 5 tokens (a) Sentence length = 5 -1  0  1  2  3  4  5  6  7  8  0  50000  100000  150000  200000  250000Solution Id LF of solutions for sentences with 10 tokens (b) Sentence length = 10 -1  0  1  2  3  4  5  6  7  8  0  50000  100000  150000  200000  250000  300000  350000Solution Id LF of solutions for sentences with 15 tokens (c", "acronym": "LF", "label": "Log frequency", "ID": "132"}, {"sentence": "LF     This feature takes the logarithm of the co- occurrence frequency of the phrase pair.", "acronym": "LF", "label": "Log frequency", "ID": "133"}, {"sentence": "LF biased MD log P (xy) 2 P (x?)P (?", "acronym": "LF", "label": "Log frequency", "ID": "134"}, {"sentence": "4  6  8  10  12  0  5000  10000  15000  20000Solution Id LF of solutions for sentences with 5 tokens (a) Sentence length = 5 -1  0  1  2  3  4  5  6  7  8  0  50000  100000  150000  200000  250000Solution Id LF of solutions for sentences with 10 tokens (b) Sentence length = 10 -1  0  1  2  3  4  5  6  7  8  0  50000  100000  150000  200000  250000  300000  350000Solution Id LF of solutions for sentences with 15 tokens (c) Sentence length = 15 Figure 3: These plots show the log-frequencies of occurrences of part-of-speech sequences for sentences with five, ten and fifteen tokens.", "acronym": "LF", "label": "Log frequency", "ID": "135"}, {"sentence": "Middle column: post submission LF1- score.", "acronym": "LF", "label": "labelled F", "ID": "136"}, {"sentence": "Us- ing a stochastic variant of Constraint Dependency Grammar (Wang and Harper, 2004) reached a 92.4% LF-score on the Penn Treebank, which slightly outperforms (Collins, 1999) who reports 92.0% on dependency structures automati- cally derived from phrase structure results.", "acronym": "LF", "label": "labelled F", "ID": "137"}, {"sentence": "Right column: post submission unLF1- score. ?", "acronym": "LF", "label": "labelled F", "ID": "138"}, {"sentence": "60.61 65.18 84.29 Table 1: Semantic labelled and unLF1-scores for each language and domain.", "acronym": "LF", "label": "labelled F", "ID": "139"}, {"sentence": "18 According to the results, the unLF 1 (UF 1 ) is a closer estimation than the labelled one.", "acronym": "LF", "label": "labelled F", "ID": "140"}, {"sentence": "My official submission scores are given in table 1, together with post submission labelled and un- LF1-scores.", "acronym": "LF", "label": "labelled F", "ID": "141"}, {"sentence": "Initial experiments on the de- velopment data indicates that these simple heuristics slightly improves semantic parsing quality measured with LF1-score.", "acronym": "LF", "label": "labelled F", "ID": "142"}, {"sentence": "Combining treebank transfor- mation techniques with a suffix analysis, (Dubey, 2005) trained a probabilistic parser and reached a LF-score of 76.3% on phrase structure an- notations for a subset of the sentences used here (with a maximum length of 40).", "acronym": "LF", "label": "labelled F", "ID": "143"}, {"sentence": "Learn- ing to Map Sentences to LF: Structured Classification with Probabilistic Categorial Grammars.", "acronym": "LF", "label": "Logical Form", "ID": "144"}, {"sentence": "Working with LFs Skolemization: Skolemization of an existential formula of type (some x R S), where x is a variable, R is a restrictor formula and S is the nuclear scope, is performed via the transduction (/ (some ! !", "acronym": "LF", "label": "Logical Form", "ID": "145"}, {"sentence": "Since the introduction of Quasi LF (Al- shawi and Crouch, 1992), there has been a lot of work on designing constraint-based underspecifica- tion formalisms where the readings of a UR are not defined in a constructive fashion as shown above, but rather by a set of constraints.", "acronym": "LF", "label": "Logical Form", "ID": "146"}, {"sentence": "Proceedings of the AAAI Spring Symposium on LFalization of Commonsense Rea-soning.", "acronym": "LF", "label": "Logical Form", "ID": "147"}, {"sentence": "Pragmatics: lmplicature, Pre-  supposition, and LF.", "acronym": "LF", "label": "Logical Form", "ID": "148"}, {"sentence": "After this operation, the module produces Predicate-Argument Structures or PAS on the basis of a previously produced LF.", "acronym": "LF", "label": "Logical Form", "ID": "149"}, {"sentence": "He says that the red ball is the one on the LF.", "acronym": "LF", "label": "left", "ID": "150"}, {"sentence": "This algorithm searches the parse tree in a LF-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.", "acronym": "LF", "label": "left", "ID": "151"}, {"sentence": "The item 4) is situated at the edge E of domain D.  This delinition covers a family of constraints depending on  the instantiations of the arguments: E is either LF (L) or  right (R), domain \\]nay be syllable, foot or word, and ~b can  be any phonological object, such as stress or an affix.", "acronym": "LF", "label": "left", "ID": "152"}, {"sentence": "The representation for sentence 1 states that the first element of the 5-gram (-3; third word to the LF of the adjective) is empty (because the second element is a phrase boundary marker), that the sec- ond element is a", "acronym": "LF", "label": "left", "ID": "153"}, {"sentence": "The  cases where \"it\" is merely a dummy subject in  a cLF sentence (example 1) or has conventional  unspecified referents (example 2) are excluded  from computing the precision:  ?", "acronym": "LF", "label": "left", "ID": "154"}, {"sentence": "In resolving inter-sentential  pronouns, the algorithm searches the previous  sentence, again in LF-to-right, breadth-first or-  der.", "acronym": "LF", "label": "left", "ID": "155"}, {"sentence": "tences (target adjective in bold face, word window in italics; negative numbers indicate positions to the LF, positive ones positions to the right): 1.", "acronym": "LF", "label": "left", "ID": "156"}, {"sentence": "The lemmata were modelled using pairs of bi- grams: in a 4-word window (three to the LF and one to the right of the adjective), the first two tags formed a feature and the second two tags another feature.", "acronym": "LF", "label": "left", "ID": "157"}, {"sentence": "For this, we propose a lan- guage model for generating reviews that incorporates a description of objects and a generic RLM.", "acronym": "RLM", "label": "review language model", "ID": "158"}, {"sentence": "On the other hand, our pro- posed bidirectional models include the full source sentence relying on recurrency, yielding improve- ments over competitive baselines already includ- ing a RLM.", "acronym": "RLM", "label": "recurrent language model", "ID": "159"}, {"sentence": "The continu- ous representations are obtained by applying a se- quence of convolutions, and the result is fed into the hidden layer of a RLM.", "acronym": "RLM", "label": "recurrent language model", "ID": "160"}, {"sentence": "All results include a RLM.", "acronym": "RLM", "label": "recurrent language model", "ID": "161"}, {"sentence": "Kim et al (2015) propose a RLM that re- places the word-indexed projection matrix with a convolution layer fed with the character sequence that constitutes each word to find morphological pat- terns.", "acronym": "RLM", "label": "recurrent language model", "ID": "162"}, {"sentence": "Our own models are also implemented in Torch7 for easier comparison.1 Fi- nally, we selected the best performing convolutional and RLMs on Europarl-NC and the Baseline FFLM to be evaluated on the ukWaC corpus.", "acronym": "RLM", "label": "recurrent language model", "ID": "163"}, {"sentence": "However, while previous work on translation modeling with recurrent neural net- works shows its effectiveness on standard base- lines, so far no notable gains have been presented on top of RLMs (Auli et al.,", "acronym": "RLM", "label": "recurrent language model", "ID": "164"}, {"sentence": "Recognition  Sentence Classification  Background Classification  SVM Comorbidity Classification  SVM Diagnosis Classification  SVM Family History Classification  SVM ISF Classification  SVM Lifestyle Classification  SVM Symptom Classification  SVM Test Classification  Candidate Generation  SVM Candidate Filter  Question Recognition  SVM Sentence Classification  Question  Candidate Generation  SVM CRing  Exemplification Recognition  Candidate Filter  Candidate Generation  SVM CRing  Coordination Recognition  SVM Candidate Filter Coordination  Exemplification  Stanford  Parser  WordNet  SVM Treatment Classification  Figure 1: Question Decomposition Architecture.", "acronym": "CR", "label": "Candidate Rank", "ID": "165"}, {"sentence": "This is a positive result, especially given 97 05 10 15 20 1 - 2 0 2 1 - 4 0 4 1 - 6 0 6 1 - 8 0 8 1 - 1 0 0 1 0 1 - 1 2 0 1 2 1 - 1 4 0 1 4 1 - 1 6 0 1 6 1 - 1 8 0 1 8 1 - 2 0 0 2 0 1 - 2 2 0 2 2 1 - 2 4 0 2 4 1 - 2 6 0 2 6 1 - 2 8 0 2 8 1 - 3 0 0 CR %   T i m e   C h o s e n   f o r   C o m p a r i s o n Figure 4: Histogram of the rank of the lower-ranked can- didate chosen in pair comparisons.", "acronym": "CR", "label": "Candidate Rank", "ID": "166"}, {"sentence": "4.3 CRing Model Given an anaphor ai and a set of candidate antecedents C = {C1,C2, ...,Ck}, the problem of anaphora resolution is to choose the best candidate antecedent for ai.", "acronym": "CR", "label": "Candidate Rank", "ID": "167"}, {"sentence": "ion  SVM Diagnosis Classification  SVM Family History Classification  SVM ISF Classification  SVM Lifestyle Classification  SVM Symptom Classification  SVM Test Classification  Candidate Generation  SVM Candidate Filter  Question Recognition  SVM Sentence Classification  Question  Candidate Generation  SVM CRing  Exemplification Recognition  Candidate Filter  Candidate Generation  SVM CRing  Coordination Recognition  SVM Candidate Filter Coordination  Exemplification  Stanford  Parser  WordNet  SVM Treatment Classification  Figure 1: Question Decomposition Architecture.", "acronym": "CR", "label": "Candidate Rank", "ID": "168"}, {"sentence": "1 http://lhncbc.nlm.nih.gov/project/consumer-health- question-answering 30 Sentence Splitting  Request  Question  Sentence  Ignore  Sentence  Background  Sentence  Candidate Generation  UMLS  SVM CRing  Boundary Fixing  Focus  Focus Recognition  Sentence Classification  Background Classification  SVM Comorbidity Classification  SVM Diagnosis Classification  SVM Family History Classification  SVM ISF Classification  SVM Lifestyle Classification  SVM Symptom Classification  SVM Test Classification  Candidate Generation  SVM Candidate Filter  Question Recognition  SVM Sentence Cla", "acronym": "CR", "label": "Candidate Rank", "ID": "169"}, {"sentence": "0 5 10 15 20 25 30 1 2 3 4 5 6 7 8 9 10 CR %   T i m e   C h o s e n   f o r   C o m p a r i s o n Figure 2: Histogram of the rank of the higher-ranked candidate chosen in pair comparisons.", "acronym": "CR", "label": "Candidate Rank", "ID": "170"}, {"sentence": "Among the TAG el- ementary trees that correspond to a given lexical entry, there is the CR, and all the other representatives are represented by adding features to the CR.", "acronym": "CR", "label": "canonical representative", "ID": "171"}, {"sentence": "If a match is found, the GO node name (deemed the CR for its set of synonyms) is associated with the abstract.", "acronym": "CR", "label": "canonical representative", "ID": "172"}, {"sentence": "In practice, we still use paths and spans, and hash to a CR if desired.", "acronym": "CR", "label": "canonical representative", "ID": "173"}, {"sentence": "Note that for idempotent f , image(f) consists of CRs f(s) of ker(f)?s equiva- lence classes {s ?", "acronym": "CR", "label": "canonical representative", "ID": "174"}, {"sentence": "Derivation trees can then be  seen as CRs of classes of derivations  producing the same string, differing only in the order in  which the same productions are applied.", "acronym": "CR", "label": "canonical representative", "ID": "175"}, {"sentence": "In addition,  we plan to automatically adjust cross-document  event aggregation operations according to specific  CR provided by the users.", "acronym": "CR", "label": "compression ratios", "ID": "176"}, {"sentence": "Comparison of CR with other  techniques is sheik) in Table 2.", "acronym": "CR", "label": "compression ratios", "ID": "177"}, {"sentence": "In the future, we would like to apply a similar ex- haustive search strategy, but this time with differ- ent CR, in order to see the impact of CR on the pdf of each domain.", "acronym": "CR", "label": "compression ratios", "ID": "178"}, {"sentence": "4.1.1  Recall, Coverage, Retention and  Weighted Retention   Recall at different CR has been  used in summarization research (Mani 2001) to  measure how well an automatic system retains  important content of original documents.", "acronym": "CR", "label": "compression ratios", "ID": "179"}, {"sentence": "i  sentence compression ratio of 0.2% and a character com-  pression of 0.3%, approximately two orders of magni-  tude different with CR used in single doc-  ument summarization.", "acronym": "CR", "label": "compression ratios", "ID": "180"}, {"sentence": "These POS-based patterns are quite generic, al- lowing for the CR of large sets of characters.", "acronym": "CR", "label": "creation", "ID": "181"}, {"sentence": "Despite large typological differ- ences between Wambaya and the languages on which the development of the resource was based, the Grammar Matrix is found to pro- vide a significant jump-start in the CR of the grammar for Wambaya: With less than 5.5 person-weeks of development, the Wambaya grammar was able to assign correct seman- tic representations to 76% of the sentences in a naturally occurring text.", "acronym": "CR", "label": "creation", "ID": "182"}, {"sentence": "In (Celli, 2012), a list of linguis- tic features were used for the CR of character models in terms of the the Big Five personality di- mensions (Norman, 1963).", "acronym": "CR", "label": "creation", "ID": "183"}, {"sentence": "Riddles and metaphors: The CR of meaning.", "acronym": "CR", "label": "creation", "ID": "184"}, {"sentence": "A clear candidate is the use of named entities, but the CR of templates has also been tried in open domains (Srihari and Li 2000) and restricted domains (Weischedel, Xu, and Licuanan 2004).", "acronym": "CR", "label": "creation", "ID": "185"}, {"sentence": "It is not difficult to see that replacing any set of two or three nonterminals in p?s right-hand side forces the CR of a fresh nonterminal of fan-out larger than two.", "acronym": "CR", "label": "creation", "ID": "186"}, {"sentence": "Constraint Equations (11) and (12) help to re- cover CR.", "acronym": "CR", "label": "causal relations", "ID": "187"}, {"sentence": "To aid in extracting CR, we leverage on the identification of discourse relations to provide addi- tional contextual information.", "acronym": "CR", "label": "causal relations", "ID": "188"}, {"sentence": "(Beamer and Girju, 2009) tried to detect CR be- tween verbs in a corpus of screen plays, but limited themselves to consecutive, or adjacent verb pairs.", "acronym": "CR", "label": "causal relations", "ID": "189"}, {"sentence": "In this work, we auto- matically detect and extract CR between events in text.", "acronym": "CR", "label": "causal relations", "ID": "190"}, {"sentence": "Beamer and Girju, 2009) tried to detect CR be- tween verbs in a corpus of screen plays, but limited themselves to consecutive, or adjacent verb pairs.", "acronym": "CR", "label": "causal relations", "ID": "191"}, {"sentence": "Thus, their focus was not on identifying CR between events in a given text document.", "acronym": "CR", "label": "causal relations", "ID": "192"}, {"sentence": "Thus, their focus was not on identifying CR between events in a given", "acronym": "CR", "label": "causal relations", "ID": "193"}, {"sentence": "Why-question answering using intra- and inter-sentential CR.", "acronym": "CR", "label": "causal relations", "ID": "194"}, {"sentence": "This means that with the aid of discourse relations, we are able to recover more CR, as well as reduce false-positive predictions.", "acronym": "CR", "label": "causal relations", "ID": "195"}, {"sentence": "1 Introduction  CR aims to identify which  noun phrases (NPs, or mentions) refer to the  same real-world entity in a text.", "acronym": "CR", "label": "Coreference resolution", "ID": "196"}, {"sentence": "c?2013 Association for Computational Linguistics Dynamic Knowledge-Base Alignment for Coreference Resolution Jiaping Zheng Luke Vilnis Sameer Singh Jinho D. Choi Andrew McCallum School of Computer Science University of Massachusetts Amherst MA 01003 {jzheng,luke,sameer,jdchoi,mccallum}@cs.umass.edu Abstract CR systems can benefit greatly from inclusion of global context, and a number of recent approaches have demonstrated improvements when precom- puting an alignment to external knowledge sources.", "acronym": "CR", "label": "Coreference resolution", "ID": "197"}, {"sentence": "CR forms an important component for natural language process- ing and information extraction pipelines due to its utility in relation extraction, cross-document coref- erence, text summarization, and question answer- ing.", "acronym": "CR", "label": "Coreference resolution", "ID": "198"}, {"sentence": "CR using competi- tion learning approach.", "acronym": "CR", "label": "Coreference resolution", "ID": "199"}, {"sentence": "CR is first recast as a classifica- tion task, in which a pair of NPs is classified as co- referring or not based on constraints that are learned from an annotated corpus.", "acronym": "CR", "label": "Coreference resolution", "ID": "200"}, {"sentence": "1 Introduction CR is the task of identifying sets of noun phrase mentions from a document that refer to the same real-world entities.", "acronym": "CR", "label": "Coreference resolution", "ID": "201"}, {"sentence": "Two  large text databases about CR (1.4 ?", "acronym": "CR", "label": "conference r gistration", "ID": "202"}, {"sentence": "Although this shared stack mechanism ac-  counts for highly task-oriented and cooperative dia-  logues where one can assume that both speakers share  3Dialogue 1is extracted from a corpus of Japanese ATR  (Advanced Telecommunication Research) recorded simu-  lated CR telephone conversations.", "acronym": "CR", "label": "conference r gistration", "ID": "203"}, {"sentence": "Ti~e  dictionaries and the rules are made by extracting  the entries from the ATR corporaU0l concerning  CR (Table 1).", "acronym": "CR", "label": "conference r gistration", "ID": "204"}, {"sentence": "V: vocabulary size (= Vc + Vf )  Vc: number of content words  Vf : number of function words  Table 1: Number of parameters of each model  90  Figure 4: Network representation f the proposed language model  Task  Vocabulary Size  Speaker  Test Data  International CR  1,500 words  1 male speaker  261 sentences  (7.0 words/sentence, on average)  Table 2: Experimental conditions for speech recognition  The proposed model was compared with the word bigram and tri-  gram models in their perplexities for test sentences and in sentence  recognition rates.", "acronym": "CR", "label": "conference r gistration", "ID": "205"}, {"sentence": "We are implementing this model using the Span-  ish travel agency domain corpus and the Japanese  ATR CR corpus.", "acronym": "CR", "label": "conference r gistration", "ID": "206"}, {"sentence": "The corpus contains conversations about  international CR (Ogura et al  1989).", "acronym": "CR", "label": "conference r gistration", "ID": "207"}, {"sentence": "Anaphoric resolu- tion can sometimes be CR for these inferences.", "acronym": "CR", "label": "critical", "ID": "208"}, {"sentence": "In more general terms, we have identified the following features of currently used tagsets for Slavic in general and Polish in particular which seem problematic from the point of view of their reusability and cross-linguistic applicability: \u0000 unCR adoption of traditional and some- times ill-defined POS classes, such as ?", "acronym": "CR", "label": "critical", "ID": "209"}, {"sentence": "Introduction  Portability and domain independence are CR  challenges for Natural Language Processing (NLP)  systems.", "acronym": "CR", "label": "critical", "ID": "210"}, {"sentence": "For such domains, the balance between reacting to the domain events as they occur and maintaining the overall, high- level consistency is CR.", "acronym": "CR", "label": "critical", "ID": "211"}, {"sentence": "11 2 Background and related work Hawley et al (2007) described an experiment in which 8 dysarthric individuals (with either cere- bral palsy or multiple sclerosis) controlled non- CR devices in their home (e.g., TV) with auto- matic speech recognition.", "acronym": "CR", "label": "critical", "ID": "212"}, {"sentence": "University of Pennsylvania Word-level alignment of bilingual text is a CR resource for a growing variety of tasks.", "acronym": "CR", "label": "critical", "ID": "213"}, {"sentence": "Another type of CR are user stories.", "acronym": "CR", "label": "controlled requirements", "ID": "214"}, {"sentence": "For instance, the NFR corpus4 covers the sys- tem?s perspective of CR spec- ifications.", "acronym": "CR", "label": "controlled requirements", "ID": "215"}, {"sentence": "For the acquisi- tion of unCR, we adapted the idea of Vlas and Robinson (2011) that is based on feature requests gathered from the open-source soft- ware platform SourceForge5.", "acronym": "CR", "label": "controlled requirements", "ID": "216"}, {"sentence": "Special attention is paid to CRs.", "acronym": "CR", "label": "corequirement", "ID": "217"}, {"sentence": "Let us look now more carefully at several linguistic issues we consider to be important to characterize the notion of linguistic requirement: extensionality/ intensionality, soft/hard requirements, the scope of a condition, syntactic/semantic requirements, and CRs.", "acronym": "CR", "label": "corequirement", "ID": "218"}, {"sentence": "This phenomenon is called here CR.", "acronym": "CR", "label": "corequirement", "ID": "219"}, {"sentence": "Special attention will be paid to both the relativized view on word sense (i.e., contextual sense) and CRs.", "acronym": "CR", "label": "corequirement", "ID": "220"}, {"sentence": "The final linguistic issue to be introduced is the phenomenon referred to as CRs.", "acronym": "CR", "label": "corequirement", "ID": "221"}, {"sentence": "This notion is modeled by means of what we call CRs.", "acronym": "CR", "label": "corequirement", "ID": "222"}, {"sentence": "1 In t roduct ion   We present a statistical method for deTEin-  ing pronoun anaphora.", "acronym": "TE", "label": "term", "ID": "223"}, {"sentence": "In equation (4), the TE  P(h.", "acronym": "TE", "label": "term", "ID": "224"}, {"sentence": "Finally we attempted a fully automatic di-  rect test of the accuracy of both pronoun meth-  ods for gender deTEination.", "acronym": "TE", "label": "term", "ID": "225"}, {"sentence": "One can judge the pro-  gram informally by simply examining the re-  sults and deTEining if the program's gender  decisions are correct (occasionally ooking at the  text for difficult cases).", "acronym": "TE", "label": "term", "ID": "226"}, {"sentence": "This decision is made by ranking the refer-  ents by log-likelihood ratio, TEed salience, for  each referent.", "acronym": "TE", "label": "term", "ID": "227"}, {"sentence": "First, as one might expect given the al-  ready noted superior performance of the Hobbs  scheme over last-noun, Hobbs also performs bet-  ter at deTEining ender.", "acronym": "TE", "label": "term", "ID": "228"}, {"sentence": "This demonstrates that given a correct underlying analysis the development of the high level TE application is relatively trivial.", "acronym": "TE", "label": "template element", "ID": "229"}, {"sentence": "2 Core ference  in  the  LaS IE  sys tem  The LaSIE system (Gaizauskas et al, 1995) has  been designed as a general purpose IE system  which can conform to the MUC task specific-  ations for named entity identification, corefer-  ence resolution, IE TE and re-  lation identification, and the construction of  scenario-specific IE templates.", "acronym": "TE", "label": "template element", "ID": "230"}, {"sentence": "Therefore an extra TE was generated for both TCI and News Corp. We also did not recognize ING Barings as a company, and so did not print a template for it.", "acronym": "TE", "label": "template element", "ID": "231"}, {"sentence": "Information Extraction Template Element For the TE task our o\u000ecial MUC-6 scores were: R: 66; P: 74; P & R: 69.8 and, with the addition of the two missed articles: R: 68; P: 74; P & R: 70.8.", "acronym": "TE", "label": "template element", "ID": "232"}, {"sentence": "In the context of MUC, the coreference layer provides input to the TE task, where each named  entity is represented as a single template which collects information about that element from the multiple  mentions in the text.", "acronym": "TE", "label": "template element", "ID": "233"}, {"sentence": "solely restricted to, carrying out the tasks specified  in MUC-6: named entity recognition, coreference  resolution, TE filling, and scenario  template filling tasks (see DARPA (1995) for further  details of the task descriptions).", "acronym": "TE", "label": "template element", "ID": "234"}, {"sentence": "horacio@Isi.upc.es  Abst ract   This paper describes the enhancements made, within a  unification framework, based on typed feature  structures, in order to support linking of lexical  entries to their TE.", "acronym": "TE", "label": "translation equivalents", "ID": "235"}, {"sentence": "This contrastive analysis serves as a basis for the work of a group of experts (the Harmonising Group) who will determine TE in French, Italian, German and Slovene (one-to- one correspondence) in the fields of spatial plan- ning and sustainable development for use within the Convention, thus optimising the understanding between the Alpine states at supranational level.", "acronym": "TE", "label": "translation equivalents", "ID": "236"}, {"sentence": "c) A German noun list with English TE, gen-  der, and inflectional information.", "acronym": "TE", "label": "translation equivalents", "ID": "237"}, {"sentence": "These  are based on a contrastive approach; they are included only for those  sets of TE for which these relations are not identical.", "acronym": "TE", "label": "translation equivalents", "ID": "238"}, {"sentence": "In this way, we compiled the following lists:  a) A German verb list with English TE.", "acronym": "TE", "label": "translation equivalents", "ID": "239"}, {"sentence": "This paper describes the enhancements made, to the LKB  system \\[6\\], in order to support linking of lexical entries to  their TE.", "acronym": "TE", "label": "translation equivalents", "ID": "240"}, {"sentence": "Measuring comparability of documents in non-parallel corpora for efficient extraction of (semi-) parallel TE.", "acronym": "TE", "label": "translation equivalents", "ID": "241"}, {"sentence": "Snow et al (2008) were among the first to use MTurk to obtain data for several NLP tasks, such as TE and word sense dis- ambiguation.", "acronym": "TE", "label": "textual entailment", "ID": "242"}, {"sentence": "Mirkin et al (2009), inter alia, frame paraphrasing as a special, symmetrical case of (WordNet-based) TE.", "acronym": "TE", "label": "textual entailment", "ID": "243"}, {"sentence": "STS is treated as computing the prob- ability of two TEs T |= H and H |= T , where T and H are the two sentences whose similarity is being judged.", "acronym": "TE", "label": "textual entailment", "ID": "244"}, {"sentence": "Contradictions and jus- tifications: Extensions to the TE task.", "acronym": "TE", "label": "textual entailment", "ID": "245"}, {"sentence": "The setting is also appropriate for cases that may require making  global decisions that involve multiple components, possibly pre-designed or pre- learned, as in summarization, paraphrasing, TE and question answering.", "acronym": "TE", "label": "textual entailment", "ID": "246"}, {"sentence": "There is recent interest on the use of graph methods for Natural Language Processing, such as document summarisation (Mihalcea, 2004) doc- ument retrieval (Montes-y-Go?mez et al, 2000; Mishne, 2004), and recognition of TE (Pazienza et al, 2005).", "acronym": "TE", "label": "textual entailment", "ID": "247"}, {"sentence": "The false friend candidates were categorized in the three categories defined in Section 2: false friends, partial false friends and TE.", "acronym": "TE", "label": "true equivalents", "ID": "248"}, {"sentence": "The second, more important factor, was that the translated documents were not TE of the original Korean documents.", "acronym": "TE", "label": "true equivalents", "ID": "249"}, {"sentence": "The function of this module is to recog- nize numerical, TE and others like product number, telephone number, credit number or alphabets.", "acronym": "TE", "label": "temporal expression", "ID": "250"}, {"sentence": "Terms which are not related to the central themes of a text, such as TEs, will be given a lower weight.", "acronym": "TE", "label": "temporal expression", "ID": "251"}, {"sentence": "GUTime (Mani and Wilson, 2000) annotates TEs according to the TimeML schema and normalizes their values.", "acronym": "TE", "label": "temporal expression", "ID": "252"}, {"sentence": "We pre- pare a phrase lookup table consisting of 350 frozen  phrases and TEs which are identi- fied before the input text is parsed.", "acronym": "TE", "label": "temporal expression", "ID": "253"}, {"sentence": "The system achieves F-scores of 85% and 82% for identification and normalization of TEs, respectively.", "acronym": "TE", "label": "temporal expression", "ID": "254"}, {"sentence": "However, before parsing, the input text is passed to  the preprocessing unit, where we try to identify the  frozen phrases2 and TEs3 which  the syntactic parser is unable to identify.", "acronym": "TE", "label": "temporal expression", "ID": "255"}, {"sentence": "However, the flexible numerical and TE cannot be solved by the above two methods.", "acronym": "TE", "label": "temporal expression", "ID": "256"}, {"sentence": "The scores for TE and Template  Relations are also high enough to make the technology  reliable for use by analysts.", "acronym": "TE", "label": "Template Element", "ID": "257"}, {"sentence": "Although the corefercnce scores are lower than  the TE scores, enough coreference is  being processed to achieve reliable results in Template  Element.", "acronym": "TE", "label": "Template Element", "ID": "258"}, {"sentence": "TEs Task The overall score on this task was: P&R  66.75 2P&R  69.74 P&2R  64.01 This result is probably the most satisfying of the three MUC tasks that the system entered.", "acronym": "TE", "label": "Template Element", "ID": "259"}, {"sentence": "Although the corefercnce scores are lower than  the TE scores, enough coreference is  being processed to achieve reliable results in Template  Eleme", "acronym": "TE", "label": "Template Element", "ID": "260"}, {"sentence": "The TEs  extracted from newswire articles are indicative of the  content of the article for most purposes.", "acronym": "TE", "label": "Template Element", "ID": "261"}, {"sentence": "As part of this research, the TE development keys were analyzed to determine how  often the descriptors of an organization and person are directly associated by syntax.", "acronym": "TE", "label": "Template Element", "ID": "262"}, {"sentence": "Task TE Scenario Template Recall 71 31 Precision 83 68 F-Measure 76.50 42.73 Figure 5: NYU scores on MUC-7 tasks candidate.", "acronym": "TE", "label": "Template Element", "ID": "263"}, {"sentence": "In ACL-PASCAL Workshop on TE and Paraphrasing, Prague, Czech Republic.", "acronym": "TE", "label": "Textual Entailment", "ID": "264"}, {"sentence": "In Proceedings of the ACL-PASCAL Workshop on TE and Paraphrasing, pages 1?9, Prague, June.", "acronym": "TE", "label": "Textual Entailment", "ID": "265"}, {"sentence": "With the introduction of the MSR alignment cor- pus (Brockett, 2007) developed from the second Recognizing TE challenge data (Bar- Haim et al.,", "acronym": "TE", "label": "Textual Entailment", "ID": "266"}, {"sentence": "The PASCAL Recognising TE Chal- lenge.,", "acronym": "TE", "label": "Textual Entailment", "ID": "267"}, {"sentence": "Meth- ods for Using TE in Open-Domain Question Answering.", "acronym": "TE", "label": "Textual Entailment", "ID": "268"}, {"sentence": "Ask not what TE can do for You...?", "acronym": "TE", "label": "Textual Entailment", "ID": "269"}, {"sentence": "Automatic  TE and Document Similarity in Special  Text Corpora.", "acronym": "TE", "label": "Term Extraction", "ID": "270"}, {"sentence": "A Simple but  Powerful Automatic TE Method.", "acronym": "TE", "label": "Term Extraction", "ID": "271"}, {"sentence": "Bilingual TE The last phase consists in finding the translation of the domain terminol- ogy.", "acronym": "TE", "label": "Term Extraction", "ID": "272"}, {"sentence": "approach 2778 2688 2549 Multiterm Extract 1337 N/A N/A Table 4: Figures after Log-Likelihood and Mutual Expectation reduction Anchor chunk approach Correct Not correct Maybe correct Multiwords 78.5% 19% 2.5% Single words 89.5% 9.5% 1% All terms 83% 15% 2% Multiterm Extract Correct Not correct Maybe correct Multiwords 51% 48.5% 0.5% Single words 83% 16% 1% All terms 66% 33.5% 0.5% Table 5: Results TE Module ?", "acronym": "TE", "label": "Term Extraction", "ID": "273"}, {"sentence": "1436 4 Chinese Key TE  In Web mining of English-Chinese OOV term  translation, an important problem is to extract  the target translation candidates from the re- turned Chinese Web documents, which can be  considered as a key term extraction task.", "acronym": "TE", "label": "Term Extraction", "ID": "274"}, {"sentence": "It is  based on the complementary use of two  tools: (1) a TE tool that  acquires term candidates from tagged  corpora through a shallow grammar of  noun phrases, and (2) a Term Cluster-  ing tool that groups syntactic variants  (insertions).", "acronym": "TE", "label": "Term Extraction", "ID": "275"}, {"sentence": "TE systems are often based on en- tailment rules which specify a directional inference relation between two fragments.", "acronym": "TE", "label": "Textual entailment", "ID": "276"}, {"sentence": "TE resolution via atomic propositions.", "acronym": "TE", "label": "Textual entailment", "ID": "277"}, {"sentence": "5.2 TE The Recognizing Textual Entailment Challenge (Dagan et al, 2005) is a task in which systems as- sess whether a sentence is entailed by a short pas- sage or sentence.", "acronym": "TE", "label": "Textual entailment", "ID": "278"}, {"sentence": "TE at EVALITA 2009.", "acronym": "TE", "label": "Textual entailment", "ID": "279"}, {"sentence": "TE through extended lexical overlap and lexico-semantic match- ing.", "acronym": "TE", "label": "Textual entailment", "ID": "280"}, {"sentence": "TE recognition based on dependency analysis and WordNet.", "acronym": "TE", "label": "Textual entailment", "ID": "281"}, {"sentence": "QAg from struc- tured knowledge sources.", "acronym": "QA", "label": "Question answerin", "ID": "282"}, {"sentence": "QAg in role-playing games.", "acronym": "QA", "label": "Question answerin", "ID": "283"}, {"sentence": "QAg using enhanced lexical se- mantic models.", "acronym": "QA", "label": "Question answerin", "ID": "284"}, {"sentence": "QAg in terminology-rich technical domains.", "acronym": "QA", "label": "Question answerin", "ID": "285"}, {"sentence": "Information extraction over structured data: QAg with freebase.", "acronym": "QA", "label": "Question answerin", "ID": "286"}, {"sentence": "QAg with subgraph embed- dings.", "acronym": "QA", "label": "Question answerin", "ID": "287"}, {"sentence": "QA from struc- tured knowledge sources.", "acronym": "QA", "label": "Question answering", "ID": "288"}, {"sentence": "QA in role-playing games.", "acronym": "QA", "label": "Question answering", "ID": "289"}, {"sentence": "QA using enhanced lexical se- mantic models.", "acronym": "QA", "label": "Question answering", "ID": "290"}, {"sentence": "QA in terminology-rich technical domains.", "acronym": "QA", "label": "Question answering", "ID": "291"}, {"sentence": "Information extraction over structured data: QA with freebase.", "acronym": "QA", "label": "Question answering", "ID": "292"}, {"sentence": "QA with subgraph embed- dings.", "acronym": "QA", "label": "Question answering", "ID": "293"}, {"sentence": "The tasks set in these conferences have molded a specific kind of QA that is easy to evaluate and that focuses on the use of fast and shallow methods that are generally independent of the application domain.", "acronym": "QA", "label": "question answering", "ID": "294"}, {"sentence": "A main characteristic of QA in restricted domains is the integration of domain-specific information that is either developed for QA or that has been developed for other purposes.", "acronym": "QA", "label": "question answering", "ID": "295"}, {"sentence": "A main characteristic of QA in restricted domains is the integration of domain-specific information that is either developed for QA or that has been developed for other pur", "acronym": "QA", "label": "question answering", "ID": "296"}, {"sentence": "article is on the use of restricted domains for automated QA.", "acronym": "QA", "label": "question answering", "ID": "297"}, {"sentence": "From this perspective, QA focuses on finding text excerpts that contain the answer within large collections of documents.", "acronym": "QA", "label": "question answering", "ID": "298"}, {"sentence": "The focus of this article is on the use of restricted domains for automated QA.", "acronym": "QA", "label": "question answering", "ID": "299"}, {"sentence": "University of Alicante, Spain Automated QA has been a topic of research and development since the earliest AI applications.", "acronym": "QA", "label": "question answering", "ID": "300"}, {"sentence": "The setting is also appropriate for cases that may require making  global decisions that involve multiple components, possibly pre-designed or pre- learned, as in summarization, paraphrasing, textual entailment and QA.", "acronym": "QA", "label": "question answering", "ID": "301"}, {"sentence": "and Vicedo QA in Restricted Domains: An Overview AQUA (Vargas-Vera, Motta, and Domingue 2003) combines knowledge encoded in a database with domain-related documents through an ontology that describes aca- demic life.", "acronym": "QA", "label": "Question Answering", "ID": "302"}, {"sentence": "96   Special Section on Restricted-Domain QA QA in Restricted Domains: An Overview Diego Molla??", "acronym": "QA", "label": "Question Answering", "ID": "303"}, {"sentence": "A good characterisation of their semantics can help identify referents in a given (con)text in dialog- based tasks, QA systems, or even advanced Information Extraction tasks.", "acronym": "QA", "label": "Question Answering", "ID": "304"}, {"sentence": "and Vicedo QA in Restricted Domains: An Overview tion prints words onto the screen?", "acronym": "QA", "label": "Question Answering", "ID": "305"}, {"sentence": "Currently we are witnessing a surge of activity in the area from the perspective of IR, initiated by the QA track of TREC1 in 1999 (Voorhees 2001).", "acronym": "QA", "label": "Question Answering", "ID": "306"}, {"sentence": "la- bels by the ITSPOKE semantic understanding com- ponent: Correct, Incorrect, PC.", "acronym": "PC", "label": "Partially Correct", "ID": "307"}, {"sentence": "Scoring involved comparing the summaries against the answer key (annotated passages from the source documents) while judging whether the summary provided a Correct, PC, or Missing answer.", "acronym": "PC", "label": "Partially Correct", "ID": "308"}, {"sentence": "Two accuracy metrics were defined, ARL (An-  swer Recall Lenient) and ARS (Answer Recall  Strict):  ARL = (nl + (.5 * n2))/n3 (4)  ARS = nl/n3 (5)  where nl is the number of Correct answers in the  summary, n2 is the number of PC  answers in the summary, and n3 is the number of  questions answered in the key.", "acronym": "PC", "label": "Partially Correct", "ID": "309"}, {"sentence": "Correct 903 463 164 309 78 PC 219 261 93 333 80 Contradictory 61 126 91 103 36 Irrelevant 209 229 119 476 189 Non-Domain 0 0 0 2 18 Table 2: Confusion matrix of Run 1 in the 5-way Unseen Domains scenario.", "acronym": "PC", "label": "Partially Correct", "ID": "310"}, {"sentence": "We also see that the system ability to distinguish Correct and PC answers need to be improved.", "acronym": "PC", "label": "Partially Correct", "ID": "311"}, {"sentence": "For system  responses, the evaluators categorized each response as  follows:  Answer: further evaluated as Correct, Incorrect  PC or Can't Decide;  System Initiated Directive: further evaluated as  Appropriate, Inappropriate, or Can't Decide;  Diagnostic Message: further evaluated as Appropriate,  Inappropriate, or Can't Decide;  Failure-to-Understand Message: no further evaluation.", "acronym": "PC", "label": "Partially Correct", "ID": "312"}, {"sentence": "Participants in these shared tasks have introduced dozens of sys- tems for event extraction, and the resulting methods have been applied to automatically analyse the entire available domain literature (Bjo?rne et al, 2010) and applied in support of applications such as semantic literature search (Ohta et al, 2010; Van Landeghem et al, 2011b) and PC support (Kemper et al, 2010).", "acronym": "PC", "label": "pathway curation", "ID": "313"}, {"sentence": "While preserving the classic event extraction tasks such as the GE task, the BioNLP-ST 2013 broad- ens the scope of application domains by introducing many new issues in biology such as cancer genetics and PC.", "acronym": "PC", "label": "pathway curation", "ID": "314"}, {"sentence": "In BioNLP-ST 2013 series, additional training data for PC including chemical entities is available.", "acronym": "PC", "label": "pathway curation", "ID": "315"}, {"sentence": "Rather, we intend the term to refer to a set of tasks where information extraction/text mining methods are applied in some role to con- tribute directly to PC, including, for example, the identification of specific texts in the literature relevant to anno- tated reactions, the automatic suggestion of further entities or reactions to add to a pathway, or even the fully automatic gen- eration of entire pathways from scratch.", "acronym": "PC", "label": "pathway curation", "ID": "316"}, {"sentence": "Overview of the PC (pc) task of bionlp shared task 2013.", "acronym": "PC", "label": "pathway curation", "ID": "317"}, {"sentence": "Analogously, the French noun compounds (NCs) were extracted from Europarl using the following pattern: a noun followed by either an adjective or a PC14.", "acronym": "PC", "label": "prepositional complement", "ID": "318"}, {"sentence": "AI/.G 1 (loe.atio~t) AI{GI (locat~tm) AI{G2 ( locat~m) AI{G2 (location)  We %rthermore assume that the semantics of  the predicates include a pointer to the semantics of  the PCs hey license.", "acronym": "PC", "label": "prepositional complement", "ID": "319"}, {"sentence": "First, the type system is augmented to al-  low for declaring the property of being an optional  or an obligatory PC, as in  figure 1.", "acronym": "PC", "label": "prepositional complement", "ID": "320"}, {"sentence": "l'he contextual factor  that resolves the ambiguity is the semantics of the  head of the PC which here  is tt~ken to specii~y whether the direct ob.ieet of the  verb is understood as the location and the oblique  complement as the locatum or v\\[ee versa.", "acronym": "PC", "label": "prepositional complement", "ID": "321"}, {"sentence": "whenever a verb is encoun- tered we generate templates that are paths be- tween v and the verb?s modifiers, either ob- jects, PCs or infinite or gerund verb forms (paths ending at stop words, e.g. pronouns, are not generated).", "acronym": "PC", "label": "prepositional complement", "ID": "322"}, {"sentence": "One of the properties of the verb  \"to be\" is that a PC qual-  ifying this verb really qualifies the subject of the  verb.", "acronym": "PC", "label": "prepositional complement", "ID": "323"}, {"sentence": "HMM word and phrase alignment for SMT.", "acronym": "SMT", "label": "statistical machine translation", "ID": "324"}, {"sentence": "3 Conclusion In this paper, we described a phrase-based unigram model for SMT.", "acronym": "SMT", "label": "statistical machine translation", "ID": "325"}, {"sentence": "Introduction The seminal work of Brown et al (1993b) introduced a series of probabilistic models (IBM Models 1?5) for SMT and the concept of ?", "acronym": "SMT", "label": "statistical machine translation", "ID": "326"}, {"sentence": "Europarl: A parallel corpus for SMT.", "acronym": "SMT", "label": "statistical machine translation", "ID": "327"}, {"sentence": "A hierarchical phrase-based model for SMT.", "acronym": "SMT", "label": "statistical machine translation", "ID": "328"}, {"sentence": "The mathematics of SMT: Parameter estimation.", "acronym": "SMT", "label": "statistical machine translation", "ID": "329"}, {"sentence": "The Mathe- matic of SMT: Parameter Estimation.", "acronym": "SMT", "label": "Statistical Machine Translation", "ID": "330"}, {"sentence": "We use a state-of-the-art machine translation system,5 and follow the experimental setup used for the 2008 shared task on machine translation (ACL 2008 Third Workshop on SMT).", "acronym": "SMT", "label": "Statistical Machine Translation", "ID": "331"}, {"sentence": "In Proceedings of the Third Workshop on SMT, pages 151?154, Columbus, Ohio, June.", "acronym": "SMT", "label": "Statistical Machine Translation", "ID": "332"}, {"sentence": "94   A Phrase-Based Unigram Model for SMT Christoph Tillmann and Fei Xia IBM T.J. Watson Research Center Yorktown Heights, NY 10598 {ctill,feixia}@us.ibm.com Abstract In this paper, we describe a phrase-based un- igram model for statistical machine transla- tion that uses a much simpler set of model parameters than similar phrase-based models.", "acronym": "SMT", "label": "Statistical Machine Translation", "ID": "333"}, {"sentence": "The Mathematics of SMT: Parameter Estima- tion.", "acronym": "SMT", "label": "Statistical Machine Translation", "ID": "334"}, {"sentence": "Distor- tion Models for SMT.", "acronym": "SMT", "label": "Statistical Machine Translation", "ID": "335"}, {"sentence": "The Mathe- matic of SMTn: Parameter Estimation.", "acronym": "SMT", "label": "Statistical Machine Translatio", "ID": "336"}, {"sentence": "We use a state-of-the-art machine translation system,5 and follow the experimental setup used for the 2008 shared task on machine translation (ACL 2008 Third Workshop on SMTn).", "acronym": "SMT", "label": "Statistical Machine Translatio", "ID": "337"}, {"sentence": "In Proceedings of the Third Workshop on SMTn, pages 151?154, Columbus, Ohio, June.", "acronym": "SMT", "label": "Statistical Machine Translatio", "ID": "338"}, {"sentence": "94   A Phrase-Based Unigram Model for SMTn Christoph Tillmann and Fei Xia IBM T.J. Watson Research Center Yorktown Heights, NY 10598 {ctill,feixia}@us.ibm.com Abstract In this paper, we describe a phrase-based un- igram model for statistical machine transla- tion that uses a much simpler set of model parameters than similar phrase-based models.", "acronym": "SMT", "label": "Statistical Machine Translatio", "ID": "339"}, {"sentence": "The Mathematics of SMTn: Parameter Estima- tion.", "acronym": "SMT", "label": "Statistical Machine Translatio", "ID": "340"}, {"sentence": "Distor- tion Models for SMTn.", "acronym": "SMT", "label": "Statistical Machine Translatio", "ID": "341"}, {"sentence": "Hence, SMT methods are not fea- sible under such a condition.", "acronym": "SMT", "label": "statistical MT", "ID": "342"}, {"sentence": "Due to the difficulty in obtaining  parallel corpora of ISL, the SMT ap- proaches may not be a feasible solu", "acronym": "SMT", "label": "statistical MT", "ID": "343"}, {"sentence": "00) have proposed an ap- proach to collect corpus for SMT research,  in his approach first, annotation standard for the  various hand shape movements was developed,  then the Sign Language performances were re- corded, and finally the recorded videos were  manually transcribed.", "acronym": "SMT", "label": "statistical MT", "ID": "344"}, {"sentence": "For example, in SMT the translation model and the language model are treated separately, characterised as faithfulness and fluency respectively (as in the treatment in Jurafsky and Martin (2000)).", "acronym": "SMT", "label": "statistical MT", "ID": "345"}, {"sentence": "Niedle et al (2000) have proposed an ap- proach to collect corpus for SMT research,  in his approach first, annotation standard for the  various hand shape movements was developed,  then the Sign Language performances were re- corded, and finally the recorded videos were  manually transcribed.", "acronym": "SMT", "label": "statistical MT", "ID": "346"}, {"sentence": "Due to the difficulty in obtaining  parallel corpora of ISL, the SMT ap- proaches may not be a feasible solution to our  problem.", "acronym": "SMT", "label": "statistical MT", "ID": "347"}, {"sentence": "2006) has  proposed a SMT system which uses Hid- den Markov Model and IBM models for training  the data.", "acronym": "SMT", "label": "statistical MT", "ID": "348"}, {"sentence": "Leveraging multiple languages to improve SMT word alignments.", "acronym": "SMT", "label": "statistical MT", "ID": "349"}, {"sentence": "We assume here that the MT system is capable of providing word alignment (or equiva- lent) information during decoding, which is gener- ally true for current SMT.", "acronym": "SMT", "label": "statistical MT systems", "ID": "350"}, {"sentence": "In SMT, au- tomatic evaluation of translations is essential for parameter optimization and system development.", "acronym": "SMT", "label": "statistical MT systems", "ID": "351"}, {"sentence": "We also included two commercial off-the-shelf MT systems, two online SMT, and five online rule-based MT systems. (", "acronym": "SMT", "label": "statistical MT systems", "ID": "352"}, {"sentence": "Compared to the general re-ordering models used in SMT, this type of feature is capable of modeling skeleton-level re-ordering, which is crucial to the fluency of MT output.", "acronym": "SMT", "label": "statistical MT systems", "ID": "353"}, {"sentence": "We propose and evaluate three computationally efficient online methods for updating SMT in a scenario where post-edited MT output is constantly being returned to the system: (1) adding new rules to the translation model from the post-edited content, (2) updating a Bayesian language model of the target language that is used by the MT system, and (3) updating the MT system?s discriminative parameters with a MIRA step.", "acronym": "SMT", "label": "statistical MT systems", "ID": "354"}, {"sentence": "We want to balance the disadvantage of rule- based systems with respect to lexical coverage when compared to SMT trained on large scale corpora.", "acronym": "SMT", "label": "statistical MT systems", "ID": "355"}, {"sentence": "Task-Specific Alignment Evaluation In this section we evaluate the alignments resulting from using the proposed constraints in two different tasks: SMT where alignments are used to restrict the number of possible minimal translation units; and syntax transfer, where alignments are used to decide how to transfer dependency links.", "acronym": "SMT", "label": "Statistical machine translation", "ID": "356"}, {"sentence": "SMT with word- and sentence-aligned parallel corpora.", "acronym": "SMT", "label": "Statistical machine translation", "ID": "357"}, {"sentence": "SMT in its origi- nal formulation disregarded the actual forms of words, focusing instead exclusively on their co- occurrence patterns.", "acronym": "SMT", "label": "Statistical machine translation", "ID": "358"}, {"sentence": "2 Background SMT is a decision prob- lem where we need decide on the best of target sentence matching a source sentence.", "acronym": "SMT", "label": "Statistical machine translation", "ID": "359"}, {"sentence": "SMT and example- based machine translation require numerous high-quality bilingual corpora.", "acronym": "SMT", "label": "Statistical machine translation", "ID": "360"}, {"sentence": "SMT ex- perimental results corroborate that the result- ing high-n approximate small language model is as effective as models obtained from other count pruning methods.", "acronym": "SMT", "label": "Statistical machine translation", "ID": "361"}, {"sentence": "The underlying  MT architecture can be classified into i) Direct  translation system, ii) Transfer based architecture  and iii) SMT.", "acronym": "SMT", "label": "Statistical MT", "ID": "362"}, {"sentence": "of the 3rd Workshop on SMT, Columbus, Ohio, 115?118.", "acronym": "SMT", "label": "Statistical MT", "ID": "363"}, {"sentence": "SMT: It requires large parallel cor- pora    which is very difficult to collect.", "acronym": "SMT", "label": "Statistical MT", "ID": "364"}, {"sentence": "so like to thank the anonymous re-viewers for their comments, the providers of the NIST MT evaluation tool, and the organizers of the Third Workshop on SMT for making available the News Corpus.", "acronym": "SMT", "label": "Statistical MT", "ID": "365"}, {"sentence": "A Decoder for Syntax-based SMT.", "acronym": "SMT", "label": "Statistical MT", "ID": "366"}, {"sentence": "of the 3rd Workshop on SMT, Columbus, 70?106.", "acronym": "SMT", "label": "Statistical MT", "ID": "367"}, {"sentence": "We would also like to thank the anonymous re-viewers for their comments, the providers of the NIST MT evaluation tool, and the organizers of the Third Workshop on SMT for making available the News Corpus.", "acronym": "SMT", "label": "Statistical MT", "ID": "368"}, {"sentence": "4.1 Acoustic Confirmation Threshold When ASR produces a hypothesis of what has been said, it also returns an acoustic confidence score which the application can utilize to decide whether to reject the utterance, confirm it, or accept it right away.", "acronym": "ASR", "label": "a speech recognizer", "ID": "369"}, {"sentence": "Furthermore, although we have not yet exper-  imented with the pruned grammar on spoken input, we  expect that the pruned grammar will improve the abil-  ity of PUNDIT to reject ungrammatical candidates from  ASR.", "acronym": "ASR", "label": "a speech recognizer", "ID": "370"}, {"sentence": "In particular,  we use a dynamic programming tabular algorithm  to find the minimal cost transduction of a word  string or word-lattice from ASR.", "acronym": "ASR", "label": "a speech recognizer", "ID": "371"}, {"sentence": "Finally, it should also be pos-  sible to embed our phonetic shift model P(jle) in-  side ASR, to help adjust for a heavy  Japanese accent, although we have not experimented  in this area.", "acronym": "ASR", "label": "a speech recognizer", "ID": "372"}, {"sentence": "Estimation of probabilities from  sparse data for the language model compo-  nent of ASR.", "acronym": "ASR", "label": "a speech recognizer", "ID": "373"}, {"sentence": "Estimation of probabilities from  sparse data for the language model component  of ASR.", "acronym": "ASR", "label": "a speech recognizer", "ID": "374"}, {"sentence": "For data col-  lected at SRI, this was true; all other sites used some  ASR and/or natural language  understanding, with varying amounts of human tran-  scription and error correction.", "acronym": "ASR", "label": "automatic speech recognition", "ID": "375"}, {"sentence": "For ASR experiments, we used as test set the 1997 Hub4 evaluation set consisting of 32,689 words.", "acronym": "ASR", "label": "automatic speech recognition", "ID": "376"}, {"sentence": "Minimum Bayes risk ASR.", "acronym": "ASR", "label": "automatic speech recognition", "ID": "377"}, {"sentence": "The most salient difference, however, is that they are motivated by the goal of improving ASR, and have an acoustic signal parallel to the undiacritized text.", "acronym": "ASR", "label": "automatic speech recognition", "ID": "378"}, {"sentence": "1 Introduction CRIM?s ASR system has been applied to live closed-captioning of french- canadian television programs (Boulianne et al, 2006).", "acronym": "ASR", "label": "automatic speech recognition", "ID": "379"}, {"sentence": "THE SPEECH RECOGNITION PROBLEM  Natural language ASR typically proceeds as follows.", "acronym": "ASR", "label": "automatic speech recognition", "ID": "380"}, {"sentence": "We use a state of the art ASR system to transcribe the calls between agents and customers, which still results in high word error rates (40%) and show that even from these noisy transcrip- tions of calls we can automatically build a domain model.", "acronym": "ASR", "label": "Automatic Speech Recognition", "ID": "381"}, {"sentence": "Wald, M. 2006 Creating Accessible Educational Multi- media through Editing ASR Captioning in Real Time.", "acronym": "ASR", "label": "Automatic Speech Recognition", "ID": "382"}, {"sentence": "In ASR and Understanding.", "acronym": "ASR", "label": "Automatic Speech Recognition", "ID": "383"}, {"sentence": "Difficulties in ASR of Dysarthric Speakers and Implications for Speech-Based Applications Used by the Elderly: A Literature Review.", "acronym": "ASR", "label": "Automatic Speech Recognition", "ID": "384"}, {"sentence": "Active learning for rule-based and corpus-based  Spoken Language Understanding models, In Proceedings of IEEE Conference on ASR  and Understanding, pp.", "acronym": "ASR", "label": "Automatic Speech Recognition", "ID": "385"}, {"sentence": "1.2 Motivation Syntactic parsing is essential for many natural lan- guage applications such as Machine Translation, Question Answering, Information Extraction, Infor- mation Retrieval, ASR.", "acronym": "ASR", "label": "Automatic Speech Recognition", "ID": "386"}, {"sentence": "SR grammar com- pilation in Grammatical Framework.", "acronym": "SR", "label": "Speech recognition", "ID": "387"}, {"sentence": "The parser is coded in C. SR is performed  by a Verbex 6000 user-dependent connected-speech recognizer unning on an IBM  PC, and the vocabulary is currently restricted to 125 words.", "acronym": "SR", "label": "Speech recognition", "ID": "388"}, {"sentence": "SR and dysarthria: a single subject study of two individuals with profound impairment of speech and motor control.", "acronym": "SR", "label": "Speech recognition", "ID": "389"}, {"sentence": "INTRODUCTION SR has advanced considerably, but has been lim- ited almost entirely either to situations in which close speaking mi- crophones are natural and acceptable (telephone, dictation, com- mand&control, etc.)", "acronym": "SR", "label": "Speech recognition", "ID": "390"}, {"sentence": "SR  by composition of weighted finite automata.", "acronym": "SR", "label": "Speech recognition", "ID": "391"}, {"sentence": "Error(%)  62.5 98.3  51.0 93.9  55.4 97.8  51.0 98.1  24.1 72.7  22.0 66.9  20.6 67.7  19.3 61.6  18.8 58.6  Table 1: SR results on the October '91 test set for the various experiments described in this paper.", "acronym": "SR", "label": "Speech recognition", "ID": "392"}, {"sentence": "More concretely, we assume that the summary  81 sentences of a given document can be iteratively  chosen (i.e., one at each iteration) from the doc- ument until the aggregated summary reaches a  predefined target SR.", "acronym": "SR", "label": "summarization ratio", "ID": "393"}, {"sentence": "The SR, defined as the ratio of  the number of words in the automatic (or manual)  summary to that in the reference transcript of a  spoken document, was set to 10% in this re- search.", "acronym": "SR", "label": "summarization ratio", "ID": "394"}, {"sentence": "We have implemented the approach and evaluated in SR and their correctness.", "acronym": "SR", "label": "summarization ratio", "ID": "395"}, {"sentence": "We computed the SR about these sentences.", "acronym": "SR", "label": "summarization ratio", "ID": "396"}, {"sentence": "2 Evaluation Metrics for Extraction In summarization through sentence or word extrac- tion under a specific SR, the order of the sentences or words and the length of the sum- maries are restricted by the original documents or sentences.", "acronym": "SR", "label": "summarization ratio", "ID": "397"}, {"sentence": "The SR is 94%.", "acronym": "SR", "label": "summarization ratio", "ID": "398"}, {"sentence": "This algorithm SRes the parse tree in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.", "acronym": "SR", "label": "search", "ID": "399"}, {"sentence": "Since the reSR described  herein we have thought of other influences on  anaphora resolution and their statistical corre-  lates.", "acronym": "SR", "label": "search", "ID": "400"}, {"sentence": "7 Prev ious  Work   The literature on pronoun anaphora is too ex-  tensive to summarize, so we concentrate here on  corpus-based anaphora reSR.", "acronym": "SR", "label": "search", "ID": "401"}, {"sentence": "8 Conc lus t ion  and  Future  ReSR   We have presented a statistical method for  pronominal anaphora that achieves an accuracy  of 84.2%.", "acronym": "SR", "label": "search", "ID": "402"}, {"sentence": "In resolving inter-sentential  pronouns, the algorithm SRes the previous  sentence, again in left-to-right, breadth-first or-  der.", "acronym": "SR", "label": "search", "ID": "403"}, {"sentence": "This too is a topic for  future reSR.", "acronym": "SR", "label": "search", "ID": "404"}, {"sentence": "This pa-per has presented a novel representation of such dialogue with a tutoring domain, and has pre-sented and evaluated a feature selection method based on a new SR metric, which can inform the development of turn-taking poli-cies in dialogue systems.", "acronym": "SR", "label": "Separation Ratio", "ID": "405"}, {"sentence": "2.1 GROUPING SRS  Both on implementational nd on  theoretical grounds, we have grouped  certain semantic roles into superclasses.", "acronym": "SR", "label": "SEMANTIC ROLE", "ID": "406"}, {"sentence": "The semantic interpretation of syntactic  subjects and objects, of prepositions and  subordinate conjunctions has been treated in  numerous books and papers with titles  including words like DEEP CASES, CASE ROLES,  SRS and SEMANTIC RELATIONS.", "acronym": "SR", "label": "SEMANTIC ROLE", "ID": "407"}, {"sentence": "The arguments to the  predicates constitute the SRS of the  verb, which are similar to cases.", "acronym": "SR", "label": "SEMANTIC ROLE", "ID": "408"}, {"sentence": "C lause AJtalysls AlKorlChm  DECOMPOSE NOMINALIZATION  FOR EACH SEMANTIC  ROLE:  IF THERE ARE SYNTACTIC CONSTITUENTS -  PROPOSE SYNTACTIC CONSTITUENT FILLER  & CALL REFERENCE RESOLUTION  & TEST SELECTIONAL RESTRICTIONS  CALL TEMPORAL ANALYSIS ON DECOMPOSITION  CALL REFERENCE RESOLUTION FOR NOMINALIZATION NOUN PHRASE  FOR EACH SR:  IF ESSENTIAL ROLE AND UNFILLED  CALL REFERENCE RESOLUTION TO HYPOTHESIZE A FILLER  TEST SELECTIONAL RESTRICTIONS  ELSE LEAVE UNFILLED  FJKure 2.", "acronym": "SR", "label": "SEMANTIC ROLE", "ID": "409"}, {"sentence": "The correlation between the valence slots of AdvD and the main  verb can be formulated IN TERMS OF SRS as follows: if a valence slot of AdvD which  corresponds to semantic role R (Agent, Theme, Recipient) is instantiated, it is either filled by an AdvD  dependent (as in v podarok do?eri ?", "acronym": "SR", "label": "SEMANTIC ROLE", "ID": "410"}, {"sentence": "The arguments of the predicates  constitute the SRS of the verb, which  are slml\\]ar to cases 4For example, fall decomposes  into become inoperat lve ,  with pat ient  as its  only semantic role.", "acronym": "SR", "label": "SEMANTIC ROLE", "ID": "411"}, {"sentence": "Research on this field is not new and varied methods have been proposed to achieve dif- ferent steps of this task including the extraction of SR (e.g. (Hearst, 1992) (Girju et al.,", "acronym": "SR", "label": "semantic relations", "ID": "412"}, {"sentence": "2009) is a lexical network consisting of triples denoting SR between words found in a dictionary.", "acronym": "SR", "label": "semantic relations", "ID": "413"}, {"sentence": "Most previous ap- proaches to this problem have focused on the interpre- tation of two word compounds whose nouns are related via a basic set of SR (e.g., CAUSE relates onion tears, FOR relates pet spray).", "acronym": "SR", "label": "semantic relations", "ID": "414"}, {"sentence": "We have already made a first approach on the extraction of relational triples from text, where, likewise Hearst (1992), we take advantage of textual patterns indicating SR.", "acronym": "SR", "label": "semantic relations", "ID": "415"}, {"sentence": "By recasting the interpretation problem in terms of paraphrasing, Lauer assumes that the SR of compound heads and modifiers can be expressed via prepositions that (in contrast to abstract SR) can be found in a corpus.", "acronym": "SR", "label": "semantic relations", "ID": "416"}, {"sentence": "Computer Science  University of Salford, UK  I.Spasic@salford.ac.uk Sophia Ananiadou  Computer Science   University of Salford, UK  S.Ananiadou@salford.ac.uk     Abstract  In this paper we discuss morpho-syntactic  clues that can be used to facilitate termi- nological processing in SR.", "acronym": "SR", "label": "Serbian", "ID": "417"}, {"sentence": "985  Morpho-syntactic Clues for Terminological Processing in SR  Goran Nenadi?", "acronym": "SR", "label": "Serbian", "ID": "418"}, {"sentence": "3.1 Term formation patterns  As a rule, the vast majority of multiword terms in  SR match the following general formation  pattern:4    (1)           (Adj | ProAdj | Num | Noun )+ Noun    which has been used for recognition of NPs in  SR (Nenadi?", "acronym": "SR", "label": "Serbian", "ID": "419"}, {"sentence": "In Sec- tion 3 we discuss morpho-syntactic clues, the  normalisation approach and the foreign word  recognition that are used for singling out terms in  SR.", "acronym": "SR", "label": "Serbian", "ID": "420"}, {"sentence": "3.1 Term formation patterns  As a rule, the vast majority of multiword terms in  SR match the following general formation  pattern:4    (1)           (Adj | ProAdj | Num | Noun )+ Noun    which has", "acronym": "SR", "label": "Serbian", "ID": "421"}, {"sentence": "3.1 Term formation patterns  As a rule, the vast majority of multiword terms in  SR match the following general formation  pattern:4    (1)", "acronym": "SR", "label": "Serbian", "ID": "422"}, {"sentence": "3 Morpho-syntactic clues for extraction  of terms in SR  In order to adjust the core C-value method for  SR, we have defined an appropriate set of  morpho-syntactic filters and rules for inflectional  normalisation of term candidates, and, addition- ally, a module for foreign word recognition.", "acronym": "SR", "label": "Serbian", "ID": "423"}, {"sentence": "means a chunk in NomBank or PropBank, which demonstrates how SR occur around a specified predicate.", "acronym": "SR", "label": "semantic roles", "ID": "424"}, {"sentence": "We decom- pose our task into three phases: identify- ing an opinion-bearing word, labeling  SR related to the word in the  sentence, and then finding the holder and  the topic of the opinion word among the  labeled SR.", "acronym": "SR", "label": "semantic roles", "ID": "425"}, {"sentence": "Frey B.J. 1998, Graphical Models for Machine  Learning and Digital Communication,  Cambridge, MA, MIT Press  Gildea D., Jurafsky D. 2002, Automatic labeling of  SR, Computational Linguistics,  28(3):245-288.", "acronym": "SR", "label": "semantic roles", "ID": "426"}, {"sentence": "Their work  looked at the problem of assigning SR to  text based on a statistical model of the FrameNet1  data.", "acronym": "SR", "label": "semantic roles", "ID": "427"}, {"sentence": "The proposition bank: An annotated cor- pus of SR.", "acronym": "SR", "label": "semantic roles", "ID": "428"}, {"sentence": "We propose an alternative, parameter-free method in which we model the co- hesive structure of a discourse as a graph structure (called cohesion graph), where the vertices of the graph correspond to the content words of the text and the edges encode the SR be- tween pairs of words.", "acronym": "SR", "label": "semantic relatedness", "ID": "429"}, {"sentence": "The value of e ij rep- resents the SR of the two tokens t i , t j that e ij connects: e ij = h(t i , t j ) (5) where h is a SR assignment function.", "acronym": "SR", "label": "semantic relatedness", "ID": "430"}, {"sentence": "id i , id j ) (8) As the SR among the MWE component words does not contain any informa- tion of how these component words are seman- tically involved in the context, we do not count the edges between the MWE component words 77 (as e 45 in Figure 1).", "acronym": "SR", "label": "semantic relatedness", "ID": "431"}, {"sentence": "e i is the average SR of the to- ken t i in the discourse.", "acronym": "SR", "label": "semantic relatedness", "ID": "432"}, {"sentence": "Figure 1: Cohesion Graph for identifying literal or non-literal usage of MWEs 78 4 Modeling Semantic Relatedness In Section 3.1, we did not define how we model the SR between two tokens (h(t i , t j )).", "acronym": "SR", "label": "semantic relatedness", "ID": "433"}, {"sentence": "Modeling SR be- tween two terms is currently an area of active re- search.", "acronym": "SR", "label": "semantic relatedness", "ID": "434"}, {"sentence": "2 The DALE System 2.1 Automatically Labeling Examples DALE assigns Concept Unique Identifiers (CUIs) from the UMLS MT.", "acronym": "MT", "label": "Metathesaurus", "ID": "435"}, {"sentence": "The difference be- tween the two approaches is that they make use of different information from the MT.", "acronym": "MT", "label": "Metathesaurus", "ID": "436"}, {"sentence": "DALE is 1 able to identify a meaning for any term that is am- biguous in the MT and therefore has far greater coverage of ambiguous terms than other su- pervised WSD systems.", "acronym": "MT", "label": "Metathesaurus", "ID": "437"}, {"sentence": "DALE uses the UMLS MT as both a sense inventory and as a source of infor- mation for automatically generating labeled training examples.", "acronym": "MT", "label": "Metathesaurus", "ID": "438"}, {"sentence": "Both approaches take a single CUI, c, as input and use information from the UMLS MT to search Medline and identi", "acronym": "MT", "label": "Metathesaurus", "ID": "439"}, {"sentence": "For biomedical documents the UMLS MT (Humphreys et al 1998b) is a more suitable lexical resource than WordNet and tech- niques have been developed to create automatically labeled examples for this resource (Stevenson and Guo, 2010).", "acronym": "MT", "label": "Metathesaurus", "ID": "440"}, {"sentence": "Both approaches take a single CUI, c, as input and use information from the UMLS MT to search Medline and identify instances of c that can be used as labeled examples.", "acronym": "MT", "label": "Metathesaurus", "ID": "441"}, {"sentence": "DALE (Disambiguation using Automatically La- beled Examples) is an online WSD system for biomedical documents that was developed by creat- ing automatically labeled examples for all ambigu- ous terms in the UMLS MT.", "acronym": "MT", "label": "Metathesaurus", "ID": "442"}, {"sentence": "t Unique Identifiers (CUIs) from the UMLS MT.", "acronym": "MT", "label": "Metathesaurus", "ID": "443"}, {"sentence": "Both approaches are provided with a set of ambiguous CUIs from the UMLS MT, which represent the possible meanings of an am- biguous term, and a target number of training ex- am", "acronym": "MT", "label": "Metathesaurus", "ID": "444"}, {"sentence": "Performance on MT sets was the lowest that can be explained by very poor quality of the training data5: models for these subsets should have been trained separately.", "acronym": "MT", "label": "machine translation test", "ID": "445"}, {"sentence": "On NIST MT sets, our reordering model  achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong baseline  hierarchical phrase-based system.", "acronym": "MT", "label": "machine translation test", "ID": "446"}, {"sentence": "We integrate our method into a state-of-the-art phrase-based baseline translation system, i.e., Moses (Koehn et al, 2007), and show that the integrated system consistently im- proves the performance of the baseline system on various NIST MT sets.", "acronym": "MT", "label": "machine translation test", "ID": "447"}, {"sentence": "We performed experiments to eliminate the pos- sibility of data overlap between the training data and the MT data as cause for the large improvements.", "acronym": "MT", "label": "machine translation test", "ID": "448"}, {"sentence": "4 Resu l ts  and  D iscuss ion   The algorithm was tested on a 3700 sentence  MT set of Japanese sen-  tences with English translations, produced by  a professional human translator.", "acronym": "MT", "label": "machine translation test", "ID": "449"}, {"sentence": "On NIST MT sets,  our reordering model achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation  over a strong baseline hierarchical phrase-based system.", "acronym": "MT", "label": "machine translation test", "ID": "450"}, {"sentence": "It is compati- ble with the broader range of DELPH-IN tools, e.g., for MTion (L?nning and Oepen, 2006), treebanking (Oepen et al, 2004) and parse selection (Toutanova et al, 2005).", "acronym": "MT", "label": "machine translat", "ID": "451"}, {"sentence": "Introduction The seminal work of Brown et al (1993b) introduced a series of probabilistic models (IBM Models 1?5) for statistical MTion and the concept of ?", "acronym": "MT", "label": "machine translat", "ID": "452"}, {"sentence": "They used this information to re- attach PPs in a MTion system, report- ing an improvement in translation quality when translating into Japanese (where PP attachment is not ambiguous and therefore matters) and a de- crease when translating into Spanish (where at- tachment ambiguities are close to the original ones and therefore need not be resolved).", "acronym": "MT", "label": "machine translat", "ID": "453"}, {"sentence": "Re-usable tools for precision MTion.", "acronym": "MT", "label": "machine translat", "ID": "454"}, {"sentence": "Confidence es- timation for MTion.", "acronym": "MT", "label": "machine translat", "ID": "455"}, {"sentence": "We also report experiments on two different tasks where word alignments are required: phrase-based MTion and syntax transfer, and show promising improvements over standard methods.", "acronym": "MT", "label": "machine translat", "ID": "456"}, {"sentence": "Nevertheless, they are key to various NLP applications, including those benefiting from deep natural language understanding (e.g., textual inference (Bobrow et al, 2007)), generation of well- formed output (e.g., natural language weather alert systems (Lareau and Wanner, 2007)) or both (as in MTion (Oepen et al, 2007)).", "acronym": "MT", "label": "machine translat", "ID": "457"}, {"sentence": "Annotation Lastly, we believe that it is important to provide alternative corrections, as the agreement on what constitutes a mistake even among native English speakers can be quite low (MT al 2011).", "acronym": "MT", "label": "Madnani et", "ID": "458"}, {"sentence": "Although there has been some recent work on paraphrasing that provided detailed error analysis of system outputs (Socher et al, 2011; MT al, 2012), more often than not such investigations are seen as above-and-beyond when assessing metrics.", "acronym": "MT", "label": "Madnani et", "ID": "459"}, {"sentence": "Sentence compression is valuable in many applica- tions, for example when displaying texts on small screens (Corston-Oliver, 2001), in subtitle genera- tion (Vandeghinste and Pan, 2004), and in text sum- marization (MT al, 2007).", "acronym": "MT", "label": "Madnani et", "ID": "460"}, {"sentence": "distance: This metric has been widely used in evaluation of sentence ordering (Lapata, 2003; Lapata, 2006; Bollegala et al 2006; MT al 2007)1.", "acronym": "MT", "label": "Madnani et", "ID": "461"}, {"sentence": "Again, this is an oversimplified application of the aligner, even more so than in STS, since a small change in linguistic properties of two sentences (e.g. polarity or modality) can turn them into non- 5https://code.google.com/p/jacana/ 6http://research.microsoft.com/en-us/downloads/607d14d9- 20cd-47e3-85bc-a2f65cd28042/ 227 System Acc.% P% R% F1% MT al. (", "acronym": "MT", "label": "Madnani et", "ID": "462"}, {"sentence": "On the other hand, training a classifier for ranking can- didate answers allows the exploitation of various features extracted from the question, candidate an- swer, and surrounding context (MT al, 2007; Zhang et al, 2007).", "acronym": "MT", "label": "Madnani et", "ID": "463"}, {"sentence": "It is compati- ble with the broader range of DELPH-IN tools, e.g., for MT (L?nning and Oepen, 2006), treebanking (Oepen et al, 2004) and parse selection (Toutanova et al, 2005).", "acronym": "MT", "label": "machine translation", "ID": "464"}, {"sentence": "Introduction The seminal work of Brown et al (1993b) introduced a series of probabilistic models (IBM Models 1?5) for statistical MT and the concept of ?", "acronym": "MT", "label": "machine translation", "ID": "465"}, {"sentence": "They used this information to re- attach PPs in a MT system, report- ing an improvement in translation quality when translating into Japanese (where PP attachment is not ambiguous and therefore matters) and a de- crease when translating into Spanish (where at- tachment ambiguities are close to the original ones and therefore need not be resolved).", "acronym": "MT", "label": "machine translation", "ID": "466"}, {"sentence": "Re-usable tools for precision MT.", "acronym": "MT", "label": "machine translation", "ID": "467"}, {"sentence": "Confidence es- timation for MT.", "acronym": "MT", "label": "machine translation", "ID": "468"}, {"sentence": "We also report experiments on two different tasks where word alignments are required: phrase-based MT and syntax transfer, and show promising improvements over standard methods.", "acronym": "MT", "label": "machine translation", "ID": "469"}, {"sentence": "Nevertheless, they are key to various NLP applications, including those benefiting from deep natural language understanding (e.g., textual inference (Bobrow et al, 2007)), generation of well- formed output (e.g., natural language weather alert systems (Lareau and Wanner, 2007)) or both (as in MT (Oepen et al, 2007)).", "acronym": "MT", "label": "machine translation", "ID": "470"}, {"sentence": "85 1 1.5 2 2.5 3 3.5 429 29.5 30 30.5 31 log10(tau) BL EU   %     devtest test   Figure 2: MT results on Europarl,  English to French track, devtest and test sets.", "acronym": "MT", "label": "Machine translation", "ID": "471"}, {"sentence": "1 1.5 2 2.5 3 3.5 420 20.5 21 21.5 22 log10(tau) BL EU   %     nc-test   Figure 3: MT results on Europarl,  English to French track, out-of", "acronym": "MT", "label": "Machine translation", "ID": "472"}, {"sentence": "5.1 MT We obtain packed-forest English outputs from 116 short Chinese sentences computed by a string-to- tree machine translation system based on (Galley, et.", "acronym": "MT", "label": "Machine translation", "ID": "473"}, {"sentence": ".5 429 29.5 30 30.5 31 log10(tau) BL EU   %     devtest test   Figure 2: MT results on Europarl,  English to French track, devtest and test sets.", "acronym": "MT", "label": "Machine translation", "ID": "474"}, {"sentence": "--91  MT \\[12\\]  As the first step to the machine  translation, we are implementing a program which  generates Japanese from the LE obtained by  analyzing English.", "acronym": "MT", "label": "Machine translation", "ID": "475"}, {"sentence": "1 1.5 2 2.5 3 3.5 420 20.5 21 21.5 22 log10(tau) BL EU   %     nc-test   Figure 3: MT results on Europarl,  English to French track, out-of-domain test sets.", "acronym": "MT", "label": "Machine translation", "ID": "476"}, {"sentence": "MT diver- gences: A formal description and proposed solution.", "acronym": "MT", "label": "Machine translation", "ID": "477"}, {"sentence": "MT.", "acronym": "MT", "label": "Machine translation", "ID": "478"}, {"sentence": "\\[NAGAO 86\\] Nagao, M., Tsuj i i ,  J., The Transfer  Phase  of the Mu MT  System, Proc.", "acronym": "MT", "label": "Machine Trans lat ion", "ID": "479"}, {"sentence": "For example, one of the tasks  for the Wayne State Univers i ty  MT group was  to program a routine to group each nominal in a Russ ian sen-  tence with its preceding (dependent) modif iers.", "acronym": "MT", "label": "Machine Trans lat ion", "ID": "480"}, {"sentence": "Sumita, E. and Iida, H.: \"Transfer  Dr iven MT  Ut i l iz ing  Empirical Knowledge,\" Transactions of  Information Processing Society of Japan, Vol.", "acronym": "MT", "label": "Machine Trans lat ion", "ID": "481"}, {"sentence": "Theoret i ca l  and Methodo lo l i ca l   l i nes  in  MT,  Cambridge University Press, to appear 1986.", "acronym": "MT", "label": "Machine Trans lat ion", "ID": "482"}, {"sentence": "ln ter l ingua l  MT:  a Parame-   ter ized Approach .", "acronym": "MT", "label": "Machine Trans lat ion", "ID": "483"}, {"sentence": "Shake-and-Bake  MT .", "acronym": "MT", "label": "Machine Trans lat ion", "ID": "484"}, {"sentence": "In MT Summit IX, New Orleans, Louisiana, USA.", "acronym": "MT", "label": "Machine Translation", "ID": "485"}, {"sentence": "Fifth MT Summit.", "acronym": "MT", "label": "Machine Translation", "ID": "486"}, {"sentence": "BLEU: a Method for Automatic Evaluation of MT.", "acronym": "MT", "label": "Machine Translation", "ID": "487"}, {"sentence": "The Mathe- matic of Statistical MT: Parameter Estimation.", "acronym": "MT", "label": "Machine Translation", "ID": "488"}, {"sentence": "In Proceedings of the Third Workshop on Statistical MT, pages 151?154, Columbus, Ohio, June.", "acronym": "MT", "label": "Machine Translation", "ID": "489"}, {"sentence": "Distor- tion Models for Statistical MT.", "acronym": "MT", "label": "Machine Translation", "ID": "490"}, {"sentence": "We determined order based on  the following precedence ranking:   Subject \u0001 Direct Object \u0001 IOt Any remaining ties (e.g., an utterance with two  direct objects) were resolved according to a left- to-right breadth-first traversal of the parse tree.", "acronym": "IO", "label": "Indirect Objec", "ID": "491"}, {"sentence": "IOt relationship between a verb and an indirect object (a verb expressing a mental state; expressing a cause) 8.", "acronym": "IO", "label": "Indirect Objec", "ID": "492"}, {"sentence": "Direct Object NPs and IOt NPs are all untagged? (", "acronym": "IO", "label": "Indirect Objec", "ID": "493"}, {"sentence": "PRD Predicative Elements VP, PREDP SBJ Grammatical Subjects NP, SBAR OBJ Direct Objects NP COM IOts NP, PP FInite Complements SBAR IC Infinitival Complements VP CNJ A Conjunct within a Conjunction Structure All Table 2: Grammatical Functions in the MHTB SP-PCFG Expansion P(C l n , . . . ,", "acronym": "IO", "label": "Indirect Objec", "ID": "494"}, {"sentence": "for non- possessive pronouns  Non-immediate  Clause (neither the  current or immediate  clause)  50  Possessive NP 65  Existential NP 70  Subject 80  Direct Object 50  IOt 40  Compliment of PP 30     Table 1: Salience Factors and weights    Improving pronominal resolution Using Name  Entity (NE) and WordNet: Pronouns such as  ?", "acronym": "IO", "label": "Indirect Objec", "ID": "495"}, {"sentence": "IOt Constructions in English and the Ordering of Transformations.", "acronym": "IO", "label": "Indirect Objec", "ID": "496"}, {"sentence": "We determined order based on  the following precedence ranking:   Subject \u0001 Direct Object \u0001 IO Any remaining ties (e.g., an utterance with two  direct objects) were resolved according to a left- to-right breadth-first traversal of the parse tree.", "acronym": "IO", "label": "Indirect Object", "ID": "497"}, {"sentence": "IO relationship between a verb and an indirect object (a verb expressing a mental state; expressing a cause) 8.", "acronym": "IO", "label": "Indirect Object", "ID": "498"}, {"sentence": "Direct Object NPs and IO NPs are all untagged? (", "acronym": "IO", "label": "Indirect Object", "ID": "499"}, {"sentence": "PRD Predicative Elements VP, PREDP SBJ Grammatical Subjects NP, SBAR OBJ Direct Objects NP COM IOs NP, PP FInite Complements SBAR IC Infinitival Complements VP CNJ A Conjunct within a Conjunction Structure All Table 2: Grammatical Functions in the MHTB SP-PCFG Expansion P(C l n , . . . ,", "acronym": "IO", "label": "Indirect Object", "ID": "500"}, {"sentence": "for non- possessive pronouns  Non-immediate  Clause (neither the  current or immediate  clause)  50  Possessive NP 65  Existential NP 70  Subject 80  Direct Object 50  IO 40  Compliment of PP 30     Table 1: Salience Factors and weights    Improving pronominal resolution Using Name  Entity (NE) and WordNet: Pronouns such as  ?", "acronym": "IO", "label": "Indirect Object", "ID": "501"}, {"sentence": "IO Constructions in English and the Ordering of Transformations.", "acronym": "IO", "label": "Indirect Object", "ID": "502"}, {"sentence": "For example, the ERG marker ne only occurs on subjects.", "acronym": "ERG", "label": "ergative", "ID": "503"}, {"sentence": "Finally, line 3 calls a template that assigns the volitionality features associated with ERG noun phrases.", "acronym": "ERG", "label": "ergative", "ID": "504"}, {"sentence": "Merlo and Stevenson (2001) report inter-judge \u000b values of 0.53 to 0.66 for a task we consider to be comparable to ours, that of classifying verbs into unERG, unaccusative and object-drop, and argue that Car- letta?s ?", "acronym": "ERG", "label": "ergative", "ID": "505"}, {"sentence": "However, not all subjects are ERG.", "acronym": "ERG", "label": "ergative", "ID": "506"}, {"sentence": "To the contrary, subjects can occur in the ERG, nominative, dative, genitive, and instrumental cases.", "acronym": "ERG", "label": "ergative", "ID": "507"}, {"sentence": "Line 2 provides the inside-out functional uncertainty statement; it states that the f- structure of the ERG noun phrase, referred to as ?,", "acronym": "ERG", "label": "ergative", "ID": "508"}, {"sentence": "The lexical entry for the ERG case, for example, states that it applies to a subject.", "acronym": "ERG", "label": "ergative", "ID": "509"}, {"sentence": "n\u0003/ne ko/ko s\u0003/se m\u0003\\/meN pr/par kA/kaa (ERG) (Dative) (Instrumental) (Locative) (Locative) (Genitive) k1(agent) 7222 575 21 11 3 612 k2(patient) 0 3448 451 8 24 39 k3(instrument) 0 0 347 0 0 1 k4(recipient) 0 1851 351 0 1 4 k4a(experiencer) 0 420 8 0 0 2 k5(source) 0 2 1176 12 1 0 k7(location) 0 1140 308 8707 3116 19 r6(possession) 0 3 1 0 0 2251 Table 4 : Distribution of case markers across case function.", "acronym": "ERG", "label": "Ergative", "ID": "510"}, {"sentence": "2009) and verb classification (Merlo and Steven- 1ERG, Genitive, Instrumental, Dative, Accusative and Locative in the given order.", "acronym": "ERG", "label": "Ergative", "ID": "511"}, {"sentence": "NPErg: ERG case with case marker  ?", "acronym": "ERG", "label": "Ergative", "ID": "512"}, {"sentence": "Nominals marked with ERG case have highest control and the ones marked with Locative have lowest.", "acronym": "ERG", "label": "Ergative", "ID": "513"}, {"sentence": "Following is an example profile: <Profile> <language code=\"WBP\">Warlpiri</language> <ontologyNamespace prefix=\"gold\"> http://linguistic-ontology.org/gold.owl# </ontologyNamespace> <feature=\"word_order\"><value>SVO</value></feature> <feature=\"det_order\"><value>DT-NN</value></feature> <feature=\"case\"> <value>gold:DativeCase</value> <value>gold:ERGCase</value> <value>gold:NominativeCase</value> . . .", "acronym": "ERG", "label": "Ergative", "ID": "514"}, {"sentence": "No- 796 tably the work of Merlo and Stevenson (2001) at- tempts to induce three main English verb classes on a large scale from parsed corpora, the class of ERG, Unaccusative, and Object-drop verbs.", "acronym": "ERG", "label": "Ergative", "ID": "515"}, {"sentence": "Joshua 2.0: A toolkit for parsing-based machine translation with syn- tax, semirings, disCRF and other good- ies.", "acronym": "CRF", "label": "criminative training", "ID": "516"}, {"sentence": "DisCRF meth- ods for Hidden Markov Models: theory and experiments with perceptron algorithms.", "acronym": "CRF", "label": "criminative training", "ID": "517"}, {"sentence": "DisCRF meth- ods for hidden Markov models: Theory and exper- iments with perceptron algorithms.", "acronym": "CRF", "label": "criminative training", "ID": "518"}, {"sentence": "Instead of using disCRF meth- ods to tune the weights of generative models, in this paper we propose to use a discrimina- tive word aligner to produce reliable constraints for the EM algorithm.", "acronym": "CRF", "label": "criminative training", "ID": "519"}, {"sentence": "However, the disCRF in these methods is re- stricted in using the model components of gener- ative models, in other words, incorporating new features is difficult.", "acronym": "CRF", "label": "criminative training", "ID": "520"}, {"sentence": "CRFs: Probabilis- tic models for segmenting and labeling sequence data.", "acronym": "CRF", "label": "Conditional random field", "ID": "521"}, {"sentence": "CRFs: Prob- abilistic models for segmenting and labeling se- quence data.", "acronym": "CRF", "label": "Conditional random field", "ID": "522"}, {"sentence": "CRFs: probabilistic models for segmenting and labeling sequence data.", "acronym": "CRF", "label": "Conditional random field", "ID": "523"}, {"sentence": "CRFs: Probabilistic mod- els for segmenting and labeling sequence data.", "acronym": "CRF", "label": "Conditional random field", "ID": "524"}, {"sentence": "Identifying Sources of Opinions with CRF and Extraction Patterns.", "acronym": "CRF", "label": "Conditional Random  Fields", "ID": "525"}, {"sentence": "We have developed a domain  independent system for ATR  from unrestricted text.", "acronym": "ATR", "label": "automatic term recognition", "ID": "526"}, {"sentence": "The result of ATR for ???", "acronym": "ATR", "label": "automatic term recognition", "ID": "527"}, {"sentence": "A proba- bilistic framework for ATR.", "acronym": "ATR", "label": "automatic term recognition", "ID": "528"}, {"sentence": "We use C-value, a measurement of ATR, to score source phrases.", "acronym": "ATR", "label": "automatic term recognition", "ID": "529"}, {"sentence": "Method for ATR.", "acronym": "ATR", "label": "automatic term recognition", "ID": "530"}, {"sentence": "Statistical and  linguistic approaches to ATR  NTCIR experiments at Matsushita.", "acronym": "ATR", "label": "automatic term recognition", "ID": "531"}, {"sentence": "Methods of  ATR: A Review.", "acronym": "ATR", "label": "Automatic Term Recognition", "ID": "532"}, {"sentence": "An Application  and Evaluation of the C/NC-Value Approach for the  ATR of Multi-Word Units in  Japanese.", "acronym": "ATR", "label": "Automatic Term Recognition", "ID": "533"}, {"sentence": "Lauriston, A. (1996) ATR :  performance of Linguistic and Statistical  Techniques.", "acronym": "ATR", "label": "Automatic Term Recognition", "ID": "534"}, {"sentence": "Nakagawa, H. and Mori, T.: ATR based on Statistics of Compound Nouns and their Components.", "acronym": "ATR", "label": "Automatic Term Recognition", "ID": "535"}, {"sentence": "ATR using Contextual Cues.", "acronym": "ATR", "label": "Automatic Term Recognition", "ID": "536"}, {"sentence": "Jong-Hoon Oh, Jae-Ho Kim, Key-Sun Choi, ATR Through EM  Algorithm, http://nlplab.kaist.ac.kr/, 2003  15.", "acronym": "ATR", "label": "Automatic Term Recognition", "ID": "537"}, {"sentence": "Noisy-parallel texts are characterised  by heavy reformatting at the translation stage, in-  cluding large sections of uATRlated text and tex-  tual reordering.", "acronym": "ATR", "label": "atrans", "ID": "538"}, {"sentence": "Data Translator/Verifier -- DTV  The data translation can be seen as ATRlat ion  from linearized character strings to certain  organized structures.", "acronym": "ATR", "label": "atrans", "ID": "539"}, {"sentence": "If  we had wanted to take prefixes into consideration then  the as  attribute, containing ATRcription of the whole  word, could have been used instead.", "acronym": "ATR", "label": "atrans", "ID": "540"}, {"sentence": "Our present effort is to describe ATRformation ISD  such that ISD _C L ?", "acronym": "ATR", "label": "atrans", "ID": "541"}, {"sentence": "Since it does not exis~ i~:  this first parse, DM~COMMAND creates that concept 7, This  newly created concept is mi instance of 'rATRqi'amC~ m~d  its object slot is now filled not by genetic ~*CONCt?!~r  N~t in-  stead by '*HAVE.-A--PATN'~ specific to Ollr input so~t_e~i:eo '\\['h~',  final concept.-mfined concept is the result of the pacse?'o  5One firing to note here is that the concept 'aIIAVE-.AopAIN' at i~ acti-  vate, d by input \"*IIAVE-A-IPAIN\" is not part of the memo W Petwock tb~ Ne  I)M-COMMAND~S MT system eonnna", "acronym": "ATR", "label": "atrans", "ID": "542"}, {"sentence": "It is still  arbitrary  to be seen whether the proposed system can be used for an ATRforma-  tional grammar.", "acronym": "ATR", "label": "atrans", "ID": "543"}, {"sentence": "stitute University of Amsterdam, Kruislaan 403 NL-1098 SJ Amsterdam, The Netherlands erikt@science.uva.nl Sander Canisius, Antal van den Bosch, Toine Bogers ILK / Computational Linguistics and AI Tilburg University, P.O. Box 90153, NL-5000 LE Tilburg, The Netherlands {S.V.M.Canisius,Antal.vdnBosch, A.M.Bogers}@uvt.nl 1 Introduction This paper describes our approach to the CoNLL- 2005 shared task: SRL.", "acronym": "SRL", "label": "semantic role labelling", "ID": "544"}, {"sentence": "Collective SRL with markov logic.", "acronym": "SRL", "label": "semantic role labelling", "ID": "545"}, {"sentence": "We adapted the method to our needs and applied it for improving SRL output.", "acronym": "SRL", "label": "semantic role labelling", "ID": "546"}, {"sentence": "1998) to provide associations between verbs and semantic roles, that are then mapped onto the cur- rent instance, as shown by the systems competing in SRL competitions (Carreras and Marquez, 2004; Carreras and Marquez, 2005) and also (Gildea and Jurafsky, 2002; Pradhan et al, 2005; Shi and Mihalcea, 2005).", "acronym": "SRL", "label": "semantic role labelling", "ID": "547"}, {"sentence": "c?2005 Association for Computational Linguistics Applying spelling error correction techniques for improving SRL Erik Tjong Kim Sang Informatics Institute University of Amsterdam, Kruislaan 403 NL-1098 SJ Amsterdam, The Netherlands erikt@science.uva.nl Sander Canisius, Antal van den Bosch, Toine Bogers ILK / Computational Linguistics and AI Tilburg University, P.O. Box 90153, NL-5000 LE Tilburg, The Netherlands {S.V.M.Canisius,Antal.vdnBosch, A.M.Bogers}@uvt.nl 1 Introduction This pap", "acronym": "SRL", "label": "semantic role labelling", "ID": "548"}, {"sentence": "2013), and she and her colleagues have shown that an abduction engine using a knowledge base de- rived from these sources is competitive with the best of the statistical systems in recognizing tex- tual entailment and in SRL.", "acronym": "SRL", "label": "semantic role labelling", "ID": "549"}, {"sentence": "The proposed system consists of five mod- ules: syntactic dependency parser, predi- cate identifier, local SRL, global role sequence candidate generator, and role sequence selector.", "acronym": "SRL", "label": "semantic role labeler", "ID": "550"}, {"sentence": "2005) and Toutanova et al (2005), our local SRL needs to enhance the perfor- mance.", "acronym": "SRL", "label": "semantic role labeler", "ID": "551"}, {"sentence": "In this paper, we test this hypothesis by combining an incremental TAG parser with an incremental SRL in a discriminative framework.", "acronym": "SRL", "label": "semantic role labeler", "ID": "552"}, {"sentence": "Currently, our graph takes non-auxiliary verbs and a few eventive nouns as predicates provided by a SRL.", "acronym": "SRL", "label": "semantic role labeler", "ID": "553"}, {"sentence": "The Chi- nese propositional structure was predicted with the Chinese SRL described in (Xue, 2008), retrained on the OntoNotes v5.0 data.", "acronym": "SRL", "label": "semantic role labeler", "ID": "554"}, {"sentence": "We parsed all data using the dependency parser, the SRL, the named entity tagger, and the coreference resolution in ClearNLP (Choi and McCallum, 2013; Choi, 2012).", "acronym": "SRL", "label": "semantic role labeler", "ID": "555"}, {"sentence": "2.2 Semantic Role Labeling  SRL is the task of identifying  semantic roles such as Agent, Patient, Speaker,  or Topic, in a sentence.", "acronym": "SRL", "label": "Semantic role labeling", "ID": "556"}, {"sentence": "SRL contains two problems:  identification and labeling.", "acronym": "SRL", "label": "Semantic role labeling", "ID": "557"}, {"sentence": "SRL using maximum entropy model.", "acronym": "SRL", "label": "Semantic role labeling", "ID": "558"}, {"sentence": "SRL us- ing dependency trees.", "acronym": "SRL", "label": "Semantic role labeling", "ID": "559"}, {"sentence": "SRL using dependency trees.", "acronym": "SRL", "label": "Semantic role labeling", "ID": "560"}, {"sentence": "3.1.1 Semantic Role Constraints SRL generally includes the three subtasks: predicate identification; argument role la- beling; sense disambiguation.", "acronym": "SRL", "label": "Semantic role labeling", "ID": "561"}, {"sentence": "This setting includes a  broad range of structured prediction problems such as SRL, named  entity and relation recognition, co-reference resolution, dependency parsing and  semantic parsing.", "acronym": "SRL", "label": "semantic role labeling", "ID": "562"}, {"sentence": "2 Related Work  This section reviews previous works in both  sentiment detection and SRL.", "acronym": "SRL", "label": "semantic role labeling", "ID": "563"}, {"sentence": "1 Introduction The joint parsing of syntactic and semantic depen- dencies introduced by the shared task of CoNLL- 08 is more complicated than syntactic dependency parsing or SRL alone (Surdeanu et al, 2008).", "acronym": "SRL", "label": "semantic role labeling", "ID": "564"}, {"sentence": "Cal- ibrating features for SRL.", "acronym": "SRL", "label": "semantic role labeling", "ID": "565"}, {"sentence": "introduces related work both in sentiment  analysis and SRL.", "acronym": "SRL", "label": "semantic role labeling", "ID": "566"}, {"sentence": "These advantages and the availability of off-the-shelf solvers have led to a  large variety of NLP tasks being formulated within it, including SRL,  syntactic parsing, co-reference resolution, summarization, transliteration and joint  information extraction.", "acronym": "SRL", "label": "semantic role labeling", "ID": "567"}, {"sentence": "This paper is organized as follows: Section 2  briefly introduces related work both in sentiment  analysis and SRL.", "acronym": "SRL", "label": "semantic role labeling", "ID": "568"}, {"sentence": "2.3 Local SRL Prediate identification is followed by argument la- beling.", "acronym": "SRL", "label": "Semantic Role Labeling", "ID": "569"}, {"sentence": "2.2 SRL  Semantic role labeling is the task of identifying  semantic roles such as Agent, Patient, Speaker,  or Topic, in a sentence.", "acronym": "SRL", "label": "Semantic Role Labeling", "ID": "570"}, {"sentence": "3.2 SRL  To find a potential holder and topic of an opinion  word in a sentence, we first label semantic roles  in a sentence.", "acronym": "SRL", "label": "Semantic Role Labeling", "ID": "571"}, {"sentence": "SRL: an Introduc- tion to the Special Issue.", "acronym": "SRL", "label": "Semantic Role Labeling", "ID": "572"}, {"sentence": "In this paper, we propose a novel method that  employs SRL, a task of iden- tifying semantic roles given a sentence.", "acronym": "SRL", "label": "Semantic Role Labeling", "ID": "573"}, {"sentence": "Graph Aligment for Semi-Supervised SRL.", "acronym": "SRL", "label": "Semantic Role Labeling", "ID": "574"}, {"sentence": "c?2007 Association for Computational Linguistics Support SVM for Query-focused Summarization trained and evaluated on Pyramid data Maria Fuentes TALP Research Center Universitat Polite`cnica de Catalunya mfuentes@lsi.upc.edu Enrique Alfonseca Computer Science Departament Universidad Auto?noma de Madrid Enrique.Alfonseca@gmail.com Horacio Rodr??guez TALP Research Center Universitat Polite`cnica de Catalunya horacio@lsi.upc.edu Abstract This p", "acronym": "SVM", "label": "Vector Machines", "ID": "575"}, {"sentence": "Learning to Classify Text  Using Support SVM: Methods, Theory,  and Algorithms.", "acronym": "SVM", "label": "Vector Machines", "ID": "576"}, {"sentence": "Learning with Kernels: Support SVM,  Regularization, Optimization, and Beyond.", "acronym": "SVM", "label": "Vector Machines", "ID": "577"}, {"sentence": "Shallow Se- mantic Parsing Using Support SVM.", "acronym": "SVM", "label": "Vector Machines", "ID": "578"}, {"sentence": "In this  paper, we describe the use of annotated  datasets and Support SVM  to induce larger monolingual para- phrase corpora from a comparable cor- pus of news clusters found on the  World Wide Web.", "acronym": "SVM", "label": "Vector Machines", "ID": "579"}, {"sentence": "931  Support SVM for Paraphrase Identification   and Corpus Construction  Chris Brockett and William B. Dolan  Natural Language Processing Group  Microsoft Research  One Microsoft Way, Redmond, WA 98502, U.S.A.  {chrisbkt, billdol}@microsoft.com  Abstract  The lack of readily-available large cor- pora of aligned monolingual sentence  pairs is a major obstacle to the devel- opment of Statistical Mac", "acronym": "SVM", "label": "Vector Machines", "ID": "580"}, {"sentence": "SVM learning for interdependent and structured output spaces.", "acronym": "SVM", "label": "Support vector machine", "ID": "581"}, {"sentence": "For this, for each training question, our approach is to hire 7SVMs yielded worse results.", "acronym": "SVM", "label": "Support vector machine", "ID": "582"}, {"sentence": "SVM learning for interdepen- dent and structured output spaces.", "acronym": "SVM", "label": "Support vector machine", "ID": "583"}, {"sentence": "SVM active learning with applications to text classification.", "acronym": "SVM", "label": "Support vector machine", "ID": "584"}, {"sentence": "Svmlight: SVM.", "acronym": "SVM", "label": "Support vector machine", "ID": "585"}, {"sentence": "SVMs for mapping histories to parser actions (Kudo and Matsumoto, 2002).", "acronym": "SVM", "label": "Support vector machine", "ID": "586"}, {"sentence": "Learning to Classify Text  Using SVMes: Methods, Theory,  and Algorithms.", "acronym": "SVM", "label": "Support Vector Machin", "ID": "587"}, {"sentence": "931  SVMes for Paraphrase Identification   and Corpus Construction  Chris Brockett and William B. Dolan  Natural Language Processing Group  Microsoft Research  One Microsoft Way, Redmond, WA 98502, U.S.A.  {chrisbkt, billdol}@microsoft.com  Abstract  The lack of readily-available large cor- pora of aligned monolingual sentence  pairs is a major obstacle to the devel- opment of Statist", "acronym": "SVM", "label": "Support Vector Machin", "ID": "588"}, {"sentence": "Learning with Kernels: SVMes,  Regularization, Optimization, and Beyond.", "acronym": "SVM", "label": "Support Vector Machin", "ID": "589"}, {"sentence": "Shallow Se- mantic Parsing Using SVMes.", "acronym": "SVM", "label": "Support Vector Machin", "ID": "590"}, {"sentence": "c?2007 Association for Computational Linguistics SVMes for Query-focused Summarization trained and evaluated on Pyramid data Maria Fuentes TALP Research Center Universitat Polite`cnica de Catalunya mfuentes@lsi.upc.edu Enrique Alfonseca Computer Science Departament Universidad Auto?noma de Madrid Enrique.Alfonseca@gmail.com Horacio Rodr??guez TALP Research Center Universitat Polite`cnica de Catalunya horacio@lsi.upc.edu Abstrac", "acronym": "SVM", "label": "Support Vector Machin", "ID": "591"}, {"sentence": "In this  paper, we describe the use of annotated  datasets and SVMes  to induce larger monolingual para- phrase corpora from a comparable cor- pus of news clusters found on the  World Wide Web.", "acronym": "SVM", "label": "Support Vector Machin", "ID": "592"}, {"sentence": "A com- parative study of SVMs applied  to the supervised word sense disambiguation prob- lem in the medical domain.", "acronym": "SVM", "label": "support vector machine", "ID": "593"}, {"sentence": "While memory-based and margin-based learn- ing approaches such as SVMs are popularly applied to shift-reduce parsing, our work provides evidence that the maximum en- tropy model can achieve a comparative perfor- mance with the aid of a suitable feature set.", "acronym": "SVM", "label": "support vector machine", "ID": "594"}, {"sentence": "SVMs (Cortes and Vapnik, 1995) and voted perceptrons (Freund 3A spurious solution is a string that does not belong to the language under consideration.", "acronym": "SVM", "label": "support vector machine", "ID": "595"}, {"sentence": "Since our  final  goal  was to  build  a  comma checker, we would have to have chosen  the classifier that gave us the best precision, that  is, the SVM based one.", "acronym": "SVM", "label": "support vector machine", "ID": "596"}, {"sentence": "Chunking with SVM.", "acronym": "SVM", "label": "support vector machine", "ID": "597"}, {"sentence": "But the  recall of the SVM based classi?", "acronym": "SVM", "label": "support vector machine", "ID": "598"}, {"sentence": "The first two rows are devoted to the probabilities of particular kind of antecedent (pronouns, proper nouns, and CN generating a pronoun, holding ev- erything constant except the type of antecedent.", "acronym": "CN", "label": "common nouns)", "ID": "599"}, {"sentence": "His implementation is different from ours: he annotates the case of pro- nouns and common nouns, whereas we focus on ar- ticles and pronouns (articles are pronouns are more strongly marked for case than CN.", "acronym": "CN", "label": "common nouns)", "ID": "600"}, {"sentence": "Starting with it set of basic categoric.v, which for  tile purposes of this paper will be {txt, s, n, cn}  (for texts, sentences, names and CN,  we define it category to be either a basic category  or anything of one of the forms a / b or b \\ a,  where a and b are categories.", "acronym": "CN", "label": "common nouns)", "ID": "601"}, {"sentence": "A dependency path is the shortest path of lexico-syntactic elements, i.e. shortest lexico-syntactic pattern, connecting enti- ties (proper and CN in their parse- trees.", "acronym": "CN", "label": "common nouns)", "ID": "602"}, {"sentence": "1 Introduction Coreference resolution is used to determine which noun phrases (including pronouns, proper names, and CN refer to the same entities in documents.", "acronym": "CN", "label": "common nouns)", "ID": "603"}, {"sentence": "2.3 Reference Detection between Entities   We assume that the syntactic relationships between  entities (proper or CN in a text give us  information on their semantic reference status.", "acronym": "CN", "label": "common nouns)", "ID": "604"}, {"sentence": "The experiment has been carried out on 60 sentences  with 1201 different lectures, and formed by using seven  verbs (wr~te, eat, smell, corrode, buy, receive, assocza~e)  coupled with fifty CNs and two proper nouns.", "acronym": "CN", "label": "common oun", "ID": "605"}, {"sentence": "Translation  ( Selection of Head Noun )  ~t  n ( Dete =tion of Type of Noun )  I ~, Proper Noun  \\[ Determination f Semantic \\]  Attributes for Proper Nouns  \\[ Determination f Semantic )  Attributes for Common Nouns  ~t  _\\[ Translation by ALT-J/E )  Fig.2 Method of Auto .rm.atically Determining  Semantic Attributes  The procedures consist of determining the head  noun, noun type (proper and/or CN), proper  noun SAs (for proper nouns) and CN SAs (for  beth common and proper nouns).", "acronym": "CN", "label": "common oun", "ID": "606"}, {"sentence": "We have the following rules for  CNs:  (R2a) n(Vl,pizza(V1)) ~ \\[pizza\\]  (R2b) n(Vl,man(V1))--* \\[man\\]  (R2c) n(Vl,woman(V1))~ \\[woman\\].", "acronym": "CN", "label": "common oun", "ID": "607"}, {"sentence": "For  proper nouns, both CN SAs and proper noun  S/ks(beth more than one) are given.", "acronym": "CN", "label": "common oun", "ID": "608"}, {"sentence": "Common nouns in the semantic word dictionary are  given CN SAs (generally more than one).", "acronym": "CN", "label": "common oun", "ID": "609"}, {"sentence": "A potential subject is not a subject if it has no de-  terminer, unless it is a proper noun or it is a coordinated  CN.", "acronym": "CN", "label": "common oun", "ID": "610"}, {"sentence": "In Chinese language Processing, Chert and  Lee (1996) present various strategies to identify  and classify three types of proper nouns, i.e.,  CNs, Chinese transliterated  person names and organization names.", "acronym": "CN", "label": "Chinese person name", "ID": "611"}, {"sentence": "A CN is composed of  surname and name parts.", "acronym": "CN", "label": "Chinese person name", "ID": "612"}, {"sentence": "(4) special verbs  The same set of speech-act verbs used in  CNs are also used for  transliterated person names.", "acronym": "CN", "label": "Chinese person name", "ID": "613"}, {"sentence": "(2) titles  Titles used in CNs are  233  also applicable to transliterated person  names.", "acronym": "CN", "label": "Chinese person name", "ID": "614"}, {"sentence": "Compared with CNs, the  length of transliterated names is not restricted to  2 to 6 characters.", "acronym": "CN", "label": "Chinese person name", "ID": "615"}, {"sentence": "Thus the length of CNs range  from 2 to 6 characters.", "acronym": "CN", "label": "Chinese person name", "ID": "616"}, {"sentence": "cn CN *pe preposition co coordinating elem.", "acronym": "CN", "label": "common noun", "ID": "617"}, {"sentence": "Verbs whose subjects are CNs account for 57.8% of all verbs that have subjects (verbs with different types of subjects, most of which are personal pronouns, are not considered here, since these subjects are not part of the noun classifier).", "acronym": "CN", "label": "common noun", "ID": "618"}, {"sentence": "The last column of Table 5 shows that valid subject-verb structures account for 67.5% of all verbs whose subjects are CNs (51.7% are cases where the words are adjacent).", "acronym": "CN", "label": "common noun", "ID": "619"}, {"sentence": "If all CNs (e.g.  'person', 'share price', etc.)", "acronym": "CN", "label": "common noun", "ID": "620"}, {"sentence": "CN?", "acronym": "CN", "label": "common noun", "ID": "621"}, {"sentence": "For words which failed to be guessed by  tile guessing rules we applied the standard method  of classifying them as CNs (NN) if they  are not capitalised inside a sentence and proper  nouns (NP) otherwise.", "acronym": "CN", "label": "common noun", "ID": "622"}, {"sentence": "The representation for sentence 1 states that the first element of the 5-gram (-3; third word to the left of the adjective) is empty (because the second element is a phrase boundary marker), that the sec- ond element is a clause delimiter (conjunction that), the third one (-1; word preceding the adjective) is a definite determiner, and the fourth one (+1; word following the adjective) is a CN.", "acronym": "CN", "label": "common noun", "ID": "623"}, {"sentence": "It should be noted that the live SLU used the word CN, not made available in the challenge.", "acronym": "CN", "label": "confusion network", "ID": "624"}, {"sentence": "Nevertheless, despite this hand- icap, the best results were obtained from word- based tracking directly on the ASR output, rather than using the CN generated SLU output.", "acronym": "CN", "label": "confusion network", "ID": "625"}, {"sentence": "Moreover, this log-linear model includes a word penalty, a language model trained on the in- put hypotheses, a binary feature which penalizes word deletions in the CN and a pri- mary feature which marks the system which pro- vides the word order.", "acronym": "CN", "label": "confusion network", "ID": "626"}, {"sentence": "Recent several years have witnessed the rapid development of system combination methods based on CNs (e.g., (Rosti et al, 2007; He et al, 2008)), which show state-of-the- art performance in MT benchmarks.", "acronym": "CN", "label": "confusion network", "ID": "627"}, {"sentence": "automatic translations of the same sentence, we show how to com- bine them into a CN, whose various paths represent composite translations that could be considered in a subsequent rescoring step.", "acronym": "CN", "label": "confusion network", "ID": "628"}, {"sentence": "The word CN is known to provide stronger features than theN -best list for language understanding (Henderson et al.,", "acronym": "CN", "label": "confusion network", "ID": "629"}, {"sentence": "Note that a subject in subject-verb structures is always third person, since all subjects in subject-verb structures are CN; other subjects, including pronouns, are ex- cluded.", "acronym": "CN", "label": "common nouns", "ID": "630"}, {"sentence": "Verbs whose subjects are CN account for 57.8% of all verbs that have subjects (verbs with different types of subjects, most of which are personal pronouns, are not considered here, since these subjects are not part of the noun classifier).", "acronym": "CN", "label": "common nouns", "ID": "631"}, {"sentence": "The last column of Table 5 shows that valid subject-verb structures account for 67.5% of all verbs whose subjects are CN (51.7% are cases where the words are adjacent).", "acronym": "CN", "label": "common nouns", "ID": "632"}, {"sentence": "For words which failed to be guessed by  tile guessing rules we applied the standard method  of classifying them as CN (NN) if they  are not capitalised inside a sentence and proper  nouns (NP) otherwise.", "acronym": "CN", "label": "common nouns", "ID": "633"}, {"sentence": "If all CN (e.g.  'person', 'share price', etc.)", "acronym": "CN", "label": "common nouns", "ID": "634"}, {"sentence": "SIM2 Whether there are CN modifying the common nucleus noun or not in the problem report and aid message.", "acronym": "CN", "label": "common nouns", "ID": "635"}, {"sentence": "In particular, on the IT168 test set, the best accuracy is achieved by CN with GoogleTranslate, and on the 360BUY test set, the best result is achieved by CN with YahooTranslate.", "acronym": "CN", "label": "CoTrain", "ID": "636"}, {"sentence": "Tables 9 and 10 show the results of significance tests between CN and the baseline methods on the two test sets, respectively.", "acronym": "CN", "label": "CoTrain", "ID": "637"}, {"sentence": "As can be seen in Tables 1 through 8, no matter which machine translation service is used, the proposed co-training approach (CN) outperforms all baseline methods on the overall accuracy metric and most other metrics on the two test sets.", "acronym": "CN", "label": "CoTrain", "ID": "638"}, {"sentence": "Even the two component classifiers in CN can perform as well as or better than the baseline methods.", "acronym": "CN", "label": "CoTrain", "ID": "639"}, {"sentence": "ts of significance tests between CN and the baseline methods on the two test sets, respectively.", "acronym": "CN", "label": "CoTrain", "ID": "640"}, {"sentence": "Even the two component classifiers in CN can perform as well as or better than the basel", "acronym": "CN", "label": "CoTrain", "ID": "641"}, {"sentence": "The p-values for sign tests are presented; the performance difference between CN and a baseline method is", "acronym": "CN", "label": "CoTrain", "ID": "642"}, {"sentence": "Our model is based on the DLCN algo- rithm proposed by (Collins and Singer, 1999), which applies a co-training procedure to decision list classifiers for two independent sets of fea- tures.", "acronym": "CN", "label": "CoTrain", "ID": "643"}, {"sentence": "The parameter values for CN and SelfTrain are set as I = 80 and p = n = 5.", "acronym": "CN", "label": "CoTrain", "ID": "644"}, {"sentence": "The p-values for sign tests are presented; the performance difference between CN and a baseline method is statistically significant at a 95% level if the p-value is smaller than 0.05.", "acronym": "CN", "label": "CoTrain", "ID": "645"}, {"sentence": "154 Sixth SIGHAN Workshop on CN Processing  Lexicon Design Using a Paradigmatic Approach  Cristian Dumitrescu  Research Institute for Informatics  Alex.", "acronym": "CN", "label": "Chinese Language", "ID": "646"}, {"sentence": "In Pro- ceedings of the Second CIPS-SIGHAN Joint Conr- erence on CN Processing, pages 35?", "acronym": "CN", "label": "Chinese Language", "ID": "647"}, {"sentence": "The Third International Chi- nese Language Processing Bakeoff: Word Seg- mentation and Named Entity Recognition, In  Proceedings of SIGHAN5 the 3rd International  CN Processing Bakeoff at Col- ing/ACL 2006, July, Sydney, Australia, 108-117.", "acronym": "CN", "label": "Chinese Language", "ID": "648"}, {"sentence": "Top 5 left, right and bi-direction WB to- kens derived from the AS corpus  151 Sixth SIGHAN Workshop on CN Processing 2.2 WBT Frequency and WBT Probability  We first give the computation of WBT frequency,  then, the computation of WBT probability.", "acronym": "CN", "label": "Chinese Language", "ID": "649"}, {"sentence": "152 Sixth SIGHAN Workshop on CN Processing Table 2 is an example of applying  WBTM(2, ???,", "acronym": "CN", "label": "Chinese Language", "ID": "650"}, {"sentence": "153 Sixth SIGHAN Workshop on CN Processing                Coverage (%)     2-char 3-char 4-char > 4-char  CKIP  68% 24% 4% 4%  CityU  78% 19% 2% 1%   Total  75% 21% 3% 1%  Table 7.", "acronym": "CN", "label": "Chinese Language", "ID": "651"}, {"sentence": "Some of the patterns gen- erated were revised manually and inconsisten- 1CNs are essentially case frames which are triggered through a lexical item and its corresponding linguistic context (Riloff, 1993) cies were removed.", "acronym": "CN", "label": "Concept node", "ID": "652"}, {"sentence": "CNs take the following form: [concept_label] or [concept_label: role_indicator].", "acronym": "CN", "label": "Concept node", "ID": "653"}, {"sentence": "CNs are frame-like structures that are triggered by relevant words or phrases an d filled by local syntactic constituents using semantic constraints and preferences .", "acronym": "CN", "label": "Concept node", "ID": "654"}, {"sentence": "CNs refer  to words and features in Hownet.", "acronym": "CN", "label": "Concept node", "ID": "655"}, {"sentence": "CNs included in the summary graph are shaded.", "acronym": "CN", "label": "Concept node", "ID": "656"}, {"sentence": "CNs and Textref nodes are linked by an event with the internal action words_used.", "acronym": "CN", "label": "Concept node", "ID": "657"}, {"sentence": "(v, svo) (subj,nil) know air pilots bring (co mp l,n il) AGENT OB JE CT According to the metric, the networks match and the pairs (watch, wWEAman) and (know, air pi- lots) match, so the semantic relation for the pair (know, air pilots) is proposed as a possible relation for pair (watch, wWEAman) .", "acronym": "WEA", "label": "eather", "ID": "658"}, {"sentence": "WWEA forecasts \u0001 System by Korea University and Sangmyung University (Chung et al 2004) 8.", "acronym": "WEA", "label": "eather", "ID": "659"}, {"sentence": "For example, for the sentence: WWEAmen watch the clouds day and night.", "acronym": "WEA", "label": "eather", "ID": "660"}, {"sentence": "A German Sign Language corpus of the domain wWEA report.", "acronym": "WEA", "label": "eather", "ID": "661"}, {"sentence": "the system builds the following network centered on the predicate watch2: cloudwWEAman watch (v, svo) (subj,nil) (c om pl ,n il) day and night (co mp l,n il) The system locates among previously stored net- works those centered around verbs3.", "acronym": "WEA", "label": "eather", "ID": "662"}, {"sentence": "Nevertheless, they are key to various NLP applications, including those benefiting from deep natural language understanding (e.g., textual inference (Bobrow et al, 2007)), generation of well- formed output (e.g., natural language wWEA alert systems (Lareau and Wanner, 2007)) or both (as in machine translation (Oepen et al, 2007)).", "acronym": "WEA", "label": "eather", "ID": "663"}, {"sentence": "In general,/Generally speaking,/Generally,  <clause>  Term.example  ... matters of process, such as exposing the WEAs  industry's influence ...  The EXAMPLE discourse relation discussed above links  entire relations: I like dogs.", "acronym": "WEA", "label": "weapon", "ID": "664"}, {"sentence": "\\[Rather,\\] We should  limit our involvement in defense and WEAry to  matters of process ...  The writer negates a proposition and, furnishing a  correction, asserts a sharply contrasting one.", "acronym": "WEA", "label": "weapon", "ID": "665"}, {"sentence": "articles required in one language but optional in the other (e.g., English Cars use gas and Portuguese Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English WEAs of mass destruction and German Massenvernichtungswaffen), and expres- sions translated indirectly.", "acronym": "WEA", "label": "weapon", "ID": "666"}, {"sentence": "We  should limit our involvement in defense and WEAry  to matters of process, such as exposing the WEAs  industry's influence of the political process.", "acronym": "WEA", "label": "weapon", "ID": "667"}, {"sentence": "Rather, we should limit out involvement in defense and  WEAry to matters of process, for instance exposing the  WEAs industry's influence of the political process.", "acronym": "WEA", "label": "weapon", "ID": "668"}, {"sentence": "As shown in Figure 2, the MO may oc- cur in different grammatical relations with respect to the verb (subject, direct object, object of a prepo- sition), but they each have an ARG1 semantic role label in PropBank.3 Furthermore, only one of the MO needs to be specified, as in Exam- ple 3 where the second matched object (presumably the company?s prices) is unstated.", "acronym": "MO", "label": "matched objects", "ID": "669"}, {"sentence": "The tem-  plate pattern search recognizes relationships  between MO in the defined pat-  tern a.s well a.s recognizing the.", "acronym": "MO", "label": "matched objects", "ID": "670"}, {"sentence": "The MO constitute here a FL expres- sion instead of a video sequence track.", "acronym": "MO", "label": "matched objects", "ID": "671"}, {"sentence": "ions of how the arguments and  modifiers of the light verbs and their true  predicates are annotated are mentioned in Table  1, but notably, none of the examples in it  currently include the annotation of arguments   Pass 1: Pass 2: Pass 3:   Light Verb Annotation True Predicate Annotation Merge of Pass1&2 Annotation  Relation Light verb True predicate Light verb + true predicate  Arguments  and  MOs  - Predicating expression is  annotated with ARG-PRX  - Arguments and modifiers of  the light verb are annotated  - Arguments and modifiers of  the true predicate are annotated  - Arguments and modifiers  found in the two passes are  merged, preferably  automatically.", "acronym": "MO", "label": "Modifier", "ID": "672"}, {"sentence": "MO head and modifier 6The tags outside of a given XP are approximated using the marginally most likely tags given the parse.", "acronym": "MO", "label": "Modifier", "ID": "673"}, {"sentence": "Pass 1: Pass 2:  Pass 3:   Light Verb Identification LVC Annotation Deterministic relation merge  Relation Light verb True predicate Light verb + true predicate  Arguments  & MOs  - Predicating expression is  annotated with ARG-PRX  - Arguments and modifiers of  the LVCs are annotated  - Arguments and modifiers  are taken from Pass 2  Frame File <no Frame File needed> LVC?s Frame File LVC?s Frame File    Example  ?", "acronym": "MO", "label": "Modifier", "ID": "674"}, {"sentence": "on (one) roof-of  house    3.2 Post MOs    Nouns in prepositional phrases can expand  with post modifiers while nouns in our structure  cannot.", "acronym": "MO", "label": "Modifier", "ID": "675"}, {"sentence": "Subsequent work (McDonald & Pereira, 2006; Carreras, 2007; Koo & Collins, 2010) looked at allowing features to access more complex, higher-order relationships, including tri- gram and 4-gram relationships, e.g., all features apart from MO, below.", "acronym": "MO", "label": "Modifier", "ID": "676"}, {"sentence": "wing features to access more complex, higher-order relationships, including tri- gram and 4-gram relationships, e.g., all features apart from MO, below.", "acronym": "MO", "label": "Modifier", "ID": "677"}, {"sentence": "MOs such as species designators were excluded from annota- tions whenever possible.", "acronym": "MO", "label": "Modifier", "ID": "678"}, {"sentence": "As shown in Figure 2, the MOs may oc- cur in different grammatical relations with respect to the verb (subject, direct object, object of a prepo- sition), but they each have an ARG1 semantic role label in PropBank.3 Furthermore, only one of the MOs needs to be specified, as in Exam- ple 3 where the second MO (presumably the company?s prices) is unstated.", "acronym": "MO", "label": "matched object", "ID": "679"}, {"sentence": "The tem-  plate pattern search recognizes relationships  between MOs in the defined pat-  tern a.s well a.s recognizing the.", "acronym": "MO", "label": "matched object", "ID": "680"}, {"sentence": "Max- imum likelihood models can be estimated from mil- lions of sentences of bitext, but optimize a mis- MOive, predicting events observed in word aligned bitext instead of optimizing translation quality.", "acronym": "MO", "label": "matched object", "ID": "681"}, {"sentence": "The MOs constitute here a FL expres- sion instead of a video sequence track.", "acronym": "MO", "label": "matched object", "ID": "682"}, {"sentence": "FFin~l OBJ?", "acronym": "OBJ", "label": "Object", "ID": "683"}, {"sentence": "i :  \"~)   Fin de la  f igure  ~5  Macro  : ~rn   Initial OBJ?", "acronym": "OBJ", "label": "Object", "ID": "684"}, {"sentence": "OBJ recognition from local scale-invariant features.", "acronym": "OBJ", "label": "Object", "ID": "685"}, {"sentence": "Theory and Prac-  tice of OBJ Systems, 2(1):31-41.", "acronym": "OBJ", "label": "Object", "ID": "686"}, {"sentence": "OBJs in cluster 1, corresponding to binary ad- jectives, have high values for most of the features containing a preposition after the adjective (observe +1pe, ?", "acronym": "OBJ", "label": "Object", "ID": "687"}, {"sentence": "OBJs in cluster 0 (unary adjectives), symmetrically, have low values for these features, and high values for the default adjective position in Catalan (directly postnominal: -1cn).", "acronym": "OBJ", "label": "Object", "ID": "688"}, {"sentence": "Here we would compare the degree to which  each possible candidate antecedent (A Japanese  company, television picture tubes, Japan, TV   sets, and Malaysia in this example) could serve  as the direct OBJ of \"export\".", "acronym": "OBJ", "label": "object", "ID": "689"}, {"sentence": "We specifically extract the character reference CH either from the dependency relation nsubj, which links a speech verb SV with a CH that is the syntactic subject of a clause, or from the dependency relation dobj, which links a SV with a CH that is the direct OBJ of the speech verb, across a conjunct (e.g., and).", "acronym": "OBJ", "label": "object", "ID": "690"}, {"sentence": "A canonical example of selectional  restriction is that of the verb \"eat\", which se-  lects food as its direct OBJ.", "acronym": "OBJ", "label": "object", "ID": "691"}, {"sentence": "Characters in chil- dren?s stories can either be human or non-human entities, i.e., animals and non-living OBJs, ex- hibiting anthropomorphic traits.", "acronym": "OBJ", "label": "object", "ID": "692"}, {"sentence": "The passive char- acters were identified via the following relations extracted by dependency parsing: nsubjpass (passive nominal subject) and pobj (OBJ of a preposition).", "acronym": "OBJ", "label": "object", "ID": "693"}, {"sentence": "To that end, we  devised a more OBJive test, useful only for  scoring the subset of referents that are names  of people.", "acronym": "OBJ", "label": "object", "ID": "694"}, {"sentence": "would compare the degree to which  each possible candidate antecedent (A Japanese  company, television picture tubes, Japan, TV   sets, and Malaysia in this example) could serve  as the direct OBJ of \"export\".", "acronym": "OBJ", "label": "object", "ID": "695"}, {"sentence": "4 OBJ Functions for Parsing A split PCFG is a grammar G over symbols of the form X-k where X is an evaluation symbol (such as NP) and k is some indicator of a subcategory, such as a parent annotation.", "acronym": "OBJ", "label": "Objective", "ID": "696"}, {"sentence": "SEMI-AUTOMATED FORCES (SAF) PROJECT 8  C/1 TB is to the east and its mission is to attack  OBJ GAMMA from ES646905 to ES758911  at 141423 Apr. All TB is to the south.", "acronym": "OBJ", "label": "Objective", "ID": "697"}, {"sentence": "The content of Figure 1 can be reconstructed  straightforwardly asa category structure subject o a set  of L c constraints (for a closely related analysis of this  10  {~ Animate  Question _ _  -- Subjective  Case OBJ  Reflex ve  Possessive Possessive-Determ ner  _ I First  Personal ~_ .P__~ Second _ _ I Feminine  Ingu la r - -  J Neuter  f \\[ | Plural  Demonstrative - - l~  Near  / Far  Figure 1: Systemic Network for English Pronouns  example, developed independently, see Mellish (1986).", "acronym": "OBJ", "label": "Objective", "ID": "698"}, {"sentence": "These include, on the one hand, inner participants or arguments,  such as Actor, Addressee, OBJ, and, on the other hand, free modifications, uch  as Locative, Means, Manner, Cause, several temporal and directional modifications,  those of Condition, Regard, Accompaniment, etc.", "acronym": "OBJ", "label": "Objective", "ID": "699"}, {"sentence": "I. OBJs of the Parser  We designed a new syntactic analy-  sis system (a parser) with the follow-  ing objectives.", "acronym": "OBJ", "label": "Objective", "ID": "700"}, {"sentence": "While these three methods yield 409 OBJ P R F1 EX BEST DERIVATION Viterbi Derivation 89.6 89.4 89.5 37.4 RERANKING Random 87.6 87.7 87.7 16.4 Precision (sampled) 91.1 88.1 89.6 21.4 Recall (sampled) 88.2 91.3 89.7 21.5 F1 (sampled) 90.2 89.3 89.8 27.2 Exact (sampled) 89.5 89.5 89.5 25.8 Exact (non-sampled) 90.8 90.8 90.8 41.7 Exact/F1 (oracle) 95.3 94.4 95.0 63.9 DYNAMIC PROGRAMMING VARIATIONAL 90.7 90.9 90.8 41.4 MAX-RULE-SUM", "acronym": "OBJ", "label": "Objective", "ID": "701"}, {"sentence": "4.2 Collaborative Filtering CF techniques make prediction using information from similar users.", "acronym": "CF", "label": "Collaborative filtering", "ID": "702"}, {"sentence": "CF is the technique of using  peer opinions to predict the interests of others.", "acronym": "CF", "label": "Collaborative filtering", "ID": "703"}, {"sentence": "CF via gaussian probabilistic latent semantic analysis.", "acronym": "CF", "label": "Collaborative filtering", "ID": "704"}, {"sentence": "0.14  0.16  0.18  0.2  0.22  1  10  100 MA P  10 Number of Groups Ubuntu Forum Latent Group SVM  0.15  0.2  0.25  0.3  0.35  1  10  100 MA P  10 Number of Groups Wow Gaming Latent Group SVM  0.2  0.3  0.4  0.5  0.6  1  10  100 MA P  10 Number of Groups Fitness Forum Latent Group SVM Figure 4: CF results: MAP@10 vs. user group number.", "acronym": "CF", "label": "Collaborative filtering", "ID": "705"}, {"sentence": "CF, on the other hand, uses co-occurrence information from a collection of users for recommendation.", "acronym": "CF", "label": "Collaborative filtering", "ID": "706"}, {"sentence": "CF systems can be ei- ther model based or memory based (Breese et al 1998).", "acronym": "CF", "label": "Collaborative filtering", "ID": "707"}, {"sentence": "when estimating a higher-order gram, instead of using the raw occurrence count, only a portion is used and the remainder is computed using a lower- order model in which one of the CFs 2 Note that factors at indices 0,?1, . . . ,?(", "acronym": "CF", "label": "context factor", "ID": "708"}, {"sentence": "The proposed  model for reference resolution elaborates on Alshawi's (1987) notions of CFs and  salience and integrates both linguistic and perceptual context effects.", "acronym": "CF", "label": "context factor", "ID": "709"}, {"sentence": "The CF is also recognized by Dybkj?r and Bernsen (2000).", "acronym": "CF", "label": "context factor", "ID": "710"}, {"sentence": "Finally, new termhood esti- mations (NC-values) are calculated as a linear  combination of the C-values and CFs.", "acronym": "CF", "label": "context factor", "ID": "711"}, {"sentence": "Subse- quently, CFs are assigned to candidate  terms according to their co-occurrence with top- ranked context words.", "acronym": "CF", "label": "context factor", "ID": "712"}, {"sentence": "Additionally, we remove low CF alignment links from the word alignment of a bilingual training corpus, which increases the alignment F-score, improves Chinese-English and Arabic-English translation quality and sig- nificantly reduces the phrase translation table size.", "acronym": "CF", "label": "confidence", "ID": "713"}, {"sentence": "c?2009 ACL and AFNLP Confidence Measure for Word Alignment Fei Huang IBM T.J.Watson Research Center Yorktown Heights, NY 10598, USA huangfe@us.ibm.com Abstract In this paper we present a CF mea- sure for word alignment based on the posterior probability of alignment links.", "acronym": "CF", "label": "confidence", "ID": "714"}, {"sentence": "ter-judge (J1, J2, J3), and with GS J1 J2 J3 %agr \u000b %agr \u000b %agr \u000b J2 0.83 0.74 J3 0.88 0.80 0.80 0.68 GS 0.93 0.89 0.83 0.74 0.92 0.87 Table 3: Agreement for the basic/event/object pa- rameter: inter-judge (J1, J2, J3), and with GS As can be seen, the agreement among judges is remarkably high for a lexical semantics task: All but one values of the kappa statistics are above 0.6 (+/-0.13 for a 95% CF interval).", "acronym": "CF", "label": "confidence", "ID": "715"}, {"sentence": "nt Fei Huang IBM T.J.Watson Research Center Yorktown Heights, NY 10598, USA huangfe@us.ibm.com Abstract In this paper we present a CF mea- sure for word alignment based on the posterior probability of alignment links.", "acronym": "CF", "label": "confidence", "ID": "716"}, {"sentence": "We illustrate the correlation between the align- ment CF measure and the alignment qual- ity on the sentence level, and present several ap- proaches to improve alignment accuracy based on the proposed CF measure: sentence align- ment selection, alignment link", "acronym": "CF", "label": "confidence", "ID": "717"}, {"sentence": "In this paper we introduce a CF mea- sure for word alignment, which is robust to extra or missing words in the bilingual sentence pairs, as well as word alignment errors.", "acronym": "CF", "label": "confidence", "ID": "718"}, {"sentence": "Based on these mea- sures, we improve the alignment qual- ity by selecting high CF sentence alignments and alignment links from mul- tiple word alignments of the same sen- tence pair.", "acronym": "CF", "label": "confidence", "ID": "719"}, {"sentence": "We propose a sentence alignment CF measure based on the alignment?s posterior probability, and ex- tend it to the alignment link CF measure.", "acronym": "CF", "label": "confidence", "ID": "720"}, {"sentence": "We illustrate the correlation between the align- ment CF measure and the alignment qu", "acronym": "CF", "label": "confidence", "ID": "721"}, {"sentence": "244   Figure 1: A NE type task in the CF interface   After these initial tasks, inter-annotator agree- ment was estimated at 91%, which can be taken  to be a reasonable upper bound for our automat- ed system.", "acronym": "CF", "label": "Crowdflower", "ID": "722"}, {"sentence": "Merging the type labels produced by Turkers  (with the help of CF) is an interesting  problem in itself.", "acronym": "CF", "label": "Crowdflower", "ID": "723"}, {"sentence": "CF is a  crowdsourcing service built on top of AMT that  associates a trust level with workers based on  their performance on gold data and uses these  trust levels to determine the correctness of work- er responses.", "acronym": "CF", "label": "Crowdflower", "ID": "724"}, {"sentence": "Instead of putting these tasks directly onto  AMT, we chose to leverage CF for its  added quality control.", "acronym": "CF", "label": "Crowdflower", "ID": "725"}, {"sentence": "0.4 0.5 0.6 0.7  0  2  4  6 Number of human annotations x 1,000 Ac cu ra cy algorithm MomResp LogResp Majority CF Annotations Figure 5: Inferred label accuracy on annotations gathered from CrowdFlower over a subset of 1000 instances of the 20 Newsgroups dataset.", "acronym": "CF", "label": "Crowdflower", "ID": "726"}, {"sentence": "4.2 Human Judgements of Image Relevance Human judgements of the suitability of each im- age were obtained using an online crowdsourcing platform, CF7.", "acronym": "CF", "label": "Crowdflower", "ID": "727"}, {"sentence": "CF: avoid rare words where possible; ?", "acronym": "CF", "label": "corpus frequency", "ID": "728"}, {"sentence": "As we expected, the CF baseline does very well.", "acronym": "CF", "label": "corpus frequency", "ID": "729"}, {"sentence": "Note that the \\[S NP VP\\]  fragment with the unspecified NUM value is produced  for both sentences and thus its CF is 2.", "acronym": "CF", "label": "corpus frequency", "ID": "730"}, {"sentence": "Then we multiply these measures by the  CF of this particular word and av-  erage them.", "acronym": "CF", "label": "corpus frequency", "ID": "731"}, {"sentence": "and the scaled CF are interpolated to get a smoothed model tpos.", "acronym": "CF", "label": "corpus frequency", "ID": "732"}, {"sentence": "Its designers relied mainly on linguistic evidence, such as CF, rather than psycholinguistic motivations.", "acronym": "CF", "label": "corpus frequency", "ID": "733"}, {"sentence": "into single, nonhierarchical argumentative moves (i.e., rhetorically coherent pieces of text, which perform the same CF).", "acronym": "CF", "label": "communicative function", "ID": "734"}, {"sentence": "Some subordinate it entirely to structure, some  attempt to combine structure and function felici-  tously, others place CF clearly  in the foreground.", "acronym": "CF", "label": "communicative function", "ID": "735"}, {"sentence": "June 21-24, 1994  produce texts whose CF has to be interpreted in terms of the concrete  situation in which they were produced.", "acronym": "CF", "label": "communicative function", "ID": "736"}, {"sentence": "All studies discussed so far are only concerned with sequences of CFs, and disregard the semantic content of dialogue acts.", "acronym": "CF", "label": "communicative function", "ID": "737"}, {"sentence": "For the most part, however, it is assumed that  there is a stereotyped set of functions involved  in performing a global CF  in a restricted omain.", "acronym": "CF", "label": "communicative function", "ID": "738"}, {"sentence": "Acces- sed online on 16 March 2012 from: http://www- bcf.usc.edu/~billmann/diversity/DDivers-site.htm   Misu T, Ohtake K, Hori C, Kashioka H, Nakamura S  (2009) Annotating CF and se- mantic content in dialogue act for construction of  consulting dialogue systems.", "acronym": "CF", "label": "communicative function", "ID": "739"}, {"sentence": "Heuristic answers are expressed in terms of  CFs which, as in the MYCIN system  (Shortliffe 1976), take their values in the range  (-1,+ 15: \"-I\" expresses absolute disbelief; \"0\"  expresses complete uncertainty; \"1\" expresses ab-  solute befief.", "acronym": "CF", "label": "certainty factor", "ID": "740"}, {"sentence": "checking for a PARTOF relation be-  tween a head and a \"with\" complement:  I. if the head is not a noun, the relation  doesn't hold (CF = - 15;  2.", "acronym": "CF", "label": "certainty factor", "ID": "741"}, {"sentence": "follows:  HI- for checking for an INSTRUMENT relation  between a head and a \"with\" complement:  I. if the head is not a verb, the relation  doesn't hold (CF = -15;  2.", "acronym": "CF", "label": "certainty factor", "ID": "742"}, {"sentence": "if some \"part-of pattern\" (see below)  exists in the dictionary definition of the  complement, and if this pattern points  to a defining term that can be linked  with the head, then the PARTOF re-  lation probably holds (CF  = 0.7);  3.", "acronym": "CF", "label": "certainty factor", "ID": "743"}, {"sentence": "The two main heuristics that are used to  evaluate the plausability of (la) against (Ib) can  be described in English as follows:  HI- for checking for an INSTRUMENT relation  between a head and a \"with\" complement:  I. if the head is not a verb, the relation  doesn't hold (CF = -15;  2.", "acronym": "CF", "label": "certainty factor", "ID": "744"}, {"sentence": "n  of the complement, and if this pattern  points to a defining term that can be  linked with the head, then the relation  probably holds (CF = 0.7);  3.", "acronym": "CF", "label": "certainty factor", "ID": "745"}, {"sentence": "H2- for  Each CF refers to the specific  proposition (or goal) to which the heuristic is  applied.", "acronym": "CF", "label": "certainty factor", "ID": "746"}, {"sentence": "hold (CF = - 15;  2.", "acronym": "CF", "label": "certainty factor", "ID": "747"}, {"sentence": "Thus, if clause 3 of heuristic 112 is used  when applied to the proposition (lb), the result-  ing CF -0.3 will indicate a relatively  moderate disbelief in this proposition, stemming  from the fact that the system has not been able  to find any positive evidence in the dictionary to  s", "acronym": "CF", "label": "certainty factor", "ID": "748"}, {"sentence": "Thus, if clause 3 of heuristic 112 is used  when applied to the proposition (lb), the result-  ing CF -0.3 will indicate a relatively  moderate", "acronym": "CF", "label": "certainty factor", "ID": "749"}, {"sentence": "if some \"instrument pattern\" (see be-  low) exists in the dictionary def'mition  of the complement, and if this pattern  points to a defining term that can be  linked with the head, then the relation  probably holds (CF = 0.7);  3.", "acronym": "CF", "label": "certainty factor", "ID": "750"}, {"sentence": "CDC research has re- cently become more popular due to the increasing interests in the web person search task (Artiles et al, 2007).", "acronym": "CDC", "label": "Cross document coreference", "ID": "751"}, {"sentence": "3.6 Cross Document Coreference Resolution CDC resolution is performed via the phylogenetic entity clustering model of Andrews et al (2014).", "acronym": "CDC", "label": "Cross document coreference", "ID": "752"}, {"sentence": "CDC, on the other hand, is a more challenging task because these linguistics cues and sentence structures no longer apply, given the wide variety of context and styles in different documents.", "acronym": "CDC", "label": "Cross document coreference", "ID": "753"}, {"sentence": "Purity improves because of the removal of noise entities, though at the sacrifice of inverse purity and the Table 2: CDC performance on subsets (I. Purity denotes inverse purity).", "acronym": "CDC", "label": "Cross document coreference", "ID": "754"}, {"sentence": "CDC Detection Ran Levy Yonatan Bilu Daniel Hershcovich Ehud Aharoni Noam Slonim IBM Haifa Research Lab / Mount Carmel, Haifa, 31905, Israel {ranl,yonatanb,danielh,aehud,noams}@il.ibm.com Abstract While discussing a concrete controversial topic, most humans will find it challenging to swiftly raise a diverse set of convincing and relevant claims that should set the basis of thei", "acronym": "CDC", "label": "Context Dependent Claim", "ID": "755"}, {"sentence": "Besides the LMl trained in riddle style, we also train a general lan- guage model with the web documents.", "acronym": "LM", "label": "language mode", "ID": "756"}, {"sentence": "For this technique, we represent the  snippets in the form of a probability distribution of  words, creating a so-called entity LMl (Allan  and Raghavan, 2002).", "acronym": "LM", "label": "language mode", "ID": "757"}, {"sentence": "These techniques were first discussed by Brill et al (2001), who observed that, as the size of the text corpus increases, it becomes more likely that the answer to a specific question can be found with data-intensive methods that do not require a complex LMl.", "acronym": "LM", "label": "language mode", "ID": "758"}, {"sentence": "Im- proving word alignment with LMl based confidence scores.", "acronym": "LM", "label": "language mode", "ID": "759"}, {"sentence": "The data for training LMl in riddle style include two parts: One is the corpus of rid- dles mentioned above, and the other is a corpus of Chinese poem and Chinese couplets because of the similar language style.", "acronym": "LM", "label": "language mode", "ID": "760"}, {"sentence": "In providing the fundamental organization of the gram- 983 mar, to the extent that that organization is consistent with the LMled, these types significantly ease the path to creating a working grammar.", "acronym": "LM", "label": "language mode", "ID": "761"}, {"sentence": "Some of the factors effecting this decision are object 4See (Gorniak and Roy, 2004) for further discussion on the use of spatial extrema of the scene and groups of objects in the scene as landmarks Figure 5: LM salience salience and the functional relationships between objects.", "acronym": "LM", "label": "Landmark", "ID": "762"}, {"sentence": "spatialRelation (UsedLM) ?", "acronym": "LM", "label": "Landmark", "ID": "763"}, {"sentence": "name (LM) ?", "acronym": "LM", "label": "Landmark", "ID": "764"}, {"sentence": "LMs in OpenLS: A data structure for cognitive ergonomic route directions.", "acronym": "LM", "label": "Landmark", "ID": "765"}, {"sentence": "LM Locality: Binary feature indicat- ing if l?", "acronym": "LM", "label": "Landmark", "ID": "766"}, {"sentence": "3.2 LMs and Descriptions If we want to use a locative expression, we must choose another object in the scene to function as landmark.", "acronym": "LM", "label": "Landmark", "ID": "767"}, {"sentence": "Statistical LM Using Leaving-One-Out\", in S. Young & G. Bloothooft (eds.),", "acronym": "LM", "label": "Language Modeling", "ID": "768"}, {"sentence": "Combining Semantic and Syntactic Structure for LM\", Proceed- ings ICSLP-2000, Beijing, China.", "acronym": "LM", "label": "Language Modeling", "ID": "769"}, {"sentence": "Toward a Unified Approach to Statistical LM for Chinese.", "acronym": "LM", "label": "Language Modeling", "ID": "770"}, {"sentence": "Structured  LM.", "acronym": "LM", "label": "Language Modeling", "ID": "771"}, {"sentence": "\"Principles of Lexical LM for  Speech Recognition\".", "acronym": "LM", "label": "Language Modeling", "ID": "772"}, {"sentence": "A LM Approach for Georeferenc- ing Textual Documents.", "acronym": "LM", "label": "Language Modeling", "ID": "773"}, {"sentence": "Statistical LMing Using Leaving-One-Out\", in S. Young & G. Bloothooft (eds.),", "acronym": "LM", "label": "Language Model", "ID": "774"}, {"sentence": "Combining Semantic and Syntactic Structure for LMing\", Proceed- ings ICSLP-2000, Beijing, China.", "acronym": "LM", "label": "Language Model", "ID": "775"}, {"sentence": "Estima.tion of Probabilities fi:om Sparse Data for the LM Com-  ponent of a. Speech Recogniser.", "acronym": "LM", "label": "Language Model", "ID": "776"}, {"sentence": "Structured  LMing.", "acronym": "LM", "label": "Language Model", "ID": "777"}, {"sentence": "A LMing Approach for Georeferenc- ing Textual Documents.", "acronym": "LM", "label": "Language Model", "ID": "778"}, {"sentence": "3.2.2 LM For our language model, we use a Kneser-Ney smoothed trigram model learned from a version of the British National Corpus modified to use Americanized spellings (Chen and Goodman, 1996; Burnard, 1995).", "acronym": "LM", "label": "Language Model", "ID": "779"}, {"sentence": "In Section 2, we detail how the task of sequential labeling is formalized in terms of LM classifi- cation, and explain the Viterbi algorithm required for prediction.", "acronym": "LM", "label": "linear", "ID": "780"}, {"sentence": "LMity  The reader may be concerned that tim regularity consmtint  in\\]poses tmdue restrictions (51' LMity (m the candidate  forms, and, in doing so, vitiates the phonological advan-  tages of non-LM epresentations.", "acronym": "LM", "label": "linear", "ID": "781"}, {"sentence": "We will first explain LM classification, and then apply a Markov assumption to the classification formal- ism.", "acronym": "LM", "label": "linear", "ID": "782"}, {"sentence": "We next present several algorithms for optimizing the weight vector in a LM classi- fier in Section 3.", "acronym": "LM", "label": "linear", "ID": "783"}, {"sentence": "For a cleaner presentation, we detail the gradient derivation in Appendix A. Given that the optimization problem is not convex, initializing the model from a good projection matrix often helps re- duce training time and may lead to convergence to a better LM.", "acronym": "LM", "label": "local minimum", "ID": "784"}, {"sentence": "Thus, we could land in a non-optimal LM using online learning.", "acronym": "LM", "label": "local minimum", "ID": "785"}, {"sentence": "Unlike correlative models (Amari S. and Maginu  K., 1988), neither distortion of pattern nor pseudo  LM solutions arise from memorizing  other patterns.", "acronym": "LM", "label": "local minimum", "ID": "786"}, {"sentence": "Then, in a manner quite similar to (Hearst, 1994), the algorithm determines for every LM mi how sharp of a change there is in the lexical cohesion function.", "acronym": "LM", "label": "local minimum", "ID": "787"}, {"sentence": "However, these for- mulations are often non-convex and thus suffer from LM.", "acronym": "LM", "label": "local minimum", "ID": "788"}, {"sentence": "The phrase acquisition method is  a greedy algorithm that performs local optimization  based on an iterative process which converges to a  LM of PP(T) .", "acronym": "LM", "label": "local minimum", "ID": "789"}, {"sentence": "Besides the LM trained in riddle style, we also train a general lan- guage model with the web documents.", "acronym": "LM", "label": "language model", "ID": "790"}, {"sentence": "For this technique, we represent the  snippets in the form of a probability distribution of  words, creating a so-called entity LM (Allan  and Raghavan, 2002).", "acronym": "LM", "label": "language model", "ID": "791"}, {"sentence": "These techniques were first discussed by Brill et al (2001), who observed that, as the size of the text corpus increases, it becomes more likely that the answer to a specific question can be found with data-intensive methods that do not require a complex LM.", "acronym": "LM", "label": "language model", "ID": "792"}, {"sentence": "Im- proving word alignment with LM based confidence scores.", "acronym": "LM", "label": "language model", "ID": "793"}, {"sentence": "The data for training LM in riddle style include two parts: One is the corpus of rid- dles mentioned above, and the other is a corpus of Chinese poem and Chinese couplets because of the similar language style.", "acronym": "LM", "label": "language model", "ID": "794"}, {"sentence": "In providing the fundamental organization of the gram- 983 mar, to the extent that that organization is consistent with the LMed, these types significantly ease the path to creating a working grammar.", "acronym": "LM", "label": "language model", "ID": "795"}, {"sentence": "4 LM 4.1 The Hierarchical Dirichlet Process Model The results of our unigram experiments suggested that word segmentation could be improved by taking into account dependencies between words.", "acronym": "LM", "label": "Bigram Model", "ID": "796"}, {"sentence": "LM This model, inspired by the ap- proach of Collins et al (1999) for parsing the Prague Dependency Treebank, builds on Collins?", "acronym": "LM", "label": "Bigram Model", "ID": "797"}, {"sentence": "4.1 LM The simplest model we define over dialogs is the bi- gram model of Eckert et al1997): p (ui|m) = p (ui|mi?1) (1) p (u|m) = ?", "acronym": "LM", "label": "Bigram Model", "ID": "798"}, {"sentence": "4.1 LMs {DPI, Feedback} Model Indeed the first signifi- cant models that include a DA bigram include the {DPI, Feedback} DA sequence.", "acronym": "LM", "label": "Bigram Model", "ID": "799"}, {"sentence": "4.2 LM The problem of the unigram model can be alleviated by the bigram model based on a hierarchical Dirich- let process (Goldwater et al, 2009).", "acronym": "LM", "label": "Bigram Model", "ID": "800"}, {"sentence": "5.2 Statistical LM Similar to Ringger et al (2004), we find the order with the highest probability conditioned on syntac- tic and semantic categories.", "acronym": "LM", "label": "Bigram Model", "ID": "801"}, {"sentence": "The contribution of this paper is to show how to model a troubleshooting SDS as a partially observable Markov decision process (POMDP).", "acronym": "SDS", "label": "spoken dialog system", "ID": "802"}, {"sentence": "(4) 3 Timestep n Timestep n+1 x' am au    y' au      r' x am au   y au     r d'd c,au ~com com com'ts ts' ' c,au ~com' Figure 3: Influence diagram of a troubleshooting SDS.", "acronym": "SDS", "label": "spoken dialog system", "ID": "803"}, {"sentence": "The conditional probability tables composing the model were handcrafted based on conversations with troubleshooting experts and past experience with SDSs.", "acronym": "SDS", "label": "spoken dialog system", "ID": "804"}, {"sentence": "Section 2 re- views POMDPs, the general troubleshooting prob- lem, and POMDP-based SDSs; section 3 explains how these two POMDPs can be combined to model a troubleshooting SDS; sections 4-5 present results from simulation; and section 6 concludes.", "acronym": "SDS", "label": "spoken dialog system", "ID": "805"}, {"sentence": "on: DSL-1 To illustrate the general framework, we first created a very simple troubleshooting SDS called DSL-1.", "acronym": "SDS", "label": "spoken dialog system", "ID": "806"}, {"sentence": "4 Illustration: DSL-1 To illustrate the general framework, we first created a very simple troubleshooting SDS called DSL-1.", "acronym": "SDS", "label": "spoken dialog system", "ID": "807"}, {"sentence": "6 Conclusions This paper has shown how a SDS for troubleshooting can be cast as a POMDP.", "acronym": "SDS", "label": "spoken dialog system", "ID": "808"}, {"sentence": "1 Introduction SDS are complex frameworks, involving the integration of speech recognition, speech synthesis, natural language understanding and generation, dialogue management, and interac- tion with domain-specific applications.", "acronym": "SDS", "label": "Spoken dialogue systems", "ID": "809"}, {"sentence": "1 Introduction 1.1 Spoken Dialogue Systems and Non-Native Speakers SDS rely on models of human lan- guage to understand users?", "acronym": "SDS", "label": "Spoken dialogue systems", "ID": "810"}, {"sentence": "1 Introduction SDS that can detect and adapt to user affect1 are fast becoming reality (Schuller et al, 2009b; Batliner et al, 2008; Prendinger and Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee ?", "acronym": "SDS", "label": "Spoken dialogue systems", "ID": "811"}, {"sentence": "What?s the Trouble: Automatically Identifying Problematic Dialogues in DARPA Communicator Dialogue Systems Helen Wright Hastie, Rashmi Prasad, Marilyn Walker AT& T Labs - Research 180 Park Ave, Florham Park, N.J. 07932, U.S.A. hhastie,rjprasad,walker@research.att.com Abstract SDS promise effi- cient and natural access to information services from any phone.", "acronym": "SDS", "label": "Spoken dialogue systems", "ID": "812"}, {"sentence": "2 Introduction SDS for cars emerged in the late 1990s with the appearance of advanced information and communication systems.", "acronym": "SDS", "label": "Spoken dialogue systems", "ID": "813"}, {"sentence": "1 Introduction SDS have traditionally fo- cused on task-oriented dialogues, such as mak- ing flight bookings or providing public transport timetables.", "acronym": "SDS", "label": "Spoken dialogue systems", "ID": "814"}, {"sentence": "1 Introduction SDSs are complex frameworks, involving the integration of speech recognition, speech synthesis, natural language understanding and generation, dialogue management, and interac- tion with domain-specific applications.", "acronym": "SDS", "label": "Spoken dialogue system", "ID": "815"}, {"sentence": "1 Introduction 1.1 Spoken Dialogue Systems and Non-Native Speakers SDSs rely on models of human lan- guage to understand users?", "acronym": "SDS", "label": "Spoken dialogue system", "ID": "816"}, {"sentence": "1 Introduction SDSs that can detect and adapt to user affect1 are fast becoming reality (Schuller et al, 2009b; Batliner et al, 2008; Prendinger and Ishizuka, 2005; Vidrascu and Devillers, 2005; Lee ?", "acronym": "SDS", "label": "Spoken dialogue system", "ID": "817"}, {"sentence": "What?s the Trouble: Automatically Identifying Problematic Dialogues in DARPA Communicator Dialogue Systems Helen Wright Hastie, Rashmi Prasad, Marilyn Walker AT& T Labs - Research 180 Park Ave, Florham Park, N.J. 07932, U.S.A. hhastie,rjprasad,walker@research.att.com Abstract SDSs promise effi- cient and natural access to information services from any phone.", "acronym": "SDS", "label": "Spoken dialogue system", "ID": "818"}, {"sentence": "2 Introduction SDSs for cars emerged in the late 1990s with the appearance of advanced information and communication systems.", "acronym": "SDS", "label": "Spoken dialogue system", "ID": "819"}, {"sentence": "1 Introduction SDSs have traditionally fo- cused on task-oriented dialogues, such as mak- ing flight bookings or providing public transport timetables.", "acronym": "SDS", "label": "Spoken dialogue system", "ID": "820"}, {"sentence": "This  eliminates the potential BS that clustering the entities  becomes easier if the entities are clearly from different  genres.", "acronym": "BS", "label": "bias", "ID": "821"}, {"sentence": "However, this would introduce systematic BSes into the counts, because nouns do in fact very often occur adjacently to prepositions that modify them, but many verbs do not.", "acronym": "BS", "label": "bias", "ID": "822"}, {"sentence": "(Volk, 2002) already suggested that this count- ing method introduced a general BS toward verb attachment, and when comparing the results for very frequent words (for which more reliable evi- dence is available from the treebank) we find that verb attachments are in fact systematically over- estimated.", "acronym": "BS", "label": "bias", "ID": "823"}, {"sentence": "Nor  do we use the gender/animiticity information  gathered from the much smaller hand-marked  text, both because we were interested in seeing  what unsupervised learning could accomplish,  and because we were concerned with inherit-  ing strong BSes from the limited hand-marked  data.", "acronym": "BS", "label": "bias", "ID": "824"}, {"sentence": "7.2 Domain-specific sub-corpora         The person-x corpus may appear to be BSed due to  the manner of its construction.", "acronym": "BS", "label": "bias", "ID": "825"}, {"sentence": "Both methods face obvi- ous problems: The data-driven approach is at the mercy of its training set and cannot easily avoid mistakes that result from BSed or scarce data.", "acronym": "BS", "label": "bias", "ID": "826"}, {"sentence": "A first set of 23,000 doc- uments was retrieved, identifying the presence of the bacterium BS in the text and/or in the MeSH terms.", "acronym": "BS", "label": "Bacillus subtilis", "ID": "827"}, {"sentence": "In this study we aimed at extracting a gene regulatory network of the popular model organism the BS.", "acronym": "BS", "label": "Bacillus subtilis", "ID": "828"}, {"sentence": "Con- necting parts with processes: SubtiWiki and Subti- Pathways integrate gene and pathway annotation for BS.", "acronym": "BS", "label": "Bacillus subtilis", "ID": "829"}, {"sentence": "A community-curated con- sensual annotation that is continuously updated: The BS centred wiki SubtiWiki.", "acronym": "BS", "label": "Bacillus subtilis", "ID": "830"}, {"sentence": "3.1 Bacteria Gene Interactions corpus The source of the Bacteria GI Task corpus is a set of PubMed abstracts mainly dealing with the tran- 70 scription of genes in BS.", "acronym": "BS", "label": "Bacillus subtilis", "ID": "831"}, {"sentence": "Operon | Regulon | Family PMID 3127379 : Three promoters direct transcription of the sigA (rpoD) operon in BS.", "acronym": "BS", "label": "Bacillus subtilis", "ID": "832"}, {"sentence": "2012) use a simple two-level genera- tive hierarchical approach using Naive Bayes, but to our knowledge no previous work implements a multi-level discriminative hierarchical model with BS for geolocation.", "acronym": "BS", "label": "beam search", "ID": "833"}, {"sentence": "We use a DP-based BS procedure similar to the one presented in (Tillmann, 2001).", "acronym": "BS", "label": "beam search", "ID": "834"}, {"sentence": "Rather than computing the 339 probability of every leaf cell using equation 3, we use a stratified BS: starting at the root cell, keep the b highest-probability cells at each level until reaching the leaf node level.", "acronym": "BS", "label": "beam search", "ID": "835"}, {"sentence": "In addition to allowing one to use many classi- fiers that each have a manageable number of out- comes, the hierarchical approach naturally lends itself to BS.", "acronym": "BS", "label": "beam search", "ID": "836"}, {"sentence": "For reference, we show the most accurate models from Alberti et al (2015) and Weiss et al (2015), which use a deeper model and BS for inference.", "acronym": "BS", "label": "beam search", "ID": "837"}, {"sentence": "2004; Kaji and Kitsuregawa, 2013) or BS (Jiang et al.,", "acronym": "BS", "label": "beam search", "ID": "838"}, {"sentence": "Clear Concept: PaSbPlSSb, SSbSbPaPl, PaPlSSbSb, PlSSbSbPa (a combination of SeekBw and BS clicks, indicating high tussle with the video lecture content) ?", "acronym": "BS", "label": "ScrollBw", "ID": "839"}, {"sentence": "clicks in the follow- ing 8 categories: Play (Pl), Pause (Pa), SeekFw (Sf), SeekBw (Sb), ScrollFw (SSf), BS (SSb), RatechangeFast (Rf), RatechangeSlow (Rs).", "acronym": "BS", "label": "ScrollBw", "ID": "840"}, {"sentence": "Building applied NLG systems.", "acronym": "NLG", "label": "natural language generation", "ID": "841"}, {"sentence": "In these approaches to NLG  there is a gap between the plan, which is usually  represented in the terms of the application program, and  the resources used by the realization component tocarry  out that plan, which are the concrete words, syntax,  morphemes, etc.", "acronym": "NLG", "label": "natural language generation", "ID": "842"}, {"sentence": "The order of prenominal adjectives in NLG.", "acronym": "NLG", "label": "natural language generation", "ID": "843"}, {"sentence": "5 Ordering of Prenominal Adjectives The ordering of prenominal modifiers is important for NLG systems where the text must be both fluent and grammatical.", "acronym": "NLG", "label": "natural language generation", "ID": "844"}, {"sentence": "Building NLG systems.", "acronym": "NLG", "label": "natural language generation", "ID": "845"}, {"sentence": "The role of the University of Edinburgh in this project  is the development of a NLG com-  ponent which can automatically derive various kinds of  specification documents from the common underlying  database.", "acronym": "NLG", "label": "natural language generation", "ID": "846"}, {"sentence": "59  References  \\[Barnett, 90\\] J. Barnett and I. Mani, \"Using Bidi-  rectional Semantic Rules for Generation\",  Proceedings of the Fifth International Work-  shop on NLG, pp.47-  53, Dawson, Pa., 3-6 June, 1990.", "acronym": "NLG", "label": "Natural Language Generation", "ID": "847"}, {"sentence": "In International Workshop on NLG, pages 178?187, Niagara-on-the-Lake, Canada, August.", "acronym": "NLG", "label": "Natural Language Generation", "ID": "848"}, {"sentence": "framenet.icsi.berkeley.edu  9  CORECT: Combining CSCW with NLG  for Collaborative Requirements Capture  John Levine\" and Chris Mellish t Department of Artificial Intelligence,  University of Edinburgh,  80 South Bridge,  Edinburgh EH1 1HN, Scotland, UK.", "acronym": "NLG", "label": "Natural Language Generation", "ID": "849"}, {"sentence": "In Proceedings of the Eleventh Eu- ropean Workshop on NLG, pages 139?142.", "acronym": "NLG", "label": "Natural Language Generation", "ID": "850"}, {"sentence": "I  I  I  I  I  i  I  I  i  i  I  I  I  I   Towards Automatic Generation of NLG Systems John Chen?,", "acronym": "NLG", "label": "Natural Language Generation", "ID": "851"}, {"sentence": "While at first glance, one might think that prob-  lems of NLG only occur  in the last phase of the system processing, we  believe that a \"generation\" perspective on the  entire process is extremely beneficial.", "acronym": "NLG", "label": "Natural Language Generation", "ID": "852"}, {"sentence": "I would like a higher optical zoom, the W200 does a great digital zoom translation... Table 3: Opinion Organization Result for Sony Cybershot DSC-W200 Camera listed in Freebase, but we can find it in people?s online discussion using MI.", "acronym": "MI", "label": "mutual information", "ID": "853"}, {"sentence": "Note that this binary reference matrix can  be converted to a strength-of-association matrix by multiplying it with a diagonal matrix that con- tains the strength-of-association scores, e.g. log  likelihood ratio, Mutual Information, Pointwise MI, Chi-squared to name a few.", "acronym": "MI", "label": "mutual information", "ID": "854"}, {"sentence": "2 statistic and the log- likelihood ratio perform best, the dice coefficient the second best, and the MI the worst.", "acronym": "MI", "label": "mutual information", "ID": "855"}, {"sentence": "Ld Then, based on the contingency table of co- occurrence document frequencies of tE and tJ below, we estimate bilingual term correspon- dences according to the statistical measures such as the MI, the ?", "acronym": "MI", "label": "mutual information", "ID": "856"}, {"sentence": "i.e., we label the largest clusters by se- lecting phrases with top MI.", "acronym": "MI", "label": "mutual information", "ID": "857"}, {"sentence": "Word association norms, MI, and  lexicography.", "acronym": "MI", "label": "mutual information", "ID": "858"}, {"sentence": "Since our focus 225 is not on finding the best extraction method, but on judging the benefit of statistical components to parsing, we employ a collocation measure related to the idea of MI: a collocation between a word w and a preposition p is judged more likely the more often it appears, and the less often its component words appear.", "acronym": "MI", "label": "mutual information", "ID": "859"}, {"sentence": "Schafer and Graham (2002) review techniques to deal with missing data, and recommend two approaches: maximum likelihood estimation and Bayesian MI.", "acronym": "MI", "label": "multiple imputation", "ID": "860"}, {"sentence": "This method is known in the liter- ature as MI (Rubin, 1987).", "acronym": "MI", "label": "multiple imputation", "ID": "861"}, {"sentence": "Tree-bank  text (Marcus et al, 1993) that has been marked  with co-reference inforMI.", "acronym": "MI", "label": "mation", "ID": "862"}, {"sentence": "We present some experiments il-  lustrating the accuracy of the method and note  that with this inforMI added, our pronoun  resolution method achieves 84.2% accuracy.", "acronym": "MI", "label": "mation", "ID": "863"}, {"sentence": "The first piece of useful inforMI we con-  sider is the distance between the pronoun  and the candidate antecedent.", "acronym": "MI", "label": "mation", "ID": "864"}, {"sentence": "forMI added, our pronoun  resolution method achieves 84.2% accuracy.", "acronym": "MI", "label": "mation", "ID": "865"}, {"sentence": "The second half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of English words, inforMI that is  itself used in the pronoun resolution program.", "acronym": "MI", "label": "mation", "ID": "866"}, {"sentence": "This program differs  from earlier work in its almost complete lack of  hand-crafting, relying instead on a very small  corpus of Penn Wall Street Journal Tree-bank  text (Marcus et al, 1993) that has been marked  with co-reference inforMI.", "acronym": "MI", "label": "mation", "ID": "867"}, {"sentence": "The second  experiment investigates a method for unsuper-  vised learning of gender/number/animaticity  inforMI.", "acronym": "MI", "label": "mation", "ID": "868"}, {"sentence": "Our first experiment  shows the relative contribution of each source  Of inforMI and demonstrates a uccess rate  of 82.9% for all sources combined.", "acronym": "MI", "label": "mation", "ID": "869"}, {"sentence": "We also study the behavior of approxi- mate Pointwise MI and Log Likeli- hood Ratio for the sketches.", "acronym": "MI", "label": "Mutual Information", "ID": "870"}, {"sentence": "Note that this binary reference matrix can  be converted to a strength-of-association matrix by multiplying it with a diagonal matrix that con- tains the strength-of-association scores, e.g. log  likelihood ratio, MI, Pointwise mutual information, Chi-squared to name a few.", "acronym": "MI", "label": "Mutual Information", "ID": "871"}, {"sentence": "However, the raw frequency is certainly not  the only way to rank the verbs; we plan on explor- ing other metrics such as MI.", "acronym": "MI", "label": "Mutual Information", "ID": "872"}, {"sentence": "(Baroni and Vegnaduzzo, 2004) and (Grefenstette et al, 2004) gathered subjective adjectives from the web calculating the MI score.", "acronym": "MI", "label": "Mutual Information", "ID": "873"}, {"sentence": "III, 2011a), computing approximate as- sociation scores like Pointwise MI (Li et al 2008; Van Durme and Lall, 2009b; Goyal and Daume?", "acronym": "MI", "label": "Mutual Information", "ID": "874"}, {"sentence": "In ad- dition to those clustering algorithms, we are also exam- ining the use of various lexical association measures such as MI, Dice coefficient, ?", "acronym": "MI", "label": "Mutual Information", "ID": "875"}, {"sentence": "2MI, though potentially of interest as a  measure of collocational status, was not tested due to its  well-known property of overemphasising the significance of  rare events (Church and Hanks, 1990).", "acronym": "MI", "label": "Mutual information", "ID": "876"}, {"sentence": "These pairs were manually tagged as  arguments, therefore MI makes  the right prediction.", "acronym": "MI", "label": "Mutual information", "ID": "877"}, {"sentence": "t t I t t t t  150 200 250 300 350 400 450 500  Window size (sentences)  Figure 1: MI gain varying window  size  5 Discussion  The main result of this paper is to show that  analogous to the case of words in language mod-  elling, a significant amount of extrasentential infor-  mation can be extracted from the long-range his-  tory of a document, using trigger pairs for tags and  rules.", "acronym": "MI", "label": "Mutual information", "ID": "878"}, {"sentence": "MI values in table 2 go  along with the manual tagging for these last pairs  as well, because the MI values are  low as should correspond to adjuncts.", "acronym": "MI", "label": "Mutual information", "ID": "879"}, {"sentence": "In contrast, the precisions of MI and Dice formula  were not so ideal.", "acronym": "MI", "label": "Mutual information", "ID": "880"}, {"sentence": "Mutual Information Bits MI clustering, as  described in \\[10\\], creates a a class \"tree\" for a given vocab-  ulary.", "acronym": "MI", "label": "Mutual information", "ID": "881"}, {"sentence": "Chinese NLP resources     Part 2: Text Processing  2.1 Lexical processing   a. Segmentation   b. Disambiguation   c. Unknown word detection   d. NERn  2.2 Syntactic processing   a. Issues in PoS tagging   b. Hidden Markov Models  2.3 NLP Applications  References   Academia Sinica Balance Corpus of Mandarin Chi- nese.", "acronym": "NER", "label": "Named Entity Recognitio", "ID": "882"}, {"sentence": "Workshop on NERn.", "acronym": "NER", "label": "Named Entity Recognitio", "ID": "883"}, {"sentence": "The Third International Chi- nese Language Processing Bakeoff: Word Seg- mentation and NERn, In  Proceedings of SIGHAN5 the 3rd International  Chinese Language Processing Bakeoff at Col- ing/ACL 2006, July, Sydney, Australia, 108-117.", "acronym": "NER", "label": "Named Entity Recognitio", "ID": "884"}, {"sentence": "3) We employed IE methods (including pattern sets  and NERn) as initial extraction  steps.", "acronym": "NER", "label": "Named Entity Recognitio", "ID": "885"}, {"sentence": "A Maximum Entropy Ap- proach to NERn.", "acronym": "NER", "label": "Named Entity Recognitio", "ID": "886"}, {"sentence": "Ex- tracting Personal Names from Email: Applying  NERn to Informal Text, Proc.", "acronym": "NER", "label": "Named Entity Recognitio", "ID": "887"}, {"sentence": "Annotation guide- lines for machine learning-based NER in microbiology.", "acronym": "NER", "label": "named entity recognition", "ID": "888"}, {"sentence": "Early results for NER with conditional random fields, feature induction and web-enhanced lexicons.", "acronym": "NER", "label": "named entity recognition", "ID": "889"}, {"sentence": "We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) NER was used for identifying proper names, e.g., ?", "acronym": "NER", "label": "named entity recognition", "ID": "890"}, {"sentence": "Joint inference of NER and normal- ization for tweets.", "acronym": "NER", "label": "named entity recognition", "ID": "891"}, {"sentence": "This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) NER, (vi) dependency parsing, and (vii) co-reference analysis.", "acronym": "NER", "label": "named entity recognition", "ID": "892"}, {"sentence": "Two broad approaches for the iden- tification of story characters were followed: (i) NER, and (ii) identification of character nominals, e.g., ?", "acronym": "NER", "label": "named entity recognition", "ID": "893"}, {"sentence": "Chinese NLP resources     Part 2: Text Processing  2.1 Lexical processing   a. Segmentation   b. Disambiguation   c. Unknown word detection   d. NER  2.2 Syntactic processing   a. Issues in PoS tagging   b. Hidden Markov Models  2.3 NLP Applications  References   Academia Sinica Balance Corpus of Mandarin Chi- nese.", "acronym": "NER", "label": "Named Entity Recognition", "ID": "894"}, {"sentence": "Workshop on NER.", "acronym": "NER", "label": "Named Entity Recognition", "ID": "895"}, {"sentence": "The Third International Chi- nese Language Processing Bakeoff: Word Seg- mentation and NER, In  Proceedings of SIGHAN5 the 3rd International  Chinese Language Processing Bakeoff at Col- ing/ACL 2006, July, Sydney, Australia, 108-117.", "acronym": "NER", "label": "Named Entity Recognition", "ID": "896"}, {"sentence": "3) We employed IE methods (including pattern sets  and NER) as initial extraction  steps.", "acronym": "NER", "label": "Named Entity Recognition", "ID": "897"}, {"sentence": "A Maximum Entropy Ap- proach to NER.", "acronym": "NER", "label": "Named Entity Recognition", "ID": "898"}, {"sentence": "Ex- tracting Personal Names from Email: Applying  NER to Informal Text, Proc.", "acronym": "NER", "label": "Named Entity Recognition", "ID": "899"}, {"sentence": "However, the main disadvantage of relying on semantic analyzers, NER and the like, is that for some languages these tools are not yet well developed.", "acronym": "NER", "label": "named entity taggers", "ID": "900"}, {"sentence": "4 Recommended Uses We have found the component corpora of MedTag to be useful for the following functions: 1) Training and evaluating part-of-speech taggers 2) Training and evaluating gene/protein NER 1http://cancerweb.ncl.ac.uk/omd/copyleft.html http://www.onelook.com/ 3) Developing and evaluating a noun phrase bracketer for PubMed phrase indexing 4) Statistical analysis of grammatical usage in medical text 5) Feature generation for machine learn- ing The MedPost tagger was recently ported to Java and is currently being employed in MetaMap, a pro- gram that maps natural la", "acronym": "NER", "label": "named entity taggers", "ID": "901"}, {"sentence": "The CALBC Silver Standard Corpus for biomedical named entities: A study in harmonizing the contributions from four indepen- dent NER.", "acronym": "NER", "label": "named entity taggers", "ID": "902"}, {"sentence": "We plan to utilise existing NER de- veloped in our group as a pre-annotation step in the creation of our reference standard.", "acronym": "NER", "label": "named entity taggers", "ID": "903"}, {"sentence": "This research was partially funded by the EC?s 7th Framework Pro- gramme within the CALBC project (FP7-231727) and the GERONTOSYS research initiative from the 13We used a gold standard in which some unusual entities (e.g., protein families) had been annotated for which most NER have not been trained.", "acronym": "NER", "label": "named entity taggers", "ID": "904"}, {"sentence": "NER.2 The various contributions covered, among others, an- notations for genes and proteins, chemicals, dis- eases, etc (Rebholz-Schuhmann et al, 2010b).", "acronym": "NER", "label": "named entity taggers", "ID": "905"}, {"sentence": "The item 4) is situated at the edge E of domain D.  This deliNER covers a family of constraints depending on  the instantiations of the arguments: E is either left (L) or  right (R), domain \\]nay be syllable, foot or word, and ~b can  be any phonological object, such as stress or an affix.", "acronym": "NER", "label": "nition", "ID": "906"}, {"sentence": "Therefore, the first task in future work will be to review the defiNER and characterisation of this class.", "acronym": "NER", "label": "nition", "ID": "907"}, {"sentence": "We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named entity recogNER was used for identifying proper names, e.g., ?", "acronym": "NER", "label": "nition", "ID": "908"}, {"sentence": "Unsupervised personality recogNER for social network sites.", "acronym": "NER", "label": "nition", "ID": "909"}, {"sentence": "This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) named entity recogNER, (vi) dependency parsing, and (vii) co-reference analysis.", "acronym": "NER", "label": "nition", "ID": "910"}, {"sentence": "Two broad approaches for the iden- tification of story characters were followed: (i) named entity recogNER, and (ii) identification of character nominals, e.g., ?", "acronym": "NER", "label": "nition", "ID": "911"}, {"sentence": "A high precision rate means a low false 2 Base named entities such as PERSON and LOCATION were found using Stanford?s NER (Finkel et al, 2005).", "acronym": "NER", "label": "named entity recogniser", "ID": "912"}, {"sentence": "2.1 Protein and small molecule recognition Two dictionary-based NERs were used to detect the names of proteins and small molecules in the full collection of MEDLINE ab- stracts, with the two source dictionaries constructed using the resources UniProt (Apweiler et al, 2004) and ChEBI (De Matos et al, 2006) respectively.", "acronym": "NER", "label": "named entity recogniser", "ID": "913"}, {"sentence": "corpus annotation and evaluation: use the default NER to boot- strap the manual annotation of the test data for the dialogue application; evaluate the performance of the default NE gram- mars on the dialogue texts; suggest possi- ble improvements on the basis of the infor- mation about missed and incorrect anno- tations provided by the corpus benchmark tool.", "acronym": "NER", "label": "named entity recogniser", "ID": "914"}, {"sentence": "The whole collection of MED- LINE was filtered using a co-occurrence approach and a NER.", "acronym": "NER", "label": "named entity recogniser", "ID": "915"}, {"sentence": "A conventional NER is then trained on the data containing the joined labels.", "acronym": "NER", "label": "named entity recogniser", "ID": "916"}, {"sentence": "We used a state-of-the-art  NER that was part of a larger  toolbox for named entity recognition.", "acronym": "NER", "label": "named entity recogniser", "ID": "917"}, {"sentence": "Training and Evalu- ating a German NER with Se- mantic Generalization.", "acronym": "NER", "label": "Named Entity Recognizer", "ID": "918"}, {"sentence": "We first ex- tract named entities from scattered opinions DT using Stanford NER (Finkel et al, 2005).", "acronym": "NER", "label": "Named Entity Recognizer", "ID": "919"}, {"sentence": "For the Named Entity classifier, we added the feature Named-Entity-type as obtained by the NER.", "acronym": "NER", "label": "Named Entity Recognizer", "ID": "920"}, {"sentence": "We run the Stanford NER (Finkel et al, 2005) and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs.", "acronym": "NER", "label": "Named Entity Recognizer", "ID": "921"}, {"sentence": "We use a NER which has an accuracy above 90% for proper names.", "acronym": "NER", "label": "Named Entity Recognizer", "ID": "922"}, {"sentence": "M. Faruqui, S. Pado, Training and Evaluating a German NER with Semantic Generaliza- tion, Proceedings of Konvens 2010, Saarbrucken, Ger- many.", "acronym": "NER", "label": "Named Entity Recognizer", "ID": "923"}, {"sentence": "Using a  normal html parser, such as lynx, the text may  lack its usual grammatical structure which may  drastically decrease the performances of sen- tence splitters, NERs and  parsers.", "acronym": "NER", "label": "Name Entity Recognizer", "ID": "924"}, {"sentence": "Accord- ingly, we extract all the named entities in a sentence using Stanford?s NER (Finkel et al.,", "acronym": "NER", "label": "Name Entity Recognizer", "ID": "925"}, {"sentence": "5.1 Modules The pipeline for processing archaeological articles integrates three main modules: a module for recov- ering the logical structure of the documents, a mod- ule for Italian and English POS tagging and a gen- eral NER and finally, a Gazetteer Based NER.", "acronym": "NER", "label": "Name Entity Recognizer", "ID": "926"}, {"sentence": "We resort to Stanford NER 5 to extract seven types of named entities including time, location, organization, person, money, percent and date.", "acronym": "NER", "label": "Name Entity Recognizer", "ID": "927"}, {"sentence": "To identify words that belong to such semantic classes, NERs are used, since most of these words represent names.", "acronym": "NER", "label": "Name Entity Recognizer", "ID": "928"}, {"sentence": "NER through classifier combination.", "acronym": "NER", "label": "Named entity recognition", "ID": "929"}, {"sentence": "NER would help in cases like the following, LHIP provides a processing method which allows selected portions of the input to be ignored or handled differently. (", "acronym": "NER", "label": "Named entity recognition", "ID": "930"}, {"sentence": "NER in tweets: An exper- imental study.", "acronym": "NER", "label": "Named entity recognition", "ID": "931"}, {"sentence": "NER in tweets: an experimental study.", "acronym": "NER", "label": "Named entity recognition", "ID": "932"}, {"sentence": "NER for bacterial type IV secretion systems.", "acronym": "NER", "label": "Named entity recognition", "ID": "933"}, {"sentence": "NER through classifier combina- tion.", "acronym": "NER", "label": "Named entity recognition", "ID": "934"}, {"sentence": "The used patterns are: 1) (DT|CD) (NN|NNS), 2) DT JJ (NN|NNS), 3) NNPOS (NN|NNS), and 4) PRP$ JJ (NN|NNS).", "acronym": "NP", "label": "N P", "ID": "935"}, {"sentence": "The evaluation metrics were precision P (the number of true pos- K TP FP FNP (%) R (%) F1 (%) 10 641 80 149 88.90 81.14 84.84 20 644 79 146 89.07 81.52 85.13 30 644 80 146 88.95 81.52 85.07 40 645 81 145 88.84 81.65 85.09 50 645 80 145 88.97 81.65 85.15 Table 2: Effects of K in Bayes Point Machines itives divided by the total number of elements la- beled as belonging to the positive class) recall R (the number of true positives divided by the to- tal number of elements", "acronym": "NP", "label": "N P", "ID": "936"}, {"sentence": "TP FP FNP (%) R (%) F1 (%) 1 647 79 143 89.12 81.90 85.36 2 647 80 143 89.00 81.90 85.30 1,2 647 81 143 88.87 81.90 85.24 Table 3: Effects of removing features (1) or (2), or both Table 3 shows the effect of removing (1), (2), or both (1) and (2), showing that they overfit the training data.", "acronym": "NP", "label": "N P", "ID": "937"}, {"sentence": "Perceptron TP FP FNP (%) R (%) F1 (%) 671 128 119 83.98 84.94 84.46 Conditional Random Fields TP FP FNP (%) R (%) F1 (%) 643 78 147 89.18 81.39 85.11 Bayes Point Machines TP FP FNP (%) R (%) F1 (%) 647 79 143 89.12 81.90 85.36 Table 4: Performance of different optimization strategies 6 Conclusion To tackle the hedge cue detection problem posed by the CoNLL-2010 shared task, we utilized a classifier for sequentia", "acronym": "NP", "label": "N P", "ID": "938"}, {"sentence": "Perceptron TP FP FNP (%) R (%) F1 (%) 671 128 119 83.98 84.94 84.46 Conditional Random Fields TP FP FNP (%) R (%) F1 (%) 643 78 147 89.18 81.39 85.11 Bayes Point Machines TP FP FNP (%) R (%) F1 (%) 647 79 143 89.12 81.90 85.36 Table 4: Performance of different optimization strategies 6 Conclusion To tackle the hedge cue detection problem posed by the CoNLL-2010 shared task, we utilized a classifier for sequential labeling following previ- ous work (Morante and Daelemans, 2009).", "acronym": "NP", "label": "N P", "ID": "939"}, {"sentence": "2013) use a su- pervised machine learning approach to address the same problem, though many of their preliminary steps and iNPut features are similar to those used in (Elson and McKeown, 2010).", "acronym": "NP", "label": "np", "ID": "940"}, {"sentence": "48 P. J. Stone, D. C. DuNPhy, M. S. Smith, and D. M. Ogilvie.", "acronym": "NP", "label": "np", "ID": "941"}, {"sentence": "For binary constrainls, the ewl-  luation of harmony is a sitNPle affair.", "acronym": "NP", "label": "np", "ID": "942"}, {"sentence": "The iNPut to the system is simply the text of a story with no additional annotation.", "acronym": "NP", "label": "np", "ID": "943"}, {"sentence": "Consider, lbr exatNPle, tile hinary constraint  ONS(P&S:25).", "acronym": "NP", "label": "np", "ID": "944"}, {"sentence": "INTRODUCTION  Recent years have sect) two major trends in phonology:  I heor ies  \\[ lave begOlllC \\[11o1{3 or iented  arOl.llld const la i \\ [ l t s   Ihan transformations, while itNPlenmntations have come to  rely increasingly on finite state attlomata nd transducers.", "acronym": "NP", "label": "np", "ID": "945"}, {"sentence": "A Stochastic Parts Program and NPe Parser for Unrestricted Text.", "acronym": "NP", "label": "Noun Phras", "ID": "946"}, {"sentence": "Towards an Annotation  Scheme for NPe Generation.", "acronym": "NP", "label": "Noun Phras", "ID": "947"}, {"sentence": "Corpus-based Identifi- cation of Non-Anaphoric NPes.", "acronym": "NP", "label": "Noun Phras", "ID": "948"}, {"sentence": "Formal Model of  NPes in Serbo-Croatian.", "acronym": "NP", "label": "Noun Phras", "ID": "949"}, {"sentence": "In particular, only NPes were anno-  tated (thereby circumventing problems of null  anaphora, summation, abstraction, etc.,", "acronym": "NP", "label": "Noun Phras", "ID": "950"}, {"sentence": "The Semantics of Definite and  Indefinite NPes PhD Thesis, University  of Massachussetts, 1982.", "acronym": "NP", "label": "Noun Phras", "ID": "951"}, {"sentence": "A Stochastic Parts Program and NP Parser for Unrestricted Text.", "acronym": "NP", "label": "Noun Phrase", "ID": "952"}, {"sentence": "Towards an Annotation  Scheme for NP Generation.", "acronym": "NP", "label": "Noun Phrase", "ID": "953"}, {"sentence": "Corpus-based Identifi- cation of Non-Anaphoric NPs.", "acronym": "NP", "label": "Noun Phrase", "ID": "954"}, {"sentence": "Formal Model of  NPs in Serbo-Croatian.", "acronym": "NP", "label": "Noun Phrase", "ID": "955"}, {"sentence": "In particular, only NPs were anno-  tated (thereby circumventing problems of null  anaphora, summation, abstraction, etc.,", "acronym": "NP", "label": "Noun Phrase", "ID": "956"}, {"sentence": "The Semantics of Definite and  Indefinite NPs PhD Thesis, University  of Massachussetts, 1982.", "acronym": "NP", "label": "Noun Phrase", "ID": "957"}, {"sentence": "6.2 Representation and Structured Knowledge Transfer Then for each expected English entity e h , if there is a cross-lingual link to link it to an LL (Chinese) entry e l in the KB, we added the title of the LL entry or its redirected/reNP c l as its LL translation.", "acronym": "NP", "label": "named page", "ID": "958"}, {"sentence": "NP zasady i dlaczego trzeba je stosowa? ?", "acronym": "NP", "label": "najprostsze", "ID": "959"}, {"sentence": "We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and NP repetition.", "acronym": "NP", "label": "noun phrase", "ID": "960"}, {"sentence": "Figure 1 shows the 43  NPs with the highest salience figures  (run using the Hobbs algorithm).", "acronym": "NP", "label": "noun phrase", "ID": "961"}, {"sentence": "However a singular NP can be the ref-  erent of a plural pronoun, as illustrated by the  following example:  \"I think if I tell Viacom I need more  time, they will take 'Cosby' across the  street,\" says the general manager ol a  network a~liate.", "acronym": "NP", "label": "noun phrase", "ID": "962"}, {"sentence": "GREENSPAN 120(68.7807) 0.9083 0 0.0916  AT&T 198(67.9668) 0.0252 0.0050 0.9696  MINISTER 125(67.7475) 0.864 0.064 0.072  JUDGE - 239(67.5899) 0.7154 0.0836 0.2008  Figure 1: Top 43 NPs according to salience  168  o  o~  o  l .O -   0.8-   0 .5 -   U  ? ?", "acronym": "NP", "label": "noun phrase", "ID": "963"}, {"sentence": "The method described  here is based on simply counting co-occurrences  of pronouns and NPs, and thus can  employ any method of analysis of the text  stream that results in referent/pronoun pairs  (cf. (", "acronym": "NP", "label": "noun phrase", "ID": "964"}, {"sentence": "Gener-  ally, a singular pronoun cannot refer to a plural  NP, so that in resolving such a pro-  noun any plural candidates should be ruled out.", "acronym": "NP", "label": "noun phrase", "ID": "965"}, {"sentence": "In a very  large corpus, it is possible to find many reasonable  paraphrases  of  NPs.", "acronym": "NP", "label": "noun  phrase", "ID": "966"}, {"sentence": "the use of a  delctlc In a definite NP, such as \"thls X\" or  \"the last X\", hints that the object was either mentioned  prev ious ly  or that  it p robab ly  was evoked by some  prev ious  re ference ,  and  that  it is searchab le )  but  we  will not  examine them here .", "acronym": "NP", "label": "noun  phrase", "ID": "967"}, {"sentence": "In our dictionary, however, the verbal  entry also contains organizing elements: in  the first instance, the prepositions that the  verb requires or allows to be placed before  the NPs.", "acronym": "NP", "label": "noun  phrase", "ID": "968"}, {"sentence": "In  case  o f  NPs   there  are  two  kerne ls :V  and  one  VG,  and   these  are  mapped to  oneParea .", "acronym": "NP", "label": "noun  phrase", "ID": "969"}, {"sentence": "For  example,  Thai  NP  '???(", "acronym": "NP", "label": "noun  phrase", "ID": "970"}, {"sentence": "2 Uses  o f  a NP  parser   The recognition and analysis of subclausal structural  units, e.g. noun phrases, is useful for several pur-  poses.", "acronym": "NP", "label": "noun  phrase", "ID": "971"}, {"sentence": "In terms of the NPs m, the time complexity in Step 3.b is in O(n m): for every markable, the corresponding term element is to be found, tak- ing at most n repositioning operations on the term layer.", "acronym": "NPs", "label": "number of markables", "ID": "972"}, {"sentence": "We present the absolute NPs se- lected as relevant by separate annotators and in two Gold Standards.", "acronym": "NPs", "label": "number of markables", "ID": "973"}, {"sentence": "In the rows below the line, N shows  the total NPs, while Z gives the num-  ber of agreements between the annotations.", "acronym": "NPs", "label": "number of markables", "ID": "974"}, {"sentence": "Also, we indicate the percentage, given the total NPs 3275.", "acronym": "NPs", "label": "number of markables", "ID": "975"}, {"sentence": "N is the total NPs, Z is total num-  ber of agreements between annotators, PE is the expected  agreement by chance.", "acronym": "NPs", "label": "number of markables", "ID": "976"}, {"sentence": "Our results show that it is hard to build a justifiable hy- pothesis on the NPs which is larger than the number of actually annotated entities while keeping ?", "acronym": "NPs", "label": "number of markables", "ID": "977"}, {"sentence": "Presumably, this was done as a consideration for the NPs?", "acronym": "NPs", "label": "NPB nodes", "ID": "978"}, {"sentence": "For consistency, an extra NP bracket is inserted around NPs not already dominated by an NP.", "acronym": "NPs", "label": "NPB nodes", "ID": "979"}, {"sentence": "These NPs are removed before evaluation.", "acronym": "NPs", "label": "NPB nodes", "ID": "980"}, {"sentence": "NNs for Pattern Recogni- tion, chapter 6.4: Modelling conditional distributions.", "acronym": "NN", "label": "Neural Network", "ID": "981"}, {"sentence": "Part of Speech Tagging with  NNs.", "acronym": "NN", "label": "Neural Network", "ID": "982"}, {"sentence": "c?2014 Association for Computational Linguistics Learning Image Embeddings using Convolutional NNs for Improved Multi-Modal Semantics Douwe Kiela ?", "acronym": "NN", "label": "Neural Network", "ID": "983"}, {"sentence": "In NNs: Tricks of the Trade, pages 599?619.", "acronym": "NN", "label": "Neural Network", "ID": "984"}, {"sentence": "Second Turkish Symposium on  Artificial Intelligence and NNs?,", "acronym": "NN", "label": "Neural Network", "ID": "985"}, {"sentence": "NN based Embedding.", "acronym": "NN", "label": "Neural Network", "ID": "986"}, {"sentence": "NN phrases are proper nouns and personal pronouns.", "acronym": "NN", "label": "Noun", "ID": "987"}, {"sentence": "NN phrases that are mentioned  repeatedly are preferred.", "acronym": "NN", "label": "Noun", "ID": "988"}, {"sentence": "For example, a set  of general filters for English may include the fol- lowing patterns:3    NN+ NN   (Adj | NN)+ NN   (Adj | NN)+| ((Adj | NN)* Prep?) (", "acronym": "NN", "label": "Noun", "ID": "989"}, {"sentence": "Adj | NN)* NN     Although these patterns are regular expressions,  the filters are implemented as unification-like  LR(1) rules (Mima et al, 1995) in order to facili- tate processing of grammatical agreements (if  any) within term candidates.", "acronym": "NN", "label": "Noun", "ID": "990"}, {"sentence": "2.2.3 NN Cosine The nearest neighbor cosine classifier required the creation of a term-document matrix, which contains a row for each training instance of an ambiguous word, and a column for each feature that can occur in the context of an ambiguous word.", "acronym": "NN", "label": "Nearest Neighbor", "ID": "991"}, {"sentence": "1)  PEBLS, a K-NN algorithm (Cost  and Salzberg 1993); (2) C4.5, a decision tree al-  gorithm (Quinlan 1994); (3) Ripper, an induc-  tive rule based classifier (Cohen 1996); (4) the  Naive Bayes classifier; and (5), a probabilistic  model search procedure (Bruce & Wiebe 1994)  using the public domain software CoCo (Bads-  berg 1995).", "acronym": "NN", "label": "Nearest Neighbor", "ID": "992"}, {"sentence": "2007) combine a naive Bayes classifier that uses a vocabulary-based language model with a k-NNs classifier us- ing grammatical features and interpolate the two to predict reading grade level.", "acronym": "NN", "label": "Nearest Neighbor", "ID": "993"}, {"sentence": "The classifiers that use PMI features are Decision Trees, Decision Rules, Na??ve Bayes, K-NN, Kernel Density, and Boosting a weak classifier (De- cision Stumps ?", "acronym": "NN", "label": "Nearest Neighbor", "ID": "994"}, {"sentence": "ML method (Weka) Features Accuracy Decision Trees PMI scores 65.4% Decision Rules PMI scores 65.5% Na??ve Bayes PMI scores 52.5% K-NN PMI scores 64.5% Kernel Density PMI scores 60.5% Boosting (Dec. Stumps) PMI scores 67.7% Na??ve Bayes 500 words 68.0% Decision Trees 500 words 67.0% Na??ve Bayes PMI + 500 words 66.5% Boosting (Dec. Stumps) PMI + 500 words 69.2% Table 6: Comparative results for the supervised learning method using various ML learning algo- rithms (Weka), averaged over the seven groups of near-syno", "acronym": "NN", "label": "Nearest Neighbor", "ID": "995"}, {"sentence": "We also implemented a k-NNs clas- sifier, which treats each individual training instance 1http://tedlab.mit.edu/?dr/SVDLIBC/ 309 as a separate vector (instead of treating each set of training instances that makes up a given sense as a single vector), and finds the k-nearest training in- stances to the test instance.", "acronym": "NN", "label": "Nearest Neighbor", "ID": "996"}, {"sentence": "In particular, the scheme infers the gender of a  referent from the gender of the proNN that  161  refer to it and selects referents using the pro-  noun anaphora program.", "acronym": "NN", "label": "nouns", "ID": "997"}, {"sentence": "In resolving inter-sentential  proNN, the algorithm searches the previous  sentence, again", "acronym": "NN", "label": "nouns", "ID": "998"}, {"sentence": "In resolving inter-sentential  proNN, the a", "acronym": "NN", "label": "nouns", "ID": "999"}, {"sentence": "3.1 The  gender /an imat ic i ty  stat ist ics   After we have identified the correct antecedents  it is a simple counting procedure to compute  P(p\\[wa) where wa is in the correct antecedent  for the pronoun p (Note the proNN are  grouped by their gender):  \\[ wain the antecedent for p \\[  P(pl o) =  When there are multiple relevant words in the  antecedent we apply the likelihood test designed  by Dunning (1993) on all the words in the candi-  date NP.", "acronym": "NN", "label": "nouns", "ID": "1000"}, {"sentence": "Given the above possible sources of informar  tion, we arrive at the following equation, where  F(p) denotes a function from proNN to their  antecedents:  F(p) = argmaxP( A(p) = alp, h, l~', t, l, so, d~ A~')  where A(p) is a random variable denoting the  referent of the pronoun p and a is a proposed  antecedent.", "acronym": "NN", "label": "nouns", "ID": "1001"}, {"sentence": "flexive  proNN.", "acronym": "NN", "label": "nouns", "ID": "1002"}, {"sentence": "In resolving inter-sentential  proNN, the algorithm searches the previous  sentence, again in left-to-right, breadth-first or-  der.", "acronym": "NN", "label": "nouns", "ID": "1003"}, {"sentence": "One classical approach to resolving  proNN in text that takes some syntactic fac-  tors into consideration is that of Hobbs (1976).", "acronym": "NN", "label": "nouns", "ID": "1004"}, {"sentence": "The most well  studied constraints are those involving reflexive  proNN.", "acronym": "NN", "label": "nouns", "ID": "1005"}, {"sentence": "An alterNN, more objec- tive selection method would be to perform ANOVA, which we plan to test in the near future.", "acronym": "NN", "label": "native", "ID": "1006"}, {"sentence": "Our alterNN mind ergonomic  approach addresses the following aspects: the  constitution of vast knowledge spaces, the best  example being the Web; the evolution of  communication models (one-to-one, one-to-many,  many-to-many); the variety of forms of access,  from the traditional question-answer model to  hyperlinks and timescaling.", "acronym": "NN", "label": "native", "ID": "1007"}, {"sentence": "DiscrimiNN training meth- ods for hidden Markov models: Theory and exper- iments with perceptron algorithms.", "acronym": "NN", "label": "native", "ID": "1008"}, {"sentence": "2 Sequential Labeling We discrimiNNly train a Markov model us- ing Bayes Point Machines (BPM).", "acronym": "NN", "label": "native", "ID": "1009"}, {"sentence": "We then prepared features, and fed the training data to a sequential labeling system, a discrimiNN Markov model much like Conditional Random Fields (CRF), with the difference being that the model parameters are tuned using Bayes Point Machines (BPM), and then compared our model against an equivalent CRF model.", "acronym": "NN", "label": "native", "ID": "1010"}, {"sentence": "The regression testing facilities of [incr tsdb()] allowed for rapid experimentation with alterNN analyses as new phenomena were brought into the grammar (cf.", "acronym": "NN", "label": "native", "ID": "1011"}, {"sentence": "NN for Pattern Recogni- tion, chapter 6.4: Modelling conditional distributions.", "acronym": "NN", "label": "Neural Networks", "ID": "1012"}, {"sentence": "Part of Speech Tagging with  NN.", "acronym": "NN", "label": "Neural Networks", "ID": "1013"}, {"sentence": "Second Turkish Symposium on  Artificial Intelligence and NN?,", "acronym": "NN", "label": "Neural Networks", "ID": "1014"}, {"sentence": "c?2014 Association for Computational Linguistics Learning Image Embeddings using Convolutional NN for Improved Multi-Modal Semantics Douwe Kiela ?", "acronym": "NN", "label": "Neural Networks", "ID": "1015"}, {"sentence": "In NN: Tricks of the Trade, pages 599?619.", "acronym": "NN", "label": "Neural Networks", "ID": "1016"}, {"sentence": "2016 Association for Computational Linguistics Globally Normalized Transition-Based NN Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov and Michael Collins ?", "acronym": "NN", "label": "Neural Networks", "ID": "1017"}, {"sentence": "This could be explained 7a + plural noun; to + past tense format; the more + the + base form of adjective 8NN + the 3rd person singular present format; have + past participle format + since by two reasons: (1) A sentence may contain sev- eral kinds of errors.", "acronym": "NN", "label": "singular or mass noun", "ID": "1018"}, {"sentence": "condition EMPTY are NNs  6b *  The expansion of branch 4d is one of the more interesting aspects of  the context-sensitive analysis since it involves a relative clause.", "acronym": "NN", "label": "singular or mass noun", "ID": "1019"}, {"sentence": "In fact, the only difference  from orthography alone is the way succeeding orthography can signal a discourse use  for a NN, and a sentential use for adverbs.", "acronym": "NN", "label": "singular or mass noun", "ID": "1020"}, {"sentence": "However, the interaction plot reveals that manhat- tan distance with RI is the best com- bination, outperforming the second best (cosine without dimensionality reduction) by a consider- able margin.", "acronym": "RI", "label": "random indexing", "ID": "1021"}, {"sentence": "Singu- lar value decomposition (parameter dimensional- ity reduction, figure 15) weakens the correlation values achieved by the models, but no significant difference is found between RI and the unreduced data.", "acronym": "RI", "label": "random indexing", "ID": "1022"}, {"sentence": "Examples of the first category are HiDeX (Shaoul and Westbury, 2010), a modern reimplementation of the HAL model, and Semantic Vectors (Widdows and Cohen, 2010), which enforces a RI representation in order to improve scalability.", "acronym": "RI", "label": "random indexing", "ID": "1023"}, {"sentence": "Interest- ingly, dimensionality reduction is found to neg- atively affect model performance: as shown in figure 7, both RI (ri) and singular 11Backward rank is equivalent to distance in this task.", "acronym": "RI", "label": "random indexing", "ID": "1024"}, {"sentence": "2009) or RI, for computing a distance matrix between a set of row vectors, and for the identification of the nearest neighbours of a given target term.", "acronym": "RI", "label": "random indexing", "ID": "1025"}, {"sentence": "Based on the partial effects of the indi- vidual parameters, any combination of manhattan or cosine distance with RI or no di- mensionality reduction should be close to optimal.", "acronym": "RI", "label": "random indexing", "ID": "1026"}, {"sentence": "To achieve that goal, we devel- oped a strategy based on RI and vec- tor permutations.", "acronym": "RI", "label": "Random Indexing", "ID": "1027"}, {"sentence": "D. Then, the method adopted to construct a semantic space that takes into account both syntactic dependencies and RI can be defined as follows: 1.", "acronym": "RI", "label": "Random Indexing", "ID": "1028"}, {"sentence": "Section 2 describes RI, the strategy for build- ing our WordSpace, while details about the method used to encode syntactic dependencies are reported in Section 3.", "acronym": "RI", "label": "Random Indexing", "ID": "1029"}, {"sentence": "Section 2 describes RI, the strategy for build- ing our WordSpace, while details about the method used to encode syntactic dependencies are report", "acronym": "RI", "label": "Random Indexing", "ID": "1030"}, {"sentence": "6 Conclusions In this work, we propose an approach to encode syn- tactic dependencies in WordSpace using vector per- mutations and RI.", "acronym": "RI", "label": "Random Indexing", "ID": "1031"}, {"sentence": "a context vector is assigned to each term, as de- scribed in Section 2 (RI); 2.", "acronym": "RI", "label": "Random Indexing", "ID": "1032"}, {"sentence": "We propose an approach based on vector permutation and RI to encode several syntactic contexts in a single WordSpace.", "acronym": "RI", "label": "Random Indexing", "ID": "1033"}, {"sentence": "BLANC: Implementing the RI for Coreference Evaluation.", "acronym": "RI", "label": "Rand Index", "ID": "1034"}, {"sentence": "For completeness, in Table 2 we report the results of evaluation against Gold PoS tags us- ing two metrics, Variation of Information (Meila, 2003) and Adjusted RI (Hubert & Arabie, 1985).", "acronym": "RI", "label": "Rand Index", "ID": "1035"}, {"sentence": "4.3 Qualitative Analysis Higher values measured with automated metrics such as log-likelihood on a held-out set and the cross-validation classification metric discussed here 246 0 10 20 30 40 50Word Error Rate0.0 0.1 0.2 0.3 0.4 0.5 ari 20_newsgroupsreuters21578enron (a) Adjusted RI results 0 10 20 30 40 50Word Error Rate2.0 2.5 3.0 3.5 4.0 4.5 vi 20_newsgroupsreuters21578enron (b) Variation of Information results (lower is better) Figure 5: Results for the clustering experiments on the three synthetic OCR datasets with TNPD feature selection.", "acronym": "RI", "label": "Rand Index", "ID": "1036"}, {"sentence": "However, this may be an artifact of the relatively poor starting performance for this dataset, a result of the fact that the gold standard labels do not align 245 0 5 10 15 20 25 30 35 40 45Word Error Rate0.0 0.1 0.2 0.3 0.4 0.5 ari 20_newsgroupsreuters21578enron (a) Adjusted RI results 0 5 10 15 20 25 30 35 40 45Word Error Rate2.0 2.5 3.0 3.5 4.0 4.5 vi 20_newsgroupsreuters21578enron (b) Variation of Information results (lower is better) Figure 4: Results for the clustering experiments on the three synthetic datasets without feature selection.", "acronym": "RI", "label": "Rand Index", "ID": "1037"}, {"sentence": "BLANC: Implementing the RI for coreference evalua- tion.", "acronym": "RI", "label": "Rand Index", "ID": "1038"}, {"sentence": "Clustering per- formance is measured on the Indo-European task according to the RI, F-score, Normalized Edit Score (Pantel, 2003) and Normalized Variation of Information (Meila, 2003).", "acronym": "RI", "label": "Rand Index", "ID": "1039"}, {"sentence": "For example, if we want to compare named entity  (NE) processing for a broadcast news source,  created via automatic speech recognition and NE  tagging, we need to compare it to data created by  careful HT and manual NE  tagging.. But the underlying texts--the  recogmzer output and the gold standard  transcription--differ, and the MUC algorithm  cannot be used.", "acronym": "HT", "label": "human transcription", "ID": "1040"}, {"sentence": "Fifth, we do not require HT - a labor-intensive step that hinders broader use in a clinical setting.", "acronym": "HT", "label": "human transcription", "ID": "1041"}, {"sentence": "The correlations decrease somewhat due to recog- nition errors when evaluated on the output of an automatic speech recognition system; how- ever, the additional use of word confidence scores can achieve correlations at a similar level as for HTs.", "acronym": "HT", "label": "human transcription", "ID": "1042"}, {"sentence": "We  tested on the HT (0% WER)  and the ASR (15% WER) versions of the 1998  evaluation transcripts.", "acronym": "HT", "label": "human transcription", "ID": "1043"}, {"sentence": "The tool also serves as a fast and systematic method of checking HT accuracy and thereby facilitates better methods of phonetic transcription (Cucchiarini, 1996; Shriberg, Hinke, & Trost-Steffen, 1987).", "acronym": "HT", "label": "human transcription", "ID": "1044"}, {"sentence": "training date Figure 1: Dependency between amount of training data and transcription automation rate less errors than a human being as it is trained on HTs?", "acronym": "HT", "label": "human transcription", "ID": "1045"}, {"sentence": "They are: retweet; HT; reply; link, if the tweet con- tains a link; punctuation (exclamation and ques- tions marks); emoticons (textual expression rep- resenting facial expressions); and upper cases (the number of words that starts with upper case in the tweet).", "acronym": "HT", "label": "hashtag", "ID": "1046"}, {"sentence": "Another measured how high-level tweet features (i.e., link, mention, and HT fre- quencies) vary across languages (Weerkamp et al 2011).", "acronym": "HT", "label": "hashtag", "ID": "1047"}, {"sentence": "e.g., k-top HTs) refers to the k most discriminating items of that type for each label (i.e., Male/Female).", "acronym": "HT", "label": "hashtag", "ID": "1048"}, {"sentence": "The features we employed were: k-top words, k-top digrams and trigrams, k-top HTs, k-top mentions, tweet/retweet/HT/link/mention frequencies, and out/in-neighborhood size.", "acronym": "HT", "label": "hashtag", "ID": "1049"}, {"sentence": "However, few of these studies have considered measures beyond simple HT fre- quencies, relative mention counts among politicians, and retweet counts.", "acronym": "HT", "label": "hashtag", "ID": "1050"}, {"sentence": "1 Introduction Since its early years, the CLs field has devoted much effort to the development of formal systems for modeling the syntax of nat- ural language.", "acronym": "CL", "label": "computational linguistic", "ID": "1051"}, {"sentence": "He has developed several machine learning based natural language  processing systems that are widely used in the CLs community and  in industry and has presented invited talks and tutorials in several major conferences.", "acronym": "CL", "label": "computational linguistic", "ID": "1052"}, {"sentence": "Generalizing the core hierarchy and libraries of the Matrix to sup- port languages like Wambaya can extend its typo- logical reach and further its development as an in- vestigation in CL typology.", "acronym": "CL", "label": "computational linguistic", "ID": "1053"}, {"sentence": "Most of the early work attempted to implement QA systems from the early per- spective of AI or CLs.", "acronym": "CL", "label": "computational linguistic", "ID": "1054"}, {"sentence": "During the 1970s and 1980s there was intensive research on the development of the- oretical bases for CLs.", "acronym": "CL", "label": "computational linguistic", "ID": "1055"}, {"sentence": "1 In t roduct ion   Attempts to the automatic identification of a struc-  ture in discourse have so far met with a limited  success in the CLs literature.", "acronym": "CL", "label": "computational linguistic", "ID": "1056"}, {"sentence": "tag gloss tag gloss *cd CL delimiter aj adjective *dd def.", "acronym": "CL", "label": "clause", "ID": "1057"}, {"sentence": "Lexical adverbs, including manner, time, and loca- tion, and adverbs of negation, which vary by CL type (declarative, imperative, or interrogative) ?", "acronym": "CL", "label": "clause", "ID": "1058"}, {"sentence": "Verbless CLs: nouns, adjectives, and adverbs, lexical or derived, functioning as predicates ?", "acronym": "CL", "label": "clause", "ID": "1059"}, {"sentence": "We specifically extract the character reference CH either from the dependency relation nsubj, which links a speech verb SV with a CH that is the syntactic subject of a CL, or from the dependency relation dobj, which links a SV with a CH that is the direct object of the speech verb, across a conjunct (e.g., and).", "acronym": "CL", "label": "clause", "ID": "1060"}, {"sentence": "and CLs expressing prior or simultaneous events ?", "acronym": "CL", "label": "clause", "ID": "1061"}, {"sentence": "Subordinate CLs: clausal complements of verbs like ?", "acronym": "CL", "label": "clause", "ID": "1062"}, {"sentence": "That is, aside from the constraint that verbal CLs require a clitic cluster (marking subject and object agreement and tense, aspect and mood) in second position, the word order is otherwise free, to the point that noun phrases can be non-contiguous, with head nouns and their modifiers separated by un- related words.", "acronym": "CL", "label": "clause", "ID": "1063"}, {"sentence": "non-finite subor- dinate CLs such as purposives (?", "acronym": "CL", "label": "clause", "ID": "1064"}, {"sentence": "The representation for sentence 1 states that the first element of the 5-gram (-3; third word to the left of the adjective) is empty (because the second element is a phrase boundary marker), that the sec- ond element is a CL delimiter (conjunction that), the third one (-1; word preceding the adjective) is a definite determiner, and the fourth one (+1; word following the adjective) is a common noun.", "acronym": "CL", "label": "clause", "ID": "1065"}, {"sentence": "Validating the Cover- age of Lexical Resources for Affect Analysis and Au- tomatically CLifying New Words along Semantic Axes.", "acronym": "CL", "label": "Class", "ID": "1066"}, {"sentence": "2 CLification and Hypothesis As mentioned above, the semantic classification of adjectives is not settled in theoretical linguistics.", "acronym": "CL", "label": "Class", "ID": "1067"}, {"sentence": "3.2 Graph-based CLifier The cohesion graph based classifier compares the cohesion graph connectivity of the discourse in- cluding the MWE component words with the con- nectivity of the discourse excluding the MWE component words to check how well the MWE component words are semantically connected to the context.", "acronym": "CL", "label": "Class", "ID": "1068"}, {"sentence": "CLification is to choose y such that y = argmaxy?(w>?(x,y?)).", "acronym": "CL", "label": "Class", "ID": "1069"}, {"sentence": "CLifying latent user attributes in twitter.", "acronym": "CL", "label": "Class", "ID": "1070"}, {"sentence": "701,3   Acquisition of Semantic CLes for Adjectives from Distributional Evidence Gemma Boleda GLiCom Universitat Pompeu Fabra La Rambla 30-32 08002 Barcelona gemma.boleda@upf.edu Toni Badia GLiCom Universitat Pompeu Fabra La Rambla 30-32 08002 Barcelona toni.badia@upf.edu Eloi Batlle Audiovisual Institute Universitat Pompeu Fabra Pg.", "acronym": "CL", "label": "Class", "ID": "1071"}, {"sentence": "Finally, we introduced our graph- based CL for distinguishing literal and non- literal use of MWEs.", "acronym": "CL", "label": "classifier", "ID": "1072"}, {"sentence": "While there are other large combination features such as ones involving F 4, F 9, F 12, F 15 and F 19, we find that they do help improving the performance of the CL.", "acronym": "CL", "label": "classifier", "ID": "1073"}, {"sentence": "As increasing the number of perceptrons re- sults in more thorough exploration of the version space V (D), we expect that the performance of the CL would improve as K increases.", "acronym": "CL", "label": "classifier", "ID": "1074"}, {"sentence": "They report an average accuracy of 72% for their canonical form (CForm) CL.", "acronym": "CL", "label": "classifier", "ID": "1075"}, {"sentence": "We also evaluated thispenalized version, varying the trade-off parameter C. Bayes Point Machines (BPM) for structured prediction (Corston-Oliver et al, 2006) is an en- semble learning algorithm that attempts to set the weight w to be the Bayes Point which approxi- mates to Bayesian inference for linear CLs.", "acronym": "CL", "label": "classifier", "ID": "1076"}, {"sentence": "We therefore integrate a data-driven CL for the special task of PP attachment into an existing rule- based parser and measure the effect that the addi- tional information has on the overall accuracy.", "acronym": "CL", "label": "classifier", "ID": "1077"}, {"sentence": "Perceptron TP FP FN P (%) R (%) F1 (%) 671 128 119 83.98 84.94 84.46 Conditional Random Fields TP FP FN P (%) R (%) F1 (%) 643 78 147 89.18 81.39 85.11 Bayes Point Machines TP FP FN P (%) R (%) F1 (%) 647 79 143 89.12 81.90 85.36 Table 4: Performance of different optimization strategies 6 Conclusion To tackle the hedge cue detection problem posed by the CoNLL-2010 shared task, we utilized a CL for sequential labeling following previ- ous work (Morante and Daelemans, 2009).", "acronym": "CL", "label": "classifier", "ID": "1078"}, {"sentence": "Therefore we can- not calculate CL by simply taking the dis- tance between the first and last points in the chain.", "acronym": "CL", "label": "chain length", "ID": "1079"}, {"sentence": "The coreferential CL of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution sys- tems (Iida et al, 2003; Mitkov, 1998; Paul et al.,", "acronym": "CL", "label": "chain length", "ID": "1080"}, {"sentence": "E.g., the non-parallel bitext map in Figure 1, which was created without the CL param- eter, has on average 630", "acronym": "CL", "label": "chain length", "ID": "1081"}, {"sentence": "In contrast, running SIMR on the same pair of non- parallel documents with a maximum CL of 700 yielded only 22 points of correspondence, or 3032 characters between points on average.", "acronym": "CL", "label": "chain length", "ID": "1082"}, {"sentence": "E.g., the non-parallel bitext map in Figure 1, which was created without the CL param- eter, has on average 630 characters between points.", "acronym": "CL", "label": "chain length", "ID": "1083"}, {"sentence": "Therefore, in parallel texts SIMR will find many chains and limiting the CL will have a minimal effect on the number of chains SIMR will find.", "acronym": "CL", "label": "chain length", "ID": "1084"}, {"sentence": "SIMR-cl uses the standard SIMR parameters plus the additional CL parameter discussed above.", "acronym": "CL", "label": "chain length", "ID": "1085"}, {"sentence": "SIMR will find many chains and limiting the CL will have a minimal effect on the number of chains SIMR will find.", "acronym": "CL", "label": "chain length", "ID": "1086"}, {"sentence": "More detailed documentation includes lower-level design decisions, such as the reasons for the CL or data structures.", "acronym": "CL", "label": "chosen algorithms", "ID": "1087"}, {"sentence": "Habash et al(2009)  proposed methods to tackle the Arabic enCL.", "acronym": "CL", "label": "clitics", "ID": "1088"}, {"sentence": "In addition to these  simple sentences, difficult problems are also han-  dled: CL, complex determiners, completives, var-  ious forms of questions, extraction and non limited  dependancies, coordinations, comparatives.", "acronym": "CL", "label": "clitics", "ID": "1089"}, {"sentence": "There are also grammatical rules, such as those  concerning the positions of the verb (e.g., in the \"second position\" in German), of the  adjective or another modifier before or after the head noun in a noun group, and of  CL.", "acronym": "CL", "label": "clitics", "ID": "1090"}, {"sentence": "the syntactic structure that hosts the verb, the  patterns of verbal inflections for every instance  of verb use (e.g. subject number, person, and  gender, as well as other morphosyntactic aspects  of the Arabic verb), the semantic properties of  other components of the construction (e.g. se- mantic properties of the subject), as well as the  inclusion or exclusion of phrases, lexical items,  or CL denoting a starting point of the event  (SOURCE), a terminal point of the event (GOAL),  etc.", "acronym": "CL", "label": "clitics", "ID": "1091"}, {"sentence": "The arc list can  be conceptually divided in two parts: one contains  the stems of the verbs, nouns and adjectives; the  other contains a number of sub-lexicons that provide  the endings for these lexical categories as well as the  CL?", "acronym": "CL", "label": "clitics", "ID": "1092"}, {"sentence": "E.g., the French pronominal CL can be treated in this way.", "acronym": "CL", "label": "clitics", "ID": "1093"}, {"sentence": "3.1 Gloss~me - -  A Closed Subsystem of  English  A dictionary is a CL paraphrasing system of nat-  ural language.", "acronym": "CL", "label": "closed", "ID": "1094"}, {"sentence": "Gloss~me is a CL subsystem of English.", "acronym": "CL", "label": "closed", "ID": "1095"}, {"sentence": "This is also evi- dent by examination of average lexeme sentiment of top loaded terms of each vector, not disCL in the paper.", "acronym": "CL", "label": "closed", "ID": "1096"}, {"sentence": "Appendix A. Structure of Paradigme  w Mapping Gloss~me onto Paradigme  The semantic network Paradigme is systematically  constructed from the small and CL English dictio-  nary Glossdme.", "acronym": "CL", "label": "closed", "ID": "1097"}, {"sentence": "In case of a single constraint, this pro- gram has a CL-form solution.", "acronym": "CL", "label": "closed", "ID": "1098"}, {"sentence": "We adopted Longman Dictionary of Contemporary  English (LDOCE) \\[1987\\] assuch a CL system of  English.", "acronym": "CL", "label": "closed", "ID": "1099"}, {"sentence": "1 Introduction Since its early years, the CL field has devoted much effort to the development of formal systems for modeling the syntax of nat- ural language.", "acronym": "CL", "label": "computational linguistics", "ID": "1100"}, {"sentence": "He has developed several machine learning based natural language  processing systems that are widely used in the CL community and  in industry and has presented invited talks and tutorials in several major conferences.", "acronym": "CL", "label": "computational linguistics", "ID": "1101"}, {"sentence": "This account contributes to the  discipline of CL in labeling  prepositions in Farsi, as this area of preposition  labeling has been very challenging.", "acronym": "CL", "label": "computational linguistics", "ID": "1102"}, {"sentence": "Most of the early work attempted to implement QA systems from the early per- spective of AI or CL.", "acronym": "CL", "label": "computational linguistics", "ID": "1103"}, {"sentence": "During the 1970s and 1980s there was intensive research on the development of the- oretical bases for CL.", "acronym": "CL", "label": "computational linguistics", "ID": "1104"}, {"sentence": "1 In t roduct ion   Attempts to the automatic identification of a struc-  ture in discourse have so far met with a limited  success in the CL literature.", "acronym": "CL", "label": "computational linguistics", "ID": "1105"}, {"sentence": "mplates  * Here and elsewhere, I use the following ARPABET-Iike substitutions for the  standard phonetic symbols:  \\[I\\] = high front lax vowel  \\[E\\] = mid front lax vowel  \\[ae\\] = low front vowel  \\[U\\] = high back lax vowel  \\[0\\] = low-mld back lax vowel  \\[uh\\] = mid-central or reduced vowel (\"carrot\" or schwa)  \\[R\\] = rhotaclzed mid-central vowel (i.e., syllabic \\[r\\])  \\[sh\\] = voiceless AP stop  \\[zh\\] = voiced AP stop  \\[dh\\] = voiced interdental fricative  \\[th\\] = voiceless interdental fricative  \\[D\\] = flap  14  for each variant pronunciation or by listing alternate paths in an allophonlc-  segment-based HHM model (Kopec and Bush 1985).", "acronym": "AP", "label": "alveopalatal", "ID": "1106"}, {"sentence": "The only difference between our symbols  and the ones used by IPA are in voiceless and  voiced AP fricatives [S] and [Z], the  voiceless and voiced affricates [C] and [J], and the  palatal glide [y].", "acronym": "AP", "label": "alveopalatal", "ID": "1107"}, {"sentence": "e the following ARPABET-Iike substitutions for the  standard phonetic symbols:  \\[I\\] = high front lax vowel  \\[E\\] = mid front lax vowel  \\[ae\\] = low front vowel  \\[U\\] = high back lax vowel  \\[0\\] = low-mld back lax vowel  \\[uh\\] = mid-central or reduced vowel (\"carrot\" or schwa)  \\[R\\] = rhotaclzed mid-central vowel (i.e., syllabic \\[r\\])  \\[sh\\] = voiceless AP stop  \\[zh\\] = voiced AP stop  \\[dh\\] = voiced interdental fricative  \\[th\\] = voiceless interdental fricative  \\[D\\] = flap  14  for each variant pronunciation or by listing alternate paths in an allophonlc-  segment-based HHM model (Kopec and Bush 1985).", "acronym": "AP", "label": "alveopalatal", "ID": "1108"}, {"sentence": "high tone,  W long syllable, ~ rhythm break, ~ voiced retro-  flex AP fricative, ~ retroflex flap, cuV  labiovelar stop.", "acronym": "AP", "label": "alveopalatal", "ID": "1109"}, {"sentence": "verb+pAP combinations such as break in on;  ?", "acronym": "AP", "label": "article+preposition", "ID": "1110"}, {"sentence": "In the AP, the val- ues of every coefficient are added up at each up- date, which happens (possibly) at each training sentence, and their arithmetic average is used in- stead.9 This trick makes the algorithm more re- sistant to weight oscillations during training (or, more precisely, at the end of it) and as a result, it substantially improves its performance.10 7And lemmas, which are th", "acronym": "AP", "label": "averaged perceptron", "ID": "1111"}, {"sentence": "It is a reimplementation of the AP de- scribed in (Collins, 2002), which uses such fea- tures that it behaves like an HMM tagger and thus the standard Viterbi decoding is possible.", "acronym": "AP", "label": "averaged perceptron", "ID": "1112"}, {"sentence": "4 The perceptron feature sets The AP?s accuracy is determined (to a large extent) by the set of features used.", "acronym": "AP", "label": "averaged perceptron", "ID": "1113"}, {"sentence": "Morphological tagging based on AP.", "acronym": "AP", "label": "averaged perceptron", "ID": "1114"}, {"sentence": "Because we find that the AP significantly outperforms L 1 SVM.", "acronym": "AP", "label": "averaged perceptron", "ID": "1115"}, {"sentence": "The transition and out- put scores for the candidate tags are based on a large number of binary-valued features and their weights, which are determined during iterative training by the AP algorithm.", "acronym": "AP", "label": "averaged perceptron", "ID": "1116"}, {"sentence": "The model makes use of the AP algorithm (Freund and Schapire, 1996) and is trained on the training data of the shared task with rich features.", "acronym": "AP", "label": "Averaged Perceptron", "ID": "1117"}, {"sentence": "There has been a lot of work on correcting preposition and determiner errors, where discriminative models such as Maximum Entropy and AP (De Felice and Pulman, 2008; Rozovskaya and Roth, 2011) and/or probablistic language models (Gamon, 2010) are generally used.", "acronym": "AP", "label": "Averaged Perceptron", "ID": "1118"}, {"sentence": "Morphological Tagging Based on AP.", "acronym": "AP", "label": "Averaged Perceptron", "ID": "1119"}, {"sentence": "AP Algorithm  5 Experiments  We evaluate our method on both Chinese and  English syntactic parsing task with the standard  division on Chinese Penn Treebank Version 5.0  and WSJ English Treebank 3.0 (Marcus et al  1993) as shown in Table 1.", "acronym": "AP", "label": "Averaged Perceptron", "ID": "1120"}, {"sentence": "Simulated Annealing Algorithm  4.2 AP  Another algorithm we apply is the averaged per- ceptron algorithm.", "acronym": "AP", "label": "Averaged Perceptron", "ID": "1121"}, {"sentence": "c?2009 Association for Computational Linguistics Semi-supervised Training for the AP POS Tagger Drahom??ra ?", "acronym": "AP", "label": "Averaged Perceptron", "ID": "1122"}, {"sentence": "Predicting character-AP voices for a TTS- based storyteller system.", "acronym": "AP", "label": "appropriate", "ID": "1123"}, {"sentence": "It is, therefore, saliz and AP  to delete it.", "acronym": "AP", "label": "appropriate", "ID": "1124"}, {"sentence": "2006) have shown that assign- ing an AP voice for a character in a digi- tal storyteller system is significant for understand- ing a story, perceiving affective content, perceiv- ing the voice as credible, and overall listener sat- isfaction.", "acronym": "AP", "label": "appropriate", "ID": "1125"}, {"sentence": "If the arc points to a state with a  delined harmony value, the hamlony value of the better  palh is retained by that state, and its position ill tile sorted  list adjusted APly.", "acronym": "AP", "label": "appropriate", "ID": "1126"}, {"sentence": "2012) have shown that the APness of the voice assigned to a syn- thetic character is strongly related to knowing the gender, age and other salient personality attributes of the character.", "acronym": "AP", "label": "appropriate", "ID": "1127"}, {"sentence": "2006), gen- erate dialogs uttered by different characters using synthetic voices AP for each character?s gender, age and personality (Greene et al.,", "acronym": "AP", "label": "appropriate", "ID": "1128"}, {"sentence": "2012) have shown that the APness of the voice assigned to a syn- thetic character is strongly related to knowing the gender, age and other salient personality at", "acronym": "AP", "label": "appropriate", "ID": "1129"}, {"sentence": "It contains analogs of our Codomain  AP, mappings and base orders.", "acronym": "AP", "label": "Agreement Principle", "ID": "1130"}, {"sentence": "Compat ib le  mapp ings  are those  that allow the Codomain AP to  be satisfied, and they are easily identified by ex-  amining the base order component of the MTrans  rules being used.", "acronym": "AP", "label": "Agreement Principle", "ID": "1131"}, {"sentence": "One of the language-universal gram-  matical principles (the Control AP)  requires that the semantic controller of a controlled  complement always be the next grammatical relation  (in the order specified by the value of the SUBCAT  feature of the head) after the controlled complement  to combine with the head.", "acronym": "AP", "label": "Agreement Principle", "ID": "1132"}, {"sentence": "We call this  the Codomain AP, and it is  through this principle that pairability constraints  are enforced.", "acronym": "AP", "label": "Agreement Principle", "ID": "1133"}, {"sentence": "167  \\]Feature Pass ing   The theory of HPSG embodies a number of sub-  stantive hypotheses about universal granunatical prin-  ciples, Such principles as the Head Feature Princi-  ple, the Binding Inheritance Principle, and the Con-  trol AP, require that certain syntac-  tic features specified on daughters in syntactic trees are  inherited by the mothers.", "acronym": "AP", "label": "Agreement Principle", "ID": "1134"}, {"sentence": "This additional  information is used to enforce the Codomain  AP and to help in the user inter-  action described in Section 3.5.", "acronym": "AP", "label": "Agreement Principle", "ID": "1135"}, {"sentence": "AP: Given a post P i in a thread, the absolute position of post P i is i Relative Position: Given a post P i in a thread with n posts, the relative position of P i is i/n We construct the CSN corpus by randomly sampling 3,000 threads from CSN between 2000 and 2010.", "acronym": "AP", "label": "Absolute Position", "ID": "1136"}, {"sentence": "So the operat ion  of a compounding rule is enabled only i f  a  c h a r a c t e r i s t i c  aspect  is associa ted  with t he  verb; i n  English,  this is usua l ly  indicated  b y  a n  adverb o r  an AP.", "acronym": "AP", "label": "adverbial phrase", "ID": "1137"}, {"sentence": "Some annotators (but not all) seemed to decide that any AP (ADVP) headed by an ?", "acronym": "AP", "label": "adverbial phrase", "ID": "1138"}, {"sentence": "Systems that  ignore this and begin with units that are inevitably  realized as kernel clauses under-utilize the expressive  power of natural language, which can use complex noun  phrases, nominalizations, APs, and other  adjuncts to pack information from multiple units into  one clause.", "acronym": "AP", "label": "adverbial phrase", "ID": "1139"}, {"sentence": "There is  normal ly  no more than two  APs before or after  the nominal.", "acronym": "AP", "label": "adverbial phrase", "ID": "1140"}, {"sentence": "a red large ball\" and the typical ordering  of temporal before spatial APs in German.", "acronym": "AP", "label": "adverbial phrase", "ID": "1141"}, {"sentence": "The matrix phrase and the AP have IP-MAT and IP-ADV tags respectively.", "acronym": "AP", "label": "adverbial phrase", "ID": "1142"}, {"sentence": "Sec-  tion 2 presents the flmctionality of several knowledge  processing modules and describes tile NLP tech-  niques for question and AP.", "acronym": "AP", "label": "answer processing", "ID": "1143"}, {"sentence": "We have sepm'ately measured the effect  of tile integration of the knowledge-based methods  at question processing and AP level.", "acronym": "AP", "label": "answer processing", "ID": "1144"}, {"sentence": "An im-  pressive improvenmnt of 14% is achieved when more  knowledge-intensive NLP techniques are ai)plied a.t  both question and AP level.", "acronym": "AP", "label": "answer processing", "ID": "1145"}, {"sentence": "In the rest of the paper we will shortly over- view the components of such a framework and  will describe the relevant aspects of the solution  offered for each of them, aspects that should ac- count for a large variety of question types,  document collections and AP  techniques, as well as for several languages.", "acronym": "AP", "label": "answer processing", "ID": "1146"}, {"sentence": "Our work suggests that considerable gains in performance can be obtained by incorporating TE during both AP and passage re- trieval.", "acronym": "AP", "label": "answer processing", "ID": "1147"}, {"sentence": "1 Introduction  Different projects, different evaluation forums,  different tasks, different languages, different  document collections, different question types,  different AP strategies ?", "acronym": "AP", "label": "answer processing", "ID": "1148"}, {"sentence": "We introduce a new inventory of preposition relations that covers the 34 prepositions that formed the basis of the SemEval 2007 task of PSD.", "acronym": "PSD", "label": "preposition sense disambiguation", "ID": "1149"}, {"sentence": "In addition to prior work on prepositional phrase attachment, a highly re- lated problem is PSD (Hovy et al, 2011; Srikumar and Roth, 2013).", "acronym": "PSD", "label": "preposition sense disambiguation", "ID": "1150"}, {"sentence": "3.1 Preposition Relation Inventory We build our relation inventory using the sense an- notation in the Preposition Project, focusing on the 34 prepositions3 annotated for the SemEval-2007 shared task of PSD.", "acronym": "PSD", "label": "preposition sense disambiguation", "ID": "1151"}, {"sentence": "6 Conclusion We showed that using a number of simple linguis- tically motivated features can improve the accu- racy of PSD.", "acronym": "PSD", "label": "preposition sense disambiguation", "ID": "1152"}, {"sentence": "2.3 Classifier Training We chose maximum entropy (Berger et al, 1996) as our primary classifier, since it had been successfully applied by the highest performing systems in both the SemEval-2007 PSD task (Ye and Baldwin, 2007) and the general word sense disambiguation task (Tratz et al, 2007).", "acronym": "PSD", "label": "preposition sense disambiguation", "ID": "1153"}, {"sentence": "The SemEval data provides a natural method for comparing the per- formance of PSD sys- tems.", "acronym": "PSD", "label": "preposition sense disambiguation", "ID": "1154"}, {"sentence": "For example, Carpuat and Wu (2007a) extend word sense dis- ambiguation to PSD and show improved performance due to the better fit with multiple possible segmentations in a phrase- based system.", "acronym": "PSD", "label": "phrase sense disambiguation", "ID": "1155"}, {"sentence": "To summarize Experiment I, which is a vari- ant of a supervised PSD task, demonstrates that we can use LSA to distin- guish between literal and the idiomatic usage of an MWE by using local linguistic context.", "acronym": "PSD", "label": "phrase sense disambiguation", "ID": "1156"}, {"sentence": "We operate un- der the framework of PSD (Carpuat and Wu, 2007), in which we take au- tomatically align parallel data in an old domain to generate an initial old-domain sense inventory.", "acronym": "PSD", "label": "phrase sense disambiguation", "ID": "1157"}, {"sentence": "Approaches that have been tried for SMT model adaptation include mixture models, transductive learning, data selection, data weighting, and PSD.", "acronym": "PSD", "label": "phrase sense disambiguation", "ID": "1158"}, {"sentence": "How PSD outperforms word sense disambiguation for statistical machine translation.", "acronym": "PSD", "label": "phrase sense disambiguation", "ID": "1159"}, {"sentence": "Thus, the scripts are functioning as a rep- resentation language for lexical ambiguity like the  semantic PSD model for  SMT (Carpuat and Wu, 2007).", "acronym": "PSD", "label": "phrase sense disambiguation", "ID": "1160"}, {"sentence": "Melb-yb: PSD using rich seman- tic features.", "acronym": "PSD", "label": "Preposition sense disambiguation", "ID": "1161"}, {"sentence": "PSD was one of the SemEval 2007 tasks (Litkowski and Hargraves, 2007), and was subsequently explored in a number of papers using supervised approaches: O?Hara and Wiebe (2009) present a supervised preposition sense disambiguation approach which explores different settings; Tratz and Hovy (2009), Hovy et al (2010) make explicit use of the arguments for preposition sense", "acronym": "PSD", "label": "Preposition sense disambiguation", "ID": "1162"}, {"sentence": "ECt Candidates set could be any  combination of the translation of words appearing  in Chinese context.", "acronym": "EC", "label": "English Contex", "ID": "1163"}, {"sentence": "430   Back Transliteration from Japanese to English  Using Target ECt   Isao Goto?,", "acronym": "EC", "label": "English Contex", "ID": "1164"}, {"sentence": "ECt Candidates set could be  any combination of translations and each  combination could be selected as the English context.", "acronym": "EC", "label": "English Contex", "ID": "1165"}, {"sentence": "c?2013 Association for Computational Linguistics Gender Inference of Twitter Users in Non-ECts Morgane Ciot School of Computer Science McGill University Montreal, Quebec, Canada morgane.ciot@mail.mcgill.ca Morgan Sonderegger Department of Linguistics McGill University Montreal, Quebec, Canada morgan.sonderegger@mcgill.ca Derek Ruths School of Computer Science McGill University Montreal, Quebec, Canada derek.ruths@mcgill.ca Abstract While much work has considered the problem", "acronym": "EC", "label": "English Contex", "ID": "1166"}, {"sentence": "ECt Candidates set is the  translations set of the Chinese context.", "acronym": "EC", "label": "English Contex", "ID": "1167"}, {"sentence": "f   Figure 2: IITK-Roman to EC mapping    A (??)?", "acronym": "EC", "label": "English consonant", "ID": "1168"}, {"sentence": "Klatt, D.H. (1968) 'Structure of confusions in short-term memory between  ECs' J. Acoust.", "acronym": "EC", "label": "English consonant", "ID": "1169"}, {"sentence": "2, 401-7,  Miller, G.A. and Nicely, P.E. (1955) 'An analysis of perceptusl con-  fusions among same ECs' J. Acoumt.", "acronym": "EC", "label": "English consonant", "ID": "1170"}, {"sentence": "We constructed a table relating He- brew and ECs, based on common knowledge patterns that relate sound to spelling in both languages.", "acronym": "EC", "label": "English consonant", "ID": "1171"}, {"sentence": "Wickelgren, W.A. (1966) 'Distinctive features snd errors in short-term  memory for ECs' J. Acoust.", "acronym": "EC", "label": "English consonant", "ID": "1172"}, {"sentence": "A Korean vowel - (eu) is most commonly inserted between two ECs in transliteration.", "acronym": "EC", "label": "English consonant", "ID": "1173"}, {"sentence": "for Computational Linguistics Building bilingual lexicon to create Dialect Tunisian corpora and  adapt language model      Rahma Boujelbane  Miracl Laboratory, ANLP Research  Group, University of Sfax, Tunisia  Rahma.boujelbane@gmail.com  Mariem Ellouze khemekhem  Miracl Laboratory, ANLP Research  Group, University of Sfax, Tunisia  mariem.ellouze@planet.com             Siwar BenAyed  Faculty of EC and Management  of Sfax  siwar.ben.ayed@gmail.com      Lamia Hadrich Belguith  Miracl Laboratory, ANLP Research  Group, University of Sfax, Tunisia  l.belguith@fsegs.rnu.tn    Abstract  Since the Tunisian revolution, Tunisian Dialect (TD)  used in daily life, has became progressively used and  represented in interviews, news and debate programs  instead of Modern Standard Arabic (MSA).", "acronym": "EC", "label": "Economics", "ID": "1174"}, {"sentence": "Machinc-Lc~irs~abic, Non-Compositional Se-  maolic.~ fiJr Domain Independent Speech or Text  Recognition to appear in Proceedings of 2nd Hellenic-  European Conference on Mathematics and Informat-  ies (HERMIS), Athens University of EC and  Business.", "acronym": "EC", "label": "Economics", "ID": "1175"}, {"sentence": "EC Business administration, national economics, finance and accounting, trade, mar- keting and public relations, and insurance.", "acronym": "EC", "label": "Economics", "ID": "1176"}, {"sentence": "EC of Education Review, 30:682?690.", "acronym": "EC", "label": "Economics", "ID": "1177"}, {"sentence": "EC) ?", "acronym": "EC", "label": "Economics", "ID": "1178"}, {"sentence": "stling Department of Linguistics Stockholm University SE-106 91 Stockholm robert@ling.su.se Andre Smolentzov Department of Linguistics Stockholm University SE-106 91 Stockholm asmolentzov@gmail.com Bj?rn Tyrefors Hinnerich Department of EC Stockholm University SE-106 91 Stockholm bjorn.hinnerich@ne.su.se Erik H?glin National Institute of Economic Research Kungsgatan 12-14 103 62 Stockholm erik.hoglin@konj.se Abstract We present the first system developed for auto- mated grading of high school essays written in Swedish.", "acronym": "EC", "label": "Economics", "ID": "1179"}, {"sentence": "EC flourish in free markets?.", "acronym": "EC", "label": "Economies", "ID": "1180"}, {"sentence": "The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Col- lective Wisdom Shapes Business, EC, Societies and Nations.", "acronym": "EC", "label": "Economies", "ID": "1181"}, {"sentence": "To get the amount of user effort into context they should be measured against the corresponding error ratios of compara- ble non-interactive systems: Word Error Rate for WSR and CER for KSMR.", "acronym": "CER", "label": "Character Error Rate", "ID": "1182"}, {"sentence": "People Daily(96) Stories modified Kneser-Ney 5.82% 14.48% Static PPM 6.00% 16.55% Adaptive PPM 4.98% 14.24% Table 3: CERs for Kneser-Ney, Static and Adaptive PPM 5 Conclusion We have introduced a method for Pinyin input based on an adaptive PPM model.", "acronym": "CER", "label": "Character Error Rate", "ID": "1183"}, {"sentence": "Our research question is closely related to this indirect prediction of the CER hypothesis.", "acronym": "CER", "label": "Constant Entropy Rate", "ID": "1184"}, {"sentence": "Our experiments showed  substantial improvement on the translitera- tion accuracy over a state-of-the-art baseline  system, significantly reducing the  transliteration CER from  50.29% to 12.84%.", "acronym": "CER", "label": "character error rate", "ID": "1185"}, {"sentence": "The three ver-  sions of the documents were the original documents,  the documents that resulted after the originals were  subjected to an optical character recognition (OCR)  process with a CER of approximately  5%, and the documents produced through OCR with  a 20% error rate (caused by down-sampling the im-  age before doing the OCR).", "acronym": "CER", "label": "character error rate", "ID": "1186"}, {"sentence": "We report two types of error rates: word  error rate and CER.", "acronym": "CER", "label": "character error rate", "ID": "1187"}, {"sentence": "The following Table 3 shows the results in terms of CER.", "acronym": "CER", "label": "character error rate", "ID": "1188"}, {"sentence": "A detailed study of Table 3 tells us that the re- duction rate of CER (\u0006\u0004\u0004\f\u000b recall) of Model \u0011 in the target domain (9.36%) is much larger than that in the general domain (3.37%).", "acronym": "CER", "label": "character error rate", "ID": "1189"}, {"sentence": "The CER is the sum of  deletion, insertion and substitution errors.", "acronym": "CER", "label": "character error rate", "ID": "1190"}, {"sentence": "In the last step, candidate terms selected this way are tested for their senti- ment strength and PP (in other words, how positive or negative are the conotations).", "acronym": "PP", "label": "polarity", "ID": "1191"}, {"sentence": "sentiment PP: select words with the highest absolute human scores.", "acronym": "PP", "label": "polarity", "ID": "1192"}, {"sentence": "For the task of as- signing evaluative PP, it is computed as num- ber of co-occurrences of candidate words with each of the paradigm positive and negative words, denoted as pw and nw.", "acronym": "PP", "label": "polarity", "ID": "1193"}, {"sentence": "The method we followed could be substituted by any other technique which results in a set of highly sentimental lexemes, pos- sibly of varying unknown PP and strength.", "acronym": "PP", "label": "polarity", "ID": "1194"}, {"sentence": "Ide- ally, such a scoring should not only inform about PP (indication whether a word is positive or negative), but also about association strength (the degree of positivity or negativity).", "acronym": "PP", "label": "polarity", "ID": "1195"}, {"sentence": "1 Introduction This paper seeks to improve one of the main meth- ods of unsupervised lexeme sentiment PP as- signment.", "acronym": "PP", "label": "polarity", "ID": "1196"}, {"sentence": "Since no further processing of text has  been performed, another source of problems is  the detection of boundaries of frozen parts in  PPs (e.g. na osnovu (Engl.", "acronym": "PP", "label": "prepositional phrase", "ID": "1197"}, {"sentence": "Statistical models for unsu- pervised PP attachment.", "acronym": "PP", "label": "prepositional phrase", "ID": "1198"}, {"sentence": "Some believe they are  compounds without analyzing them (Mashkur  (1346), Khatib Rahbar (1367), Gharib (1371),  Meshkatodini (1366)) and still some have  defined them as PPs in one way  or another (Gholam Alizade (1371), Samiian  (1983)).", "acronym": "PP", "label": "prepositional phrase", "ID": "1199"}, {"sentence": "In addition, the first child following the head of a  PP is marked as a complement.", "acronym": "PP", "label": "prepositional phrase", "ID": "1200"}, {"sentence": "Also, the PP itself is correctly predicted.", "acronym": "PP", "label": "prepositional phrase", "ID": "1201"}, {"sentence": "An unsupervised approach to PP attachment using contextu- ally similar words.", "acronym": "PP", "label": "prepositional phrase", "ID": "1202"}, {"sentence": "Our corpus consists of the 45 news articles from the Agence France Press used in the training and test sets described by (PP et al, 2008).", "acronym": "PP", "label": "Parent", "ID": "1203"}, {"sentence": "This approach differs from that of (PP et al, 2008) in that it relies almost en- tirely on lexical processing.", "acronym": "PP", "label": "Parent", "ID": "1204"}, {"sentence": "2 A System for TimeML Annotation in French (PP et al, 2008) provide the description and evaluation of a system for the TimeML annota- tion of events and temporal expressions in French texts.", "acronym": "PP", "label": "Parent", "ID": "1205"}, {"sentence": "temporal noun) NNF Formal noun (general) NNFV Formal noun (adverbial) PX Prefix SX Suffix NUM Numeral CL Classifier VB Verb ADJ Adjective ADNOM Adnominal adjective ADV Adverb PCS Case particle PBD Binding particle PADN Adnominal particle PCO Parallel particle PCJ Conjunctive particle PEND Sentence-ending particle P Particle (others) AUX Auxiliary verb CONJ Conjunction PNC Punctuation PAR PPhesis SYM Symbol FIL Filler Table 1: Preterminal tags automatically converted from dependency structure to phrase structure by the previously described method (Uematsu et al 2013), and conversion er- rors of structures and tags were manually corrected.", "acronym": "PP", "label": "Parent", "ID": "1206"}, {"sentence": "respectively, re- quiring 1) PP-Sibling to recognize that ?", "acronym": "PP", "label": "Parent", "ID": "1207"}, {"sentence": "PPhesis.", "acronym": "PP", "label": "Parent", "ID": "1208"}, {"sentence": "3 Annotation Modules In this section, we describe an annotation system, similar to that of (PP et al, 2008) described above, although based on a rich cascade of finite state transducers and a shallow syntactic analysis, as opposed to a full dependency parse.", "acronym": "PP", "label": "Parent", "ID": "1209"}, {"sentence": "SemEval 2010 Task 8 (Hendrickx et al, 2008) provides a new standard benchmark for semantic relation classification to a wider community, where it defines 9 relations includ- ing CAUSE-EFFECT, COMPONENT-WHOLE, CONTENT-CONTAINER, ENTITY-DESTINATION, ENTITY-ORIGIN, INSTRUMENT-AGENCY, MEMBER-COLLECTION, MESSAGE-TOPIC, PP, and a tenth pseudo- relation OTHER (where relation is not one of the 9 annotated relations).", "acronym": "PP", "label": "PRODUCT-PRODUCER", "ID": "1210"}, {"sentence": "PP The boy who made the threat was arrested, charged, and had items confiscated from his home.", "acronym": "PP", "label": "PRODUCT-PRODUCER", "ID": "1211"}, {"sentence": "Seven relations were used in the task: CAUSE-EFFECT, INSTRUMENT-AGENCY, PP, ORIGIN-ENTITY, THEME- TOOL, PART-WHOLE and CONTENT-CONTAINER.", "acronym": "PP", "label": "PRODUCT-PRODUCER", "ID": "1212"}, {"sentence": "From this table it can be observed that the PP, INSTRUMENT-AGENCY, and CAUSE-EFFECT rela- tions were detected with a relatively very high per- formance score, whereas the THEME-TOOL relation classification yielded a relatively small score.", "acronym": "PP", "label": "PRODUCT-PRODUCER", "ID": "1213"}, {"sentence": "A total of 10 relations were used includ- ing CAUSE-EFFECT, COMPONENT-WHOLE, CONTENT-CONTAINER, ENTITY-ORIGIN, ENTITY-DESTINATION, INSTRUMENT-AGENCY, MEMBER-COLLECTION, MESSAGE-TOPIC, OTHER, and PP.", "acronym": "PP", "label": "PRODUCT-PRODUCER", "ID": "1214"}, {"sentence": "R1 R2 R3 R4 R5 R6 R7 before 682 1200 913 898 861 849 677 after 13 19 10 15 15 8 16 Table 4: The number of features before and af- ter Weka selection, for each semantic relation dataset: R1 CAUSE-EFFECT, R2 INSTRUMENT- AGENCY, R3 PP, R4 ORIGIN- ENTITY, R5 THEME-TOOL, R6 PART-WHOLE, and R7 CONTENT-CONTAINER.", "acronym": "PP", "label": "PRODUCT-PRODUCER", "ID": "1215"}, {"sentence": "\\[Boggus et al 1992\\] L. Boggess, R. Agarwal, R.  Davis, Disambiguation of PPs in  Automatically Labeled Technical Texts, Proc.", "acronym": "PP", "label": "Prepositional Phrase", "ID": "1216"}, {"sentence": "At- taching Multiple PPs: General- ized Backed-off Estimation.", "acronym": "PP", "label": "Prepositional Phrase", "ID": "1217"}, {"sentence": "Attachment  and Transfer of PPs with Con-  straint Propagation\".", "acronym": "PP", "label": "Prepositional Phrase", "ID": "1218"}, {"sentence": "A Rule-Based Approach to PP Attachment  Disambiguation.", "acronym": "PP", "label": "Prepositional Phrase", "ID": "1219"}, {"sentence": "A Maximum Entropy Model for PP Attachment.", "acronym": "PP", "label": "Prepositional Phrase", "ID": "1220"}, {"sentence": "combinations in Farsi that  apparently a PP almost  behaves as Compound Prepositions.", "acronym": "PP", "label": "Prepositional Phrase", "ID": "1221"}, {"sentence": "We intend to perform experi-  ments to compare the PP of the various mod-  els, and a structurally similar 'pure' PCFG 1?.", "acronym": "PP", "label": "perplexity", "ID": "1222"}, {"sentence": "We propose a set of features that model both the translations and the translators, such as country of resi- dence, LM PP of the translation, edit rate from the other translations, and (option- ally) calibration against professional transla- tors.", "acronym": "PP", "label": "perplexity", "ID": "1223"}, {"sentence": "The lower PP means the better predictive ability of the LM.", "acronym": "PP", "label": "perplexity", "ID": "1224"}, {"sentence": "The PP was improved by approximately 46% on English and 49% on Czech compared with standalone MKN.", "acronym": "PP", "label": "perplexity", "ID": "1225"}, {"sentence": "Our experiments with English and Czech corpora show significant PP re- ductions (up to 46% for English and 49% for Czech) compared with standalone 4-gram Modified Kneser-Ney language model.", "acronym": "PP", "label": "perplexity", "ID": "1226"}, {"sentence": "We measure the quality of LTLM by PP that is the standard measure used for LMs.", "acronym": "PP", "label": "perplexity", "ID": "1227"}, {"sentence": "cn common noun *pe PPosition co coordinating elem.", "acronym": "PP", "label": "prep", "ID": "1228"}, {"sentence": "PPosition to the right?).", "acronym": "PP", "label": "prep", "ID": "1229"}, {"sentence": "We then PPared features, and fed the training data to a sequential labeling system, a discriminative Markov model much like Conditional Random Fields (CRF), with the difference being that the model parameters are tuned using Bayes Point Machines (BPM), and then compared our model against an equivalent CRF model.", "acronym": "PP", "label": "prep", "ID": "1230"}, {"sentence": "The passive char- acters were identified via the following relations extracted by dependency parsing: nsubjpass (passive nominal subject) and pobj (object of a PPosition).", "acronym": "PP", "label": "prep", "ID": "1231"}, {"sentence": "Objects in cluster 1, corresponding to binary ad- jectives, have high values for most of the features containing a PPosition after the adjective (observe +1pe, ?", "acronym": "PP", "label": "prep", "ID": "1232"}, {"sentence": "4 count of 1st person singular pronouns 5 count of negative particles 6 count of numbers 7 count of PPositions 8 count of pronouns 9 count of ?", "acronym": "PP", "label": "prep", "ID": "1233"}, {"sentence": "The remaining 60% translation pairs do not only reflect word alignment errors, but also cases where we find a PPiciple in the German sentence that has a correct adverbial translation for other reasons.", "acronym": "PP", "label": "proper part", "ID": "1234"}, {"sentence": "2nd + 3rd = .\\]  Note that the PP of lst+2nd excludes 3rd person.", "acronym": "PP", "label": "proper part", "ID": "1235"}, {"sentence": "After each stage only 'maximal' fragments are  retained (a fragment is maximal if its segment is  not a PP of the segment of any other frag-  Inent).", "acronym": "PP", "label": "proper part", "ID": "1236"}, {"sentence": "Constitutive Role: the relation between an object and its constituents, or  PPs.", "acronym": "PP", "label": "proper part", "ID": "1237"}, {"sentence": "8WN has chosen a restrictive sense for the Great Divide, making it a PP of the Continental Divide.", "acronym": "PP", "label": "proper part", "ID": "1238"}, {"sentence": "A fragment is called maxi-  mal if its segment is not a PP of the segment  of any other fragment belonging to A. The set C is the  final result of partial parsing.", "acronym": "PP", "label": "proper part", "ID": "1239"}, {"sentence": "LTLM 39.9 36.4 32.8 30.3 28.1 26.0 24.9 64.4 56.1 51.5 47.3 43.4 39.9 37.2 Table 3: PPity results on the test data for LTLMs and STLMs with different number of roles.", "acronym": "PP", "label": "Perplex", "ID": "1240"}, {"sentence": "PP- ity is a measure of uncertainty.", "acronym": "PP", "label": "Perplex", "ID": "1241"}, {"sentence": "LTLM 24.9 (-43.1%) 37.2 (-49.4%) Table 2: PPity results on the test data.", "acronym": "PP", "label": "Perplex", "ID": "1242"}, {"sentence": "020406080100120140 '0 20 25 30 35 40 50 70 100 125 150 175 Top-r anked   gener al-dom ain se ntenc es (in k ) Devset PPity In-dom ain ba seline Cross - Entrop y Moore - Lewis bilingu al M-L Figure 1: Corpus Selection Results The perplexity of the dev set according to LMs trained on the top-ranked sentences varied from 77 to 120, depending on the size of the subset and the method used.", "acronym": "PP", "label": "Perplex", "ID": "1243"}, {"sentence": "PP- ity of n-gram and dependency language models.", "acronym": "PP", "label": "Perplex", "ID": "1244"}, {"sentence": "PPities on the test set are given in Table 1.", "acronym": "PP", "label": "Perplex", "ID": "1245"}, {"sentence": "But even the ostensibly disambiguating PP  sition by, is itself ambiguous, since it might introduce  a manner or locative phrase consistent with the main  clause analysis.", "acronym": "PP", "label": "prepo-", "ID": "1246"}, {"sentence": "The only manually encoded knowledge is a dictio- nary of markers (subordinators, coordinators, PP sitions).", "acronym": "PP", "label": "prepo-", "ID": "1247"}, {"sentence": "Also, instead of using the PP sition itself as the argument head, we used the ac- tual content word modifying the preposition.", "acronym": "PP", "label": "prepo-", "ID": "1248"}, {"sentence": "Therefore, we relax the adjacency con- dition for verb attachment and also count PP sitions that occur within a fixed distance of their suspected regent.", "acronym": "PP", "label": "prepo-", "ID": "1249"}, {"sentence": "Context features {Head word of first NP in PP sition phrase, left and right sibling head words and syntactic categories, first and last word in phrase yield and their PoS, parent syntactic category and head word}.", "acronym": "PP", "label": "prepo-", "ID": "1250"}, {"sentence": "Extra function words (determiners and PP sitions) in source or target language are linked together with their noun to the noun?s transla- tion.", "acronym": "PP", "label": "prepo-", "ID": "1251"}, {"sentence": "Using Decision Trees to Con- struct a PP.", "acronym": "PP", "label": "Practical Parser", "ID": "1252"}, {"sentence": "1998 : \"Using Decision Trees  to Construct a PP\".", "acronym": "PP", "label": "Practical Parser", "ID": "1253"}, {"sentence": "1998 : \"Using Decision ~l~'ees to  Construct a PP\", Procccding.s o\\[  COLING-A CL-08 pp505-511  Sadao Kurohashi, Makoto Nagao.", "acronym": "PP", "label": "Practical Parser", "ID": "1254"}, {"sentence": "Using Decision Trees to Construct a PP.", "acronym": "PP", "label": "Practical Parser", "ID": "1255"}, {"sentence": "proof, since the sum of linear elements i linear:  2 The PP   From now on we assume that strings of  nodes  are natural language sentences and discuss a  fully implemented parser (DCParser) that parses  Finnish sentences.", "acronym": "PP", "label": "Practical Parser", "ID": "1256"}, {"sentence": "Table 2: Typical hand-written compound  noun indexing rule patterns for Korean  Noun without case makers / Noun  Noun with a genitive case maker / Noun  Noun with a nominal case maker or  an accusative case maker \\[  Verbal common oun or adjectival common noun  Noun with an adnominal ending \\] Noun  Noun within predicate PP / Noun  (The two nouns before and after a slash  in the pattern can form a single compound noun.)", "acronym": "PP", "label": "particle phrase", "ID": "1257"}, {"sentence": "Currently this rescoring  is used to fine-tune attachments of PPs in  Japanese sentences.", "acronym": "PP", "label": "particle phrase", "ID": "1258"}, {"sentence": "cor-  resl)onding to the grammar of (9)-(10) is ~s follows~:  (14) (Vi, j ,k, l ,e,p)pp(i, j ,e) A pp(j,k,e)  A v(< t, ^ > ,(i, l, e)  (15) (Vi, j,k,z,e,pari),,p(i,j,;,:)  A pa,'t ide(j, k, l,a,'t ) A pa,'t(x,e)  -o pp(i, k, e)  (16) (Vi,j,k,l,.~,V),u,(i,j,~j) A pa,'iicle(j,k,,,o)  a t, A ,,,o(v,  .p(i, l, z)  (17) (Vi, j ,w,z)n(i , j ,w) A w(.~) D np(i,j,z)  pp(i, j ,  e) mean.~ that there is a PP from i  to j with the missing a.rgumenl, e. part is a particle  and the predicate it encodes.", "acronym": "PP", "label": "particle phrase", "ID": "1259"}, {"sentence": "COMLEX cites  the specific: prepositions and adverbs hi preposi-  tional anti PPs associated with partic-  ular verbs.", "acronym": "PP", "label": "particle phrase", "ID": "1260"}, {"sentence": "An idiomatic transla-  tion of sentence (2) is  pp(i, j, e) means that there is a PP from i to  j with the missing argument e. part is a particle .and  the predicate it encodes.", "acronym": "PP", "label": "particle phrase", "ID": "1261"}, {"sentence": "3 Methods Statistical PP attachment is based on the obser- vation that the identities of content words can be used to predict which PPs mod- ify which words, and achieve better-than-chance accuracy.", "acronym": "PPs", "label": "prepositional phrases", "ID": "1262"}, {"sentence": "3 Extending the structure under  discussion    3.1 Premodifiers    The noun in PPs, can be  extended in different ways while as the examples  below show, the related structures cannot:      3.1.1 Demonstratives    6.", "acronym": "PPs", "label": "prepositional phrases", "ID": "1263"}, {"sentence": "on (one) roof-of  house    3.2 Post Modifiers    Nouns in PPs can expand  with post modifiers while nouns in our structure  cannot.", "acronym": "PPs", "label": "prepositional phrases", "ID": "1264"}, {"sentence": "If  the noun here expands as other nouns in other  PPs we can conclude that the  related structure is a phrase, otherwise it is better  to think about them as compound prepositions.", "acronym": "PPs", "label": "prepositional phrases", "ID": "1265"}, {"sentence": "A linguistic analyser of spatial expressions (nominal and PPs) have been designed, which recognise them and produces a symbolic representation of their \"content\".", "acronym": "PPs", "label": "prepositional phrases", "ID": "1266"}, {"sentence": "WITH and WITHOUT  PPs containing WITH and WITH-  OUT can also be related to OF-phrases (Lees  1960:93) and to HAVE (Poldauf I967:33f.) ,", "acronym": "PPs", "label": "Prepositional phrases", "ID": "1267"}, {"sentence": "PPs are free in their location be- cause the preposition is already a unique identi- fier.", "acronym": "PPs", "label": "Prepositional phrases", "ID": "1268"}, {"sentence": "PPs, however, although they  show a high degree of locality in the syntax, are involved in  complex, non-local interactions in the semantics, with a cor-  responding complication of the processing.", "acronym": "PPs", "label": "Prepositional phrases", "ID": "1269"}, {"sentence": "PP Interpretation  PPs, in both post-copular nd post-nominal  positions, are very common in the ATIS domain (and most  other domains as well).", "acronym": "PPs", "label": "Prepositional phrases", "ID": "1270"}, {"sentence": "PPs would seem to be  trouble, however, because the preposi-  119  -9 -   tion simultaneously interacts with both  its subject and its object.", "acronym": "PPs", "label": "Prepositional phrases", "ID": "1271"}, {"sentence": "PPs: Our original method also fails to correctly determine the negation scope when the negated event is followed by a prepositional phrase, as it may be seen in Figure 2, where the syntax tree for the sen- tence: [There was] no [attempt at robbery] is shown.", "acronym": "PPs", "label": "Prepositional phrases", "ID": "1272"}, {"sentence": "Using MSR for  Word Sense Disambiguation.", "acronym": "MSR", "label": "Measures of Semantic Relatedness", "ID": "1273"}, {"sentence": "Using MSR for Word Sense Disambiguation.", "acronym": "MSR", "label": "Measures of Semantic Relatedness", "ID": "1274"}, {"sentence": "Using MSR for Word  Sense Disambiguation, LNCS 2588 - CICLing  2003, pp.", "acronym": "MSR", "label": "Measures of Semantic Relatedness", "ID": "1275"}, {"sentence": "2 MSR Approaches to measuring semantic relatedness that use lexical resources transform these resources into a network or graph and compute relatedness using paths in it (see Budanitsky & Hirst (2006) for an ex- tensive review).", "acronym": "MSR", "label": "Measures of Semantic Relatedness", "ID": "1276"}, {"sentence": "Automatically Creating Datasets for MSR.", "acronym": "MSR", "label": "Measures of Semantic Relatedness", "ID": "1277"}, {"sentence": "MSR Asia +University of Edinburgh ?", "acronym": "MSR", "label": "Microsoft Research", "ID": "1278"}, {"sentence": "MSR Paraphrase Corpus.", "acronym": "MSR", "label": "Microsoft Research", "ID": "1279"}, {"sentence": "The work was done when the first author and the third author were interns at MSR Asia.", "acronym": "MSR", "label": "Microsoft Research", "ID": "1280"}, {"sentence": "931  Support Vector Machines for Paraphrase Identification   and Corpus Construction  Chris Brockett and William B. Dolan  Natural Language Processing Group  MSR  One Microsoft Way, Redmond, WA 98502, U.S.A.  {chrisbkt, billdol}@microsoft.com  Abstract  The lack of readily-available large cor- pora of aligned monolingual sentence  pairs is a major obstacle to the devel- opment of Statistical Machine Transla- tion-based paraphrase models.", "acronym": "MSR", "label": "Microsoft Research", "ID": "1281"}, {"sentence": "MSR, Redmond, WA 98052, USA {jianshuc, xiaohe, jfgao, lihongli, deng}@microsoft.com Abstract This paper introduces a novel architec- ture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games.", "acronym": "MSR", "label": "Microsoft Research", "ID": "1282"}, {"sentence": "Role-playing games \u0001 System by MSR (Kacmarcik 2005) 61   Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 846?855, Austin, Texas, November 1-5, 2016.", "acronym": "MSR", "label": "Microsoft Research", "ID": "1283"}, {"sentence": "The quote-to-speaker attribution was evaluated in terms of PRE (AT p ), while the estimation of speakers?", "acronym": "PRE", "label": "precision", "ID": "1284"}, {"sentence": "The  cases where \"it\" is merely a dummy subject in  a cleft sentence (example 1) or has conventional  unspecified referents (example 2) are excluded  from computing the PRE:  ?", "acronym": "PRE", "label": "precision", "ID": "1285"}, {"sentence": "Thus we  compute PRE as follows:  PRE =  \\ [ ra t t r ib .", "acronym": "PRE", "label": "precision", "ID": "1286"}, {"sentence": "The PRE score computed over all phrases  containing any of the target honorifics are 66.0%  l In effect, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes across different  observ", "acronym": "PRE", "label": "precision", "ID": "1287"}, {"sentence": "The PRE score computed over all phrases  containing any of the target honorifics are 66.0%  l In effect, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes a", "acronym": "PRE", "label": "precision", "ID": "1288"}, {"sentence": "In order to exclude the second type of error, the PRE of gender estimation was also computed for only the true story speaker", "acronym": "PRE", "label": "precision", "ID": "1289"}, {"sentence": "In order to exclude the second type of error, the PRE of gender estimation was also computed for only the true story speaker identified by the system (G?", "acronym": "PRE", "label": "precision", "ID": "1290"}, {"sentence": "gender was evaluated in terms of PRE (G p ) and recall (G r ).", "acronym": "PRE", "label": "precision", "ID": "1291"}, {"sentence": "The used patterns are: 1) (DT|CD) (NN|NNS), 2) DT JJ (NN|NNS), 3) NN PRES (NN|NNS), and 4) PRP$ JJ (NN|NNS).", "acronym": "PRE", "label": "PO", "ID": "1292"}, {"sentence": "These PRES-based patterns are quite generic, al- lowing for the creation of large sets of characters.", "acronym": "PRE", "label": "PO", "ID": "1293"}, {"sentence": "No.1-20 + PRES tags for No.21 3.4.3 Estimation of Personality Attributes A machine-learning based approach was also used for personality attribute estimation.", "acronym": "PRE", "label": "PO", "ID": "1294"}, {"sentence": "MEESE 166(82.1007) 0.8734 0 0.1265  BRAZIL 285(79.7311) 0.0596 0 0.9403  SPREKESMAN 665(78", "acronym": "PRE", "label": "PO", "ID": "1295"}, {"sentence": "GORBACHEV 205(108.776) 0.8926 0.0048 0.1024  JUDGE BORK 212(108.746) 0.8820 0 0.1179  HUSBAND 91(107.438) 0.3626 0.5714 0.0659  JAPAN 450(100.727) 0.0755 0.0111 0.9133  AGENCY 476(97.4016) 0.0840 0.0147 0.9012  WIFE 153(93.7485) 0.6143 0.2875 0.0980  DOLLAR 621(90.8963) 0.1304 0.0096 0.8599  STANDARD PREOR 200(90.1062) 0 0 1  FATHER 146(89.4178-) 0.8082 0.1438 0.0479  UTILITY 242(87.1821) 0.0247 0 0.9752  MR.", "acronym": "PRE", "label": "PO", "ID": "1296"}, {"sentence": "MEESE 166(82.1007) 0.8734 0 0.1265  BRAZIL 285(79.7311) 0.0596 0 0.9403  SPREKESMAN 665(78.3441) 0.6075 0.0045 0.3879  MR.", "acronym": "PRE", "label": "PO", "ID": "1297"}, {"sentence": "This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (PRES) tagging, (iv) lemmatization, (v) named entity recognition, (vi) dependency parsing, and (vii) co-reference analysis.", "acronym": "PRE", "label": "PO", "ID": "1298"}, {"sentence": "8599  STANDARD PREOR 200(90.1062) 0 0 1  FATHER 146(89.4178-) 0.8082 0.1438 0.0479  UTILITY 242(87.1821) 0.0247 0 0.9752  MR.", "acronym": "PRE", "label": "PO", "ID": "1299"}, {"sentence": "To determine conceptual categories that POBJ belong to, WordNet (Fellbaum, 1998) appears to be an ideal tool.", "acronym": "POBJ", "label": "objects of prepositions", "ID": "1300"}, {"sentence": "generated by Stanford Parser  There are various argument relations like sub- ject, object, POBJ and clausal  complements, modifier relations like adjectival,  adverbial, participial, infinitival modifiers and  other relations like coordination, conjunct, exple- tive and punctuation.", "acronym": "POBJ", "label": "objects of prepositions", "ID": "1301"}, {"sentence": "In addition, Stanford distinguishes POBJ from objects of verbs, while PARC and GR collapse the two into a single relation.", "acronym": "POBJ", "label": "objects of prepositions", "ID": "1302"}, {"sentence": "GEN is assigned to POBJ and to possessors in idafa (possessive) construction.", "acronym": "POBJ", "label": "objects of prepositions", "ID": "1303"}, {"sentence": "Therefore, another im- portant question arises: can we automatically learn rules or patterns to achieve the same ob- 2 They are 1) verbs and their arguments, 2) adjectives and their arguments and 3) propositionally modified tokens and POBJ.", "acronym": "POBJ", "label": "objects of prepositions", "ID": "1304"}, {"sentence": "CHILDES labelled unlabelled RASP 60.1 69.2 CCG parser 39.1 66.5 MSTParser 93.8 95.4 MEGRASP 90.7 93.5 CCC labelled unlabelled RASP 66.7 72.3 CCG parser 60.2 68.5 F1-scores, such as auxiliaries, determiners, sub- jects, and POBJ.5 3.3 CCC The Cambridge Cookie-theft Corpus (CCC, TO APPEAR, 2010) contains audio-recorded mono- logues of 196 subjects that were asked to fully de- scribe a scene in a picture.", "acronym": "POBJ", "label": "objects of prepositions", "ID": "1305"}, {"sentence": "We use POBJ, subject, direct object, tense as our features.", "acronym": "POBJ", "label": "object of preposition", "ID": "1306"}, {"sentence": "b3 if (a) NP2 is a pronoun ; and (b) both NP1 and NP2 are at the same type of grammatical rol e (subject, object of verb, POBJ, etc.) .", "acronym": "POBJ", "label": "object of preposition", "ID": "1307"}, {"sentence": "Table 2 shows the recall of the easy-first dependency parser of (Goldberg and Elhadad, 2010) on Section 23 of the Penn Treebank for identifying the governor and POBJs.", "acronym": "POBJ", "label": "object of preposition", "ID": "1308"}, {"sentence": "A second problem is that CFG's naturally abstract away from  syntactic function: for example, in a CFG, a noun phrase is  described by the same set of rules whether it occurs as sub-  ject, object, POBJ or whatever.", "acronym": "POBJ", "label": "object of preposition", "ID": "1309"}, {"sentence": "Our syn- tactic errors are either selection (e.g., wrong case as POBJ) or agreement errors (e.g., subject-verb disagreement in number).", "acronym": "POBJ", "label": "object of preposition", "ID": "1310"}, {"sentence": "Recall Governor Object Parser 88.88 92.37 Best(Parser, Heuristics) 92.50 93.06 Table 2: Identifying governor and POBJs in the Penn Treebank data.", "acronym": "POBJ", "label": "object of preposition", "ID": "1311"}, {"sentence": "The 0-1 loss also has a loss of 1 for FP and false negatives.", "acronym": "FP", "label": "false positives", "ID": "1312"}, {"sentence": "The high-recall loss function penalizes FP with 0.1 and false negatives with 5.", "acronym": "FP", "label": "false positives", "ID": "1313"}, {"sentence": "are considered FP.", "acronym": "FP", "label": "false positives", "ID": "1314"}, {"sentence": "The baseline system returns nearly all cues but since it matches every string, it also returns many FP, resulting in low precision.", "acronym": "FP", "label": "false positives", "ID": "1315"}, {"sentence": "This implies that although we get a rather satisfying score in terms of precision and recall, the number of FP that we get is rather high in relation to our universe.", "acronym": "FP", "label": "false positives", "ID": "1316"}, {"sentence": "The high-precision loss function penalizes false negatives with 0.1 and FP with 5.", "acronym": "FP", "label": "false positives", "ID": "1317"}, {"sentence": "but guessed the wrong type, we call this a cross- labeling; (2) a FP occurs when the learner guessed some relation while there should have been none; (3) the reverse is a false negative.", "acronym": "FP", "label": "false positive", "ID": "1318"}, {"sentence": "The high-recall loss function penalizes FPs with 0.1 and false negatives with 5.", "acronym": "FP", "label": "false positive", "ID": "1319"}, {"sentence": "The 0-1 loss also has a loss of 1 for FPs and false negatives.", "acronym": "FP", "label": "false positive", "ID": "1320"}, {"sentence": "are considered FPs.", "acronym": "FP", "label": "false positive", "ID": "1321"}, {"sentence": "The baseline system returns nearly all cues but since it matches every string, it also returns many FPs, resulting in low precision.", "acronym": "FP", "label": "false positive", "ID": "1322"}, {"sentence": "The high-precision loss function penalizes false negatives with 0.1 and FPs with 5.", "acronym": "FP", "label": "false positive", "ID": "1323"}, {"sentence": "c?2013 Association for Computational Linguistics Open-ended, Extensible System Utterances Are Preferred, Even If They Require FPs Timo Baumann Universit?t Hamburg Department of Informatics Germany baumann@informatik.uni-hamburg.de David Schlangen University of Bielefeld Faculty of Linguistics and Literary Studies Germany david.schlangen@uni-bielefeld.de Abstract In many environments (e. g. sports com- mentary), situations incrementally unfold over time and often the future appearance of a relevant event can be", "acronym": "FP", "label": "Filled Pause", "ID": "1324"}, {"sentence": "130  Category Read Speech Spontaneous Speech  TOTAL 103 269  Mouth Clicks 60 106  Breath Noise 37 117  FPs 0 30  Others 6 16  Table 2: Number of occurrences of non-speech vocalizations for read and spontaneous speech from 9 male  and 9 female speakers.", "acronym": "FP", "label": "Filled Pause", "ID": "1325"}, {"sentence": "702  Modeling FPs in Medical Dictations  Serge)' V.. Pakhomov  University of Minnesota  190 Klaeber Court  320-16 th Ave.", "acronym": "FP", "label": "Filled Pause", "ID": "1326"}, {"sentence": "Word Accuracy  \\ [ \\ ]  Sentence ACc, lr'~ev  66.6  86.9  0.0  Partial Words FP No Non-Speech  (1.5 %) (4.5 %) (94 %)  Condition  Figure 2: Breakdown of word and sentence accuracy for the spontaneous speech test sets, depending on  whether the sentences contain false starts or filled pauses.", "acronym": "FP", "label": "Filled Pause", "ID": "1327"}, {"sentence": "The Communicative Value of FPs in Spontaneous Speech.", "acronym": "FP", "label": "Filled Pause", "ID": "1328"}, {"sentence": "FPs and Gestures: It's not  coincidence,\" Journal of Psycholinguistic  Research, Vol.", "acronym": "FP", "label": "Filled Pause", "ID": "1329"}, {"sentence": "Syntax Only  Marked Marked  as  as   Repair False Positive  Repairs 68 (96%) 56 (30%)  FP 3 (4%) 131 (70%)  Syntax and Semantics  Marked Marked  as  as   Repair False Positive  Repairs 64 (85%) 23 (20%)  FP 11 (15%) 90 (80%)  Table 5: Syntax and Semantics Results  59  dataset of 335 sentences, of which 179 contained  repairs and 176 contained false positives.", "acronym": "FP", "label": "False Positives", "ID": "1330"}, {"sentence": "The number of True Positives, True Negatives, and  FP are listed.", "acronym": "FP", "label": "False Positives", "ID": "1331"}, {"sentence": "3.6 Querying the Model and FP The construction we have described above ensures that for any n-gram xi ?", "acronym": "FP", "label": "False Positives", "ID": "1332"}, {"sentence": "rise fall stress speech  Repairs .00 1.00 .00 .00  FP .87 .00 .87 .73  6\\]  Table 7: Acoustic Characteristics of Cue Words  8000  !", "acronym": "FP", "label": "False Positives", "ID": "1333"}, {"sentence": "14  True Positives 191 246  FP 82 133  False Negatives 1587 1874  Correct Labels 189 237  Precision 0.700 0.649  Recall 0.107 0.116  F-Score 0.186 0.197  Label Accuracy 0.106 0.112    As can be seen, for entries with patterns (albeit  a low recall), a substantial number of frame ele- ments could be recognized with high precision  from a very small number of constituent match- ing functions.", "acronym": "FP", "label": "False Positives", "ID": "1334"}, {"sentence": "Syntax Only  Marked Marked  as  as   Repair FP  Repairs 68 (96%) 56 (30%)  FPs 3 (4%) 131 (70%)  Syntax and Semantics  Marked Marked  as  as   Repair FP  Repairs 64 (85%) 23 (20%)  FPs 11 (15%) 90 (80%)  Table 5: Syntax and Semantics Results  59  dataset of 335 sentences, of which 179 contained  repairs and 176 contained false positives.", "acronym": "FP", "label": "False Positive", "ID": "1335"}, {"sentence": "3.6 Querying the Model and FPs The construction we have described above ensures that for any n-gram xi ?", "acronym": "FP", "label": "False Positive", "ID": "1336"}, {"sentence": "5.2 FP Rates All n-grams explicitly inserted into our randomized language model are retrieved without error; how- ever, n-grams not stored may be incorrectly assigned a value resulting in a false positive.", "acronym": "FP", "label": "False Positive", "ID": "1337"}, {"sentence": "Links that erroneously start a new cluster when it is coreferent with other men- tions to the left is marked as FN.", "acronym": "FN", "label": "false new", "ID": "1338"}, {"sentence": "def cost_based_on_consistency(arc): ana, ante = arc consistent = \\ ana.decision_is_consistent(ante) # FN if not consistent and \\ ante.is_dummy(): return 2 # wrong link elif not consistent: return 1 # correct else: return 0 tent structure for one document into substructures, and the candidate arcs for each substructure.", "acronym": "FN", "label": "false new", "ID": "1339"}, {"sentence": "FN?", "acronym": "FN", "label": "false new", "ID": "1340"}, {"sentence": "Uturku used an SVM- based approach for extraction, and it is thus delicate to account for the FN in a simple and concise way.", "acronym": "FN", "label": "false negatives", "ID": "1341"}, {"sentence": "The 0-1 loss also has a loss of 1 for false positives and FN.", "acronym": "FN", "label": "false negatives", "ID": "1342"}, {"sentence": "The high-recall loss function penalizes false positives with 0.1 and FN with 5.", "acronym": "FN", "label": "false negatives", "ID": "1343"}, {"sentence": "For example, we con- sider every WordNet sense to be plausible, which produces FN.", "acronym": "FN", "label": "false negatives", "ID": "1344"}, {"sentence": "However, a concern was that the transformed query would cause numerous FN.", "acronym": "FN", "label": "false negatives", "ID": "1345"}, {"sentence": "Surprisingly, we also found FN in rather trivial examples (?", "acronym": "FN", "label": "false negatives", "ID": "1346"}, {"sentence": "The high-precision loss function penalizes FN with 0.1 and false positives with 5.", "acronym": "FN", "label": "false negatives", "ID": "1347"}, {"sentence": "The FN error  was high for only one user, while the majority  of - the users exhibited no FN  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  Table 2: Relevance Classes by User  User  5 0  4 0  I  2  3  4  5  1  7  8  9  10  11  12  13  14  True False True False  Positive Positive Negative Negative  5 0 0 0  4  7  1  5 0  4 0  0  4  5  0  2  0  0  0  0  0  0 0  0 1  0 0  0 1  0 2  ~0  0  0  0  0  1 0 2 2  0 1 6 0  I 1 4  errors, a worse error to commit han wasting", "acronym": "FN", "label": "False Negative", "ID": "1348"}, {"sentence": "The (1,0) point represents perfect performance with 100% True Positive Rate and 0% FN Rate.", "acronym": "FN", "label": "False Negative", "ID": "1349"}, {"sentence": "Baseline + Graph Accuracy/Kappa 0.624/0.298 0.646/0.173 FN Rate 0.095 0.482 Table 1: Performance metrics for machine learning experiments.", "acronym": "FN", "label": "False Negative", "ID": "1350"}, {"sentence": "After that we use the Word2Vec tool (Mikolov et al, 2013b) to generate FN (%) Time (s) Linear Search 0 342 LSH 14.29 69 RBV 9.08 19 Table 2: Performance of linear search, locality sensitive hashing, and redundant bit vectors, for k = 200.", "acronym": "FN", "label": "False Negative", "ID": "1351"}, {"sentence": "Graph Accuracy/Kappa 0.692/0.365 # 0.693/0.277 # FN Rate 0.157 0.397 3.", "acronym": "FN", "label": "False Negative", "ID": "1352"}, {"sentence": "14  True Positives 191 246  False Positives 82 133  FNs 1587 1874  Correct Labels 189 237  Precision 0.700 0.649  Recall 0.107 0.116  F-Score 0.186 0.197  Label Accuracy 0.106 0.112    As can be seen, for entries with patterns (albeit  a low recall), a substantial number of frame ele- ments could be recognized with high precision  from a very small number of constituent match- ing functions.", "acronym": "FN", "label": "False Negative", "ID": "1353"}, {"sentence": "Baseline Accuracy/Kappa 0.623/0.297 0.647/0.173 FN Rate 0.095 0.485 2.", "acronym": "FN", "label": "False Negative", "ID": "1354"}, {"sentence": "The FN error  was high for only one user, while the majority  of - the users exhibited no FN  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  I  Table 2: Relevance Classes by User  User  5 0  4 0  I  2  3  4  5  1  7  8  9  10  11  12  13  14  True False True False  Positive Positive Negative Negative  5 0 0 0  4  7  1  5 0  4 0  0  4  5  0  2  0  0  0  0  0  0 0  0 1  0", "acronym": "FN", "label": "False Negative", "ID": "1355"}, {"sentence": "The Berkeley FN project.", "acronym": "FN", "label": "FrameNet", "ID": "1356"}, {"sentence": "Our approach demonstrates the  use of two human built knowledge bases  (WordNet and FN) for the task of  semantic extraction.", "acronym": "FN", "label": "FrameNet", "ID": "1357"}, {"sentence": "For projects such as FN (Baker et al, 1998), 1Note that we do not state that adjectives denote objects or events, but that they imply an object or event in their denota- tion.", "acronym": "FN", "label": "FrameNet", "ID": "1358"}, {"sentence": "Their work  looked at the problem of assigning semantic roles to  text based on a statistical model of the FN1  data.", "acronym": "FN", "label": "FrameNet", "ID": "1359"}, {"sentence": "In this paper we report results obtained from  combining IE and graphical modeling techniques,  with semantic resources (WordNet and FN)  for automatic Semantic Extraction.", "acronym": "FN", "label": "FrameNet", "ID": "1360"}, {"sentence": "The ongoing development of public  knowledge bases such as WordNet, FN, CYC,  etc.", "acronym": "FN", "label": "FrameNet", "ID": "1361"}, {"sentence": "(37) If the CWS is an NP and the TOS is an S, then  construct the FN phrase and push it  to the STACK:  (NP (HEAD CWS)  (MOD (rep_emn TOS CWS)))  To illustrate how (37) works, we will trace the noun  phrase (38), which is included in all sentences cited in  (4b) through (4e).", "acronym": "FN", "label": "following noun", "ID": "1362"}, {"sentence": "In this way the parser will be forced to attach of-PPs FNs as their complements.", "acronym": "FN", "label": "following noun", "ID": "1363"}, {"sentence": "Lexical Token n-gram Token n-grams in a 2 word window around the preposition POS n-gram POS n-grams in a 2 word window around the preposition HEAD PREC VP The head verb in the preceding verb phrase HEAD PREC NP The head noun in the preceding noun phrase HEAD FOLLOW NP The head noun in the FN phrase Parsing HEAD Head of the preposition HEAD POS POS of the head COMP Complement of the preposition COMPLEMENT POS POS of the complement HEAD RELATION Prep-Head relation name COMPLEMENT RELATION Prep-Comp relation name Phrase Structure PARENT TAG TAG of the preposition?s parent GRANDPARENT TAG TAG of the preposition?s grandparent PARENT LEFT Left context of the preposition paren", "acronym": "FN", "label": "following noun", "ID": "1364"}, {"sentence": "Premodifier relations specify the proper adjective or proper noun premodifier and the FN it modifies, e.g.: [the [Seattle] zoo] Possessive indicates that the first mention is in a possessive case, e.g.: [[California] ?", "acronym": "FN", "label": "following noun", "ID": "1365"}, {"sentence": "2 (7) If the CWS is an NP and the TOS is an S, then  construct the FN phrase and push it to  the STACK:  (NP (HEAD CWS)  (MOD (rep_emn TOS CWS)))  - CWS is the word or phrase on which the PROCES-  SOR is currently working.", "acronym": "FN", "label": "following noun", "ID": "1366"}, {"sentence": "For instance, note the structure of the FN phrase, which is shown in the middle tree in Figure 2: ?-?", "acronym": "FN", "label": "following noun", "ID": "1367"}, {"sentence": "but guessed the wrong type, we call this a cross- labeling; (2) a false positive occurs when the learner guessed some relation while there should have been none; (3) the reverse is a FN.", "acronym": "FN", "label": "false negative", "ID": "1368"}, {"sentence": "The high-recall loss function penalizes false positives with 0.1 and FNs with 5.", "acronym": "FN", "label": "false negative", "ID": "1369"}, {"sentence": "itive occurs when the learner guessed some relation while there should have been none; (3) the reverse is a FN.", "acronym": "FN", "label": "false negative", "ID": "1370"}, {"sentence": "The 0-1 loss also has a loss of 1 for false positives and FNs.", "acronym": "FN", "label": "false negative", "ID": "1371"}, {"sentence": "However, a concern was that the transformed query would cause numerous FNs.", "acronym": "FN", "label": "false negative", "ID": "1372"}, {"sentence": "In order to boost recall, we defined the loss function as the num- ber of FN trigger chunks.", "acronym": "FN", "label": "false negative", "ID": "1373"}, {"sentence": "sed the wrong type, we call this a cross- labeling; (2) a false positive occurs when the learner guessed some relation while there should have been none; (3) the reverse is a FN.", "acronym": "FN", "label": "false negative", "ID": "1374"}, {"sentence": "The high-precision loss function penalizes FNs with 0.1 and false positives with 5.", "acronym": "FN", "label": "false negative", "ID": "1375"}, {"sentence": "FN-based confidence estimation, the direct phrase-based confidence measure and the count-based confidence measure calculated over N-best lists show the best performance.", "acronym": "FN", "label": "For PER", "ID": "1376"}, {"sentence": "FN_OUT,   unigram EOD kernel was used.", "acronym": "FN", "label": "For PER", "ID": "1377"}, {"sentence": "FNSONAGE and GRAG2, personality traits are mainly used to adapt linguistic styles.", "acronym": "FN", "label": "For PER", "ID": "1378"}, {"sentence": "FNSON objects, this  challenge is small, since the only additional bit of  information required is the person's title (\"Mr.,\"  \"Ms.,\" \"Dr.,\" etc.),", "acronym": "FN", "label": "For PER", "ID": "1379"}, {"sentence": "FN and ORG queries, we select named entities in DQ that contain NQ as a substring.", "acronym": "FN", "label": "For PER", "ID": "1380"}, {"sentence": "FN, the classifier based on word counts dominates all other confidence measures.", "acronym": "FN", "label": "For PER", "ID": "1381"}, {"sentence": "HMMs proposed by Miller et al (1999), and have shown to outperform tf, idf in TREC information retrieval tasks.", "acronym": "HMM", "label": "Hidden Markov Model", "ID": "1382"}, {"sentence": "We use HMM for this stochastic process, where the classes are assumed to be hid- den states.", "acronym": "HMM", "label": "Hidden Markov Model", "ID": "1383"}, {"sentence": "Fujie et al (2004) used HMMs to perform head nod recognition.", "acronym": "HMM", "label": "Hidden Markov Model", "ID": "1384"}, {"sentence": "A Second- Order HMM for Part-of-Speech Tagging.", "acronym": "HMM", "label": "Hidden Markov Model", "ID": "1385"}, {"sentence": "689  Combining Optimal Clustering and HMMs for Extractive Summarization Pascale Fung  Human Language Technology Center, Dept.", "acronym": "HMM", "label": "Hidden Markov Model", "ID": "1386"}, {"sentence": "Applying Many-to-Many Alignments and HMMs to Letter-to-Phoneme Conversion.", "acronym": "HMM", "label": "Hidden Markov Model", "ID": "1387"}, {"sentence": "Robust Part-of-Speech Tagging using a HMMl.", "acronym": "HMM", "label": "Hidden Maxkov Mode", "ID": "1388"}, {"sentence": "Discriminative training meth- ods for HMMs: Theory and exper- iments with perceptron algorithms.", "acronym": "HMM", "label": "hidden Markov model", "ID": "1389"}, {"sentence": "The EM-like method for learning dependency  relations described in Section 3.3 has also been  applied to other tasks such as HMM  training (Rabiner, 1989), syntactic relation learning  (Yuret, 1998), and Chinese word segmentation  (Gao et al, 2002a).", "acronym": "HMM", "label": "hidden Markov model", "ID": "1390"}, {"sentence": "Type- supervised HMMs for part-of- speech tagging with incomplete tag dictionaries.", "acronym": "HMM", "label": "hidden Markov model", "ID": "1391"}, {"sentence": "We focus on the simple and tractable HMM, and present an efficient learning algorithm for incorporating approximate bijectivity and symmetry constraints.", "acronym": "HMM", "label": "hidden Markov model", "ID": "1392"}, {"sentence": "A spectral algorithm for learning HMMs.", "acronym": "HMM", "label": "hidden Markov model", "ID": "1393"}, {"sentence": "The speech recognition hypotheses are obtained by using the HMM Toolkit (HTK) speech rec- ognizer adapted to our application domain in which the word error rate (WER) is 21.03%.", "acronym": "HMM", "label": "Hidden Markov model", "ID": "1394"}, {"sentence": "of Electrical & Electronic Engineering, University of Science & Technology (HKUST) Clear Water Bay, Hong Kong eepercy@ee.ust.hk Abstract We propose HMMs with unsupervised training for extractive sum- marization.", "acronym": "HMM", "label": "Hidden Markov model", "ID": "1395"}, {"sentence": "2009) presented one of the initial efforts at spectral-based parameter estimation (us- ing SVD) of observed moments for latent-variable models, in the case of HMMs.", "acronym": "HMM", "label": "Hidden Markov model", "ID": "1396"}, {"sentence": "3.1 Probabilistic Disambiguation  The HMM is the most widely  used method for statistical part of speech tag- ging.", "acronym": "HMM", "label": "Hidden Markov model", "ID": "1397"}, {"sentence": "4); Gricean Maxims (Sripada et al, 2003); Integer Linear Programming (Lampouras and An- droutsopoulos, 2013); collective content selection (Barzilay and Lapata, 2004); interest scores as- signed to content (Androutsopoulos et al, 2013); a combination of statistical and template-based ap- proaches to NLG (Kondadadi et al, 2013); statis- tical acquisition of rules (Duboue and McKeown, 2003) and the HMM approach for Content Selection and ordering (Barzilay and Lee, 2004).", "acronym": "HMM", "label": "Hidden Markov model", "ID": "1398"}, {"sentence": "Inspired on HMMs (Baum  and Petrie, 1966) and following the idea that  words combinations are finite in an evaluation  text, we decided to create a finite automata in  graph form to represent all these relations  extracted from a training corpus.", "acronym": "HMM", "label": "Hidden Markov model", "ID": "1399"}, {"sentence": "Hidden HMMs proposed by Miller et al (1999), and have shown to outperform tf, idf in TREC information retrieval tasks.", "acronym": "HMMs", "label": "Markov Models", "ID": "1400"}, {"sentence": "Fujie et al (2004) used Hidden HMMs to perform head nod recognition.", "acronym": "HMMs", "label": "Markov Models", "ID": "1401"}, {"sentence": "Com- puting word similarity and identifying cognates with Pair Hidden HMMs.", "acronym": "HMMs", "label": "Markov Models", "ID": "1402"}, {"sentence": "We are currently working on NLTK modules for Hidden HMMs, language modeling, and tree adjoining grammars.", "acronym": "HMMs", "label": "Markov Models", "ID": "1403"}, {"sentence": "689  Combining Optimal Clustering and Hidden HMMs for Extractive Summarization Pascale Fung  Human Language Technology Center, Dept.", "acronym": "HMMs", "label": "Markov Models", "ID": "1404"}, {"sentence": "Applying Many-to-Many Alignments and Hidden HMMs to Letter-to-Phoneme Conversion.", "acronym": "HMMs", "label": "Markov Models", "ID": "1405"}, {"sentence": "For CRFs with general graphical structure, calcu- lation of Ep(s|o)[ fk] is intractable, but for the linear chain case Lafferty et al (2001) describe an efficient dynamic programming procedure for inference, sim- ilar in nature to the forward-backward algorithm in HMMs.", "acronym": "HMMs", "label": "hidden Markov models", "ID": "1406"}, {"sentence": "Applying many-to-many alignments and HMMs to letter-to-phoneme con- version.", "acronym": "HMMs", "label": "hidden Markov models", "ID": "1407"}, {"sentence": "These alignment models are similar to the con- cept of HMMs (HMM) in speech recognition.", "acronym": "HMMs", "label": "Hidden Markov models", "ID": "1408"}, {"sentence": "HMMs (HMM) have  to date been accepted as an effective classification method  for large vocabulary continuous peech recognition, e.g., the  ARPA-sponsored SPHINX and DECIPHER.", "acronym": "HMMs", "label": "Hidden Markov models", "ID": "1409"}, {"sentence": "These alignment models are sire-  ilar to the concept of HMMs  (HMM) in speech recognition.", "acronym": "HMMs", "label": "Hidden Markov models", "ID": "1410"}, {"sentence": "HDPes.?", "acronym": "HDP", "label": "Hierarchical Dirichlet Process", "ID": "1411"}, {"sentence": "Information Retrieval Using HDPes.", "acronym": "HDP", "label": "Hierarchical Dirichlet Process", "ID": "1412"}, {"sentence": "Their systems use a HDP (Teh et al 2006) to automatically infer the number of senses from contextual and positional features.", "acronym": "HDP", "label": "Hierarchical Dirichlet Process", "ID": "1413"}, {"sentence": "Evolutionary HDP for Timeline Summarization.", "acronym": "HDP", "label": "Hierarchical Dirichlet Process", "ID": "1414"}, {"sentence": "Notwithstanding that, ideally we would like to avoid having to pre-specify the number of classes for the word class induction module: we thus plan to investigate non-parametric models such as HDP for this purpose.", "acronym": "HDP", "label": "Hierarchical Dirichlet Process", "ID": "1415"}, {"sentence": "These include the Latent Dirichlet Al- location (LDA) model of Blei et al (2003) and the HDP model of Teh et al (2006).", "acronym": "HDP", "label": "Hierarchical Dirichlet Process", "ID": "1416"}, {"sentence": "HierarchiHDPes.?", "acronym": "HDP", "label": "cal Dirichlet Process", "ID": "1417"}, {"sentence": "Information Retrieval Using HierarchiHDPes.", "acronym": "HDP", "label": "cal Dirichlet Process", "ID": "1418"}, {"sentence": "Their systems use a HierarchiHDP (Teh et al 2006) to automatically infer the number of senses from contextual and positional features.", "acronym": "HDP", "label": "cal Dirichlet Process", "ID": "1419"}, {"sentence": "A Hierar- chiHDP Model for Joint Part-of- Speech and Morphology Induction.", "acronym": "HDP", "label": "cal Dirichlet Process", "ID": "1420"}, {"sentence": "These include the Latent Dirichlet Al- location (LDA) model of Blei et al (2003) and the HierarchiHDP model of Teh et al (2006).", "acronym": "HDP", "label": "cal Dirichlet Process", "ID": "1421"}, {"sentence": "= { 1 2P E 0 (e) if |f | = 0 1 2P F 0 (f) if |e| = 0 The terminal translation phrase pair distribution is a hierarchiHDP in which each phrase are independently distributed according to DPs:4 PP1 (z ? ?", "acronym": "HDP", "label": "cal Dirichlet Process", "ID": "1422"}, {"sentence": "HDPes.", "acronym": "HDP", "label": "Hierarchical Dirichlet process", "ID": "1423"}, {"sentence": "2 A corpus of action images and captions Image collection and sentence annotation We have constructed a corpus consisting of 8108 pho- tographs from Flickr.com, each paired with five one-sentence descriptive captions written by Ama- zon?s MTurk workers.", "acronym": "MTurk", "label": "Mechanical Turk1", "ID": "1424"}, {"sentence": "Other such projects include Ama- zon.com?s MTurk4, LiTgloss15, The ESP Game16, and the Wiktionary17.", "acronym": "MTurk", "label": "Mechanical Turk1", "ID": "1425"}, {"sentence": "We presented 100 ran- dom samples for each of the 3 distances as well as 100 unperturbed groups (original) to annotators at Amazon MTurk, asking which word fits the group the least.", "acronym": "MTurk", "label": "Mechanical Turk1", "ID": "1426"}, {"sentence": "3 Annotation of Social Variables In order to study how entrainment in various dimen- sions correlated with perceived social behaviors of our subjects, we asked Amazon MTurk annotators to label the 168 Objects games in our cor- pus for an array of social behaviors perceived for each of the speakers, which we term here ?", "acronym": "MTurk", "label": "Mechanical Turk1", "ID": "1427"}, {"sentence": "2 Mechanical Turk Amazon?s MTurk is an online market- place for work.", "acronym": "MTurk", "label": "Mechanical Turk1", "ID": "1428"}, {"sentence": "The approach can be particularly useful for the modern crowd- sourcing approaches, such as those em- ploying the Amazon?s MTurk or CrowdFlower2.", "acronym": "MTurk", "label": "Mechanical Turk1", "ID": "1429"}, {"sentence": "This evaluation is performed us- ing a set of human-corrected sentences gathered via Amazon MTurk, an online service where workers are paid to perform a short task, and further filtered for correctness by an undergraduate research assistant.", "acronym": "MTurk", "label": "Mechanical Turk", "ID": "1430"}, {"sentence": "We recreate the NIST 2009 Urdu-to- English evaluation set with MTurk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional trans- lators.", "acronym": "MTurk", "label": "Mechanical Turk", "ID": "1431"}, {"sentence": "Noting that ESL er- rors tend to occur in groups within sentences and Figure 3: Human judgments of corrected sentences gath- ered using MTurk.", "acronym": "MTurk", "label": "Mechanical Turk", "ID": "1432"}, {"sentence": "We then utilized the Amazon MTurk Service1.", "acronym": "MTurk", "label": "Mechanical Turk", "ID": "1433"}, {"sentence": "In (Gao et al, 2010) we experimented with using a dictionary to generate such constraints, and in (Gao and Vogel, 2010) we experimented with manual word align- ments from MTurk.", "acronym": "MTurk", "label": "Mechanical Turk", "ID": "1434"}, {"sentence": "This issue is exacerbated by the fact that MTurk work- ers were instructed to change each ESL sentence as little as possible, which helps their consistency but hurts these particular models?", "acronym": "MTurk", "label": "Mechanical Turk", "ID": "1435"}, {"sentence": "To look at more perceptual measures of dialogue quality, we used Amazon MTurk to an- notate each task (the sub-units of each game) in the Games Corpus for what we term social variables, the perceived social characteristics of an interaction and ints participants.", "acronym": "MTurk", "label": "Mechanical Turk2", "ID": "1436"}, {"sentence": "Crowd-sourcing seemed particularly  appropriate, given the nature of the task, so we  opted to use Amazon MTurk (AMT).", "acronym": "MTurk", "label": "Mechanical Turk2", "ID": "1437"}, {"sentence": "The NAACL 2010 Workshop on Creating Speech and Language Data with Amazon?s MTurk presents an interesting opportunity to ex-tend this collaboration in a novel data collection task.", "acronym": "MTurk", "label": "Mechanical Turk2", "ID": "1438"}, {"sentence": "In this paper, we used mi- cro-task crowd-sourcing, i.e. a central platform  like for example Amazon MTurk  or  CrowdFlower3 assigns small tasks (called HITs,  human-intelligence tasks) to workers for monetary  compensation.", "acronym": "MTurk", "label": "Mechanical Turk2", "ID": "1439"}, {"sentence": "Deceptive reviews are gathered using Amazon MTurk.", "acronym": "MTurk", "label": "Mechanical Turk2", "ID": "1440"}, {"sentence": "Another, less time-consuming approach of crowdsourcing is us- ing platforms such as Amazon MTurk.", "acronym": "MTurk", "label": "Mechanical Turk2", "ID": "1441"}, {"sentence": "The ATIS tests consist of tests  of (1) ATIS-domain spontaneous speech (lexicons typically  less than 2,000 words), (2) natural language understanding,  and (3) SLU.", "acronym": "SLU", "label": "spoken language understanding", "ID": "1442"}, {"sentence": "D. A. Dahl, L. Hirschman, L. M. Norton, M. C.  Linebarger, D. Magerman, and C. N. Ball, \"Training and  evaluation of a SLU system,\"  in Proceedings of the DARPA Speech and Language Work-  shop, (Hidden Valley, PA), June 1990.", "acronym": "SLU", "label": "spoken language understanding", "ID": "1443"}, {"sentence": "Comparing stochastic approaches to SLU in multiple lan- guages.", "acronym": "SLU", "label": "spoken language understanding", "ID": "1444"}, {"sentence": "However,  in the field of SLU which is always dealing with noise, no complete  comparison between different active learning methods has been done.", "acronym": "SLU", "label": "spoken language understanding", "ID": "1445"}, {"sentence": "Opt ica l  Character  Recogn i t ion   Although the nbest architecture was developed in the  context, of SLU, it is in fact  applicable to any kind of input where indeterminacies  in the input result in misrecognitions.", "acronym": "SLU", "label": "spoken language understanding", "ID": "1446"}, {"sentence": "This paper compares the  best known active learning methods in noisy conditions for SLU.", "acronym": "SLU", "label": "spoken language understanding", "ID": "1447"}, {"sentence": "Multi-Site Data Collection and Evalu-  ation in SLU\".", "acronym": "SLU", "label": "Spoken Language Understanding", "ID": "1448"}, {"sentence": "Hirschman, L., et al, \"Multi-Site Data Collection and  Evaluation in SLU\", in Pro-  ceedings of the Human Language Technology Workshop,  March 1993 (M. Bates, ed.)", "acronym": "SLU", "label": "Spoken Language Understanding", "ID": "1449"}, {"sentence": "SLU: Systems for Extracting Semantic  Information from Speech, First Edition, John Wiley & Sons.", "acronym": "SLU", "label": "Spoken Language Understanding", "ID": "1450"}, {"sentence": "Active learning for rule-based and corpus-based  SLU models, In Proceedings of IEEE Conference on Automatic Speech Recognition  and Understanding, pp.", "acronym": "SLU", "label": "Spoken Language Understanding", "ID": "1451"}, {"sentence": "Improving SLU with information retrieval  and active learning methods, In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal  Processing, pp.", "acronym": "SLU", "label": "Spoken Language Understanding", "ID": "1452"}, {"sentence": "Active Learning for SLU,  In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing, vol.", "acronym": "SLU", "label": "Spoken Language Understanding", "ID": "1453"}, {"sentence": "Each decision point ID for easy reference.", "acronym": "ID", "label": "is numbered", "ID": "1454"}, {"sentence": "The syntactic structure of the sentence can be depicted as a  tree where each node ID :  sentence, i  verb.2 sdir.3 sent ~f.4  transfer NH  Det.5 Head.6 nh_pf .", "acronym": "ID", "label": "is numbered", "ID": "1455"}, {"sentence": "The position that extends the kth  subsequence to the left of the head outwards from  the head ID -2k  + 1, while the position  that extends this same subsequence inwards to-  wards the head is labeled -2k .", "acronym": "ID", "label": "is numbered", "ID": "1456"}, {"sentence": "Finally, each PERS labeled entity ID by order of ap- parition and is associated with the sentences refer- ence number where it appears (consecutive PERS labeled words, not separated by punctuation mark, receive the same index number).", "acronym": "ID", "label": "is numbered", "ID": "1457"}, {"sentence": "Each sentence in the TEXT  region ID with paragraph and sentence  numbers, so, for instance, 2-1 is the first sentence  in the second paragraph.", "acronym": "ID", "label": "is numbered", "ID": "1458"}, {"sentence": "Each word sense whether irregular or regular  ID separate ly  with Arabic numerals.", "acronym": "ID", "label": "is numbered", "ID": "1459"}, {"sentence": "plementizer choice can improve upon the prediction accuracy of a state-of- the-art realization ranking model, arguably in ways that make a substantial difference to fluency and in- telligiblity.2 In particular, we report results on a bi- nary classification task for predicting the presence or absence of a that-complementizer using features adapted from Jaeger?s (2010) investigation of the uniform ID principle in the con- text of that-mentioning.", "acronym": "ID", "label": "information density", "ID": "1460"}, {"sentence": "Furthermore, Strict Local Density (SLD) method is proposed  based on a new concept of local density and a new technique of utilizing ID  measures.", "acronym": "ID", "label": "information density", "ID": "1461"}, {"sentence": "Effect of locality degree (in computation of ID) on performance of active  learning methods.", "acronym": "ID", "label": "information density", "ID": "1462"}, {"sentence": "In Jaeger?s study, uniform ID emerges as an impor- tant predictor of speakers?", "acronym": "ID", "label": "information density", "ID": "1463"}, {"sentence": "for predicting the presence or absence of a that-complementizer using features adapted from Jaeger?s (2010) investigation of the uniform ID principle in the con- text of that-mentioning.", "acronym": "ID", "label": "information density", "ID": "1464"}, {"sentence": "We report results on a binary classification task for predicting the presence/absence of a that-complementizer using features adapted from Jaeger?s (2010) investigation of the uniform ID principle in the context of that-mentioning.", "acronym": "ID", "label": "information density", "ID": "1465"}, {"sentence": "To address this problem and yet avoid outliers we choose to compute ID for each  instance locally, i.e. using k nearest instances and not all instances.", "acronym": "ID", "label": "information density", "ID": "1466"}, {"sentence": "In this work, we address both sub- problems, namely, anaphoric speaker and implicit speaker ID.", "acronym": "ID", "label": "identification", "ID": "1467"}, {"sentence": "2003), the quote-ID module detects whether a piece of quoted speech is a new quote (NEW), spoken by a speaker dif- ferent from the previous speaker, or a continuation quote (CONT) spoken by the same speaker as that of the previous quote.", "acronym": "ID", "label": "identification", "ID": "1468"}, {"sentence": "This system performs the following story analysis tasks: ID of charac- ters in each story; attribution of quotes to specific story characters; ID of character age, gender and other salient personality attributes; and finally, affective analysis of the quoted material.", "acronym": "ID", "label": "identification", "ID": "1469"}, {"sentence": "A long list of heuristics for character ID is proposed in (Mamede and Chaleira, 2004).", "acronym": "ID", "label": "identification", "ID": "1470"}, {"sentence": "Two broad approaches for the iden- tification of story characters were followed: (i) named entity recognition, and (ii) ID of character nominals, e.g., ?", "acronym": "ID", "label": "identification", "ID": "1471"}, {"sentence": "so that they can check for case and number/gender compatibility and connect the semantic ID of the argument they modify to a role in their own semantic contribution (in this case, the ARG1 of the ?", "acronym": "ID", "label": "index", "ID": "1472"}, {"sentence": "The underlying information is provided  with a gnoseological map of known interpretations,  as well as with an IDing utility in which the  learner or researcher advances new hypotheses or new interpretations a these arise through the  continuous progress of knowledge acquisition and  discovery.", "acronym": "ID", "label": "index", "ID": "1473"}, {"sentence": "The underlying information is provided  with a gnoseological map of known interpretations,  as well as with an IDing utility in which the  learner or researcher advances new hypothe", "acronym": "ID", "label": "index", "ID": "1474"}, {"sentence": "If we treat a as an ID into the vector 1~,  then (a, I.V') is simply the ath candidate in  the list ffz.", "acronym": "ID", "label": "index", "ID": "1475"}, {"sentence": "to accomplish this goal, we suggest a  multimedia database with powerful IDing and  classification functions. (", "acronym": "ID", "label": "index", "ID": "1476"}, {"sentence": "In order to accomplish this goal, we suggest a  multimedia database with powerful IDing and  classification functions. (", "acronym": "ID", "label": "index", "ID": "1477"}, {"sentence": "The grammar is able to connect x7 (the ID of ?", "acronym": "ID", "label": "index", "ID": "1478"}, {"sentence": "When viewed in this way, a can be regarded as  an ID into these vectors that specifies which  value is relevant o the particular choice of an-  tecedent.", "acronym": "ID", "label": "index", "ID": "1479"}, {"sentence": "are constants for the  binary relations ID (par-  ent relation), dominance (reflexive transitive  closure of ID) and linear  precedence.", "acronym": "ID", "label": "immediate dominance", "ID": "1480"}, {"sentence": "The export format explicitly  encodes tokens, categories and edge labels,  linear precedence between leaves and the par-  ent (ID) relation.", "acronym": "ID", "label": "immediate dominance", "ID": "1481"}, {"sentence": "We do not restrict the dependency only to the  ID, but the words on a deeper  level share less of the value E(wz).", "acronym": "ID", "label": "immediate dominance", "ID": "1482"}, {"sentence": "pl signifies ID of  the first node over the second, p2 immedi-  ate dominance of the second over the first,  d l  dominance of the first over the second,  etc.", "acronym": "ID", "label": "immediate dominance", "ID": "1483"}, {"sentence": "are the  192  SIMPX  I I  D D  VF MF  I I  1 I  NX NX  I I  I I  PWS NE  wen Maria  I  E3  SIMPX \\]  I I  D \\[3  I I  LK MF LK  I I I  I I I  VXFIN NX VXFIN  I I I  I I I  WFIN PPER VVFIN  glaubst du liebt  Figure 2: Annotation of (3) in Verbmobil for-  mat  binary relations ID (par-  ent), dominance and linear precedence, # is  a function assigning syntactic categories or  part-of-speech tags to nodes, r/ is a function  mapping edges to grammatical functions, and  a assigns tokens to the leaves (i.e. the nodes  that do not dominate any other node).", "acronym": "ID", "label": "immediate dominance", "ID": "1484"}, {"sentence": "A tree is often def ined as a set of  elements, cal led \"nodes\", on which two  relat ions are defined, ID  (D) and l inear precedence (<), which are  required to have certa in propert ies to  the effect that a tree has exact ly one  root node, which dominates every other  node ( immediate ly  or indirect ly);  that  every node in a tree has exact ly  one  \"mother\" node, etc. (", "acronym": "ID", "label": "immediate  dominance", "ID": "1485"}, {"sentence": "IDiption isgenerated, a mechanism allows the  user to request he information that was omitted for  brevity sake (see Paris 1987).", "acronym": "ID", "label": "initial descr", "ID": "1486"}, {"sentence": "A false start occurs when the  speaker goofs on his IDiption, stops, and then  restarts the description (also see Polanyi (1978) on false  starts).", "acronym": "ID", "label": "initial descr", "ID": "1487"}, {"sentence": "A respecification of the IDiption in more  detail.", "acronym": "ID", "label": "initial descr", "ID": "1488"}, {"sentence": "For example, in the case of the descriptions the  thing that is flared at the top and the main tube which is  the biggest ube, the relative clauses are needed because  the IDiptions are too general to distinguish  any one object.", "acronym": "ID", "label": "initial descr", "ID": "1489"}, {"sentence": "then the later  references will be some subset of IDiption  (like ?", "acronym": "ID", "label": "initial descr", "ID": "1490"}, {"sentence": "which is de- fined as the prefix of the IDiption.", "acronym": "ID", "label": "initial descr", "ID": "1491"}, {"sentence": "SPIN builds a task representation  from a top level goal and an IDiption of the  world using a hierarchical non-linear planning technique.", "acronym": "ID", "label": "initial descr", "ID": "1492"}, {"sentence": "Using Internet Searches for Influenza Sur-veillance, Clinical ID Vol.", "acronym": "ID", "label": "Infectious Diseases", "ID": "1493"}, {"sentence": "4.2 ID The ID track differs from the Genia track in two important ways.", "acronym": "ID", "label": "Infectious Diseases", "ID": "1494"}, {"sentence": "This project has been funded in whole or in part with Fed- eral funds from the National Institute of Allergy and ID, National Institutes of Health, Department of Health and Human Services, under Contract No.", "acronym": "ID", "label": "Infectious Diseases", "ID": "1495"}, {"sentence": "We use custom versions of these (except for ID where we use those from Stenetorp et al (2011)).", "acronym": "ID", "label": "Infectious Diseases", "ID": "1496"}, {"sentence": "Our system performed competitively, obtain- ing 3rd place in the ID track (50.6% f-score), 5th place in Epigenetics and Post-translational Modifications (31.2%), and 7th place in Genia (50.0%).", "acronym": "ID", "label": "Infectious Diseases", "ID": "1497"}, {"sentence": "Recently there has been a surge of interest in extracting PAes from online data due to the rapid growth of E-Commerce.", "acronym": "PA", "label": "product attribut", "ID": "1498"}, {"sentence": "These sentiment factors may also be factors based on PAes.", "acronym": "PA", "label": "product attribut", "ID": "1499"}, {"sentence": "An unsupervised ap- proach to PAe extraction.", "acronym": "PA", "label": "product attribut", "ID": "1500"}, {"sentence": "c?2011 Association for Computational Linguistics Bootstrapped Named Entity Recognition for Product Attribute Extraction Duangmanee (Pew) Putthividhya eBay Inc. 2065 Hamilton Ave San Jose, CA 95125 dputthividhya@ebay.com Junling Hu eBay Inc. 2065 Hamilton Ave San Jose, CA 95125 juhu@ebay.com Abstract We present a named entity recognition (NER) system for extracting PAes and values from listing titles.", "acronym": "PA", "label": "product attribut", "ID": "1501"}, {"sentence": "Extracting PAes from such short titles faces the following challenges: ?", "acronym": "PA", "label": "product attribut", "ID": "1502"}, {"sentence": "Then there came up with research works shifting focus from overall document sentiment to sentiment analysis based on PAes (Hu and Liu, 2004; Popescu and Etzioni, 2005; Ding and Liu, 2007; Liu et al, 2005).", "acronym": "PA", "label": "product attribut", "ID": "1503"}, {"sentence": "1 Introduction At present a classical Chinese NLU architec- ture usually includes several components, such as Word Segmentation (Word-Seg), POS Tag- ging, PAs, Parsing, Word Sense Dis- ambiguation (WSD) and so on.", "acronym": "PA", "label": "Phrase Analysi", "ID": "1504"}, {"sentence": "085  0.038 0.060  +13%  +31%  +26%  +33%  +58%  +32%  Average Search Precision for  Query Statement PAs  (CACM Collection, 25 Queries)  Table 5  The special processing described up to now  is user related in the sense that user query  formulations and user relevance assessments are  utilized to improve the retrieval procedures.", "acronym": "PA", "label": "Phrase Analysi", "ID": "1505"}, {"sentence": "3.3 Sentence and PAs Tech- niques  Annotation of real text requires various tech- niques to be applied.", "acronym": "PA", "label": "Phrase Analysi", "ID": "1506"}, {"sentence": "Word Formation Approach to Noun  PAs for Thai,  Proceedings of  SNLP2002.", "acronym": "PA", "label": "Phrase Analysi", "ID": "1507"}, {"sentence": "Word Formation Approach and Noun PAs for Thai? ?,", "acronym": "PA", "label": "Phrase Analysi", "ID": "1508"}, {"sentence": "In the future we are going to add the PAs, WSD (Word Sense Disambiguation) and Semantic Analysis components into CUP, because it is impossible to analyze some sen- tences correctly without semantic understand- ing and the PAs helps to en- hance the performance of Parsing.", "acronym": "PA", "label": "Phrase Analysi", "ID": "1509"}, {"sentence": "In Proceedings of the 2005 International Conference on Acoustics, Speech, and Signal Process- ing (ICASSP 2005), PA, Pennsylvania.", "acronym": "PA", "label": "Philadelphia", "ID": "1510"}, {"sentence": "In EMNLP-02, pages 46?51, PA, USA.", "acronym": "PA", "label": "Philadelphia", "ID": "1511"}, {"sentence": "John Benjamins Publishing Company, Amsterdam/PA.", "acronym": "PA", "label": "Philadelphia", "ID": "1512"}, {"sentence": "PA, US.", "acronym": "PA", "label": "Philadelphia", "ID": "1513"}, {"sentence": "Semantic orientation applied to unsupervised clas- sification of reviews, Proceedings of ACL-02,  PA, Pennsylvania, 417-424  Wiebe, Janyce, Bruce M., Rebecca F., and Thomas P.  O'Hara.", "acronym": "PA", "label": "Philadelphia", "ID": "1514"}, {"sentence": "1044  AN AUTOMATIC  METHOD OF F INDING TOP IC   BOUNDARIES   Jeffrey C. Reynar*  Depar tment  of Computer  and In format ion  Sc ience  Un ivers i ty  of  Pennsy lvan ia   PA ,  Pennsy lvan ia ,  USA  j c reynar@unag i .c i s .upenn.edu   Abstract  This article outlines a new method of locating discourse  boundaries based on lexical cohesion and a graphical  technique called dotplotting.", "acronym": "PA", "label": "Ph i lade lph ia", "ID": "1515"}, {"sentence": "PA ,  1980.", "acronym": "PA", "label": "Ph i lade lph ia", "ID": "1516"}, {"sentence": "1190  GRADED UNIF ICAT ION:  A FRAMEWORK FOR  INTERACTIVE  PROCESSING  Alber t  K im *  Depar tment  of  Computer  and  In fo rmat ion  Sciences  Un ivers i ty  of  Pennsy lvan ia   PA ,  Pennsy lvan ia ,  USA  email :  a lk im?unagi ,  cis.", "acronym": "PA", "label": "Ph i lade lph ia", "ID": "1517"}, {"sentence": "I~oo~ of  the  18th  Meet in~ of  the  Assoc ia t ion  fo r  Computat iona l  L in~ulet??so   PA ,  1980  Kwasny 8o and Sondheimer N. \"Ungrsmmat ioa l i ty  and Ex~rn-  ~ammat ica l i ty  in  Natura l  Lant~age Unders tand ing  Syst -   ems\"?", "acronym": "PA", "label": "Ph i lade lph ia", "ID": "1518"}, {"sentence": "93   A Simple Rule-Based Part of Speech Tagger  Er ic  Br i l l  *  Depar tment  of Computer  Sc ience  Un ivers i ty  of  Pennsy lvan ia   PA ,  Pennsy lvan ia  19104  U.S.A.  br i l l@unag i .c i s .upenn.edu   Abst ract   Automatic part of speech tagging is an area  of natural anguage processing where statistical  techniques have been more successful than rule-  based methods.", "acronym": "PA", "label": "Ph i lade lph ia", "ID": "1519"}, {"sentence": "Inter -Coder  Agreement:  Inter-coder agree-  ment is a direct measure of consistency among  20  PAt  Speech-act 82.14  Dialog-act 65.48  Concept lists 88.00  Argument lists I 85.79  Table 2: Inter-coder Agreement between CMU  and IRST  C-STAR partners.", "acronym": "PA", "label": "Percent Agreemen", "ID": "1520"}, {"sentence": "to judge single terms without any context: they had to think about all the senses of Metric PAt Kappa HN 0.909 0.465 N 0.796 0.368 P 0.714 0.281 HP 0.846 0 N+HN 0.829 0.396 P+HP 0.728 0.280 ALL 0.766 0.318 Table 6: Inter-annotator agreement on checking the trian- gulated list.", "acronym": "PA", "label": "Percent Agreemen", "ID": "1521"}, {"sentence": "8 113 112 46 151 85 94 56  Al~reement 87 82 91 89 89 90 90 90 90 88 92 90 91 89 85 89 92 91 91 86  Boundary  21 16 7 10 6 5 11 5 8 22 13 17 9 11 8 7 15 11 10 6  Agreement  74 70 76 77 60 80 79 69 75 70 74 75 73 71 68 73 77 71 80 74  Non-Boundary   % Agreement   117 105 48 53 63 78 79 45 88 173 97 143 99 102 104 39 136 74 84 50  89 84 93 91 92 91 92 92 92 90 95 91 93 91 87 92 93 94 93 88  Table 1: PAt with the Majority Opinion  of boundaries assigned by subject i. In the case of  narrative 9 (j=96), one of the subjects assigned 8  boundaries.", "acronym": "PA", "label": "Percent Agreemen", "ID": "1522"}, {"sentence": "Metric PAt Kappa HN 0.804 0.523 N 0.765 0.545 P 0.686 0.405 HP 0.855 0.669 N+HN 0.784", "acronym": "PA", "label": "Percent Agreemen", "ID": "1523"}, {"sentence": "In ad- dition, annotators had to judge single terms without any context: they had to think about all the senses of Metric PAt Kappa HN 0.909 0.465 N 0.796 0.368 P 0.714 0.281 HP 0.846 0 N+HN 0.829 0.396 P+HP 0.728 0.280 ALL 0.766 0.318 Table 6: Inter-annotator agreement on checking the trian- gulated list.", "acronym": "PA", "label": "Percent Agreemen", "ID": "1524"}, {"sentence": "Metric PAt Kappa HN 0.804 0.523 N 0.765 0.545 P 0.686 0.405 HP 0.855 0.669 N+HN 0.784 0.553 P+HP 0.783 0.559 ALL 0.826 0.614 Table 7: Inter-annotator agreement on checking the can- didates.", "acronym": "PA", "label": "Percent Agreemen", "ID": "1525"}, {"sentence": "3.2.4 PAt.", "acronym": "PA", "label": "Percent Agreemen", "ID": "1526"}, {"sentence": "c?2009 AssociACL A Markov Logic Approach to Bio-Molecular Event Extraction Sebastian Riedel ??", "acronym": "ACL", "label": "ation for Computational Linguistics", "ID": "1527"}, {"sentence": "2007 AssociACL Computational Linguistics Volume 33, Number 1 Research in QA has been developed from two different scientific perspectives, artificial intelligence (AI) and information retrieval (IR).", "acronym": "ACL", "label": "ation for Computational Linguistics", "ID": "1528"}, {"sentence": "c?2016 AssociACL Solving and Generating Chinese Character Riddles Chuanqi Tan? ?", "acronym": "ACL", "label": "ation for Computational Linguistics", "ID": "1529"}, {"sentence": "Associ- ACL.", "acronym": "ACL", "label": "ation for Computational Linguistics", "ID": "1530"}, {"sentence": "c?2010 AssociACL Features for Detecting Hedge Cues Nobuyuki Shimizu Information Technology Center The University of Tokyo shimizu@r.dl.itc.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center The University of Tokyo n3@dl.itc.u-tokyo.ac.jp Abstract We present a sequential labeling approach to hedge cue detection submitted to the bi- ological portion of task 1 for the CoN", "acronym": "ACL", "label": "ation for Computational Linguistics", "ID": "1531"}, {"sentence": "In Janyce Wiebe, editor, Proceedings of the 1st Meeting of the North American Chapter of the AssociACL, Seattle, WA.", "acronym": "ACL", "label": "ation for Computational Linguistics", "ID": "1532"}, {"sentence": "2007 ACL Computational Linguistics Volume 33, Number 1 Research in QA has been developed from two different scientific perspectives, artificial intelligence (AI) and information retrieval (IR).", "acronym": "ACL", "label": "Association for Computational Linguistics", "ID": "1533"}, {"sentence": "c?2009 ACL A Markov Logic Approach to Bio-Molecular Event Extraction Sebastian Riedel ??", "acronym": "ACL", "label": "Association for Computational Linguistics", "ID": "1534"}, {"sentence": "c?2016 ACL A General Regularization Framework for Domain Adaptation Wei Lu1 and Hai Leong Chieu2 and Jonathan Lo?fgren3 1Singapore University of Technology and Design 2DSO National Laboratories 3Uppsala University luwei@sutd.edu.sg, chaileon@dso.org.sg, lofgren021@gmail.com Abstract We propose a domain adaptation framework, and formally prove that it generalizes the", "acronym": "ACL", "label": "Association for Computational Linguistics", "ID": "1535"}, {"sentence": "c?2016 ACL Solving and Generating Chinese Character Riddles Chuanqi Tan? ?", "acronym": "ACL", "label": "Association for Computational Linguistics", "ID": "1536"}, {"sentence": "c?2010 ACL Features for Detecting Hedge Cues Nobuyuki Shimizu Information Technology Center The University of Tokyo shimizu@r.dl.itc.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center The University of Tokyo n3@dl.itc.u-tokyo.ac.jp Abstract We present a sequential labeling approach to hedge cue detection submitted to the bi- ological portion of task 1 for t", "acronym": "ACL", "label": "Association for Computational Linguistics", "ID": "1537"}, {"sentence": "In Janyce Wiebe, editor, Proceedings of the 1st Meeting of the North American Chapter of the ACL, Seattle, WA.", "acronym": "ACL", "label": "Association for Computational Linguistics", "ID": "1538"}, {"sentence": "2007 ACLs Computational Linguistics Volume 33, Number 1 Research in QA has been developed from two different scientific perspectives, artificial intelligence (AI) and information retrieval (IR).", "acronym": "ACL", "label": "Association for Computational Linguistic", "ID": "1539"}, {"sentence": "c?2009 ACLs A Markov Logic Approach to Bio-Molecular Event Extraction Sebastian Riedel ??", "acronym": "ACL", "label": "Association for Computational Linguistic", "ID": "1540"}, {"sentence": "c?2016 ACLs A General Regularization Framework for Domain Adaptation Wei Lu1 and Hai Leong Chieu2 and Jonathan Lo?fgren3 1Singapore University of Technology and Design 2DSO National Laboratories 3Uppsala University luwei@sutd.edu.sg, chaileon@dso.org.sg, lofgren021@gmail.com Abstract We propose a domain adaptation framework, and formally prove that it generalizes the", "acronym": "ACL", "label": "Association for Computational Linguistic", "ID": "1541"}, {"sentence": "c?2016 ACLs Solving and Generating Chinese Character Riddles Chuanqi Tan? ?", "acronym": "ACL", "label": "Association for Computational Linguistic", "ID": "1542"}, {"sentence": "c?2010 ACLs Features for Detecting Hedge Cues Nobuyuki Shimizu Information Technology Center The University of Tokyo shimizu@r.dl.itc.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center The University of Tokyo n3@dl.itc.u-tokyo.ac.jp Abstract We present a sequential labeling approach to hedge cue detection submitted to the bi- ological portion of task 1 for t", "acronym": "ACL", "label": "Association for Computational Linguistic", "ID": "1543"}, {"sentence": "In Janyce Wiebe, editor, Proceedings of the 1st Meeting of the North American Chapter of the ACLs, Seattle, WA.", "acronym": "ACL", "label": "Association for Computational Linguistic", "ID": "1544"}, {"sentence": "brown, edu  Abst ract   This paper presents an algorithm for identiACL  fying pronominal anaphora and two experiACL  ments based upon this algorithm.", "acronym": "ACL", "label": "-", "ID": "1545"}, {"sentence": "We incorpoACL  rate multiple anaphora resolution factors into  a statistical framework ACL ACL  specifically the disACL  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informaACL  tion and noun phrase repetition.", "acronym": "ACL", "label": "-", "ID": "1546"}, {"sentence": "In Proceedings of the Association for Computational Linguistics Workshop on Computational ACL.", "acronym": "ACL", "label": "Approaches to Semitic Languages", "ID": "1547"}, {"sentence": "Compu- tational ACL, pp.", "acronym": "ACL", "label": "Approaches to Semitic Languages", "ID": "1548"}, {"sentence": "In Pro- ceedings of the Workshop on Computational ACL, pages 27?", "acronym": "ACL", "label": "Approaches to Semitic Languages", "ID": "1549"}, {"sentence": "In Proceedings of the Workshop on Compu- tational ACL, pages 9?", "acronym": "ACL", "label": "Approaches to Semitic Languages", "ID": "1550"}, {"sentence": "In Proceedings of the Workshop on Computational ACL, pages 9?16, Prague, Czech Republic.", "acronym": "ACL", "label": "Approaches to Semitic Languages", "ID": "1551"}, {"sentence": "The user should choose the area so that  it contains necessary and sufficient words to be one  ME.", "acronym": "ME", "label": "meaningful expression", "ID": "1552"}, {"sentence": "These subjective ex- pressions could then be added to a subjectivity lex- icon (Esuli and Sebastiani, 2005), and used to gain understanding about which types of complex fea- tures capture MEs that are im- portant for opinion recognition.", "acronym": "ME", "label": "meaningful expression", "ID": "1553"}, {"sentence": "Figure 1: The flow of finding the correspondences of word sequences  number of other interesting and ME that should be translated in a specific way.", "acronym": "ME", "label": "meaningful expression", "ID": "1554"}, {"sentence": "I  distinguish - as usual - two kinds of MEs in SRL, terms and  formulas.", "acronym": "ME", "label": "meaningful expression", "ID": "1555"}, {"sentence": "tional restrictions and so serve to delimit the set of  MEs in the language.", "acronym": "ME", "label": "meaningful expression", "ID": "1556"}, {"sentence": "Therefore, it makes sense to restrict our attention to a  finite subset E of A, containing all expressions of  length less than or equal to an appropriately fixed  integer n.  Let L be the set of all MEs of a  natural language, that is, of all expressions to which  humans attach a meaning.", "acronym": "ME", "label": "meaningful expression", "ID": "1557"}, {"sentence": "One such tool was the template-filling tool devel - oped by Bill Ogden and Jim Cowie at New Mexico State University (known as Locke in the version designed fo r English ME) .", "acronym": "ME", "label": "Microelectronics", "ID": "1558"}, {"sentence": "54  COMPARING HUMAN AND MACHINE PERFORMANCE FO R NATURAL LANGUAGE INFORMATION EXTRACTION : Results for English ME from the MUC-5 Evaluation Craig A. Will Institute for Defense Analyses Computer and Software Engineering Division 1801 N. Beauregard Street Alexandria, VA 2231 1 INTRODUCTION In evaluating the state of technology for extracting information from natural language text by machine, it i s valuable to compare the performance of machine extraction systems with that achieved by humans pe", "acronym": "ME", "label": "Microelectronics", "ID": "1559"}, {"sentence": "This paper discusses preparation of templates and presents results for human and machine per- formance for English ME ; a companion paper [1] presents additional experimental results.", "acronym": "ME", "label": "Microelectronics", "ID": "1560"}, {"sentence": "To improve quality and consistency, three steps were taken : First, a set of relatively detailed rules for extracting information from articles and structuring it as an object - oriented template was developed (with the rules for English ME a 40-page, single-spaced document) .", "acronym": "ME", "label": "Microelectronics", "ID": "1561"}, {"sentence": "Previous experience with the English and Japanese Joint Ventures corpus had made it clear that producin g templates with a high degree of quality and consistency is a difficult and time-consuming task 11l, and we attempte d 54 to make the best use of what had been learned in that effort in producing templates for English ME with quality and consistency appropriate to both the needs of the project and the resources we had available .", "acronym": "ME", "label": "Microelectronics", "ID": "1562"}, {"sentence": "THE PREPARATION OF TEMPLATE S The development of templates for the English ME corpus began in the fall of 1992 .", "acronym": "ME", "label": "Microelectronics", "ID": "1563"}, {"sentence": "4.1 Classifiers We use a NBian model as in Kupiec, Pedersen, and Chen?s (1995) experiment (cf.", "acronym": "NB", "label": "na??ve Bayes", "ID": "1564"}, {"sentence": "To test if such a simple approach would be enough, we performed a text categorization experiment, using the Rainbow implementation of a NB term frequency times inverse document frequency (TF*IDF) method (McCallum 1997) and considering each sentence as a ?", "acronym": "NB", "label": "na??ve Bayes", "ID": "1565"}, {"sentence": "In this paper we generalize the tri-training al- gorithm and use three different learning algo- rithms rather than bootstrap samples to create diversity: a NB algorithm (no smooth- ing), random forests (Breiman, 2001) (with 100 unpruned decision trees) and an algorithm that induces unpruned decision trees.", "acronym": "NB", "label": "na??ve Bayes", "ID": "1566"}, {"sentence": "In their exper- iments, they consider classifiers based on deci- sion trees, BP neural networks and NB inference.", "acronym": "NB", "label": "na??ve Bayes", "ID": "1567"}, {"sentence": "We have experimented with a maxi- mum entropy model, Repeated Incremental Pruning to Produce Error Reduction (RIP- PER), and decision trees; preliminary results do not show significant improvement over the NBian model.", "acronym": "NB", "label": "na??ve Bayes", "ID": "1568"}, {"sentence": "This contrasts with typical semi-supervised learning methods for text categorization that com- bine unlabeled and labeled data within a genera- tive model, such as multinomial NB, via expectation-maximization (Nigam et al, 2000) or semi-supervised frequency estimation (Su et al, 2011).", "acronym": "NB", "label": "na??ve Bayes", "ID": "1569"}, {"sentence": "For the NBian classification method, indeed, it is most important that the features be as independent of each other as possible.", "acronym": "NB", "label": "na??ve Bayes", "ID": "1570"}, {"sentence": "We used the NLTK (Bird, 2006) implementation of the NB classifier for all our experiments.", "acronym": "NB", "label": "Na??ve Bayesian", "ID": "1571"}, {"sentence": "ME classifiers outperform their generative counterparts (e.g., NB clas- sifiers) because they can easily handle overlapping, probably dependent features which might be con- tained in rich feature sets.", "acronym": "NB", "label": "Na??ve Bayesian", "ID": "1572"}, {"sentence": "For rhetorical relations, NB models achieve 84.90% and 57.87% accuracy in classifying NARRATION and BACKGROUND / ELABORATION re- lations respectively (16% and 23% above baseline).", "acronym": "NB", "label": "Na??ve Bayesian", "ID": "1573"}, {"sentence": "Fn?1; P(C): (Overall) probability of category C; P(Fj | C): Probability of feature-value pair Fj, given that the sentence is of target category C; P(Fj): Probability of feature value Fj; Figure 9 NB classifier.", "acronym": "NB", "label": "Na??ve Bayesian", "ID": "1574"}, {"sentence": "So this kind of training is misleading especially for a NB classifier that utilizes the prior prob- ability of classes.", "acronym": "NB", "label": "Na??ve Bayesian", "ID": "1575"}, {"sentence": "Obtaining Cal- ibrated Probability Estimates from Decision Trees and NB Classiers.", "acronym": "NB", "label": "Na??ve Bayesian", "ID": "1576"}, {"sentence": "In  this paper, we integrate punctuation rules, lexicon, and NB models  to recognize creation titles in Chinese documents.", "acronym": "NB", "label": "na?ve Bayesian", "ID": "1577"}, {"sentence": "An approach  of integrating punctuation rules, lexicon, and NB models is employed to  recognize creation titles in Chinese documents.", "acronym": "NB", "label": "na?ve Bayesian", "ID": "1578"}, {"sentence": "Another usage of the NB  model in summarization can be found in (Aone et  al.,", "acronym": "NB", "label": "na?ve Bayesian", "ID": "1579"}, {"sentence": "Section 4 addresses which features  may be adopted in training NB model.", "acronym": "NB", "label": "na?ve Bayesian", "ID": "1580"}, {"sentence": "The focus of this study is related to different classification techniques such as support-vector machines (SVM), multi-layer perceptron, NB nets, decision trees and random forest.", "acronym": "NB", "label": "na?ve Bayesian", "ID": "1581"}, {"sentence": "The rest of undetermined  candidates are verified by NB model.", "acronym": "NB", "label": "na?ve Bayesian", "ID": "1582"}, {"sentence": "However, we find that NBs in con- junction with Dirichlet smoothing (Smucker and Allan, 2006) works at least as well when appropri- ately tuned.", "acronym": "NB", "label": "Naive Baye", "ID": "1583"}, {"sentence": "Figure 1 shows a choropleth map of the behav- ior of NBs, plotting the rank of cells for 338 Figure 1: Relative NBs rank of cells for ENWIKI13 test document Pennsylvania Avenue (Washington, DC), surrounding the true location.", "acronym": "NB", "label": "Naive Baye", "ID": "1584"}, {"sentence": "Edlin provides gen- eral machine learning architecture for linear models and a framework with implementations of popular learning algorithms including NBs, percep- tron, maximum entropy, one-best MIRA, and condi- tional random fields (CRF) among others.", "acronym": "NB", "label": "Naive Baye", "ID": "1585"}, {"sentence": "andard mea- sures such as Kullback-Leibler (KL) divergence (Zhai and Lafferty, 2001), which seeks the cell whose language model is closest to the test doc- ument?s, or NBs (Lewis, 1998), which chooses the cell that assigns the highest probabil- ity to the test document.", "acronym": "NB", "label": "Naive Baye", "ID": "1586"}, {"sentence": "A test document?s location is then chosen based on the cell with the most sim- ilar language model according to standard mea- sures such as Kullback-Leibler (KL) divergence (Zhai and Lafferty, 2001), which seeks the cell whose language model is closest to the test doc- ument?s, or NBs (Lewis, 1998), which chooses the cell that assigns the highest probabil- ity to the test document.", "acronym": "NB", "label": "Naive Baye", "ID": "1587"}, {"sentence": "Importantly, this is the first method that improves upon straight uniform-grid NBs on all of these corpora, in contrast with k-d trees (Roller12) and the current state-of-the-art tech- nique for Twitter users of geographically-salient feature selection (Han14).", "acronym": "NB", "label": "Naive Baye", "ID": "1588"}, {"sentence": "3.2 NBs A geodesic grid of sufficient granularity creates a large decision space, when each cell is viewed as a label to be predicted by some classifier.", "acronym": "NB", "label": "Naive Baye", "ID": "1589"}, {"sentence": "However, we find that NB in con- junction with Dirichlet smoothing (Smucker and Allan, 2006) works at least as well when appropri- ately tuned.", "acronym": "NB", "label": "Naive Bayes", "ID": "1590"}, {"sentence": "Figure 1 shows a choropleth map of the behav- ior of NB, plotting the rank of cells for 338 Figure 1: Relative NB rank of cells for ENWIKI13 test document Pennsylvania Avenue (Washington, DC), surrounding the true location.", "acronym": "NB", "label": "Naive Bayes", "ID": "1591"}, {"sentence": "Edlin provides gen- eral machine learning architecture for linear models and a framework with implementations of popular learning algorithms including NB, percep- tron, maximum entropy, one-best MIRA, and condi- tional random fields (CRF) among others.", "acronym": "NB", "label": "Naive Bayes", "ID": "1592"}, {"sentence": "andard mea- sures such as Kullback-Leibler (KL) divergence (Zhai and Lafferty, 2001), which seeks the cell whose language model is closest to the test doc- ument?s, or NB (Lewis, 1998), which chooses the cell that assigns the highest probabil- ity to the test document.", "acronym": "NB", "label": "Naive Bayes", "ID": "1593"}, {"sentence": "A test document?s location is then chosen based on the cell with the most sim- ilar language model according to standard mea- sures such as Kullback-Leibler (KL) divergence (Zhai and Lafferty, 2001), which seeks the cell whose language model is closest to the test doc- ument?s, or NB (Lewis, 1998), which chooses the cell that assigns the highest probabil- ity to the test document.", "acronym": "NB", "label": "Naive Bayes", "ID": "1594"}, {"sentence": "Importantly, this is the first method that improves upon straight uniform-grid NB on all of these corpora, in contrast with k-d trees (Roller12) and the current state-of-the-art tech- nique for Twitter users of geographically-salient feature selection (Han14).", "acronym": "NB", "label": "Naive Bayes", "ID": "1595"}, {"sentence": "3.2 NB A geodesic grid of sufficient granularity creates a large decision space, when each cell is viewed as a label to be predicted by some classifier.", "acronym": "NB", "label": "Naive Bayes", "ID": "1596"}, {"sentence": "A  series  of  four  experiments  was  conducted  on  a  baseline  method:  NBs  with varying sets of attributes.", "acronym": "NB", "label": "Na?ve  Baye", "ID": "1597"}, {"sentence": "We therefore modified our  system to incorporate the position information with  the help of SVMs and we also investigated the  capability of SVMs versus NBs on this  problem.", "acronym": "NB", "label": "Na?ve  Baye", "ID": "1598"}, {"sentence": "Besides,  the  proposed  method  is  compared  with  three  baselines,  namely  NBs  classifier,  a  language  model  and an approach based on significant collo- cations.", "acronym": "NB", "label": "Na?ve  Baye", "ID": "1599"}, {"sentence": "As  an  alternative  other  teams  during  the  last  TREC and NTCIR evaluation campaigns have sug- gested  variations  of  NBs  classifier,  lan- guage models and SVM, along with the use of such  heuristics  as  word  order,  punctuation,  sentence  length, etc.", "acronym": "NB", "label": "Na?ve  Baye", "ID": "1600"}, {"sentence": "Bhalotia et al (2003)  converted this task into a binary classification  problem and trained a NBs classifier with  kernels, achieving 53.04% for CD.", "acronym": "NB", "label": "Na?ve  Baye", "ID": "1601"}, {"sentence": "Our approach was compared to the three baselines,  namely  NBs  classifier,  language  model  and an approach based on finding significant collo- cations.", "acronym": "NB", "label": "Na?ve  Baye", "ID": "1602"}, {"sentence": "ML method (Weka) Features Accuracy Decision Trees PMI scores 65.4% Decision Rules PMI scores 65.5% NB PMI scores 52.5% K-Nearest Neighbor PMI scores 64.5% Kernel Density PMI scores 60.5% Boosting (Dec. Stumps) PMI scores 67.7% NB 500 words 68.0% Decision Trees 500 words 67.0% NB PMI + 500 words 66.5% Boosting (Dec. Stumps) PMI + 500 words 69.2% Table 6: Comparative results for the supervised learning method using various ML learning algo- rithms (Weka), averaged ov", "acronym": "NB", "label": "Na??ve Bayes", "ID": "1603"}, {"sentence": "ML method (Weka) Features Accuracy Decision Trees PMI scores 65.4% Decision Rules PMI scores 65.5% NB PMI scores 52.5% K-Nearest Neighbor PMI scores 64.5% Kernel Density PMI scores 60.5% Boosting (Dec. Stumps) PMI scores 67.7% NB 500 words 68.0% Decision Trees 500 words 67.0% NB PMI + 500 words 66.5% Boosting (Dec. Stumps) PMI + 500 words 69.2% Table 6: Comparative results for the supervised learning method using various ML learning algo- rithms (Weka), averaged over the seven groups of near-synonyms from the Exp1 data set.", "acronym": "NB", "label": "Na??ve Bayes", "ID": "1604"}, {"sentence": "In particular, when using the 500 word features for each training exam- ple, only the NB algorithm was able to run in reasonable time.", "acronym": "NB", "label": "Na??ve Bayes", "ID": "1605"}, {"sentence": "The classifiers that use PMI features are Decision Trees, Decision Rules, NB, K-Nearest Neighbor, Kernel Density, and Boosting a weak classifier (De- cision Stumps ?", "acronym": "NB", "label": "Na??ve Bayes", "ID": "1606"}, {"sentence": "Then, a NB classifier that uses only the word features is presented, and the same type of classifiers with both types of features.", "acronym": "NB", "label": "Na??ve Bayes", "ID": "1607"}, {"sentence": "We noticed that the NB classifier performs very poorly on PMI features only (55% average accuracy), but performs very well on word features (68% average accuracy).", "acronym": "NB", "label": "Na??ve Bayes", "ID": "1608"}, {"sentence": "Common Local Temporal Noun expres- sions  3.2 NBian Classifier  A variety of machine learning classifiers are de- signed to resolve the classification problem,  such as SVM classifier, ME classifier and the  Decision Tree family.", "acronym": "NB", "label": "Na?ve Bayes", "ID": "1609"}, {"sentence": "Pang et al  (2002) adopt the VSM model to represent product  reviews and apply text classification algorithms  such as NB, maximum entropy and sup- port vector machines to predict sentiment polarity  of given product review.", "acronym": "NB", "label": "Na?ve Bayes", "ID": "1610"}, {"sentence": "Text  Dictionary Lookup  Found  Noun Phrase Head identifier  NB classifier  Best guess cluster Mayo Synonym Clusters M001|cholangeocarcinoma  M001|bile duct cancer  M001|?", "acronym": "NB", "label": "Na?ve Bayes", "ID": "1611"}, {"sentence": "The ML (Machine Learning) Named Entity  annotator is based on a NB classifier  trained on a combination of the UMLS entry terms  and the MCS where each diagnostic statement is  represented as a bag-of-words and used as a train- ing sample for generating a Naive Bayes classifier  which assigns MCS id?s to noun phrases identified  in the text of clinical notes.", "acronym": "NB", "label": "Na?ve Bayes", "ID": "1612"}, {"sentence": "So the NBian Classifier  that assumes independence among feature deno- tations is suitable to be applied to our method.", "acronym": "NB", "label": "Na?ve Bayes", "ID": "1613"}, {"sentence": "From the  NB formula?", "acronym": "NB", "label": "Na?ve Bayes", "ID": "1614"}, {"sentence": "7.2 Machine Learning Approaches  We evaluated the performance of four different  machine-learning approaches on the DA classifi- cation tasks: memory-based learning (k-Nearest- Neighbor), decision trees, neural networks, and  NB n-gram classifiers.", "acronym": "NB", "label": "na?ve Bayes", "ID": "1615"}, {"sentence": "For concept sequence classifi- cation, no learning approach clearly outper- formed any other (with the exception that the  NB n-gram approach performed worse  than other approaches).", "acronym": "NB", "label": "na?ve Bayes", "ID": "1616"}, {"sentence": "This time, we compare with a bag-of- words NB system (reported by Hermann and Blunsom (2014)), a system trained on the Polyglot embeddings from Al-Rfou et al (2013) (which are multilingual, but not in a shared rep- resentation space), and the two systems developed by Hermann and Blunsom (2014).", "acronym": "NB", "label": "na?ve Bayes", "ID": "1617"}, {"sentence": "Another usage of the NBian  model in summarization can be found in (Aone et  al.,", "acronym": "NB", "label": "na?ve Bayes", "ID": "1618"}, {"sentence": "We used Rainbow (McCallum, 1996) for  our NB n-gram classifiers.", "acronym": "NB", "label": "na?ve Bayes", "ID": "1619"}, {"sentence": "Although our best- performing classifiers combined word and argu- ment parse information, the NB word  bigram classifier (Rainbow) performed very well  on the SA classification task.", "acronym": "NB", "label": "na?ve Bayes", "ID": "1620"}, {"sentence": "NB makes a strict indepen- dence assumption and can be swamped by the sheer number of features we used, but it is a solid baseline and does a decent job of suggesting which features are more powerful.", "acronym": "NB", "label": "NaiveBayes", "ID": "1621"}, {"sentence": "PARAM_CORPUS_PATH, \"src/main/resources/tweets.txt\"]]), Dimension.create(\"featureMode\", \"document\"), Dimension.create(\"learningMode\", \"singleLabel\"), Dimension.create(\"featureSet\", [EmoticonRatioExtractor.name, NumberOfHashTagsExtractor.name]), Dimension.create(\"dataWriter\", WekaDataWriter.name), Dimension.create(\"classificationArguments\", [NB.name, RandomForest.name])], reports: [BatchCrossValidationReport], // collects results from folds numFolds: 10]; Listing 1: Groovy code to configure a DKPro TC cross-validation BatchTask on Twitter data.", "acronym": "NB", "label": "NaiveBayes", "ID": "1622"}, {"sentence": "NB 56.3 76.2 50.7 Table 2: Performance of several machine learn- ing algorithms on the English TempEval-1 train- ing data, with cross-validation.", "acronym": "NB", "label": "NaiveBayes", "ID": "1623"}, {"sentence": "JRip is not without its own limitations but, for our task, it shows better results than NB.", "acronym": "NB", "label": "NaiveBayes", "ID": "1624"}, {"sentence": "The top classifier uses NB.", "acronym": "NB", "label": "NaiveBayes", "ID": "1625"}, {"sentence": "NB 56.3 76.2 50.7 Table 2: Comparing different algorithms (%-acc.", "acronym": "NB", "label": "NaiveBayes", "ID": "1626"}, {"sentence": "Such a rule was used by Soubbotin (2001), who developed a system who obtained the best accuracy in the 2001 TREC (Voorhees, 2001a).", "acronym": "TREC", "label": "Text REtrieval Conference", "ID": "1627"}, {"sentence": "TREC.", "acronym": "TREC", "label": "Text REtrieval Conference", "ID": "1628"}, {"sentence": "The evaluation consisted of four tasks  designed to assess performance ofautomatic  summaries used in real world tasks and to  leverage off of previous evaluations in IR and IE,  the TRECs and Message  Understanding Conferences, respectively.", "acronym": "TREC", "label": "Text REtrieval Conference", "ID": "1629"}, {"sentence": "In TREC, pages 104?", "acronym": "TREC", "label": "Text REtrieval Conference", "ID": "1630"}, {"sentence": "1 TREC (http://trec.nist.gov/).", "acronym": "TREC", "label": "Text REtrieval Conference", "ID": "1631"}, {"sentence": "First TREC.", "acronym": "TREC", "label": "Text REtrieval Conference", "ID": "1632"}, {"sentence": "Continuing Evaluation  During Phase II, TIPSTER became primary  sponsor for both the Message Understanding  Conferences and the TRECs,  based on the belief that these forums for evaluation  of text-processing technologies are essential to  continued success in TIPSTER research and  development.", "acronym": "TREC", "label": "Text Retrieval Conference", "ID": "1633"}, {"sentence": "Proceedings of the Twelfth TREC, November 18-21, 2003, Gaithersburg, MD.", "acronym": "TREC", "label": "Text Retrieval Conference", "ID": "1634"}, {"sentence": "In Proceedings of the 15th TREC.", "acronym": "TREC", "label": "Text Retrieval Conference", "ID": "1635"}, {"sentence": "of TREC, pages 16?19.", "acronym": "TREC", "label": "Text Retrieval Conference", "ID": "1636"}, {"sentence": "In pro- ceeding of the Eight TREC 1999,  253-258.", "acronym": "TREC", "label": "Text Retrieval Conference", "ID": "1637"}, {"sentence": "In Proceedings of the 10th TREC,  pages 437-446, 2001.", "acronym": "TREC", "label": "Text Retrieval Conference", "ID": "1638"}, {"sentence": "This algorithm searches the parse tree in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a pREF to antecedents hat are closer  to the pronoun.", "acronym": "REF", "label": "reference", "ID": "1639"}, {"sentence": "Since the train-  ing corpus is tawed with REF informa-  tion, the probability P(plWo) is easily obtained.", "acronym": "REF", "label": "reference", "ID": "1640"}, {"sentence": "This implements the observed pREF  for subject position antecedents.", "acronym": "REF", "label": "reference", "ID": "1641"}, {"sentence": "This program differs  from earlier work in its almost complete lack of  hand-crafting, relying instead on a very small  corpus of Penn Wall Street Journal Tree-bank  text (Marcus et al, 1993) that has been marked  with co-REF information.", "acronym": "REF", "label": "reference", "ID": "1642"}, {"sentence": "We collect these probabilities on the  training data, which are marked with REF  links.", "acronym": "REF", "label": "reference", "ID": "1643"}, {"sentence": "The corpus  is manually tagged with REF indices and  referents\" repetition numbers.", "acronym": "REF", "label": "reference", "ID": "1644"}, {"sentence": "in some respect other than that it is a panda as opposed to a bear, and looks for something about REF which might distinguish it from previously observed bears.", "acronym": "REF", "label": "obj123", "ID": "1645"}, {"sentence": "in s refers to the focused object REF.", "acronym": "REF", "label": "obj123", "ID": "1646"}, {"sentence": "1  \" \" \" ' ' ' \" |   10 100  Number of REF  O ?", "acronym": "REF", "label": "references", "ID": "1647"}, {"sentence": "salience(re/)  = -2  log  Making the unrealistic simplifying assumption  that REF of one gender class are com-  pletely independent of REF for another  classes 1, the likelihood function in this case is  just the product over all classes of the probabil-  ities of each class of reference to the power of  the number of observations of this class.", "acronym": "REF", "label": "references", "ID": "1648"}, {"sentence": "We  would like to know, therefore, whether the pat-  tern of pronoun REF that we observe for  a given referent is the result of our supposed  \"hypothesis about pronoun reference\" - that is,  the pronoun reference strategy we have provi-  sionally adopted in order to gather statistics -  or whether the result of some other unidentified  process.", "acronym": "REF", "label": "references", "ID": "1649"}, {"sentence": "Figure 2 shows average  accuracy as a function of number of REF  for a given referent.", "acronym": "REF", "label": "references", "ID": "1650"}, {"sentence": "The constraints rule out implausible can-  didates and the pREF mphasize the selec-  tion of the most likely antecedent.", "acronym": "REF", "label": "references", "ID": "1651"}, {"sentence": "Sub- ject(S), verbs(V), and objects(O) are displayed for both VP.", "acronym": "VP", "label": "verb phrases", "ID": "1652"}, {"sentence": "Torisawa et al, (2002) presented a method to detect asso- ciative relationships between VP.", "acronym": "VP", "label": "verb phrases", "ID": "1653"}, {"sentence": "Figure 2: An example of a Chinese sentence with a coordination of VP as predicate.", "acronym": "VP", "label": "verb phrases", "ID": "1654"}, {"sentence": "uffixes for inflec- tion as clues to identify the attachment position of the verb and adjective phrases, because Japanese verbs and adjectives have inflections, which depends 110 (no label) base form cont continuative form attr attributive form neg negative form hyp hypothetical form imp imperative form stem stem Table 2: Inflection tag suffixes on their modifying words and phrases (e.g. noun and VP).", "acronym": "VP", "label": "verb phrases", "ID": "1655"}, {"sentence": "Typically, ambiguous VP of the form v rip1 p rip2 are  resolved through a model which considers values of the four head words (v, nl ,  p and 77,2).", "acronym": "VP", "label": "verb phrases", "ID": "1656"}, {"sentence": "As an option, we also added voice infor- mation (A:active, P:passive and C:causative) to the VP, because it effectively helps to discrim- inate cases.", "acronym": "VP", "label": "verb phrases", "ID": "1657"}, {"sentence": "Parser-based games accept typed-in com- mands from the player, usually in the form of VP, such as ?", "acronym": "VP", "label": "verb phrases", "ID": "1658"}, {"sentence": "Torisawa et al, (2002) presented a method to detect asso- ciative relationships between VPs.", "acronym": "VP", "label": "verb phrase", "ID": "1659"}, {"sentence": "noun and VPs).", "acronym": "VP", "label": "verb phrase", "ID": "1660"}, {"sentence": "uffixes for inflec- tion as clues to identify the attachment position of the verb and adjective phrases, because Japanese verbs and adjectives have inflections, which depends 110 (no label) base form cont continuative form attr attributive form neg negative form hyp hypothetical form imp imperative form stem stem Table 2: Inflection tag suffixes on their modifying words and phrases (e.g. noun and VPs).", "acronym": "VP", "label": "verb phrase", "ID": "1661"}, {"sentence": "Subcategorization and voice Each verb has a subcategorization frame, which is useful for build- ing VP s", "acronym": "VP", "label": "verb phrase", "ID": "1662"}, {"sentence": "This is almost cer- tainly a misleading figure, since those two words do not form a plausible VP; it is much more probable that the very strong, in fact id- iomatic, correlation ?", "acronym": "VP", "label": "verb phrase", "ID": "1663"}, {"sentence": "Subcategorization and voice Each verb has a subcategorization frame, which is useful for build- ing VP structure.", "acronym": "VP", "label": "verb phrase", "ID": "1664"}, {"sentence": "The texts used to de- scribe states and actions could be very different in nature, e.g., a state text could be long, contain- ing sentences with complex linguistic structure, whereas an action text could be very concise or just a VP.", "acronym": "VP", "label": "verb phrase", "ID": "1665"}, {"sentence": "Parser-based games accept typed-in com- mands from the player, usually in the form of VPs, such as ?", "acronym": "VP", "label": "verb phrase", "ID": "1666"}, {"sentence": "Sentence 2, are VP, ROOT 1, Exports NNS, SBJ A1 3, thought VBN, VC PRED 4, to TO, OPRD C-A1 5, have VB, IM 6, risen VBN, VC 7, strongly RB, MNR 8, in IN, TMP 9, August NNP, PMOD [...] Figure 1: a labeled example for the (part of) sentence ?", "acronym": "VP", "label": "VBP", "ID": "1667"}, {"sentence": "U.S. to get feature values of -0.8353556 and -2.0460036 per language model (see 41 Feature Example for that-CCs Example for that-less CCs Dependency length and position of CC Position of matrix verb thatCC:mvInd 7.0 noThatCC:mvInd 7.0 Dist between matrix verb & CC thatCC:mvCCDist 1.0 noThatCC:mvCCDist 1.0 Length of CC thatCC:ccLen 29.0 noThatCC:ccLen 28.0 Matrix verb features POS-tag thatCC:mvPos:VP 1.0 noThatCC:mvPos:VP 1.0 Stem thatCC:mvStem:argue 1.0 noThatCC:mvStem:argue 1.0 Form thatCC:mvForm:argue 1.0 noThatCC:mvForm:argue 1.0 CCG supertag thatCC:mvSt:s[dcl]\\np/s[em] 1.0 noThatCC:mvSt:s[dcl]\\np/s[dcl] 1.0 uniform information density (UID) Average n-gram log probs thatCC:$uidCCMean1 -0.8353556 noThatCC:$uidCCMean1 -2.5177214 of first 2 words of that-less CCs thatCC:$uidCCMean2 -2.04", "acronym": "VP", "label": "VBP", "ID": "1668"}, {"sentence": "Therefore, if we take a naive filtering method using the rule dictionary, we Original Symbol Normalized Symbol NNP, NNS, NNPS, PRP NN RBR, RBS RB JJR, JJS, PRP$ JJ VBD, VBZ VP : , ?, ?", "acronym": "VP", "label": "VBP", "ID": "1669"}, {"sentence": "Groups Member POS tags Count % Noun NN/NNP/NNS/NNPS 7511 31.30 Verb VBD/VB/VBZ/VBN/VBG/VP 3285 13.69 Adj JJ/JJR/JJS 1718 7.16 Adv RB/RBR 742 3.09 Pronoun CD/PRP/PRP$ 1397 5.82 Content Noun/Verb/Adj/Adv/Pronoun 14653 61.05 Function Other 9347 38.95 Total All 45 POS tags 24K 100.00 Table 3: Group names, members, number and per- centage of the words according to their gold POS tags.", "acronym": "VP", "label": "VBP", "ID": "1670"}, {"sentence": "ues of -0.8353556 and -2.0460036 per language model (see 41 Feature Example for that-CCs Example for that-less CCs Dependency length and position of CC Position of matrix verb thatCC:mvInd 7.0 noThatCC:mvInd 7.0 Dist between matrix verb & CC thatCC:mvCCDist 1.0 noThatCC:mvCCDist 1.0 Length of CC thatCC:ccLen 29.0 noThatCC:ccLen 28.0 Matrix verb features POS-tag thatCC:mvPos:VP 1.0 noThatCC:mvPos:VP 1.0 Stem thatCC:mvStem:argue 1.0 noThatCC:mvStem:argue 1.0 Form thatCC:mvForm:argue 1.0 noThatCC:mvForm:argue 1.0 CCG supertag thatCC:mvSt:s[dcl]\\np/s[em] 1.0 noThatCC:mvSt:s[dcl]\\np/s[dcl] 1.0 uniform information density (UID) Average n-gram log probs thatCC:$uidCCMean1 -0.8353556 noThatCC:$uidCCMean1 -2.5177214 of first 2 words of that-less CCs thatCC:$uidCCMean2 -2.0460036 noThatCC:$uidCCMe", "acronym": "VP", "label": "VBP", "ID": "1671"}, {"sentence": "Implicative verbs are a special subclass2 of such verbs which give rise to entailments involving their 1Here, * indicates that VB1 can match any verb form, e.g. VB, VBD, VP, etc.", "acronym": "VP", "label": "VBP", "ID": "1672"}, {"sentence": "L ikewise  the  p~ocedure   ~h|ch  pa~ses in f in i t i va l  complements  in   accordance  w i th  $8 accepts  a con junct ion   o f  one or  more VPs  s ta r t ing  ~ i th   in f in i t i ves .", "acronym": "VP", "label": "verb  phrase", "ID": "1673"}, {"sentence": "As ~egards  the  F i r s t ,  Par tee   observes  ( (24 \\ ] ,  C25\\] )  that  a vers ion  o f   51~ which inse~ts  labe l led  bracket ing ,  and  a vers ion  oF $4 sens i t i ve  to  such  bracket ing  and genera l i sed  to  add sub jec t   - agreement  to  the  f i r s t  verb  in  each  con junct  o f  a con jo ined  VP ,  i s   needed in  o~dey .", "acronym": "VP", "label": "verb  phrase", "ID": "1674"}, {"sentence": "Without l abe l led  bracket ing ,  PTG has d tFF -   4~  i f   i f   i f   i f   i f   i f   i f   i f   4~  4~  i f   4~  i f   and then  const ra ins  the  pred icate  to  be a  con junct ion  o f  one or  mo~e VPs", "acronym": "VP", "label": "verb  phrase", "ID": "1675"}, {"sentence": "An example of a derivation tree in  treebank comparing between CG and CDG A status  of  transformed  CDG  treebank  is  30,340  text  lines  which  include  14,744  sen- tences,  9,651  VPs  or  subject-omitted  sentences and 5,945 noun phrases.", "acronym": "VP", "label": "verb  phrase", "ID": "1676"}, {"sentence": "Without l abe l led  bracket ing ,  PTG has d tFF -   4~  i f   i f   i f   i f   i f   i f   i f   i f   4~  4~  i f   4~  i f   and then  const ra ins  the  pred icate  to  be a  con junct ion  o f  one or  mo~e VPs   ident i f i ab le  as commencing ~ i th  concordant   F in i te  Forms.", "acronym": "VP", "label": "verb  phrase", "ID": "1677"}, {"sentence": "VP   M\\[ remove nontermina l  category   N. add new nontermina l  category   Cont ro l  l ing  access  to  l ingu is t i c   in fo rmat ion  by  means  of menu ensures  that  the   updated  f i les  are appropr ia te ly  recompi led  into  the form used by the program.", "acronym": "VP", "label": "verb  phrase", "ID": "1678"}, {"sentence": "a con jo ined  VP ,  i s   needed in  o~dey .", "acronym": "VP", "label": "verb  phrase", "ID": "1679"}, {"sentence": "0 1 2 4 5 6  Rule  us ing semant ic  re lat ion  to VP   Candidate judging rule 1  When a candidate r ferent of a case component (azero  pronoun) does not satisfy the semantic marker of the  case component in the case frame, it is given -5.", "acronym": "VP", "label": "verb  phrase", "ID": "1680"}, {"sentence": "Ru le  when a pronoun refers to a VP   Like a demonstrative pronoun, a demonstrative adjec-  tive can refer to the meaning of the verb phrase in the  previous sentence.", "acronym": "VP", "label": "verb  phrase", "ID": "1681"}, {"sentence": "This can be summarized by the following  table:    Outside SimpleNP Exceptions  Prepositional Phrases  Relative Clauses  VPs  Apposition1  Some conjunctions  (Conjunctions are  marked according to the  TreeBank guidelines)2.", "acronym": "VP", "label": "Verb Phrase", "ID": "1682"}, {"sentence": "Presented at the  Linguistic Conference on East Asian Languages: VPs, in  Los Angeles, California. (", "acronym": "VP", "label": "Verb Phrase", "ID": "1683"}, {"sentence": "Robinson, A. E., \"Determining VP  Referents in Dialog~\", American Journal of  Computatlor~l Linguistics\", Jan. 1981  17.", "acronym": "VP", "label": "Verb Phrase", "ID": "1684"}, {"sentence": "The structure is not only  functional (with- function s/m/ools laloeling the const|tuents instead of  category names like Noun Phrase and VP) but i t  is  multifunctional.", "acronym": "VP", "label": "Verb Phrase", "ID": "1685"}, {"sentence": "3 Outline  3.1 Corpus, Language Usage and Computa- ble Semantic Properties of VPs  section  Basic Computational Semantic Con- cepts  Theory of Norm and Exploitation of  Language Usage   Corpus Pattern Analysis in  Sketch En- gine  Sense Discriminative Patterns  3.2   Semantic Types and Ontologies          Argument Structures         Frames and Semantic Types    Inducing Semantic Types  Discriminative Patterns  3.3 Statistical Models for Corpus Pattern  Recognition", "acronym": "VP", "label": "Verb Phrase", "ID": "1686"}, {"sentence": "4 .3  VP  Different from a noun phrase, a verb phrase may have  pre-modifiers and post-modifiers.", "acronym": "VP", "label": "Verb Phrase", "ID": "1687"}, {"sentence": "This can be summarized by the following  table:    Outside SimpleNP Exceptions  Prepositional Phrases  Relative Clauses  VPes  Apposition1  Some conjunctions  (Conjunctions are  marked according to the  TreeBank guidelines)2.", "acronym": "VP", "label": "Verb Phras", "ID": "1688"}, {"sentence": "Presented at the  Linguistic Conference on East Asian Languages: VPes, in  Los Angeles, California. (", "acronym": "VP", "label": "Verb Phras", "ID": "1689"}, {"sentence": "Robinson, A. E., \"Determining VPe  Referents in Dialog~\", American Journal of  Computatlor~l Linguistics\", Jan. 1981  17.", "acronym": "VP", "label": "Verb Phras", "ID": "1690"}, {"sentence": "The structure is not only  functional (with- function s/m/ools laloeling the const|tuents instead of  category names like Noun Phrase and VPe) but i t  is  multifunctional.", "acronym": "VP", "label": "Verb Phras", "ID": "1691"}, {"sentence": "3 Outline  3.1 Corpus, Language Usage and Computa- ble Semantic Properties of VPes  section  Basic Computational Semantic Con- cepts  Theory of Norm and Exploitation of  Language Usage   Corpus Pattern Analysis in  Sketch En- gine  Sense Discriminative Patterns  3.2   Semantic Types and Ontologies          Argument Structures         Frames and Semantic Types    Inducing Semantic Types  Discriminative Patterns  3.3 Statistical Models for Corpus Pattern  Recognition", "acronym": "VP", "label": "Verb Phras", "ID": "1692"}, {"sentence": "4 .3  VPe  Different from a noun phrase, a verb phrase may have  pre-modifiers and post-modifiers.", "acronym": "VP", "label": "Verb Phras", "ID": "1693"}, {"sentence": "Algorithm for Generating  Clustering-based VPs 5 Results We explored a range of thresholds in the final stage of the algorithm.", "acronym": "VP", "label": "Verb Pair", "ID": "1694"}, {"sentence": "Specifically, we add the feature types in this order: YAGO TYPE, YAGO MEANS, Noun Pairs, FrameNet, VPs, and Appositives.", "acronym": "VP", "label": "Verb Pair", "ID": "1695"}, {"sentence": "Algorithms for Generating Non-clustering-based VPs scope of the other frame or if the semantic scopes SemFrame?s verb classes list specific LDOCE of the two frames have significant overlap.", "acronym": "VP", "label": "Verb Pair", "ID": "1696"}, {"sentence": "3.2 Choice of VPs and Coverage To ensure a wide coverage of a variety of syntactico- semantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set3 (Nelson et al, 2004), and (2) the VerbNet verb lexicon4 (Kipper et al.,", "acronym": "VP", "label": "Verb Pair", "ID": "1697"}, {"sentence": "5 Conclusions We have examined the utility of three major sources of world knowledge for coreference resolu- tion, namely, large-scale knowledge bases (YAGO, FrameNet), coreference-annotated data (Noun Pairs, VPs), and unannotated data (Appositives), by applying them to two learning-based coreference models, the mention-pair model and the cluster- ranking model, and evaluating them on documents annotated with the ACE and OntoNotes annotation schemes.", "acronym": "VP", "label": "Verb Pair", "ID": "1698"}, {"sentence": "Specifically, since each instance now corresponds to 819 an NP, NPk, and a preceding cluster, c, we can gener- ate a noun-pair-based feature by applying the above method to NPk and each of the NPs in c, and its value is the number of times it is applicable to NPk and c. 3.2.2 Features Based on VPs As discussed above, features encoding the seman- tic roles of two NPs and the relatedness of the asso- ciated verbs could be useful for coreference resolu- tion.", "acronym": "VP", "label": "Verb Pair", "ID": "1699"}, {"sentence": "In the manner described in Section 5.1, we collected co-occurrence counts between a functor that the given it possesses concatenated with a lemma of its VP and a Czech counterpart having the same functor (denoted as csit).", "acronym": "VP", "label": "verbal parent", "ID": "1700"}, {"sentence": "TIEV: We add normal experts that tie all proba- bilities corresponding to a VP (any par- ent, using the coarse tags of Cohen et al, 2008).", "acronym": "VP", "label": "verbal parent", "ID": "1701"}, {"sentence": "Thus the three mentioned objects are defined as follows in English: new_A = regA \"new\" ; play_V2 = dirV2 (regVPplay\") ; remove_V3 = dirV3 (regVPremove\") from_Prep ; Here are the German definitions: 19 new_A = regA \"neu\" ; play_V2 = dirV2 (regVPspielen\") ; remove_V3 = dirV3 (prefixVPheraus\" nehmen_V) aus_Prep ; The lexicon definitions are gathered into a separate interface module, which the concrete syntax mod- ule depends on.", "acronym": "VP", "label": "V \"", "ID": "1702"}, {"sentence": "above) to: play_V2 = variants { dirV2 (regVPspielen\") ; dirV2 (prefixVPab\" (regVPspielen\")) } ; the extended grammar is able to parse an utterance like ?", "acronym": "VP", "label": "V \"", "ID": "1703"}, {"sentence": "UBCAT* NOO('CONV* N)VPyarak ' ) ( '$UB*  A ' r~) )   \"yan')C-AGR* 3S;G)(*C/LSE- NOl4)))  kez - Far \\[5 CONS C93a cg3a Cg3a C22ad : ( \"  e r \"  ((*CAT* N)(*R* \"yer ' )C*AGg* 3SG)(*CASE* NOH)))  burada - * \\ [1\\ ]  : ( ' . \"", "acronym": "VP", "label": "V \"", "ID": "1704"}, {"sentence": "2.2 The Cluster Type Size Zipf Constraint The motivation behind using a Zipfian constraint is the following observation: when a certain statistic is known to affect the quality of the induced clus- tering and it is not explicitly manipulated by the al- gorithm, strong fluctuations in its values are likely to imply that there are UR fluctuations in the quality of the induced clusterings.", "acronym": "UR", "label": "uncontrolled", "ID": "1705"}, {"sentence": "An UR  pkediction (used by a canonical top-down parser )  i s  t o  s e l e c t  each such r u l e   as the expansion.", "acronym": "UR", "label": "uncontrolled", "ID": "1706"}, {"sentence": "In  PEDAGLOT, t h e  choice of which r u l e  t o  t r y  can be defined as  the  r e s u l t  of  the c a l l  t o  a 'choosef funct ion (or it can be l e f t  UR] , We have  des&ned various approaches t o  such predic t ions  (e.g., a l imi ted  key-word  scan of the incoming s t r i n g ,  and the use of 'language s t a t i s t i c s  such as the   s e t  of rules which can generate the next symbol i n  the  s t r i n g  as t h e i r  l e f t   most symbol).", "acronym": "UR", "label": "uncontrolled", "ID": "1707"}, {"sentence": "puts, but at its core is still an UR expo- nential algorithm.", "acronym": "UR", "label": "uncontrolled", "ID": "1708"}, {"sentence": "and  on the other hand the UR inclusion in the XML  document of free PCDATA strings, which are written in  a specific language.", "acronym": "UR", "label": "uncontrolled", "ID": "1709"}, {"sentence": "We report labeled and UR, preci- sion and F-scores for this experiment.", "acronym": "UR", "label": "unlabeled recall", "ID": "1710"}, {"sentence": "The Petrov parser has better results by a sta- tistically significant margin for both labeled and UR and unlabeled F-score.", "acronym": "UR", "label": "unlabeled recall", "ID": "1711"}, {"sentence": "But  2The figure indicates UR and preci-  sion.", "acronym": "UR", "label": "unlabeled recall", "ID": "1712"}, {"sentence": "Most of the decrease in F1 is due to the drop in UR.", "acronym": "UR", "label": "unlabeled recall", "ID": "1713"}, {"sentence": "Even with help from the true English trees, the unsu- pervised SITGs underperform PCFGs trained on as few as 32 sentences, with the exception of UR in one experiment.", "acronym": "UR", "label": "unlabeled recall", "ID": "1714"}, {"sentence": "63.73 82.10 63.20 75.14 90.89 92.79 80.02 81.40 Table 6: Detailed results for SVM; T = transformation; P = unlabeled precision, R = UR costly to train (Sagae and Lavie, 2005).", "acronym": "UR", "label": "unlabeled recall", "ID": "1715"}, {"sentence": "POS tagging: Each word is labelled with its POS  tag following the UT proposed by  Petrov et al (2011).", "acronym": "UT", "label": "Universal Tagset", "ID": "1716"}, {"sentence": "UMPOS employs a 12-tag UT introduced by Petrov et al. (", "acronym": "UT", "label": "Universal Tagset", "ID": "1717"}, {"sentence": "For the sake of comparability we ap- plied the split to the UT (Petrov et al.,", "acronym": "UT", "label": "Universal Tagset", "ID": "1718"}, {"sentence": "As the UT did not have a separate  118 category for NEs, we chose to label and classify  them as people, locations and organizations.", "acronym": "UT", "label": "Universal Tagset", "ID": "1719"}, {"sentence": "punctuation ADV adverb NUM numeral X everything else Table 1: The 12 tags of the UT.", "acronym": "UT", "label": "Universal Tagset", "ID": "1720"}, {"sentence": "In these chains of UTs, the speaker is not explicitly mentioned because the author relies on the shared understanding with the reader that adja- cent pieces of quoted speech are not independent (Zhang et al.,", "acronym": "UT", "label": "utterance", "ID": "1721"}, {"sentence": "First is the problem of iden- tifying anaphoric speakers, i.e., in the UT ?", "acronym": "UT", "label": "utterance", "ID": "1722"}, {"sentence": "Elson and McKeown (2010) describe and address two basic types of UT chains: (i) one-character chains, and (ii) intertwined chains.", "acronym": "UT", "label": "utterance", "ID": "1723"}, {"sentence": "is the problem of iden- tifying anaphoric speakers, i.e., in the UT ?", "acronym": "UT", "label": "utterance", "ID": "1724"}, {"sentence": "The second problem is resolving UT chains with implicit speakers.", "acronym": "UT", "label": "utterance", "ID": "1725"}, {"sentence": "2013) also iden- tified similar chains of UTs and addressed their attribution to characters using a model-based approach.", "acronym": "UT", "label": "utterance", "ID": "1726"}, {"sentence": "the speakers before their UTs.", "acronym": "UT", "label": "utterance", "ID": "1727"}, {"sentence": "Elson and McKeown (2010) describe and address two basic types of UT chains: (i) one-character chains, and (ii) intertwined", "acronym": "UT", "label": "utterance", "ID": "1728"}, {"sentence": "// Vocabulary Acquisition Device // theory-based lexicon L experience OO experience OO Figure 1: (a) The Model of Concepts from the Golden Oldies: used in the present Machine Learning Paradigm; (b) The UT Model of Concepts: necessary for deep lexical acquisition The new framework: UT We have much progress to make: We can de- scribe naive theories precisely; we can describe how theory acquisition occurs; we can describe the map from naive theories to a set of lexicalizable concepts.", "acronym": "UT", "label": "Universal Theory", "ID": "1729"}, {"sentence": "For the SU component, a German corpus named SAMMIE (SAarbru?cken Multi-Modal Interface Experiment) was collected by Saarland University and DFKI2 using a Wizard of Oz experiment.", "acronym": "SU", "label": "speech understanding", "ID": "1730"}, {"sentence": "1 Introduction The SU component in a spoken dialogue system consists of an automatic speech recognition (ASR) component and a language un- derstanding (LU) component.", "acronym": "SU", "label": "speech understanding", "ID": "1731"}, {"sentence": "The correction of ill-formed input  using history-based expectation with  applications to SU.\"", "acronym": "SU", "label": "speech understanding", "ID": "1732"}, {"sentence": "Tina: a probabilistic syntactic  parser for SU systems.", "acronym": "SU", "label": "speech understanding", "ID": "1733"}, {"sentence": "Performance evaluation for SU  is being conducted with the ATIS corpus,  collected from subjects interacting with a  simulated (wizard-based) understanding system  that contains certain data from the Official  Airline Guide (OAG).", "acronym": "SU", "label": "speech understanding", "ID": "1734"}, {"sentence": "Erdman, L. D. Overview -- of the Hearsay SU research  (Computer Sc iences  Review 1974-1 975) .", "acronym": "SU", "label": "speech understanding", "ID": "1735"}, {"sentence": "Multiple SU contribute to  multiple model units.", "acronym": "SU", "label": "system units", "ID": "1736"}, {"sentence": "However, in the h igh   level dialogue description SU S-goals  see/~1 to be more important, while at the low levels  C-goal seem to prevail.", "acronym": "SU", "label": "system units", "ID": "1737"}, {"sentence": "For example, in an evaluation session an  assessor judged SU S1.1 and S10.4 as  sharing some content with model unit M2.2.", "acronym": "SU", "label": "system units", "ID": "1738"}, {"sentence": "We believe that metrics  have to take coverage score (C, Section 4.1.1)  into consideration to be reasonable since most  of the content sharing among SU and  model units is partial.", "acronym": "SU", "label": "system units", "ID": "1739"}, {"sentence": "It also allow s assessors to step  through each model unit, mark all SU  sharing content with the current model unit,  and specify that the marked SU                                                 3 Does a summary follow the rule of English  grammatical rules independent of its content?", "acronym": "SU", "label": "system units", "ID": "1740"}, {"sentence": "SUs are appropriate.", "acronym": "SU", "label": "System utterance", "ID": "1741"}, {"sentence": "Table 2: Dihana database statistics (mean for the five cross-validation partitions) Training Test Dialogues 720 180 Turns 12,330 3,083 User turns 5,024 1,256 System turns 7,206 1,827 Utterances 18,837 4,171 User utterances 7,773 1,406 SUs 11,064 2,765 Running words 162,613 40,765 User running words 42,806 10,815 System running words 119,807 29,950 Vocabulary 832 485 User vocabulary 762 417 System vocabulary 208 174 A cross-validation approach was adopted in Di- hana as well.", "acronym": "SU", "label": "System utterance", "ID": "1742"}, {"sentence": "It either waits 500 ms after a SU to speak or interrupts 200 ms after the System con- firms an misrecognized word, which is in line with human reaction time (Fry, 1975).", "acronym": "SU", "label": "System utterance", "ID": "1743"}, {"sentence": "The feature set is described by three main subsets: 25 system-utterance-level binary features4 derived from the system dialogue act(s) in the last turn; 17 user-utterance-level binary features5 derived from (a) what the user heard prior to the current turn, or (b) what keywords the system recognised in its 4SU features: heardAck, heardCantHelp, heardExample, heardExplConf, heardGoBackDAT, heard- Hello, heardImplConf, heardMoreBuses, heardRequest, heardRestartDAT, heardSchedule, heardSorry, heardDate, heardFrom, heardRoute, heardTime, heardTo, heardNext, heardPrevious, heardGoBack, heardChoices, heardRestart, heardRepeat, heardDontKnow, lastSystemDialActType.", "acronym": "SU", "label": "System utterance", "ID": "1744"}, {"sentence": "SUs have no problem.", "acronym": "SU", "label": "System utterance", "ID": "1745"}, {"sentence": "ation  that the user wants to book a room on Wednesday  this week has the highest priority, and then after  that, the interpretation that the user wants to book  a room on Wednesday next week has the highest  Dialogue ) C  s~,,~ Control ontext  Utterance I Response Understanding  (ISSS method) Generation  Wor  /  hypotheses/ ~ i o n   I  peec \"eco nition I I   eoc  o uction I  l \\  User utterance SU  Figure 3: Architecture of the experimental systems.", "acronym": "SU", "label": "System utterance", "ID": "1746"}, {"sentence": "Lin (1999) uses the SUB test 2 and mutual information (MI) to determine the compositionality of the phrase.", "acronym": "SUB", "label": "substitution", "ID": "1747"}, {"sentence": "The numerical value assigned by the func-  tion to a pair of segments i referred to as the cost,  or penalty, of SUB.", "acronym": "SUB", "label": "substitution", "ID": "1748"}, {"sentence": "Section 2.2))  A 'SUB' test can be used to confirm that  NPs that stand in anaphoric relations to NPs of  these types do not corefer with them.", "acronym": "SUB", "label": "substitution", "ID": "1749"}, {"sentence": "For in-  stance~ one may observe that Every man loves  his mother does not mean the same as Every  man loves every man's mother, contrasting with  referring NPs, which do allow such SUBs  (e.g., John loves his mother equals John loves  John's mother).", "acronym": "SUB", "label": "substitution", "ID": "1750"}, {"sentence": "The function's values  for SUBs are listed in the \"penalty\" column  in Table 2.", "acronym": "SUB", "label": "substitution", "ID": "1751"}, {"sentence": "They report an av- 2 The SUB test aims to replace part of the idiom?s component words with semantically similar words, and test how the co-occurrence frequency changes.", "acronym": "SUB", "label": "substitution", "ID": "1752"}, {"sentence": "Blood is not  tagged as 27 (SUB) or as 13 (food), though it might  well be considered as such in certain contexts.", "acronym": "SUB", "label": "substance", "ID": "1753"}, {"sentence": "By empirical inspection, concrete nouns are hyponyms of the WordNet synsets physical object.n.01, matter.n.03, or SUB.n.04.", "acronym": "SUB", "label": "substance", "ID": "1754"}, {"sentence": "Restriction WordNet Synsets animate animate being.n.01 people.n.01 person.n.01 concrete physical object.n.01 matter.n.01 SUB.n.01 organization social group.n.01 district.n.01 Table 2: DAVID?s mappings between some common VerbNet restriction types and WordNet synsets.", "acronym": "SUB", "label": "substance", "ID": "1755"}, {"sentence": "First, there is information concerning patients, such as the changes in Mr. X?s blood pressure over the past three days, or the SUBs to which Ms. Y is allergic.", "acronym": "SUB", "label": "substance", "ID": "1756"}, {"sentence": "Blood is not  tagged as 27 (SUB) or as 13 (food), though it might  well be consider", "acronym": "SUB", "label": "substance", "ID": "1757"}, {"sentence": "ct, animal, man-made artifact, attributes, body parts,  .... SUB, and time), and 15 tags for verbs (from  grooming and dressing verbs, to verbs of weather).", "acronym": "SUB", "label": "substance", "ID": "1758"}, {"sentence": "to create a general semantic tagging scheme and  to apply it on a large scale: every set of synonymous  senses, synsets, are tagged with one of 45 tags as Word-  Net version 1.5 ~. In their schema, there are3 tags for  adjectives (relational adjectives, participial adjectives  and all others), 1 tag for all adverbs, 26 tags for nouns  (act, animal, man-made artifact, attributes, body parts,  .... SUB, and time), and 15 tags for verbs (from  grooming and dressing verbs, to verbs of weather).", "acronym": "SUB", "label": "substance", "ID": "1759"}, {"sentence": "Likewise, in Example 4, the paper refers to a certain SUB from made and pulp, and therefore it is a mass noun.", "acronym": "SUB", "label": "substance", "ID": "1760"}, {"sentence": "Multiple Regression  and the ANOVA and Covariance.", "acronym": "ANOVA", "label": "Analysis of Variance", "ID": "1761"}, {"sentence": "We carry out an one-way ANOVA which shows significant differences in score as a function of system (p < 0.05, paired t-test).", "acronym": "ANOVA", "label": "Analysis of Variance", "ID": "1762"}, {"sentence": "Dimension F value Probability r2  1 111.9 p < .001 84.3%  2 32.3 p K .001 60.8%  3 31.9 p < .001 60.5%  4 4.2 p K .001 16.9%  5 28.8 p < .001 58.0%  the results of an ANOVA showing that the registers are significant dis-  criminators for each dimension, and the r 2 values show their strength (r 2 is a direct  measure of the percentage of variation in the dimension score that can be predicted  on the basis of the register distinctions).", "acronym": "ANOVA", "label": "Analysis of Variance", "ID": "1763"}, {"sentence": "The ANOVA, Wiley- Interscience.", "acronym": "ANOVA", "label": "Analysis of Variance", "ID": "1764"}, {"sentence": "More references were required to achieve optimal performance for ROUGE based on longer n-grams (using the Kruskal-Wallis test, a non-parametric ANOVA, p < 2.2 ?", "acronym": "ANOVA", "label": "analysis of variance", "ID": "1765"}, {"sentence": "We measured the reliability of the judges in the black  box evaluation by using an ANOVA technique  described in \\[16\\].", "acronym": "ANOVA", "label": "analysis of variance", "ID": "1766"}, {"sentence": "GAMMs, as implemented in the mgcv package 1.7-28, offer three important advantages for the analysis of EEG data compared to standard linear models and ANOVA.", "acronym": "ANOVA", "label": "analysis of variance", "ID": "1767"}, {"sentence": "We performed an ANOVA to de-  tect significant differences in accuracy consider-  ing algorithm, collocational property, and fea-  ture organization.", "acronym": "ANOVA", "label": "analysis of variance", "ID": "1768"}, {"sentence": "A 3-way ANOVA in- cluding non-filtered scores for proportional thresh- old resulted in F [4, 235] = 31.82, p < 0.01 for WD scores.", "acronym": "ANOVA", "label": "analysis of variance", "ID": "1769"}, {"sentence": "Table 3 shows results from the ANOVA for 10The interpretation of the N-N results in terms of spread- ing activation is also rejected by Hare et al(2009, 163).", "acronym": "ANOVA", "label": "analysis of variance", "ID": "1770"}, {"sentence": "Due to their performative na- ture, we assume that these DAs make the turn they belong to a good candidate for a cor- responding edit-turn-pair.", "acronym": "DA", "label": "dialog act", "ID": "1771"}, {"sentence": "Next a subset of the agent/user telephone calls were transcribed and annotated with simple DAs, and from these the two user models were estimated.", "acronym": "DA", "label": "dialog act", "ID": "1772"}, {"sentence": "Actually, we deviate from Grosz and Sidner here and  keep the recent subDAive near the top of the stack.", "acronym": "DA", "label": "dialog act", "ID": "1773"}, {"sentence": "As future work, we are plan- ning to test our model on DA classification and multimodal behavior recognition tasks.", "acronym": "DA", "label": "dialog act", "ID": "1774"}, {"sentence": "The re- ward function can include distinct costs for differ- ent diagnostic tests, DAions, and for success- ful/unsuccessful task completion.", "acronym": "DA", "label": "dialog act", "ID": "1775"}, {"sentence": "Lexical, prosodic and syntactic cures for DAs.", "acronym": "DA", "label": "dialog act", "ID": "1776"}, {"sentence": "Ferschke et al (2012) suggest four types of explicit perfor- matives in their annotation scheme for DAs of Wikipedia turns.", "acronym": "DA", "label": "dialog act", "ID": "1777"}, {"sentence": "27.8 25.7 8.6 BH 67.5 55.3 44.9 45.6 58.6 27.6 62.7 Mu 29.3 42.4 38.8 51.8 34.5 30.5 33.0 R 12.9 9.4 16.1 21.1 12.1 15.7 2.7 Mc 60.7 47.4 39.9 45.8 36.5 33.9 44.3 RB 17.9 12.4 26.2 36.5 15.3 25.4 1.1 H 19.4 29.3 12.2 22.2 17.3 20.9 10.3 DMVp 54.7 42.0 30.7 30.1 28.9 25.4 24.3 S2 45.2 41.9 44.2 49.8 39.7 25.4 63.8 Mp 67.8 54.3 49.6 59.4 47.7 37.7 49.7 S1 43.1 47.9 36.3 46.7 27.9 23.5 7.6 Table 6: DA results on the Penn treebank, stratified by dependency relation.", "acronym": "DA", "label": "Directed accuracy", "ID": "1778"}, {"sentence": "l l l l l l l l l 2 4 6 8 10 20 30 40 50 60 70 edge length direc ted a ccura cy (%) l TuBCBHMZ?pS?2DMV?p Figure 1: DA on the Penn treebank strat- ified by dependency length.", "acronym": "DA", "label": "Directed accuracy", "ID": "1779"}, {"sentence": "DA is the ratio of cor- rectly predicted dependencies (including direction) over total amount of predicted dependencies.", "acronym": "DA", "label": "Directed accuracy", "ID": "1780"}, {"sentence": "functional 59.6 52.4 41.5 47.4 36.2 30.6 40.0 58.5 20.9 37.2 20.6 19.3 29.2 23.7 lexical 40.6 41.9 28.5 37.3 24.8 27.7 35.5 39.5 23.5 23.1 23.0 14.4 33.1 10.1 oldLTH 41.4 43.6 28.8 37.8 24.6 28.6 36.1 39.5 22.3 23.7 21.8 14.3 32.0 10.7 standard 56.0 50.4 41.0 50.3 37.5 32.8 42.5 55.5 22.3 33.5 21.8 18.4 31.4 20.4 best 59.6 52.4 41.5 50.3 37.5 32.8 42.5 58.5 23.5 37.2 23.0 19.3 33.1 23.7 Table 5: DA results measured against different conversions of the Penn Treebank into dependency trees.", "acronym": "DA", "label": "Directed accuracy", "ID": "1781"}, {"sentence": "20 Universal Dependency Rules 1 HDP-DEP 71.9 50.4 No Rules (Random Init) 2 HDP-DEP 24.9 24.4 3 Headden III et al (2009) 68.8 - English-Specific Parsing Rules 4 Deterministic (rules only) 70.0 62.6 5 HDP-DEP 73.8 66.1 Druck et al (2009) Rules 6 Druck et al (2009) 61.3 - 7 HDP-DEP 64.9 42.2 Table 6: DA of our model (HDP-DEP) on sentences of length 10 or less and 20 or less from WSJ with different rulesets and with no rules, along with vari- ous baselines from the literature.", "acronym": "DA", "label": "Directed accuracy", "ID": "1782"}, {"sentence": ".3 32.0 10.7 standard 56.0 50.4 41.0 50.3 37.5 32.8 42.5 55.5 22.3 33.5 21.8 18.4 31.4 20.4 best 59.6 52.4 41.5 50.3 37.5 32.8 42.5 58.5 23.5 37.2 23.0 19.3 33.1 23.7 Table 5: DA results measured against different conversions of the Penn Treebank into dependency trees.", "acronym": "DA", "label": "Directed accuracy", "ID": "1783"}, {"sentence": "-ptb 75.2 69.8 69.4 73.8 67.2 64.1 71.6 69.8 49.8 67.0 49.6 44.8 53.9 68.1 portuguese 67.5 79.8 75.6 75.7 71.7 66.9 78.2 80.4 62.1 66.6 61.3 51.8 57.3 75.4 slovene 64.4 60.7 56.9 58.9 57.1 55.9 66.7 68.7 49.2 47.3 49.2 38.9 43.8 56.6 swedish 80.1 70.9 73.2 73.8 72.7 66.7 77.0 77.1 64.0 64.0 62.0 56.0 56.5 71.0 averages 71.6 63.1 68.0 68.6 66.9 63.6 68.6 72.1 59.1 59.9 57.6 50.3 57.6 68.1 Table 3: DA, undirected accuracy and NED results for the dependency task (using supplied POS).", "acronym": "DA", "label": "Directed accuracy", "ID": "1784"}, {"sentence": "The strength of Hirst's approach is his attempt o reduce the  presuppositional metric of Craln and Steedman to criteria manipul-  able by basic semantie/lexieal codings, and particularly the contrast  of definite and inDA.", "acronym": "DA", "label": "definite articles", "ID": "1785"}, {"sentence": "Examples are the distinc- tion between definite and inDA, and the distinction between hyphens, slashes, left and right parentheses, quotation marks, and other sym- bols which the Tiger treebank annotates with ?", "acronym": "DA", "label": "definite articles", "ID": "1786"}, {"sentence": "4.2 Adaptation and evaluation of HeidelTime: HTSwe The main modifications required in the adapta- tion of HeidelTime to Swedish (HTSwe) involved handling DA and plurals, e.g. adding eftermiddag(en)?(ar)?(na)? (?", "acronym": "DA", "label": "definite articles", "ID": "1787"}, {"sentence": "2.1 General Web Corpora In a first attempt, we tried to obtain a general German HTML corpus using the mean- ingless query der die das, i.e., the three German DA.", "acronym": "DA", "label": "definite articles", "ID": "1788"}, {"sentence": "Lexi-  cally reiterated items include repeated syn-  onymous noun phrases which may often be  preceded by DA or demon-  stratives.", "acronym": "DA", "label": "definite articles", "ID": "1789"}, {"sentence": "But in abstract anaphora, English prefers demonstratives to personal pronouns and DA (Pas- sonneau, 1989; Navarretta, 2011).1 Demonstra- 1This is not to say that personal pronouns and definite arti- cles do not occur in abstract anaphora, but they are not common.", "acronym": "DA", "label": "definite articles", "ID": "1790"}, {"sentence": "The inlransitive verbs.are  claksified accor .ding to semantic criteria (verbs of motion~ state) or  by their syntactic usa~ (like predicativi~ auxiliaries, urgpers6nal  verbs with DA ).W'e should-notice that he same verb maybe  transitiv 9 or intransitive, accor \"ding to its m e.", "acronym": "DA", "label": "dative", "ID": "1791"}, {"sentence": "takes three arguments (nomina- tive, accusative and DA cases), it is tagged with VB[nad].", "acronym": "DA", "label": "dative", "ID": "1792"}, {"sentence": "We also added suffixes to verb tags to de- note which arguments they require (n:nominative, a:accusative and d: DA).", "acronym": "DA", "label": "dative", "ID": "1793"}, {"sentence": "DA?.?", "acronym": "DA", "label": "dative", "ID": "1794"}, {"sentence": "takes three arguments: nominative, accusative and DA cases.", "acronym": "DA", "label": "dative", "ID": "1795"}, {"sentence": "To the contrary, subjects can occur in the ergative, nominative, DA, genitive, and instrumental cases.", "acronym": "DA", "label": "dative", "ID": "1796"}, {"sentence": "Nev-  ertheless it can still give us guiDAnce on which  candiDAtes are more probable than others.", "acronym": "DA", "label": "da", "ID": "1797"}, {"sentence": "Here we would compare the degree to which  each possible candiDAte antecedent (A Japanese  company, television picture tubes, Japan, TV   sets, and Malaysia in this example) could serve  as the direct object of \"export\".", "acronym": "DA", "label": "da", "ID": "1798"}, {"sentence": "The first piece of useful information we con-  sider is the distance between the pronoun  and the candiDAte antecedent.", "acronym": "DA", "label": "da", "ID": "1799"}, {"sentence": "We collect these probabilities on the  training DAta, which are marked with reference  links.", "acronym": "DA", "label": "da", "ID": "1800"}, {"sentence": "Gener-  ally, a singular pronoun cannot refer to a plural  noun phrase, so that in resolving such a pro-  noun any plural candiDAtes should be ruled out.", "acronym": "DA", "label": "da", "ID": "1801"}, {"sentence": "91.97 96.81 Petrov I-5 85.30 85.87 85.58 92.00 92.61 92.31 96.65 Petrov I-6 84.86 85.46 85.16 91.79 92.44 92.11 96.65 Figure 12: DA on CCGbank dependencies on all sentences from section 00.", "acronym": "DA", "label": "Dependency accuracy", "ID": "1802"}, {"sentence": "DA is the ratio of dependency relations correctly identified by the parser, while sentence accuracy is the exact match accuracy of complete dependency relations in a sentence.", "acronym": "DA", "label": "Dependency accuracy", "ID": "1803"}, {"sentence": "0  10  20  30  40  50  60  70  80  90  100  80  82  84  86  88  90  92  94  96  98  100 Sen tenc e co vera ge (% ) DA (%) Figure 1: Accuracy-coverage curve on BIO rel dev.", "acronym": "DA", "label": "Dependency accuracy", "ID": "1804"}, {"sentence": "Labeled % Unlabeled % Parser R P F R P F C&C Hybrid 84.71 86.35 85.52 90.96 92.72 91.83 Petrov I-5 85.50 86.08 85.79 92.12 92.75 92.44 p-value 0.005 0.189 0.187 < 0.001 0.437 0.001 Figure 13: DA on the section 00 sentences that receive an analysis from both parsers.", "acronym": "DA", "label": "Dependency accuracy", "ID": "1805"}, {"sentence": "DA is de- fined as the ratio of correct dependencies over the to- tal number of dependencies in a sentence. (", "acronym": "DA", "label": "Dependency accuracy", "ID": "1806"}, {"sentence": "611 Collins Head-Driven Statistical Models for NL Parsing Table 4 DA on section 0 of the treebank with Model 2.", "acronym": "DA", "label": "Dependency accuracy", "ID": "1807"}, {"sentence": "Labeled % Unlabeled % Parser R P F R P F Cover C&C Normal Form 84.39 85.28 84.83 90.93 91.89 91.41 98.95 C&C Hybrid 84.53 86.20 85.36 90.84 92.63 91.73 98.95 Petrov I-0 79.87 78.81 79.34 87.68 86.53 87.10 96.45 Petrov I-4 84.76 85.27 85.02 91.69 92.25 91.97 96.81 Petrov I-5 85.30 85.87 85.58 92.00 92.61 92.31 96.65 Petrov I-6 84.86 85.46 85.16 91.79 92.44 92.11 96.65 Figure 12: DA on CCGbank dependencies on all sentences from section 00.", "acronym": "DA", "label": "Dependency accuracy", "ID": "1808"}, {"sentence": "Labeled % Unlabeled % Parser R P F R P F C&C Hybrid 85.11 86.46 85.78 91.15 92.60 91.87 Petrov I-5 85.73 86.29 86.01 92.04 92.64 92.34 p-value 0.013 0.278 0.197 < 0.001 0.404 0.005 Figure 14: DA on the section 23 sentences that receive an analysis from both parsers.", "acronym": "DA", "label": "Dependency accuracy", "ID": "1809"}, {"sentence": "249  Computational Linguistics Volume 23, Number 2  Figure 2  noun  article  conjunction  preposition  number  left parentheses  non-punctuation character  colon or dash  sentence-ending punctuation  verb  modifier  pronoun  proper noun  comma or semicolon  right parentheses  possessive  abbreviation  others  Elements of the DA assigned to each incoming token.", "acronym": "DA", "label": "descriptor array", "ID": "1810"}, {"sentence": "3.4 Classification by a Learning Algorithm  The DAs representing the tokens in the context are used as the input to  a machine learning algorithm.", "acronym": "DA", "label": "descriptor array", "ID": "1811"}, {"sentence": "In addition to the 18 category frequencies, the DA also contains two  additional f ags that indicate if the word begins with a capital etter and if it follows a  punctuation mark, for a total of 20 items in each DA.", "acronym": "DA", "label": "descriptor array", "ID": "1812"}, {"sentence": "3.3 Descriptor Array Construction  A vector, or DA, is constructed for each token in the input text.", "acronym": "DA", "label": "descriptor array", "ID": "1813"}, {"sentence": "The context vectors, which we call DAs, are input to a machine learn-  ing algorithm trained to disambiguate s ntence boundaries.", "acronym": "DA", "label": "descriptor array", "ID": "1814"}, {"sentence": "An important component of the Satz system is the lexicon contain-  ing part-of-speech frequency data from which the DAs are constructed.", "acronym": "DA", "label": "descriptor array", "ID": "1815"}, {"sentence": "c?2014 Association for Computational Linguistics Named Entity Recognition for DA Ayah Zirikly Department of Computer Science The George Washington University Washington DC, USA ayaz@gwu.edu Mona Diab Department of Computer Science The George Washington University Washington DC, USA mtdiab@gwu.edu Abstract To date, majority of research for Ara- bic Named Entity Recognition (NER) ad- dresses the task for Modern Standard Ara- bic (MSA) and mainly focuses on the n", "acronym": "DA", "label": "Dialectal Arabic", "ID": "1816"}, {"sentence": "40 6 Current and Future Work We have already utilized the dialect labels to identify dialectal sentences to be translated into English, in an effort to create a DA-to-English par- allel dataset (also taking a crowdsourcing approach) to aid machine translation of dialectal Arabic.", "acronym": "DA", "label": "Dialectal Arabic", "ID": "1817"}, {"sentence": "However, the dataset contains English and Arabic; in this work we only target DA.", "acronym": "DA", "label": "Dialectal Arabic", "ID": "1818"}, {"sentence": "2012) show that small amounts of data from the right dialect can have a dramatic impact on the quality of DA Machine Translation systems.", "acronym": "DA", "label": "Dialectal Arabic", "ID": "1819"}, {"sentence": "5 Conclusion & Future Work In this paper we present DA NER system using state-of-the-art features in addi- tion to proposing new features that improve the performance.", "acronym": "DA", "label": "Dialectal Arabic", "ID": "1820"}, {"sentence": "4.118 88.889 91.429 83.333 23.81 37.037 88.7255 56.3495 64.233 FEA4={FEA3, EM-GAZ} 94.118 88.889 91.429 72.222 30.952 43.333 83.17 59.9205 67.381 FEA5={FEA4, PM-GAZ} 94.118 88.889 91.429 73.684 33.333 45.902 83.901 61.111 68.666 FEA6={FEA5, LVM-GAZ} 94.118 88.889 91.429 78.947 35.714 49.18 86.533 62.302 70.305 FEA7={FEA6, BC} 93.333 77.778 84.849 77.778 33.333 46.667 85.556 55.556 65.758 Table 3: DA NER the test data.", "acronym": "DA", "label": "Dialectal Arabic", "ID": "1821"}, {"sentence": "discourse or dialogue generation  Franck Panaget*  Centre National d'Etudes des T@l@communications (CNET)  LAA/TSS/RCP  Route de Tr@gastel -- 22300 Lannion - France  email: panaget@lannion.cnet.fr / tel: +33 96.05.28.52 / fax: +33 96.05.35.30  Abst ract   A natural language generation system is typically con-  stituted by two main components: a content planning  component (e.g., text planner or DA planner)  and a linguistic realization component.", "acronym": "DA", "label": "dialogue act", "ID": "1822"}, {"sentence": "Sneaker's belief set  For every condition C in DA performed:  default_ascribe(Speaker, H arer, believe(C))  That is, for every condition in the speech act, the speaker  must ascribe abelief to the hearer that the condition is satis-  fied.", "acronym": "DA", "label": "dialogue act", "ID": "1823"}, {"sentence": "Furthermore, we will incorporate past work on DA tag- ging in chat (Wu et al, 2005) to both improve sum- marization and create a framework for the next two steps.", "acronym": "DA", "label": "dialogue act", "ID": "1824"}, {"sentence": "Instead, a theory of speech acLs should be solipsistic in  that it refers olely to finite belief representations of either  the speaker or hearer of the DA.", "acronym": "DA", "label": "dialogue act", "ID": "1825"}, {"sentence": "The update  rule for the heater is the converse of the speaker's:  Update on the He.arer's belie, f set  For every condition C in DA performed:  default ascribe(Hearer;Speaker, C)  That is, given that he speaker has performed an infolxn  act, the hearer can ascribe to the speaker the preconditions of  the inform act assuming that he speaker isbeing coopera-  tive.", "acronym": "DA", "label": "dialogue act", "ID": "1826"}, {"sentence": "To overcome this, we plan to use an unsupervised learn- ing approach to discover DAs (Ritter et al, 2010).", "acronym": "DA", "label": "dialogue act", "ID": "1827"}, {"sentence": "Due to their performative na- ture, we assume that these DA make the turn they belong to a good candidate for a cor- responding edit-turn-pair.", "acronym": "DA", "label": "dialog acts", "ID": "1828"}, {"sentence": "Next a subset of the agent/user telephone calls were transcribed and annotated with simple DA, and from these the two user models were estimated.", "acronym": "DA", "label": "dialog acts", "ID": "1829"}, {"sentence": "The average length of the topics (measured using the number of DA) among all the meetings is 172.5, with a high standard devi- ation of 236.8.", "acronym": "DA", "label": "dialog acts", "ID": "1830"}, {"sentence": "These logs include information about the system DA, the N-best speech recognition hypotheses, and the hypothesized interpretation (including confidence estimates) of the user?s spo- ken utterances as provided by the dialog system?s Spoken Language Understanding (SLU) module.", "acronym": "DA", "label": "dialog acts", "ID": "1831"}, {"sentence": "Lexical, prosodic and syntactic cures for DA.", "acronym": "DA", "label": "dialog acts", "ID": "1832"}, {"sentence": "Ferschke et al (2012) suggest four types of explicit perfor- matives in their annotation scheme for DA of Wikipedia turns.", "acronym": "DA", "label": "dialog acts", "ID": "1833"}, {"sentence": "This increased the need for Question Answering (QA) systems that provide the users with DAs to their questions.", "acronym": "DA", "label": "direct answer", "ID": "1834"}, {"sentence": "Max Planck Institute for Informatics myahya@mpi-inf.mpg.de Steven Euijong Whang, Rahul Gupta, Alon Halevy Google Research {swhang,grahul,halevy}@google.com Abstract Search engines are increasingly relying on large knowledge bases of facts to provide DAs to users?", "acronym": "DA", "label": "direct answer", "ID": "1835"}, {"sentence": "In both cases, users can ask  natural language questions about, the contents of the  texts, and the system responds with DAs  along with the original text.", "acronym": "DA", "label": "direct answer", "ID": "1836"}, {"sentence": "This paper describes a sys-  tem that attempts to retrieve a much smaller section  of text, namely, a DA to a user's question.", "acronym": "DA", "label": "direct answer", "ID": "1837"}, {"sentence": "This would deliver DAs to the query rather than having the user sort through list of keyword results.", "acronym": "DA", "label": "direct answer", "ID": "1838"}, {"sentence": "6 Evaluation 6.1 The Evaluation Settings Evaluation has been carried out to determine whether Topic Indexing and Retrieval using a sim- ple and efficient IR technique for DA re- trieval can indeed make for an accurate QA sys- tem.", "acronym": "DA", "label": "direct answer", "ID": "1839"}, {"sentence": "DA (Abbreviation)  acceptance (ace)  query (query)  rejection (rej)  request comment (re-c)  request suggestion (re-s)  statement (state)  date/loc, suggestion (sag)  miscellaneous (mist)  Example  That would be \\[ine  1)o you know ttamburg  This is /,oo late for mc  Is that possible  When wouM it be ok  Right, it's a Tuesday  \\[ propose April 13th  So long, bye  Table I: DAs and e", "acronym": "DA", "label": "Dialog act", "ID": "1840"}, {"sentence": "DA (Abbreviation)  acceptance (ace)  query (query)  rejection (rej)  request comment (re-c)  request suggestion (re-s)  statement (state)  date/loc, suggestion (sag)  miscellaneous (mist)  Example  That would be \\[ine  1)o you know ttamburg  This is /,oo late for mc  Is that possible  When wouM it be ok  Right, it's a Tuesday  \\[ propose April 13th  So long, bye  Table I: DAs and examples  For example, in our example turu below there  arc several utterances and each of them has a par-  ticular dialog act as shown below.", "acronym": "DA", "label": "Dialog act", "ID": "1841"}, {"sentence": "The content word over- 1 Manual annotations for DA tags and adjacency pairs are available for the AMI corpus.", "acronym": "DA", "label": "Dialog act", "ID": "1842"}, {"sentence": "DAs  aCC  state  misc  query  r~j  sug  re-c  re-s  Total  'Ih:aining Test  88.9 72.0  90.0 90.9  54.5 73.7  40.0 0.0  9\\]..7 85.7  90.3 92.9  0.0 0.0  90.0 82.4  82.0 79.4  Table 6: Performance of simple recurrent network  with dialog plausibility vectors in percent  Table 6 shows the results for our training and  test utterances.", "acronym": "DA", "label": "Dialog act", "ID": "1843"}, {"sentence": "DAs in this domain consist of a list of component acts of the form acttype(slot=value) where the slot=value pair is optional.", "acronym": "DA", "label": "Dialog act", "ID": "1844"}, {"sentence": "Meeting recorder project: DA labeling guide.", "acronym": "DA", "label": "Dialog act", "ID": "1845"}, {"sentence": "1.3 DA, Cooperative Observatories  The development of interactive nvironments for  monolingnal writers leads to modelling new functions for  documentation, self-documentation, self-learning and  management of indivitlualized personal knowledge bases,  to bc pooled into opcn encyclopaedic 'discovery  environments', pecific components for NLP systems.", "acronym": "DA", "label": "Discovery Assistants", "ID": "1846"}, {"sentence": "Our research aims at developing such Linguistic  DA by merging hyperdocumcnts, Data  Base Management Systems and interpretive adaptive  interfaces.", "acronym": "DA", "label": "Discovery Assistants", "ID": "1847"}, {"sentence": "The ASR  hypothesis is first classified into domain-specific  DAs (DA).", "acronym": "DAs", "label": "Domain Acts", "ID": "1848"}, {"sentence": "1 10.4 5.7 58.1 10.4 WORD-UNG 43.1 29.1 34.7 63.0 39.5 48.6 PN,MN,FV,DA 66.7 48.8 56.4 72.3 54.7 62.3 PN,MN,DA 64.5 46.5 54.1 75.8 58.1 65.8 LN,PN,MN,FV 64.4 44.2 52.4 65.2 50.0 56.6 Table 2: Results Class Imbalance Handling: InstWeight: Instance weighting and SigThresh: Sigmoid thresholding Features: WORD-UNG: Word unigrams, LN: Lemma ngrams, PN: POS ngrams, MN: Mixed ngrams, FV: First verb, DA: DAs bution on positive and negative training instances.", "acronym": "DAs", "label": "Dialog acts", "ID": "1849"}, {"sentence": "Dialog act (Abbreviation)  acceptance (ace)  query (query)  rejection (rej)  request comment (re-c)  request suggestion (re-s)  statement (state)  date/loc, suggestion (sag)  miscellaneous (mist)  Example  That would be \\[ine  1)o you know ttamburg  This is /,oo late for mc  Is that possible  When wouM it be ok  Right, it's a Tuesday  \\[ propose April 13th  So long, bye  Table I: DAs and examples  For example, in our example turu below there  arc several utterances and each of them has a par-  ticular dialog act as shown below.", "acronym": "DAs", "label": "Dialog acts", "ID": "1850"}, {"sentence": "Topics are DAs while documents are utterances ; we used the S-Space Package http://code.google.com/p/ airhead-research/wiki/RandomIndexing 9.", "acronym": "DAs", "label": "Dialog acts", "ID": "1851"}, {"sentence": "DAs, according to the classic speech act theory (Austin, 1962; Searle, 1969), represent the meaning of an utterance at the level of illocutionary force, i.e. a dialog act label con- cisely characterizes the intention and the role of a contribution in a dialog.", "acronym": "DAs", "label": "Dialog acts", "ID": "1852"}, {"sentence": "N-Best List of DAs Let?s Go Score Bayesian Score inform(from=mill street) 7.8E-4 3.5998527E-16 inform(from=mission street) 0.015577 3.5998527E-16 inform(from=osceola street) 0.0037 3.5998527E-16 inform(from=robinson township) 0.007292 3.5998527E-16 inform(from=sheraden station) 0.001815 3.1346254E-8 inform(from=brushton) 2.45E-4 3.5998527E-16 inform(from=jefferson) 0.128727 0.0054255757 inform(from=mck", "acronym": "DAs", "label": "Dialogue Acts", "ID": "1853"}, {"sentence": "Predicting DAs for a Speech-To-Speech  Translation System.", "acronym": "DAs", "label": "Dialogue Acts", "ID": "1854"}, {"sentence": "N-Best List of DAs Let?s Go Score Bayesian Score inform(route=46a) 3.33E-4 1.9236763E-6 inform(route=46b) 1.0E-6 1.5243509E-16 inform(route=46d) 0.096107 7.030841E-4 inform(route=46k) 0.843685 4.9941495E-10 silence() NA 0 User input: ?", "acronym": "DAs", "label": "Dialogue Acts", "ID": "1855"}, {"sentence": "c?2009 Association for Computational Linguistics Unsupervised Classification of DAs using a Dirichlet Process Mixture Model Nigel Crook, Ramon Granell, and Stephen Pulman Oxford University Computing Laboratory Wolfson Building Parks Road, OXFORD, UK nigc@comlab.ox.ac.uk ramg@comlab.ox.ac.uk sgp@clg.ox.ac.uk Abstract In recent years DAs have be- come a popular means of modelling the communicative intentions of human and machine utterances in many modern di-", "acronym": "DAs", "label": "Dialogue Acts", "ID": "1856"}, {"sentence": "3 Re-Ranking DAs Using Multiple Bayesian Networks Figure 1 shows an illustration of our dialogue act re-ranker within a pipeline architecture.", "acronym": "DAs", "label": "Dialogue Acts", "ID": "1857"}, {"sentence": "References   Adreani, G., Di Fabbrizio, G., Gilbert, M., Gillick, D.,  Hakkani-Tur, D., and Lemon, O., 2006 Let?s DiS- CoH: Collecting an Annotated Open Corpus with  DAs and Reward Signals for Natural Lan- guage Helpdesk, in Proceedings of IEEE SLT-2006  Workshop, Aruba Beach, Aruba.", "acronym": "DAs", "label": "Dialogue Acts", "ID": "1858"}, {"sentence": "N-Best List of DAs Let?s Go Score Bayesian Score inform(from=mill street) 7.8E-4 3.5998527E-16 inform(from=mission street) 0.0", "acronym": "DAs", "label": "Dialogue Acts", "ID": "1859"}, {"sentence": "Verbs that take dative rather than OAs are a particu- lar problem, such as mistaking accusative for dative feminine objects (10.6% of occurrences) or dative for accusative feminine objects (11.9%).", "acronym": "OA", "label": "accusative object", "ID": "1860"}, {"sentence": "the rele- vant point here is that the gap between the results (23% for sub- jects, 35% for OAs) warrants further attention in the context of comparing parsing results across corpora.", "acronym": "OA", "label": "accusative object", "ID": "1861"}, {"sentence": "and OA such as ?", "acronym": "OA", "label": "accusative object", "ID": "1862"}, {"sentence": "OA?", "acronym": "OA", "label": "accusative object", "ID": "1863"}, {"sentence": "The frames in GermaNet use comple-mentation codes provided with the German version of the CELEX Lexical Database (Baayen et al, 2005) such as NN.AN for transitive verbs with OAs.", "acronym": "OA", "label": "accusative object", "ID": "1864"}, {"sentence": "In order to find structures like (1) in a Ger-  man corpus, one needs to search for  (a) a prepositional phrase modifying the ac-  cusative object and preceding the finite  verb (i.e. in the so-called vorfeld), and  (b) an OA between finite verb  and infinite verb forms (i.e. in the so-  called rnittelfeld)  Obviously, two things need to be available  in order to enable such a search.", "acronym": "OA", "label": "accusative object", "ID": "1865"}, {"sentence": "Most no-  tably, the Hobbs algorithm depends on the ex-  istence OAn/~\" parse-tree node that is absent  from the Penn Tree-bank trees.", "acronym": "OA", "label": "of a", "ID": "1866"}, {"sentence": "In particular, the scheme infers the gender OA  referent from the gender of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.", "acronym": "OA", "label": "of a", "ID": "1867"}, {"sentence": "We also ob-  serve that the position OA pronoun in a story  influences the mention count of its referent.", "acronym": "OA", "label": "of a", "ID": "1868"}, {"sentence": "We present some typ-  ical results as well as the more rigorous results  OA blind evaluation of its output.", "acronym": "OA", "label": "of a", "ID": "1869"}, {"sentence": "However a singular noun phrase can be the ref-  erent OA plural pronoun, as illustrated by the  following example:  \"I think if I tell Viacom I need more  time, they will take 'Cosby' across the  street,\" says the general manager ol a  network a~liate.", "acronym": "OA", "label": "of a", "ID": "1870"}, {"sentence": "When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is relevant o the particular choice OAn-  tecedent.", "acronym": "OA", "label": "of a", "ID": "1871"}, {"sentence": "ap-afzk durum  INT-clear situation  'Very clear situation'  uzun yol  long road  ' l ong  road'  bu- ndan ba~ka  this-ABLATIVE other  'other than this'  ktz kedi-yi gSr-dii  girl cat-ACC see-TENSE  or   ktz g6rdii kediyi  'The girl saw the cat'  Figure 2: OPs in the proposed model.", "acronym": "OP", "label": "Operator", "ID": "1872"}, {"sentence": "Table 3: Paraphrasability of OPs Paraphrase pair P ?? (", "acronym": "OP", "label": "Operator", "ID": "1873"}, {"sentence": "OP Morp.", "acronym": "OP", "label": "Operator", "ID": "1874"}, {"sentence": "OPs occur with different frequencies.", "acronym": "OP", "label": "Operator", "ID": "1875"}, {"sentence": "3 Mu l t i -domain  Combinat ion   OP   Oehrle (1988) describes a model of multi-dimen-  sional composition in which every domain Di has  an algebra with a finite set of primitive operations  1Derived and basic categories in the examples are in  fact feature structures; ee section 4.", "acronym": "OP", "label": "Operator", "ID": "1876"}, {"sentence": "Component-wise OPs Mitchell and Lap- ata (2008) investigate using component-wise op- erators to combine the vectors of verbs and their intransitive subjects.", "acronym": "OP", "label": "Operator", "ID": "1877"}, {"sentence": "So, for example the wh-NP \"whom\" in COMP-  position can only be coindexed with an empty NP  bearingthe grammatical function of direct object or  OP for example.", "acronym": "OP", "label": "prepositional object", "ID": "1878"}, {"sentence": "He argues with her arg1 arg2 (9) 6In the ID trees, pobj stands for a OP, pcomp for the complement of a preposition, pmod for a prepositional modifier, and det for a determiner.", "acronym": "OP", "label": "prepositional object", "ID": "1879"}, {"sentence": "Consider for example (17) where the  OP (realized as where) may be merged  into the clause he put the book by a Ioncl-distance  movement equation or by a simple equation of the form  (t  FOCUS) = (~ ON).", "acronym": "OP", "label": "prepositional object", "ID": "1880"}, {"sentence": "For OPs, the semantic type of the noun within the  prepositional phrase in also indicated as one of the semantic noun class-  es mentioned earlier.", "acronym": "OP", "label": "prepositional object", "ID": "1881"}, {"sentence": "For example if we assume that in English  adjuncts follow OPs, for the sentence  in (11) the parser develops 19 readings whereas without  a coherence-check it had to pursue 42 different paths.", "acronym": "OP", "label": "prepositional object", "ID": "1882"}, {"sentence": "In TPP, the lexicographer develops three pieces of information about each sense: a semantic relation name, the properties of the OP, and the properties of the word to which the prepositional phrase is attached.", "acronym": "OP", "label": "prepositional object", "ID": "1883"}, {"sentence": "To the extent hat underparsing and OP are  avoided, the description is said to be faithful to the  input.", "acronym": "OP", "label": "overparsing", "ID": "1884"}, {"sentence": "Thus, a block of cells  (the set of cells each covering the same input sub-  string) is interdependent with respect o OP  operations, meaning that an OP operation  trying to fill one cell in the block is adding structure  to a partial description from a different cell in the  same block.", "acronym": "OP", "label": "overparsing", "ID": "1885"}, {"sentence": "The first consequence of this is that the  OP operations must be considered after the  underparsing and parsing operations for that block.", "acronym": "OP", "label": "overparsing", "ID": "1886"}, {"sentence": "The second consequence is that OP oper-  ations may need to be considered more than once,  because the result of one OP operation (if it  fills a cell) could be the source", "acronym": "OP", "label": "overparsing", "ID": "1887"}, {"sentence": "There are three main types of opera-  tions, corresponding to underparsing, parsing, and  OP actions.", "acronym": "OP", "label": "overparsing", "ID": "1888"}, {"sentence": "Crucial to Optimality Theory are faithful-  ness constraints, which are violated by underparsing  and OP.", "acronym": "OP", "label": "overparsing", "ID": "1889"}, {"sentence": "PD for Multilingual Word Sense Disambiguation.", "acronym": "PD", "label": "Peripheral Diversity", "ID": "1890"}, {"sentence": "150 80% 74%  > 150 74% 70%  Table 8: PD of the ATR method   (with the usage of a stoplist)    The majority of remaining errors originate  from the ambiguous POS tagging (more than  50%, problematic words being naziv(a), igra,  kod, etc.).", "acronym": "PD", "label": "Precision", "ID": "1891"}, {"sentence": "50  55  60  65  70  75  80  85  90  95  100  0  10  20  30  40  50  60  70  80  90  100 Pe rc e n ta ge Window Size PD Recall F-Measure   Figure 6: Relationship between the window size of the  snippet and recall/precision on the John Smith corpus.", "acronym": "PD", "label": "Precision", "ID": "1892"}, {"sentence": "PD-focused textual inference.", "acronym": "PD", "label": "Precision", "ID": "1893"}, {"sentence": "150 52% 58%  > 150 69% 68%  Table 6: PD of the ATR method   (without the usage of a stoplist)    In the first 50 terms for the domain of mathe- matical analysis, there was only one false term  candidate (specijalna klasa neprekidnih pres- likavanja), which contained an ?", "acronym": "PD", "label": "Precision", "ID": "1894"}, {"sentence": "PD is the proportion of mentions in HC that are  also in TC and recall is the proportion of mentions in TC  that are also in HC.", "acronym": "PD", "label": "Precision", "ID": "1895"}, {"sentence": "Figure 2: PD using honorific scoring  scheme with syntactic Hobbs algorithm  for the last-noun method and 70.3% for the  Hobbs method.", "acronym": "PD", "label": "Precision", "ID": "1896"}, {"sentence": "Although adjectives are PDs, they have a much more limited distribution than verbs, and do not present long-distance dependencies.", "acronym": "PD", "label": "predicate", "ID": "1897"}, {"sentence": "The same applies to event adjectives, this time being PDs (flipping coin - a coin flips).", "acronym": "PD", "label": "predicate", "ID": "1898"}, {"sentence": "They are further characterised by not occur- ing as PDs (low value for -1ve).", "acronym": "PD", "label": "predicate", "ID": "1899"}, {"sentence": "Adjectives are PDs, equivalent to verbs when appearing in predicative environments.", "acronym": "PD", "label": "predicate", "ID": "1900"}, {"sentence": "The other main function of the adjective is that of PD in a copular sentence (6% of the tokens).", "acronym": "PD", "label": "predicate", "ID": "1901"}, {"sentence": "The arity is a basic parameter for the seman- tic characterisation of any PD.", "acronym": "PD", "label": "predicate", "ID": "1902"}, {"sentence": "Although adjectives are PD, they have a much more limited distribution than verbs, and do not present long-distance dependencies.", "acronym": "PD", "label": "predicates", "ID": "1903"}, {"sentence": "The same applies to event adjectives, this time being PD (flipping coin - a coin flips).", "acronym": "PD", "label": "predicates", "ID": "1904"}, {"sentence": "Verbless clauses: nouns, adjectives, and adverbs, lexical or derived, functioning as PD ?", "acronym": "PD", "label": "predicates", "ID": "1905"}, {"sentence": "They are further characterised by not occur- ing as PD (low value for -1ve).", "acronym": "PD", "label": "predicates", "ID": "1906"}, {"sentence": "Adjectives are PD, equivalent to verbs when appearing in predicative environments.", "acronym": "PD", "label": "predicates", "ID": "1907"}, {"sentence": "The SE problem is quite similar to the  Information Extraction (IE) task, in that in both cases  we are interested only in certain PD and their  argument bindings and not in full understanding.", "acronym": "PD", "label": "predicates", "ID": "1908"}, {"sentence": "The narrate  verbs require OCjects or objects of normalization phrases, the basic structures  are ?", "acronym": "OC", "label": "clausal ob", "ID": "1909"}, {"sentence": "NP+[AG]+[NP+VP]+VP(NAR), or [NP+VP] +NP +[AG]+VP(NAR), in which the  OCjects of citation verbs unusually attaches clause particle ?", "acronym": "OC", "label": "clausal ob", "ID": "1910"}, {"sentence": "For each verb, each grammatical role ( stC`A is the set of such roles) is instantiated from the stock of all con- stituents ( /8u \u0002wv x`v5y , which includes all np and pp constituents but also the verbs as potential heads of OCjects).", "acronym": "OC", "label": "clausal ob", "ID": "1911"}, {"sentence": "Note that in multiclausal  constructions there is the corresponding subordination  of different OCliqueness hlerarchles (for the salve  or comparalgility with diagrams (3) involving time  arrow, Hasse dm~ams for obliqueness are displayed  with a turn of 90~right):  (8) Kim said Lee saw Max.", "acronym": "OC", "label": "clausal ob", "ID": "1912"}, {"sentence": "but there is no frame that combines a subject, a prepositional object and a OCject.", "acronym": "OC", "label": "clausal ob", "ID": "1913"}, {"sentence": "The worst example in our tests was a verb that receives from the maxent classifier two sub- jects and three OCjects.", "acronym": "OC", "label": "clausal ob", "ID": "1914"}, {"sentence": "The narrate  verbs require OCs or objects of normalization phrases, the basic structures  are ?", "acronym": "OC", "label": "clausal object", "ID": "1915"}, {"sentence": "verbal predicates with a pronominal subject  and a OC  In order to determine whether our results hold  on sample documents of smaller size, we con- ducted a second round of experiments where  document length was scaled down to five sen- tences per document.", "acronym": "OC", "label": "clausal object", "ID": "1916"}, {"sentence": "NP+[AG]+[NP+VP]+VP(NAR), or [NP+VP] +NP +[AG]+VP(NAR), in which the  OCs of citation verbs unusually attaches clause particle ?", "acronym": "OC", "label": "clausal object", "ID": "1917"}, {"sentence": "For each verb, each grammatical role ( stC`A is the set of such roles) is instantiated from the stock of all con- stituents ( /8u \u0002wv x`v5y , which includes all np and pp constituents but also the verbs as potential heads of OCs).", "acronym": "OC", "label": "clausal object", "ID": "1918"}, {"sentence": "but there is no frame that combines a subject, a prepositional object and a OC.", "acronym": "OC", "label": "clausal object", "ID": "1919"}, {"sentence": "The worst example in our tests was a verb that receives from the maxent classifier two sub- jects and three OCs.", "acronym": "OC", "label": "clausal object", "ID": "1920"}, {"sentence": "Deter- mining the correct label for a single textual en- tailment example requires human analysts to make many smaller, LOC decisions which may de- pend on each other.", "acronym": "LOC", "label": "localized", "ID": "1921"}, {"sentence": "Each latent concept is then modeled as a LOC Gaussian distribution over the embedding space.", "acronym": "LOC", "label": "localized", "ID": "1922"}, {"sentence": "Our approach is to augment model training compared to the clean baseline by adding non-English, mixed-language, and non-language material, and to augment the model?s feature set with language- identification features more LOC than the sentence-level perplexity described above, as well as other features designed primarily to distinguish non- language material such as mark-up codes.", "acronym": "LOC", "label": "localized", "ID": "1923"}, {"sentence": "These are generic systems that can be LOC to specific domains.", "acronym": "LOC", "label": "localized", "ID": "1924"}, {"sentence": "3.2 Linguistic events  Russian gaps are LOC to second conjugation  non-past verb forms, so productions of these forms  are the focus of interest.", "acronym": "LOC", "label": "localized", "ID": "1925"}, {"sentence": "One of the two Twitter datasets is primarily LOC to the United States, while the remaining datasets cover the whole world.", "acronym": "LOC", "label": "localized", "ID": "1926"}, {"sentence": "Nouns are inflected based  on number (singular, plural), article and case [k?- raka] (nominative, accusative, instrumental, dative,  ablative, genitive, LOC and vocative).", "acronym": "LOC", "label": "locative", "ID": "1927"}, {"sentence": "Phenomenon Occurrence Agreement Named Entity 91.67% 0.856 LOC 17.62% 0.623 Numerical Quantity 14.05% 0.905 temporal 5.48% 0.960 nominalization 4.05% 0.245 implicit relation 1.90% 0.651 Table 4: Occurrence statistics for hypothesis struc- ture features.", "acronym": "LOC", "label": "locative", "ID": "1928"}, {"sentence": "The problem arises during the parsing  process with the fact that the \" to\"  argument of  \"prefer\" in Italian may occur before the verb, and the  LOC preposition \" in\"  is a, the same word as the  marking preposition corresponding to \"to\".", "acronym": "LOC", "label": "locative", "ID": "1929"}, {"sentence": "LOC-, con- junctive-, and objective particles) ?? (", "acronym": "LOC", "label": "locative", "ID": "1930"}, {"sentence": "Typical target positions  for transitions corresponding to noun phrase mod-  ification (noun phrases are head-final in Chinese)  are as follows:  head: 0 ( f l ight )   nominal: -1 (a i r l ine)   ad ject ive:  -3 (cheap)  possessive: -5 (Continental~s)  relative: -6 (that leaves NYC)  LOC: -8 (from NYC)  temporal: -9 (before one pm)  classifier: -I0 (pint)  specifier: -11 (a11)  cardinal: -II (five)  ordinal: -11 (first)  DE: -2, -4, or -6  The position for transitions emitting the Chi-  nese particle pronounced DE may be either -2, -4,  or -6, depending on the transducer states for the  transition.", "acronym": "LOC", "label": "locative", "ID": "1931"}, {"sentence": "But even the ostensibly disambiguating prepo-  sition by, is itself ambiguous, since it might introduce  a manner or LOC phrase consistent with the main  clause analysis.", "acronym": "LOC", "label": "locative", "ID": "1932"}, {"sentence": "n+ 1 2 )2 (1) In order to make pairwise comparisons between samples and to establish the LOC of the difference, we rely on the Mann?Whitney test.", "acronym": "LOC", "label": "locus", "ID": "1933"}, {"sentence": "Biological proof PMID 1744050 : DNA sequencing established that spoIIIF and spoVB are a single monocistronic LOC encoding a 518-amino-acid polypeptide with features of an integral membrane protein.", "acronym": "LOC", "label": "locus", "ID": "1934"}, {"sentence": "C11b Subfact: p19ARF induces cell cycle arrest in a p53-dependent manner INK4a/ARF is perhaps the second most commonly disrupted LOC in cancer cells.", "acronym": "LOC", "label": "locus", "ID": "1935"}, {"sentence": "In  what follows then we will concentrate on terms  and their meaning as expressed in definitions,  the typical LOC tbr conceptual meaning  information.", "acronym": "LOC", "label": "locus", "ID": "1936"}, {"sentence": "The lesson learned can then be  used to LOC on what knowledge needs malmal an-  notation.", "acronym": "LOC", "label": "locus", "ID": "1937"}, {"sentence": "In this way, reformulations iden- tify the LOC of any error, and hence the existence of an error.", "acronym": "LOC", "label": "locus", "ID": "1938"}, {"sentence": "A constraint that de- scribes this individual word pair would be trivial to write, but it is not feasible to model the general phenomenon in this way; thousands of constraints would be needed just to reflect the more impor- tant colLOC in a language, and the exact set of collocating words is impossible to predict ac- curately.", "acronym": "LOC", "label": "locations", "ID": "1939"}, {"sentence": "In another  approach, she used regular expression patterns to  extract term colLOC from a morpho- syntactically tagged corpus.", "acronym": "LOC", "label": "locations", "ID": "1940"}, {"sentence": "The ex- tracted colLOC were filtered with a stop- word list, and only colLOC containing sin- gle-word terms (devised previously by bilingual  alignment) were accepted as relevant.", "acronym": "LOC", "label": "locations", "ID": "1941"}, {"sentence": "For ex- ample, Vintar (2000) presented two methods for  extraction of terminological colLOC in order  to assist the translation process in Slovene.", "acronym": "LOC", "label": "locations", "ID": "1942"}, {"sentence": "Extracting Terms and Terminological  ColLOC from the ELAN Slovene-English Par- allel Corpus.", "acronym": "LOC", "label": "locations", "ID": "1943"}, {"sentence": "defined as follows: RuleID: ON; TriggeringCondition: (DMOVE:specifyCommand, TYPE:SwitchOn); DeclareExpectations: { LOC, DeviceType } ActionsExpectations: { [DeviceType] => {NLG(DeviceType);} } PostActions: { ExecuteAction(@is-ON); } The DTAC obtained for switch on triggers the dialogue rule ON.", "acronym": "LOC", "label": "Location", "ID": "1944"}, {"sentence": "LOC prediction in social media based on tie strength.", "acronym": "LOC", "label": "Location", "ID": "1945"}, {"sentence": "However, since two declared expectations are still missing (LOC and De- viceType), the dialogue manager will activate the ActionExpectations and prompt the user for the kind of device she wants to switch on, by means of a call to the natural language generation mod- ule NLG(DeviceType).", "acronym": "LOC", "label": "Location", "ID": "1946"}, {"sentence": "Using our detailed dependency annotations we can also determine how many instances need addi- 175 LOC Main Fact Subfact Synonym Extra Title 3.3 ( 0.2) 1.9 ( 0.7) 0.0 ( 0.0) 0.8 ( 0.8) Abstract 19.1 (10.1) 9.3 ( 5.1) 36.2 (21.7) 25.8 (14.8) Introduction 11.3 ( 5.2) 8.3 ( 3.4) 30.4 (17.4) 17.2 ( 7.8) Results 31.0 (13.8) 37.6 (16.1) 20.3 (15.9) 32.0 (12.5) Discussion 21.8 ( 7.3) 19.5 ( 6.6) 2.9 ( 1.4) 9.4 ( 3.1) Figure Heading 5.0 ( 0.6) 10.7 ( 3.8) 1.4 ( 1.4) 2.3 ( 0.0) Figure Legend 3.1 (", "acronym": "LOC", "label": "Location", "ID": "1947"}, {"sentence": "However, since two declared expectations are still missing (LOC and De- viceType), the dialogue manager will activate the ActionExpectations and prompt the user for the kind of device she wants to switch on, b", "acronym": "LOC", "label": "Location", "ID": "1948"}, {"sentence": "FrameNet II contains                                                    5 http://www.wjh.harvard.edu/~inquirer/homecat.htm  Table 1: Example of opinion related frames  and lexical units  Frame  name Lexical units Frame elements  Desiring want, wish, hope,  eager, desire,  interested,  Event,  Experiencer,  LOC_of_event Emotion _directed agitated, amused,  anguish, ashamed,  angry, annoyed,  Event, Topic  Experiencer,  Expressor,  Mental  _property absurd, brilliant,  careless, crazy,  cunning, foolish  Behavior,  Protagonist,  Domain, Degree  Subject  _stimulus delightful, amazing,  annoying, amusing,  aggravating,  Stimulus, Degree Experiencer,  Circumstances,     Figure 1: An overview of our", "acronym": "LOC", "label": "Location", "ID": "1949"}, {"sentence": "DMOVE specifyCommand TYPE SwitchOn ARGS [ LOC, DeviceType ] META INFO ?", "acronym": "LOC", "label": "Location", "ID": "1950"}, {"sentence": "defined as follows: RuleID: ON; TriggeringCondition: (DMOVE:specifyCommand, TYPE:SwitchOn); DeclareExpectations: { LOC, DeviceType } ActionsExpectations: { [DeviceType] => {NLG(DeviceType);} } PostActions: { ExecuteAction(@", "acronym": "LOC", "label": "Location", "ID": "1951"}, {"sentence": "salience(re/)  = -2  LO  Making the unrealistic simplifying assumption  that references of one gender class are com-  pletely indepe", "acronym": "LO", "label": "log", "ID": "1952"}, {"sentence": "salience(re/)  = -2  LO  Making the unrealistic simplifying assumption  that references of one gender class are com-  pletely independent of references for another  classes 1, the likelihood function in this case is  just the product over all classes of the probabil-  ities of each class of reference to the power of  the number of observations of this class.", "acronym": "LO", "label": "log", "ID": "1953"}, {"sentence": "2006), gen- erate diaLOs uttered by different characters using synthetic voices appropriate for each character?s gender, age and personality (Greene et al.,", "acronym": "LO", "label": "log", "ID": "1954"}, {"sentence": "HatzivassiLOlou and McKeown, 1997)  for another application in which no explicit  indicators are available in the stream).", "acronym": "LO", "label": "log", "ID": "1955"}, {"sentence": "This decision is made by ranking the refer-  ents by LO-likelihood ratio, termed salience, for  each referent.", "acronym": "LO", "label": "log", "ID": "1956"}, {"sentence": "Shared story reading with parents or teachers helps children to learn about vocabulary, syntax and phonoLOy, and to develop narrative comprehension and awareness of the concepts of print, all of which are linked to developing reading and writing skills (National Early Literacy Panel 2008).", "acronym": "LO", "label": "log", "ID": "1957"}, {"sentence": "Vasileios HatzivassiLOlou and Kathleen R.  McKeown.", "acronym": "LO", "label": "log", "ID": "1958"}, {"sentence": "Textual Entailment Through Ex- tended LO.", "acronym": "LO", "label": "Lexical Overlap", "ID": "1959"}, {"sentence": "Literary News Subtitles Parallel Text Articles Corpus # Tokens 1879 3379 1632 1581 # Unique Tokens 811 1473 824 609 # Shared Tokens 519 1125 402 354 LO 72.5 82.9 63.2 62.7 LO 68.4 67.2 48.6 45 (lem.", "acronym": "LO", "label": "Lexical Overlap", "ID": "1960"}, {"sentence": "Textual Entailment through Ex- tended LO and Lexico-Semantic  Matching.", "acronym": "LO", "label": "Lexical Overlap", "ID": "1961"}, {"sentence": "c?2007 Association for Computational Linguistics Textual Entailment Through Extended LO and Lexico-Semantic Matching Rod Adams, Gabriel Nicolae, Cristina Nicolae and Sanda Harabagiu Human Language Technology Research Institute University of Texas at Dallas Richardson, Texas {rod, gabriel, cristina, sanda}@hlt.utdallas.edu Abstract This paper presents two systems for textual entailment, both employing decision trees as a supervised learning algorithm.", "acronym": "LO", "label": "Lexical Overlap", "ID": "1962"}, {"sentence": "6.4 Evaluating Summaries using LO ROUGE (Lin 2004) is a package for automatically evaluating summaries.", "acronym": "LO", "label": "Lexical Overlap", "ID": "1963"}, {"sentence": "P e t r i c k ,  Trans fo rmat iona l  Analysis, II  I n  NLP, ed.", "acronym": "NLP", "label": "Natural  Language Processing", "ID": "1964"}, {"sentence": "3.3 NLP Tools  We used the N I,P COml)onents of kl/ l ' - .", "acronym": "NLP", "label": "Natural  Language Processing", "ID": "1965"}, {"sentence": "204  Bruce and Wiebe Decomposable Model ing in NLP  Data Set Naive Model  Word Number (10% used as a test Entropy Bayes Selection MS - NB Majority Best  of Senses set on each fold) (NB) (MS) Classifier Model  Tagged Word  Instances Count  sick 14 659 15066 2.969 56.8 65.1 +8.3 30.8 67.4  storm 18 752 20806 2.895 63.4 71.6 +8.2 39.6 73.6  drift 17 520 13484 2.889 56.0 63.3 +7.3 31.7 66.0  curious 3 459 12950 0.833 83.0", "acronym": "NLP", "label": "Natural  Language Processing", "ID": "1966"}, {"sentence": "In J. Vicedo, P. Martnez- Barco, R. Muoz, and M. Saiz Noeda, editors, Ad- vances in NLP, volume 3230 of Lecture Notes in Computer Science, pages 82?90.", "acronym": "NLP", "label": "Natural Language Processing", "ID": "1967"}, {"sentence": "He  has worked on Machine Learning in the context of NLP and  has published papers in several conferences.", "acronym": "NLP", "label": "Natural Language Processing", "ID": "1968"}, {"sentence": "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in NLP, pages 779?786, Vancouver.", "acronym": "NLP", "label": "Natural Language Processing", "ID": "1969"}, {"sentence": "In Proceedings of 2010 Workshop on Do- main Adaptation for NLP.", "acronym": "NLP", "label": "Natural Language Processing", "ID": "1970"}, {"sentence": "940  Proceedings of the 2016 Conference on Empirical Methods in NLP, pages 950?954, Austin, Texas, November 1-5, 2016.", "acronym": "NLP", "label": "Natural Language Processing", "ID": "1971"}, {"sentence": "Role-playing games \u0001 System by Microsoft Research (Kacmarcik 2005) 61   Proceedings of the 2016 Conference on Empirical Methods in NLP, pages 846?855, Austin, Texas, November 1-5, 2016.", "acronym": "NLP", "label": "Natural Language Processing", "ID": "1972"}, {"sentence": "CONCI.USIONS  There are many important sources of infor-  mation for NLP.", "acronym": "NLP", "label": "natural anguage processing", "ID": "1973"}, {"sentence": "& Whitney R.A., A general organization of ~he  knowledge for NLP: the 1lehman  Upper Model, ISI research report, California, 1990.", "acronym": "NLP", "label": "natural anguage processing", "ID": "1974"}, {"sentence": "1 In t roduct ion   This paper describes a new discourse module within  our multilingual NLP system  which has been used for understanding texts in En-  glish, Spanish and Japanese (el.", "acronym": "NLP", "label": "natural anguage processing", "ID": "1975"}, {"sentence": "t r   Abst rac t   Automatic text tagging is an important  component in higher level analysis of text  corpora, and its output can be used in  many NLP applica-  tions.", "acronym": "NLP", "label": "natural anguage processing", "ID": "1976"}, {"sentence": "It should be noted that  all the processing steps, those performed by the backbone system,  and those performed by the NLP com-  ponents, are fully automated, and no human intervention rmanual  encoding isrequired.", "acronym": "NLP", "label": "natural anguage processing", "ID": "1977"}, {"sentence": "This environment is based on a  set of NLP compo-  nents, at the morphologic, syntactic and  semantic levels.", "acronym": "NLP", "label": "natural anguage processing", "ID": "1978"}, {"sentence": "We believe that we have presented a viable approach to the automatic generation  of a NLP.", "acronym": "NLP", "label": "natural language processor", "ID": "1979"}, {"sentence": "CONCLUSIONS The LOLITA system is a NLP that has a core functionality on which a number of different applications can be built.", "acronym": "NLP", "label": "natural language processor", "ID": "1980"}, {"sentence": "To interpret a comparative xpression (CE) a  'NLP must determine (1)  the entities to he compared, and (2) the at-  tribute(s) of those entities to consider in per-  forming the comparison.", "acronym": "NLP", "label": "natural language processor", "ID": "1981"}, {"sentence": "We believe  -that automatic, or semi-automatic acquisition of  the lexicon is a critical factor in determining how  widespread the use of NLPs  will be in the next few years. '", "acronym": "NLP", "label": "natural language processor", "ID": "1982"}, {"sentence": "We believe that this approach to machine learning of  a NLP that involves a 1/uman in-  formant in an elicit-generate-test loop and uses scaffold-  ing provided by the human informant in machine learn-  ing, is a very viable approach that avoids the noise and  opaqueness of other induction schemes.", "acronym": "NLP", "label": "natural language processor", "ID": "1983"}, {"sentence": "A Method for the Acquisition and Interpretation of a Semantic Lexicon  Our research on lexical acquisition from corpora started in 1988, when a first version  of the system was built as utility for the DANTE NLP (Velardi  1989), a system that analyzes press agency releases on finance and economics.", "acronym": "NLP", "label": "natural language processor", "ID": "1984"}, {"sentence": "INTRODUCTION  NLPg systems  need much larger lexicons than those  available today.", "acronym": "NLP", "label": "Natural language processin", "ID": "1985"}, {"sentence": "1 Introduction  1.1 Genera l i t ies   NLPg is nowadays trongly  related to Cognitive Science, since linguistics, psy-  chology and computer science have to collaborate  to produce systems that are useful for man-machine  communication.", "acronym": "NLP", "label": "Natural language processin", "ID": "1986"}, {"sentence": "NLPg (almost) from scratch.", "acronym": "NLP", "label": "Natural language processin", "ID": "1987"}, {"sentence": "\\[Shinghal 92\\] Shinghal R., NLPg:  a prescriptive grammar, In Formal conceptions in Arti-  ficial Intelligence, pp 131-232, Chapman & Hall, 1992.", "acronym": "NLP", "label": "Natural language processin", "ID": "1988"}, {"sentence": "INTRODUCTION  NLP systems  need much larger lexicons than those  available today.", "acronym": "NLP", "label": "Natural language processing", "ID": "1989"}, {"sentence": "1 Introduction  1.1 Genera l i t ies   NLP is nowadays trongly  related to Cognitive Science, since linguistics, psy-  chology and computer science have to collaborate  to produce systems that are useful for man-machine  communication.", "acronym": "NLP", "label": "Natural language processing", "ID": "1990"}, {"sentence": "NLP (almost) from scratch.", "acronym": "NLP", "label": "Natural language processing", "ID": "1991"}, {"sentence": "\\[Shinghal 92\\] Shinghal R., NLP:  a prescriptive grammar, In Formal conceptions in Arti-  ficial Intelligence, pp 131-232, Chapman & Hall, 1992.", "acronym": "NLP", "label": "Natural language processing", "ID": "1992"}, {"sentence": "In J. Vicedo, P. Martnez- Barco, R. Muoz, and M. Saiz Noeda, editors, Ad- vances in NLPg, volume 3230 of Lecture Notes in Computer Science, pages 82?90.", "acronym": "NLP", "label": "Natural Language Processin", "ID": "1993"}, {"sentence": "He  has worked on Machine Learning in the context of NLPg and  has published papers in several conferences.", "acronym": "NLP", "label": "Natural Language Processin", "ID": "1994"}, {"sentence": "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in NLPg, pages 779?786, Vancouver.", "acronym": "NLP", "label": "Natural Language Processin", "ID": "1995"}, {"sentence": "In Proceedings of 2010 Workshop on Do- main Adaptation for NLPg.", "acronym": "NLP", "label": "Natural Language Processin", "ID": "1996"}, {"sentence": "940  Proceedings of the 2016 Conference on Empirical Methods in NLPg, pages 950?954, Austin, Texas, November 1-5, 2016.", "acronym": "NLP", "label": "Natural Language Processin", "ID": "1997"}, {"sentence": "Role-playing games \u0001 System by Microsoft Research (Kacmarcik 2005) 61   Proceedings of the 2016 Conference on Empirical Methods in NLPg, pages 846?855, Austin, Texas, November 1-5, 2016.", "acronym": "NLP", "label": "Natural Language Processin", "ID": "1998"}, {"sentence": "2 Motivation PP attachment disambiguation has often been studied as a benchmark test for empirical meth- ods in NLPg.", "acronym": "NLP", "label": "natural language processin", "ID": "1999"}, {"sentence": "Mitigating the paucity- of-data problem: Exploring the effect of training corpus size on classifier performance for NLPg.", "acronym": "NLP", "label": "natural language processin", "ID": "2000"}, {"sentence": "Roth has published broadly in machine learning, NLPg,  knowledge representation and reasoning and received several paper, teaching and  research awards.", "acronym": "NLP", "label": "natural language processin", "ID": "2001"}, {"sentence": "Web-based models for NLPg.", "acronym": "NLP", "label": "natural language processin", "ID": "2002"}, {"sentence": "In practical NLPg applic- ations the fan-out of the grammar is typically bounded by some small number.", "acronym": "NLP", "label": "natural language processin", "ID": "2003"}, {"sentence": "Nevertheless, as current NLPg 54 Molla?", "acronym": "NLP", "label": "natural language processin", "ID": "2004"}, {"sentence": "2 Motivation PP attachment disambiguation has often been studied as a benchmark test for empirical meth- ods in NLP.", "acronym": "NLP", "label": "natural language processing", "ID": "2005"}, {"sentence": "Mitigating the paucity- of-data problem: Exploring the effect of training corpus size on classifier performance for NLP.", "acronym": "NLP", "label": "natural language processing", "ID": "2006"}, {"sentence": "Roth has published broadly in machine learning, NLP,  knowledge representation and reasoning and received several paper, teaching and  research awards.", "acronym": "NLP", "label": "natural language processing", "ID": "2007"}, {"sentence": "Web-based models for NLP.", "acronym": "NLP", "label": "natural language processing", "ID": "2008"}, {"sentence": "In practical NLP applic- ations the fan-out of the grammar is typically bounded by some small number.", "acronym": "NLP", "label": "natural language processing", "ID": "2009"}, {"sentence": "Nevertheless, as current NLP 54 Molla?", "acronym": "NLP", "label": "natural language processing", "ID": "2010"}, {"sentence": "PS Metaphorical Literal0 1 Vs.", "acronym": "PS", "label": "Previous Sentence", "ID": "2011"}, {"sentence": "Ad- ditionally, PS Arg1 model perfor- mance is much lower than that of the other models due to the fact that it only considers immediately previous sentence; which, as was mentioned ear- lier, covers only 71.7% of the inter-sentential re- lations.", "acronym": "PS", "label": "Previous Sentence", "ID": "2012"}, {"sentence": "PS 0 1 MetaphoricalLiteral T0 T1 T2 T3 T4 T5 T6 T7 T8 T9 Vs.", "acronym": "PS", "label": "Previous Sentence", "ID": "2013"}, {"sentence": "PS l l l l l l ll Metaphorical Literal0 1 Vs.", "acronym": "PS", "label": "Previous Sentence", "ID": "2014"}, {"sentence": "models in all but PS Arg2 category.", "acronym": "PS", "label": "Previous Sentence", "ID": "2015"}, {"sentence": "In anal--  ogy to a eontext-flee PS rule, a DATR  sentence has a left hand side that consists of exactly  one non-terminal symbol (i.e. a node-path pair) and  a right hand side that consists of an arbitrary num-  ber of non-terminal and terminal symbols (i.e. DATR  atoms).", "acronym": "PS", "label": "phrase structure", "ID": "2016"}, {"sentence": "Us- ing a stochastic variant of Constraint Dependency Grammar (Wang and Harper, 2004) reached a 92.4% labelled F-score on the Penn Treebank, which slightly outperforms (Collins, 1999) who reports 92.0% on dependency structures automati- cally derived from PS results.", "acronym": "PS", "label": "phrase structure", "ID": "2017"}, {"sentence": "I I l  contrast to context-free PS  grmmnar, DATR nonterminals are not atomic sym-  hols, but highly structured complex objects.", "acronym": "PS", "label": "phrase structure", "ID": "2018"}, {"sentence": "Subcategorization and voice Each verb has a subcategorization frame, which is useful for build- ing verb PS.", "acronym": "PS", "label": "phrase structure", "ID": "2019"}, {"sentence": "Classifier VB Verb ADJ Adjective ADNOM Adnominal adjective ADV Adverb PCS Case particle PBD Binding particle PADN Adnominal particle PCO Parallel particle PCJ Conjunctive particle PEND Sentence-ending particle P Particle (others) AUX Auxiliary verb CONJ Conjunction PNC Punctuation PAR Parenthesis SYM Symbol FIL Filler Table 1: Preterminal tags automatically converted from dependency structure to PS by the previously described method (Uematsu et al 2013), and conversion er- rors of structures and tags were manually corrected.", "acronym": "PS", "label": "phrase structure", "ID": "2020"}, {"sentence": "Keyaki Treebank: PS with functional informa- tion for Japanese.", "acronym": "PS", "label": "phrase structure", "ID": "2021"}, {"sentence": "Combining treebank transfor- mation techniques with a suffix analysis, (Dubey, 2005) trained a probabilistic parser and reached a labelled F-score of 76.3% on PS an- notations for a subset of the sentences used here (with a maximum length of 40).", "acronym": "PS", "label": "phrase structure", "ID": "2022"}, {"sentence": "The Penn Chinese Treebank: PS annotation of a large corpus.", "acronym": "PS", "label": "Phrase structure", "ID": "2023"}, {"sentence": "PS surprisal and phase structure en- tropy reduction are good predictors of the sort of human parsing difficulty that is measured by re- gression path duration, for these sentence types.", "acronym": "PS", "label": "Phrase structure", "ID": "2024"}, {"sentence": "6.2 PS surprisal PS surprisal predicted that the am- biguous cases would be harder then the unambigu- ous cases; and that the disadvantage of sentence type 1 in the ambiguous cases would turn around into a disadvantage of sentence type 2 in the unam- biguous conditions.", "acronym": "PS", "label": "Phrase structure", "ID": "2025"}, {"sentence": "PS  grammars are entirely concerned with assigning termi-  nal strings to categories and determining dominance and  precedence between constituents on the basis of their  categories.", "acronym": "PS", "label": "Phrase structure", "ID": "2026"}, {"sentence": "6.3 PS entropy reduction The directions of the entropy reduction hypothesis predictions were the same as for phrase structure surprisal, although there was a relatively greater difficulty with the type 2 cases versus surprisal.", "acronym": "PS", "label": "Phrase structure", "ID": "2027"}, {"sentence": "5.1 The basic elements  The minimal machinery necessary for generating tense is  composed of a time axis with a PS (now)  and a means of locating an action/event with respect to  this PS (Figure 5).", "acronym": "PS", "label": "point of speech", "ID": "2028"}, {"sentence": "PS  o  o  time axis O  Figure 5  Reichenbach used the following concepts to characterize  tense: a PS (S), a point of reference (R) and a  point of event (E).", "acronym": "PS", "label": "point of speech", "ID": "2029"}, {"sentence": "where additional information may be added (e.g.  structure can only he added at leaves and nodes of  type COMPOSITE; furthermore, in an incremental  pipeline architecture such as this, information can  only be added ahead of the PS)  ?", "acronym": "PS", "label": "point of speech", "ID": "2030"}, {"sentence": "The point of reference can be used in order  to represent some instant which plays the role of a  translated PS (ht 1980 Paul had already  visited Paris twice.).", "acronym": "PS", "label": "point of speech", "ID": "2031"}, {"sentence": "Note  that 17r \", d'~ and A~ are vector quantities in which  each entry corresponds to a PS antecedent.", "acronym": "PS", "label": "possible", "ID": "2032"}, {"sentence": "Here we would compare the degree to which  each PS candidate antecedent (A Japanese  company, television picture tubes, Japan, TV   sets, and Malaysia in this example) could serve  as the direct object of \"export\".", "acronym": "PS", "label": "possible", "ID": "2033"}, {"sentence": "It is quite PS that a  word appears in the test data that the program  never saw in the training data and low which it  hence has no P(plwo) probability.", "acronym": "PS", "label": "possible", "ID": "2034"}, {"sentence": "Given the above PS sources of informar  tion, we arrive at the following equation, where  F(p) denotes a function from pronouns to their  antecedents:  F(p) = argmaxP( A(p) = alp, h, l~', t, l, so, d~ A~')  where A(p) is a random variable denoting the  referent of the pronoun p and a is a proposed  antecedent.", "acronym": "PS", "label": "possible", "ID": "2035"}, {"sentence": "We  also transform our trees under certain condi-  tions to meet Hobbs' assumptions as much as  PS.", "acronym": "PS", "label": "possible", "ID": "2036"}, {"sentence": "Once we have the trees in the proper form  (to the degree this is PS) we run Hobbs'  algorithm repeatedly for each pronoun until it  has proposed n (= 15 in our experiment) can-  didates.", "acronym": "PS", "label": "possible", "ID": "2037"}, {"sentence": "23-28, 1992  in Southern Sweden, Sun in  Southern Sweden, Fair in Southern  Sweden,  or descriptive It will be  sunny.., There will be sun .... The  weather will be fair...  The generation process may be  set to generate telegraphic or full  utterances, single or coordinated  utterances, texts where the area is  kept in focus, e.g. Wales will get sun  and light winds or CC  with different areas such as Wales   will get sun, but Cornwall will get  rain.", "acronym": "CC", "label": "coordinations", "ID": "2038"}, {"sentence": "In addition to these  simple sentences, difficult problems are also han-  dled: clitics, complex determiners, completives, var-  ious forms of questions, extraction and non limited  dependancies, CC, comparatives.", "acronym": "CC", "label": "coordinations", "ID": "2039"}, {"sentence": "For binary trees, the coordination is represented by a left-branching tree, which is a conjunction or a con- junction particle that first joined a left hand con- stituent; the phrase is marked as a modifier consist- ing of coordination (-NCOORD and -VCOORD for NP and VP CC), as shown on the left side of Figure 3.", "acronym": "CC", "label": "coordinations", "ID": "2040"}, {"sentence": "Indeed, according to this approach  (Chomsky, 1957; Banfield, 1981), only coordination  of sentences are basic and other syntagmatic coordi-  nations should be considered as CC of re-  duced sentences, the reduction being performed by  deleting repeated elements.", "acronym": "CC", "label": "coordinations", "ID": "2041"}, {"sentence": "However, this solution cannot be applied gener-  ally because all CC have not such \"natu-  ral\" intersection (see (2)).", "acronym": "CC", "label": "coordinations", "ID": "2042"}, {"sentence": "Last, note that gapping the verb is less compati-  ble with head-driven mechanisms (and the comma in  (4f) could be such a head mark, see (BEF, 1996) for  an analysis of Gapping CC).", "acronym": "CC", "label": "coordinations", "ID": "2043"}, {"sentence": "denote the CC between the relevance ranking and the performance ranking for a given question.", "acronym": "CC", "label": "correlation coefficient", "ID": "2044"}, {"sentence": "The results in Figure 2 shows strong correlation be- tween the confidence measure and the alignment F-score, with the CCs equals to -0.69.", "acronym": "CC", "label": "correlation coefficient", "ID": "2045"}, {"sentence": "For a sample of size n, the n raw scores X i , Y i are converted to ranks x i , y i , and the Spearman CC ?", "acronym": "CC", "label": "correlation coefficient", "ID": "2046"}, {"sentence": "Pearson CC with hu- man annotations was computed individually for each test set and a weighted sum of the correlations was used as the final evaluation metric (the weight for each dataset was proportional to its size).", "acronym": "CC", "label": "correlation coefficient", "ID": "2047"}, {"sentence": "Regarding the interpretation of the absolute value of (Pearson?s) CCs, both here and in the rest of the paper, we adopt Cohen?s scale (Co- hen, 1988) for use in human judgements, given in Table 1; we use this as most of this work is to do with human judgements of fluency.", "acronym": "CC", "label": "correlation coefficient", "ID": "2048"}, {"sentence": "We adopt the Spearman?s rank CC as the test measure.", "acronym": "CC", "label": "correlation coefficient", "ID": "2049"}, {"sentence": "This might be due to the slow convergence of SimRank for higher values of C. Figure 1 shows that by varying thresholds the im- provement of the CCs Cluster- ing over the random clustering baseline at the same granularity first increases and then decreases.", "acronym": "CC", "label": "Connected Component", "ID": "2050"}, {"sentence": "55  0.6  0.65 FSco re Threshold CCs ClusteringRandom Clustering (a)  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1  0.35  0.4  0.45  0.5  0.55  0.6  0.65  0.7  0.75 FSco re Threshold CCs ClusteringRandom Clustering (b) Figure 1: Improvement in (a) average performance of best 3 Supervised Systems and (b) performance of best Unuper- vised System in Senseval-3 using CC Clustering Vs Random Clustering at the same granularity with C = 0.6 for experimentation.", "acronym": "CC", "label": "Connected Component", "ID": "2051"}, {"sentence": "Definition (CC): Let G = (V,E) be an undirected graph where V is the set of vertices and E is the set of edges.", "acronym": "CC", "label": "Connected Component", "ID": "2052"}, {"sentence": "uster senses but the generic nature of the similarity gives us the flexibility to use other clustering algorithms 17  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1  0.25  0.3  0.35  0.4  0.45  0.5  0.55  0.6  0.65 FSco re Threshold CCs ClusteringRandom Clustering (a)  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1  0.35  0.4  0.45  0.5  0.55  0.6  0.65  0.7  0.75 FSco re Threshold CCs ClusteringRandom Clustering (b) Figure 1: Improvement in (a) average performance of best 3 Supervised Systems and (b) performance of best Unuper- vised System in Senseval-3 using CC Clustering Vs Random Clustering at the same granularity with C = 0.6 for experimentation.", "acronym": "CC", "label": "Connected Component", "ID": "2053"}, {"sentence": "For coarsening senses, we used one of the simplest approaches to cluster senses but the generic nature of the similarity gives us the flexibility to use other clustering algorithms 17  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1  0.25  0.3  0.35  0.4  0.45  0.5  0.55  0.6  0.65 FSco re Threshold CCs ClusteringRandom Clustering (a)  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1  0.35  0.4  0.45  0.5  0.55  0.6  0.65  0.7  0.75 FSco re Threshold CCs ClusteringRandom Clustering (b) Figure 1: Improvement in (a) average performance of best 3 Supervised Systems and (b) performance of best Unuper- vised System in Senseval-3 using CC Clustering V", "acronym": "CC", "label": "Connected Component", "ID": "2054"}, {"sentence": "6 0.52 0.8453 0.7864 0.0589 SenseLearner 0.7104 0.49 0.8541 0.8097 0.0444 KOC University 0.7191 0.52 0.8448 0.7911 0.0538 IRST-DDD 0.6367 0.49 0.7970 0.7402 0.0568 0.8 GAMBL 0.7116 0.59 0.8419 0.7843 0.0577 SenseLearner 0.7104 0.56 0.8439 0.7984 0.0455 KOC University 0.7191 0.59 0.8414 0.7879 0.0535 IRST-DDD 0.6367 0.47 0.8881 0.8324 0.0557 Table 4: Improvement in Senseval-3 WSD performance using CC Clustering Vs Random Cluster- ing at the same granularity credit if it belongs to the cluster of the correct an- swer.", "acronym": "CC", "label": "Connected Component", "ID": "2055"}, {"sentence": "3.1 Argument Position Classification Discourse connectives have a very strong prefer- ence on the location of the Arg1 with respect to their syntactic category (Subordinating Conjunc- tion, CC, and Discourse Adverbial) and position in the sentence (sentence initial or sentence medial); thus, classification of discourse connectives into inter-sentential or intra- sentential is an easy task yielding high supervised machine learning performance (Stepanov and Ric- cardi, 2013; Lin et al.,", "acronym": "CC", "label": "Coordinating Conjunction", "ID": "2056"}, {"sentence": "6.5 The Punctuation and CC Parameter Classes 6.5.1 Inconsistent Model.", "acronym": "CC", "label": "Coordinating Conjunction", "ID": "2057"}, {"sentence": "5.2.1.2 CC.", "acronym": "CC", "label": "Coordinating Conjunction", "ID": "2058"}, {"sentence": "6.7 Handling CCs Headless constructions such as coordinating conjunctions have been one of the weaker points of dependency grammar approaches.", "acronym": "CC", "label": "Coordinating Conjunction", "ID": "2059"}, {"sentence": "The 114 English question Frequency Is Next Preposition 1523 Is Prev Determiner 1444 Is Prev Preposition 1209 Is Prev Adjective 864 Is Next Noun Singular Mass 772 Is Prev Noun Singular Mass 690 Is Next Noun Plural 597 Is Next Noun 549 Arabic question Frequency Is Prev Preposition 1110 Is Next Preposition 993 Is Prev Noun 981 Is Next Noun 912 Is Prev CC 627 Is Prev Noun SingularMass 607 Is Next Punctuation 603 Is Next Adjective Adverb 559 Table 1: Most frequent root node context questions models were then evaluated using AER at each train- ing iteration.", "acronym": "CC", "label": "Coordinating Conjunction", "ID": "2060"}, {"sentence": "CC: We will assume that a coordinating conjunction like but may link spans within a text-clause, or across text-clauses and text- sentences, so that no constraint on the levels of A and B results.", "acronym": "CC", "label": "Coordinating Conjunction", "ID": "2061"}, {"sentence": "Miyoshi, H.; Ogino, T. and Sugiyarna, K. (1997)  EDR's CCification and Description for  Interlingual Representation.", "acronym": "CC", "label": "Concept Class", "ID": "2062"}, {"sentence": "ncept Dictionary is one of the five types of EDR  dictionaries, the others are the Word Dmtlonarms  for English and Japanese the Blhngual Dictio-  nary, the Coocurrence Dictionary, and the Tech-  mcal Telmmology Dxctlonar} The EDR Con-  cept Dictionary consists of three sub-dmuonanes  the Headconcept Dlctxonaz} contains concept ex-  planations m natural language (both m Engh~h  and Japanese),~the CCification Dmuo-  nar} contains a set of ls-a relationships, and the  Concept Description Dictionary contains pairs of  concepts that have certain semantic relationships  other than ls-a relationship 1 e object, agent  9oal, zmplement a-object (object of a particular at-  tribute), place, scene and cause  The CCification Dmtlonar~ classifies  all the 400 000 concepts based o", "acronym": "CC", "label": "Concept Class", "ID": "2063"}, {"sentence": "NP1 NP1 Concept Vs Vs Concept VPc VPc  concept VPe VPe CC \u0001\u0002\u0003\u0004\u0005\u0003\u0006 Naringi  crenulata herb \u0007\b\t \u000b\f  use as \u000e\u000f\u0010\u0011\u0012\u0013\u0006\u0014 cure  poison be- antipyretic \u0004\u0001\t\u000b\u0015\u000e  \u0016\u0017\u0018\u0019\u001a relieve muscle  pain n \u0010\u000f\u0015\u0010\u0001\u0006 Asiatic  Pennyworth herb leaf \u0007\b\t \u000b\f  use as \u001b\u001c \u001d\u001c\u001a \u0019\u0001\u0006 apply  externally apply  topically \u0002\u000f\u0001\u0013\u001c\u0004\u001e\u001f heal wound y  \u0019\u0016\u0004\u000e!\u0006 red onion herb  \u000b\f  is \"#\u001c\u001a\u0006 excrete be-lexative \u0004\u0001\t\u001b\t\u0019!", "acronym": "CC", "label": "Concept Class", "ID": "2064"}, {"sentence": "c?2010 Association for Computational Linguistics CCification with Bayesian Multi-task Learning Marcel van Gerven Radboud University Nijmegen Intelligent Systems Heyendaalseweg 135 6525 AJ Nijmegen, The Netherlands marcelge@cs.ru.nl Irina Simanova Max Planck Institute for Psycholinguistics Wundtlaan 1 6525 XD Nijmegen, The Netherlands irina.simanova@mpi.nl Abstract Multivariate analysis allows decoding of sin- gle trial data in individ", "acronym": "CC", "label": "Concept Class", "ID": "2065"}, {"sentence": "Cross-Document Summarization by  CCification.", "acronym": "CC", "label": "Concept Class", "ID": "2066"}, {"sentence": "Real-time Visual CCification.", "acronym": "CC", "label": "Concept Class", "ID": "2067"}, {"sentence": "The position we argue for in this article, is that whereas adjacency and explicit conjunction (CCs such as and, or, so, and but; subordinating con- junctions such as although, whereas, and when) imply discourse relations between (the interpretation of) adjacent or conjoined discourse units, discourse adverbials such as then, otherwise, nevertheless, and instead are anaphors, signaling a relation between the interpretation of their matrix clause and an entity in or deri", "acronym": "CC", "label": "coordinating conjunction", "ID": "2068"}, {"sentence": "CCs: tokens morpho- logical tag of which starts with the pair J?", "acronym": "CC", "label": "coordinating conjunction", "ID": "2069"}, {"sentence": "The conjuncts are sister nodes separated by CCs; we call these configu- rations coordination domains.", "acronym": "CC", "label": "coordinating conjunction", "ID": "2070"}, {"sentence": "interjection D determiner P pre- or postposition, or subordinating conjunction & CC T verb particle X existential there, predeterminers Y X + verbal # hashtag (indicates topic/category for tweet) @ at-mention (indicates a user as a recipient of a tweet) ~ discourse marker, indications of continuation across multiple tweets U URL or email address E emoticon $ numeral , punctuation G other abbreviations, foreign words, possessive endings, symbols, garbage T", "acronym": "CC", "label": "coordinating conjunction", "ID": "2071"}, {"sentence": "We  used  parts  of  speech  often  found  in  G?TA systems:  V  verb,  N  noun,  A adjunct,  R  pronoun,  S 51 subordination (preposition,  subordinating conjunction and linking word),  C CC.", "acronym": "CC", "label": "coordinating conjunction", "ID": "2072"}, {"sentence": "Coor- dinating elements are commata and CCs.", "acronym": "CC", "label": "coordinating conjunction", "ID": "2073"}, {"sentence": "c?2008 Association for Computational Linguistics Learning CC in Sentence Realization Ranking Crystal Nakatsu Department of Linguistics The Ohio State Univeristy Columbus, OH, USA cnakatsu@ling.osu.edu Abstract We look at the average frequency of con- trastive connectives in the SPaRKy Restaurant Corpus with respect to realization ratings by human judges.", "acronym": "CC", "label": "Contrastive Connectives", "ID": "2074"}, {"sentence": "3 CC Usage 3.1 Usage Restrictions Previous work on contrastive connectives have found that these connectives often have different re- strictions on their location in the discourse struc- ture, with respect to maintaining discourse coher- ance (Quirk et al, 1972; Grote et al, 1995).", "acronym": "CC", "label": "Contrastive Connectives", "ID": "2075"}, {"sentence": "Face to face vs. CCs:  A controlled experiment.", "acronym": "CC", "label": "computerized conference", "ID": "2076"}, {"sentence": "CC Earplugs relieve the discomfort from traveling with a cold allergy or sinus condition.", "acronym": "CC", "label": "CAUSE-EFFECT", "ID": "2077"}, {"sentence": "A total of 10 relations were used includ- ing CC, COMPONENT-WHOLE, CONTENT-CONTAINER, ENTITY-ORIGIN, ENTITY-DESTINATION, INSTRUMENT-AGENCY, MEMBER-COLLECTION, MESSAGE-TOPIC, OTHER, and PRODUCT-PRODUCER.", "acronym": "CC", "label": "CAUSE-EFFECT", "ID": "2078"}, {"sentence": "Seven relations were used in the task: CC, INSTRUMENT-AGENCY, PRODUCT-PRODUCER, ORIGIN-ENTITY, THEME- TOOL, PART-WHOLE and CONTENT-CONTAINER.", "acronym": "CC", "label": "CAUSE-EFFECT", "ID": "2079"}, {"sentence": "From this table it can be observed that the PRODUCT-PRODUCER, INSTRUMENT-AGENCY, and CC rela- tions were detected with a relatively very high per- formance score, whereas the THEME-TOOL relation classification yielded a relatively small score.", "acronym": "CC", "label": "CAUSE-EFFECT", "ID": "2080"}, {"sentence": "User-centric decompositions are based on the idea that each user generates a sequence of questions that repre- sents a path from the complex question to a series of ques- tions that are connected through coherence relations of the type ELABORATION or CC.", "acronym": "CC", "label": "CAUSE-EFFECT", "ID": "2081"}, {"sentence": "R1 R2 R3 R4 R5 R6 R7 before 682 1200 913 898 861 849 677 after 13 19 10 15 15 8 16 Table 4: The number of features before and af- ter Weka selection, for each semantic relation dataset: R1 CC, R2 INSTRUMENT- AGENCY, R3 PRODUCT-PRODUCER, R4 ORIGIN- ENTITY, R5 THEME-TOOL, R6 PART-WHOLE, and R7 CONTENT-CONTAINER.", "acronym": "CC", "label": "CAUSE-EFFECT", "ID": "2082"}, {"sentence": "The position we argue for in this article, is that whereas adjacency and explicit conjunction (CC such as and, or, so, and but; subordinating con- junctions such as although, whereas, and when) imply discourse relations between (the interpretation of) adjacent or conjoined discourse units, discourse adverbials such as then, otherwise, nevertheless, and instead are anaphors, signaling a relation between the interpretation of their matrix clause and an entity in or deri", "acronym": "CC", "label": "coordinating conjunctions", "ID": "2083"}, {"sentence": "CC: tokens morpho- logical tag of which starts with the pair J?", "acronym": "CC", "label": "coordinating conjunctions", "ID": "2084"}, {"sentence": "The conjuncts are sister nodes separated by CC; we call these configu- rations coordination domains.", "acronym": "CC", "label": "coordinating conjunctions", "ID": "2085"}, {"sentence": "ent speakers 2.1 2 2 6 7.5 Turns 242 276 25 96 140 Sentences 280 366 101 281 304 Sentences per turn 1.2 1.3 4.1 2.9 2.2 Questions (in %) 3.7 6.4 6.3 9.8 4.0 False starts (in %) 12.1 11.0 2.0 7.2 13.9 Words 1685 1905 1224 3165 2355 Words per sentence 6.0 5.2 12.1 11.3 7.7 Disfluent (in %) 16.0 16.3 5.1 4.2 13.2 Disfluencies 222 259 48 95 266 Disfluencies per sentence 0.79 0.71 0.48 0.34 0.87 Empty CC (in %) 30.3 30.4 64.8 50.7 24.3 Lexicalized filled pauses (in %) 18.8 21.0 17.2 23.5 13.9 Editing terms (in %) 3.6 1.6 3.4 5.7 3.3 Nonlexicalized filled pauses (in %) 20.8 29.9 0.7 2.3 29.5 Repairs (in %) 26.6 17.1 13.8 17.8 29.0 (8E-CH) and four dialogues for the eval set (4E-CH).4 These are recordings of phone conversations between two family members or friends, typical", "acronym": "CC", "label": "coordinating conjunctions", "ID": "2086"}, {"sentence": "Coor- dinating elements are commata and CC.", "acronym": "CC", "label": "coordinating conjunctions", "ID": "2087"}, {"sentence": "CC that don?t serve their usual connective role, but act more as links between subsequent speech acts of a speaker (e.g., and then; we call these empty CC in this work) 2.", "acronym": "CC", "label": "coordinating conjunctions", "ID": "2088"}, {"sentence": "1 Introduction: Language Variations in the CCt  Commonly dichotomy of language and dialect is not easily maintained in the context of Chinese  language(s).", "acronym": "CC", "label": "Chinese Contex", "ID": "2089"}, {"sentence": "c?2007 Association for Computational Linguistics Extending a Thesaurus in the Pan-CCt  Oi Yee Kwong and Benjamin K. Tsou  Language Information Sciences Research Centre  City University of Hong Kong  Tat Chee Avenue, Kowloon, Hong Kong  {rlolivia,rlbtsou}@cityu.edu.hk    Abstract  In this paper, we address a unique problem  in Chinese language processing and report  on our study on extending a Chinese the- saurus with region-specific words, mostly  from the financial", "acronym": "CC", "label": "Chinese Contex", "ID": "2090"}, {"sentence": "Interpretation  of Biblical Texts in CCts?,", "acronym": "CC", "label": "Chinese Contex", "ID": "2091"}, {"sentence": "Kwong, O.Y. and Tsou, B.K. (2007)  Extending a  Thesaurus in the Pan-CCt.", "acronym": "CC", "label": "Chinese Contex", "ID": "2092"}, {"sentence": "section headers (History of Present Illness, CCs, Physical  Examination, Laboratory Data etc.).", "acronym": "CC", "label": "Chief Complaint", "ID": "2093"}, {"sentence": "Re- cent works (Halasz et al, 2006) also utilize NGram techniques to automatically create CCs classifiers based on ICD9 groupings.", "acronym": "CC", "label": "Chief Complaint", "ID": "2094"}, {"sentence": "MT Summit XII, Beyond TMs: New Tools for Translators Workshop, pp 20?27.", "acronym": "TMs", "label": "Translation Memories", "ID": "2095"}, {"sentence": "c?2012 Association for Computational Linguistics Language Resources Factory: case study on the acquisition of TMs?", "acronym": "TMs", "label": "Translation Memories", "ID": "2096"}, {"sentence": "In Proceedings of Machine Translation Summit XII - Workshop: Beyond TMs: New Tools for Translators.", "acronym": "TMs", "label": "Translation Memories", "ID": "2097"}, {"sentence": "Beyond TMs: Finding similar documents in comparable corpora.", "acronym": "TMs", "label": "Translation Memories", "ID": "2098"}, {"sentence": "MT Summit XII-Workshop: Beyond TMs: New Tools for Translators MT.", "acronym": "TMs", "label": "Translation Memories", "ID": "2099"}, {"sentence": "A Compact Data Structure for  Searchable TMs.", "acronym": "TMs", "label": "Translation Memories", "ID": "2100"}, {"sentence": "TMs are used in computational biology, com- puter vision, music, and, of course, text analysis.", "acronym": "TMs", "label": "Topic models", "ID": "2101"}, {"sentence": "TMs for word sense disambiguation and token-based idiom detection.", "acronym": "TMs", "label": "Topic models", "ID": "2102"}, {"sentence": "TMs can also be inspected man- ually by a human to understand the themes of the underlying corpus.", "acronym": "TMs", "label": "Topic models", "ID": "2103"}, {"sentence": "TMs posit doc- ument collection exhibits multiple latent semantic topics where each topic is represented as a multino- mial distribution over a given vocabulary and each document is a mixture of hidden topics.", "acronym": "TMs", "label": "Topic models", "ID": "2104"}, {"sentence": "TMs for meaning similarity in context.", "acronym": "TMs", "label": "Topic models", "ID": "2105"}, {"sentence": "1 Introduction TMs, exemplified by latent Dirichlet aloca- tion (LDA) (Blei et al, 2003), discover latent themes present in text collections. ?", "acronym": "TMs", "label": "Topic models", "ID": "2106"}, {"sentence": "2016 Association for Computational Linguistics A Latent Concept TMl for Robust Topic Inference Using Word Embeddings Weihua Hu ?", "acronym": "TM", "label": "Topic Mode", "ID": "2107"}, {"sentence": "Supervised TMls.", "acronym": "TM", "label": "Topic Mode", "ID": "2108"}, {"sentence": "c?2015 Association for Computational Linguistics Lifelong Machine Learning for TMling and Beyond Zhiyuan Chen Department of Computer Science University of Illinois at Chicago czyuanacm@gmail.com Abstract Machine learning has been popularly used in numerous natural language processing tasks.", "acronym": "TM", "label": "Topic Mode", "ID": "2109"}, {"sentence": "Leveraging  Multi-Domain Prior Knowledge in TMls.", "acronym": "TM", "label": "Topic Mode", "ID": "2110"}, {"sentence": "A TMl for Word Sense Disambigua- tion.", "acronym": "TM", "label": "Topic Mode", "ID": "2111"}, {"sentence": "Software Framework for TMlling with Large Corpora.", "acronym": "TM", "label": "Topic Mode", "ID": "2112"}, {"sentence": "The used patTM are: 1) (DT|CD) (NN|NNS), 2) DT JJ (NN|NNS), 3) NN POS (NN|NNS), and 4) PRP$ JJ (NN|NNS).", "acronym": "TM", "label": "terns", "ID": "2113"}, {"sentence": "These POS-based patTM are quite generic, al- lowing for the creation of large sets of characters.", "acronym": "TM", "label": "terns", "ID": "2114"}, {"sentence": "using syntactic patTM.", "acronym": "TM", "label": "terns", "ID": "2115"}, {"sentence": "Variations of the following basic patTM (Elson and McKe- own, 2010) were used: 1) QT SV CH, 2) QT CH SV, and 3) CH SV QT,", "acronym": "TM", "label": "terns", "ID": "2116"}, {"sentence": "Variations of the following basic patTM (Elson and McKe- own, 2010) were used: 1) QT SV CH, 2) QT CH SV, and 3) CH SV QT, where QT denotes a quote boundary and CH stands for a story character.", "acronym": "TM", "label": "terns", "ID": "2117"}, {"sentence": "Several syntac- tic patTM were applied to associate quotes with explicit mention of speakers in their vicinity to characters from the pruned list of story charac- ters.", "acronym": "TM", "label": "terns", "ID": "2118"}, {"sentence": "2) a set of part-of-speech patTM was used for the extraction of human and non-human characters that were not represented by proper names, e.g., ?", "acronym": "TM", "label": "terns", "ID": "2119"}, {"sentence": "These patTM were developed around SV.", "acronym": "TM", "label": "terns", "ID": "2120"}, {"sentence": "The NLP technology known as TM finds its place here.", "acronym": "TM", "label": "translation memory", "ID": "2121"}, {"sentence": "5 Experiments We performed experiments on translation from En- glish into Swedish and Danish on two different cor- pora, an automotive corpus collected from a propri- etary TM, and on Europarl (Koehn, 2005) for the merging experiments.", "acronym": "TM", "label": "translation memory", "ID": "2122"}, {"sentence": "A stand-alone sub- sentential alignment module however, is also use- ful for human translators if incorporated in CAT- tools, e.g. sophisticated bilingual concordance sys- tems, or in sub-sentential TM sys- tems (Gotti et al, 2005).", "acronym": "TM", "label": "translation memory", "ID": "2123"}, {"sentence": "The au- thor automatically derived from the Hansard corpus what he calls a TM: ac- tually a collection of pairs of source and target word sequences that are in a translation rela- tion according to the viterbi alignment run with an IBM4 model that was also trained on the Hansard corpus.", "acronym": "TM", "label": "translation memory", "ID": "2124"}, {"sentence": "The GREYC Translation Memory for the IWSLT 2009 Evaluation Campaign: one step be- yond TM.", "acronym": "TM", "label": "translation memory", "ID": "2125"}, {"sentence": "It resembles algorithms used in TM search for locating orthographically similar sentences.", "acronym": "TM", "label": "translation memory", "ID": "2126"}, {"sentence": "Table 1 lists three  common local weighting methods, namely raw term frequency (tf ), term presence (tp) and augmented  TM (atf ).", "acronym": "TM", "label": "term  frequency", "ID": "2127"}, {"sentence": "ABSTRACT We present an adaptation for the French TM challenge (DEFT 2012) of the KX system for multilingual unsupervised key-concept extraction.", "acronym": "TM", "label": "text mining", "ID": "2128"}, {"sentence": "As well as serving as a dataset for future tool de- velopment, our corpus is an excellent case study pro- viding valuable guidance to developers of biomedi- cal TM and retrieval systems.", "acronym": "TM", "label": "text mining", "ID": "2129"}, {"sentence": "This work provides important empirical guidance for developers of biomedical TM systems.", "acronym": "TM", "label": "text mining", "ID": "2130"}, {"sentence": "1 Introduction  With the rapidly increasing biomedical literature,  TM has become popular for finding bio- medical information in text.", "acronym": "TM", "label": "text mining", "ID": "2131"}, {"sentence": "dical Literature by  Combining High-Precision Gene Identifiers      Sun Kim, Won Kim, Don Comeau, and W. John Wilbur  National Center for Biotechnology Information  National Library of Medicine, National Institutes of Health  Bethesda, MD 20894, USA  {sun.kim,won.kim,donald.comeau,john.wilbur}@nih.gov              Abstract  Gene name identification is a fundamental  step to solve more complicated TM  problems such as gene normalization and pro- tein-protein interactions.", "acronym": "TM", "label": "text mining", "ID": "2132"}, {"sentence": "5 Discussion and Future Work We have developed a dataset for IFLOW in the con- text of financial TM and demonstrated it is a Features P (%) R (%) F (%) Radford et al (2009) 90.9 88.1 89.5 + Metadata Features 91.1 ??", "acronym": "TM", "label": "text mining", "ID": "2133"}, {"sentence": "For the efficient computation of the de- nominator, we use the lexical TM.", "acronym": "TM", "label": "translation model", "ID": "2134"}, {"sentence": "Generat- ing chinese classical poems with statistical machine TMs.", "acronym": "TM", "label": "translation model", "ID": "2135"}, {"sentence": "In order to improve the efficiency of a human translator, the k-best output of a translation system could be displayed as word or phrase choices which are color coded based on the confidence value assigned by the TM.", "acronym": "TM", "label": "translation model", "ID": "2136"}, {"sentence": "Here, n sign means the number of sign words in 69 Figure 2: Two ways of learning TMs the alignment pair, and n JP means the number of Japanese words in the alignment pair.", "acronym": "TM", "label": "translation model", "ID": "2137"}, {"sentence": "The source-to- target lexical TM p(t|s) and target- to-source model p(s|t) can be obtained through IBM Model-1 or HMM training.", "acronym": "TM", "label": "translation model", "ID": "2138"}, {"sentence": "Although no longer competitive as end-to-end TMs, the IBM Models, as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996), are still widely used for word alignment.", "acronym": "TM", "label": "translation model", "ID": "2139"}, {"sentence": "National Centre for TM, UK {sebastian,chun,takagi}@dbcls.rois.ac.jp tsujii@is.s.u-tokyo.ac.jp Abstract In this paper we describe our entry to the BioNLP 2009 Shared Task regarding bio- molecular event extraction.", "acronym": "TM", "label": "Text Mining", "ID": "2140"}, {"sentence": "TM Tools: Instruments for  Scientific Discovery, in IMA TM Work- shop, Institute for", "acronym": "TM", "label": "Text Mining", "ID": "2141"}, {"sentence": "National Centre for TM, University of Manchester, Manchester, UK ?", "acronym": "TM", "label": "Text Mining", "ID": "2142"}, {"sentence": "TM as Integration of Several Re- lated Research Areas, KDD 2000 Workshop on  TM, Boston, USA  Hatzivassiloglou V., Duboue P. and Rzetsky A. 2001.", "acronym": "TM", "label": "Text Mining", "ID": "2143"}, {"sentence": "TM Tools: Instruments for  Scientific Discovery, in IMA TM Work- shop, Institute for Mathematics and its Applica- tions, Minneapolis, USA, 2000  Jacquemin C. 2001.", "acronym": "TM", "label": "Text Mining", "ID": "2144"}, {"sentence": "ML method (Weka) Features Accuracy DT PMI scores 65.4% Decision Rules PMI scores 65.5% Na??ve Bayes PMI scores 52.5% K-Nearest Neighbor PMI scores 64.5% Kernel Density PMI scores 60.5% Boosting (Dec. Stumps) PMI scores 67.7% Na??ve Bayes 500 words 68.0% DT 500 words 67.0% Na??ve Bayes PMI + 500 words 66.5% Boosting (Dec. Stumps) PMI + 500 words 69.2% Table 6: Comparative results for the supervised learning method using various ML learning algo- rithms (Weka), averaged over the seven groups of near-synonyms from the Exp1 data set.", "acronym": "DT", "label": "Decision Trees", "ID": "2145"}, {"sentence": "1345  Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 777?784 Manchester, August 2008 Estimation of Conditional Probabilities With DT and an Application to Fine-Grained POS Tagging Helmut Schmid and Florian Laws IMS, University of Stuttgart {schmid,lawsfn}@ims.uni-stuttgart.de Abstract We present a HMM part-of-speech tag- ging method which is particularly suited for POS tagsets with a large number of fine-grained tags.", "acronym": "DT", "label": "Decision Trees", "ID": "2146"}, {"sentence": "3 Constructing a Classifier  3.1 Sequential Minimal Optimization  Although any of a number of machine learning  algorithms, including DT, might be  equally applicable here, Support Vector Ma- chines (Vapnik, 1995) have been extensively  used in text classification  problems and with  considerable success (Dumais 1998; Dumais et  al.,", "acronym": "DT", "label": "Decision Trees", "ID": "2147"}, {"sentence": "Helmut Schmid, 1994, Probabilistic Part-of-Speech Tag- ging Using DT, Intl.", "acronym": "DT", "label": "Decision Trees", "ID": "2148"}, {"sentence": "ML method (Weka) Features Accuracy DT PMI scores 65.4% Decision Rules PMI scores 65.5% Na??ve Bayes PMI scores 52.5% K-Nearest Neighbor PMI scores 64.5% Kernel Density PMI scores 60.5% Boosting (Dec. Stumps) PMI scores 67.7% Na??ve Bayes 500 words 68.0% DT 500 words 67.0% Na??ve Bayes PMI + 500 words 66.5% Boosting (Dec. Stumps) PMI + 500 words 69.2% Table 6: Comparative results for the supervised learning m", "acronym": "DT", "label": "Decision Trees", "ID": "2149"}, {"sentence": "The classifiers that use PMI features are DT, Decision Rules, Na??ve Bayes, K-Nearest Neighbor, Kernel Density, and Boosting a weak classifier (De- cision Stumps ?", "acronym": "DT", "label": "Decision Trees", "ID": "2150"}, {"sentence": "Named Ent i ty  (NE) -- Insert SGML  tags into the text to mark each string that  represents a person, organization, or location  name, or a DT stamp, or a currency or  percentage figure.", "acronym": "DT", "label": "date or time", "ID": "2151"}, {"sentence": "Named Entity Task [NE]: Insert SGML tags into the text to mark each string that represents a person, organization, or location name, or a DT stamp, or a currency or percentage figure ?", "acronym": "DT", "label": "date or time", "ID": "2152"}, {"sentence": "Every  normalized expression is made up of two dates  although it refers to a concrete DT.", "acronym": "DT", "label": "date or time", "ID": "2153"}, {"sentence": "The system applies different functions to get a valid ISO DT in a valid gran- ularity from DCT3 dates.", "acronym": "DT", "label": "date or time", "ID": "2154"}, {"sentence": "ISO: Apply rules to convert any-format ex- plicit DT into a valid ISO 8601 stan- dard date.", "acronym": "DT", "label": "date or time", "ID": "2155"}, {"sentence": "which inquire about the DT when certain events affected certain properties of the instances everquest and radon re- spectively.", "acronym": "DT", "label": "date or time", "ID": "2156"}, {"sentence": "already available on the DT in the given domain.", "acronym": "DT", "label": "different topics", "ID": "2157"}, {"sentence": "We propose a new method to identify the author of a document on a topic using a predictive model trained on examples from DT.", "acronym": "DT", "label": "different topics", "ID": "2158"}, {"sentence": "Therefore, we try to improve the performance of cross-topic AA (CTAA), one of the dimensions of cross-domain AA (CDAA) where training and test data come from DT.", "acronym": "DT", "label": "different topics", "ID": "2159"}, {"sentence": "For example, people on Social Media might talk about DT dur- ing and after work on weekdays, talk every Friday about the weekend ahead, or comment about their favorite weekly TV show during its air time.", "acronym": "DT", "label": "different topics", "ID": "2160"}, {"sentence": "And do DT benefit from different indicators?", "acronym": "DT", "label": "different topics", "ID": "2161"}, {"sentence": "Many differences between the male and female lists can be linked to men and women talking about DT, or to differ- ent people.", "acronym": "DT", "label": "different topics", "ID": "2162"}, {"sentence": "Cotterill 2010 Li & Roth 2002 % Person(s) HUM{individual,title} 2.53 Group or Organisation HUM{group} 0.17 Descriptive text HUM{description} 11.51 DESC{manner, definition, description} Reason DESC{reason} 1.57 DT NUM{date, period} 3.57 Numeric NUM{weight, volume/size, ordinal, percentage, count, speed, money, temperature, distance, other} 1.92 Phone NUM{code}1 0.40 URL 0.17 Email 0.17 Place LOC{country, state, city, mountain, other} 0.96 Animal ENTY{animal} 0.00 Physical Object ENTY{instrument, plant, body part, vehicle, food, product, substance} 0.30 Concept ENTY{language, religion, letter, c", "acronym": "DT", "label": "Date or Time", "ID": "2163"}, {"sentence": "For instance, general projec- tivity rules ensure that the DT corre- sponds to a properly nested syntax structure with- out crossing brackets1 .", "acronym": "DT", "label": "dependency tree", "ID": "2164"}, {"sentence": "4.4 Results We trained the PP attachment predictor both with the counts acquired from the DTbank (supervised) and those from the newspaper cor- pus (unsupervised).", "acronym": "DT", "label": "dependency tree", "ID": "2165"}, {"sentence": "4.3 Corpus For our parsing experiments, we used the first 1,000 sentences of technical newscasts from the DTbank mentioned above.", "acronym": "DT", "label": "dependency tree", "ID": "2166"}, {"sentence": "Weighted Constraint Dependency Grammar (Schro?der, 2002) models syntax structure as la- belled DTs as shown in the exam- ple.", "acronym": "DT", "label": "dependency tree", "ID": "2167"}, {"sentence": "How- ever, comparable results are also achieved when applying the parser to the standard test set from the NEGRA corpus of German, as used by (Schiehlen, 2004; Foth et al, 2005): adding the PP predic- tor trained on our DTbank raises the overall attachment accuracy from 89.3% to 90.6%.", "acronym": "DT", "label": "dependency tree", "ID": "2168"}, {"sentence": "4 Experiments 4.1 Sources To obtain the counts to base our estimates of at- traction on, we first turned to the DT- bank that accompanies the WCDG parsing suite.", "acronym": "DT", "label": "dependency tree", "ID": "2169"}, {"sentence": "DT-based sentiment classifica- tion using CRFs with hidden variables.", "acronym": "DT", "label": "Dependency tree", "ID": "2170"}, {"sentence": "DT kernels for relation extrac- tion.", "acronym": "DT", "label": "Dependency tree", "ID": "2171"}, {"sentence": "Perceptron with DT kernel.", "acronym": "DT", "label": "Dependency tree", "ID": "2172"}, {"sentence": "le  imnl  quitte/l  ~g>~do~  Ne pas/l l me~12  Figure 7: DT for Ne me quitte pas t  4 A Computational Rendering  A close approximation of the described stochastic  model of dependency syntax has been realized as a  type of prohabilistic bottom-up chart parser.", "acronym": "DT", "label": "Dependency tree", "ID": "2173"}, {"sentence": "1 The architecture of our parser  (i)Preprocessor (neighboring  relation tagger) (ii)Get contextual features (iii)Estimate dependency attachment by SVM (iv)Tag label by MaxEnt Construct Subtree No more construction DT False True Left or Right attachment None Input sentence (word tokens) 191 Fig.", "acronym": "DT", "label": "Dependency tree", "ID": "2174"}, {"sentence": "This means  that the dependency links establish a tree structure,  main main  ate ate  ~bj~d~j  ~bj~su~i  John beans beans John  Fignre 1: DTs for John ate beans.", "acronym": "DT", "label": "Dependency tree", "ID": "2175"}, {"sentence": "DT Pars-  ing using a Hidden Derivation Model.", "acronym": "DT", "label": "Decision Tree", "ID": "2176"}, {"sentence": "3 Constructing a Classifier  3.1 Sequential Minimal Optimization  Although any of a number of machine learning  algorithms, including DTs, might be  equally applicable here, Support Vector Ma- chines (Vapnik, 1995) have been extensively  used in text classification  problems and with  considerable success (Dumais 1998; Dumais et  al.,", "acronym": "DT", "label": "Decision Tree", "ID": "2177"}, {"sentence": "Kettei-gi o mochiita ni-  hongo kakariuke kaiseki (A Japanese Dependency  Parser Using A DT).", "acronym": "DT", "label": "Decision Tree", "ID": "2178"}, {"sentence": "Helmut Schmid, 1994, Probabilistic Part-of-Speech Tag- ging Using DTs, Intl.", "acronym": "DT", "label": "Decision Tree", "ID": "2179"}, {"sentence": "ML method (Weka) Features Accuracy DTs PMI scores 65.4% Decision Rules PMI scores 65.5% Na??ve Bayes PMI scores 52.5% K-Nearest Neighbor PMI scores 64.5% Kernel Density PMI scores 60.5% Boosting (Dec. Stumps) PMI scores 67.7% Na??ve Bayes 500 words 68.0% DTs 500 words 67.0% Na??ve Bayes PMI + 500 words 66.5% Boosting (Dec. Stumps) PMI + 500 words 69.2% Table 6: Comparative results for the supervised learning m", "acronym": "DT", "label": "Decision Tree", "ID": "2180"}, {"sentence": "112   Discourse Parsing: A DT Approach  Tadash i  Nomoto   Advanced Research  Laboratory   H i tach i  L td .", "acronym": "DT", "label": "Decision Tree", "ID": "2181"}, {"sentence": "such graphs are called DET curves.", "acronym": "DET", "label": "detection error tradeoff", "ID": "2182"}, {"sentence": "The rules are the following:  (defrule NPRule ; NP --> DET Noun  (STATUS active)  (CNTXTLORNOPR NIL)  (PRODUCTION (NP (DET Noun)))  (SYN-TESTS T)  (SEM-TESTS T)  (SYN-ACTIONS  (ralsef \"(* DefiniteneSS DET)  ;raise the values of the specified features from the  ;son node into the parent node  \"(* * Noun)l}}  ; second *means all features of the son node  ;first * means the storing of the features as they are  ;in the son node into t", "acronym": "DET", "label": "Determiner", "ID": "2183"}, {"sentence": "The rules are the following:  (defrule NPRule ; NP --> DET Noun  (STATUS active)  (CNTXTLORNOPR NIL)  (PRODUCTION (NP (DET Noun)))  (SYN-TESTS T)  (SEM-TESTS T)  (SYN-ACTIONS  (ralsef \"(* DefiniteneSS DET)  ;raise the values of the specified features from the  ;son node into the parent node  \"(* * Noun)l}}  ; second *means all features of the son node  ;first * means the storing of the features as they are  ;in the son node into the parent node  (defrule VPRule ; VP --> Verb NP NP  (STATUS active)  (", "acronym": "DET", "label": "Determiner", "ID": "2184"}, {"sentence": "The rules are the following:  (defrule NPRule ; NP --> DET Noun  (STATUS active)  (CNTXTLORNOPR NIL)  (PRODUCTION (NP (DET Noun)))  (SYN-TESTS T)  (SEM-TESTS T)  (SYN-ACTIONS  (ralsef \"(* DefiniteneSS DET)  ;raise the values of the specified features from the  ;son node into the parent node  \"(* * Noun)l}}  ; second *means all features of the son node  ;first * means the storing of the features as they are  ;in the son node into the parent node  (defrule VPRule ; VP --> Verb NP NP  (STATUS active)  (CNTXTLORN OPR NIL)  (PRODUCTION (Via (Verb NP NP)))  (SYN-TESTS T)  (SEM-TESTS T)  (SYN-A", "acronym": "DET", "label": "Determiner", "ID": "2185"}, {"sentence": "AN EXAMPLE  The example concerns a simple fragment of a LFG  written in SAIL according to the CGU model, Our example  is taken f rom/Kap lan  1982/and/Winograd  1983/.   The lexical entries for this grammar in SAIL are the  following:  a {(DET NIL (Definiteness) (Indefinite)  {Number) (singular)))  baby ((Noun NIL (Number) (Singular)  (Predicate) {Baby)}}  girl {(Noun NIL {Number} (Singular}  (Predicate) {Girl)l}  handed ((Verb NIL (Tense) (Past)  (Predicate) (Hand)))  the ((DET NIL (Definiteness) {Definite)))  toys ((Noun NIL (Number) (Plural}  (Predicate) (Toys)))  Rules in SAIL are written using a def~ttle format wh", "acronym": "DET", "label": "Determiner", "ID": "2186"}, {"sentence": "DETs were inserted before nouns result- ing in queries of the type story/stories about and about the/a/0 war/wars for the compound war story.", "acronym": "DET", "label": "Determiner", "ID": "2187"}, {"sentence": "to the CGU model, Our example  is taken f rom/Kap lan  1982/and/Winograd  1983/.   The lexical entries for this grammar in SAIL are the  following:  a {(DET NIL (Definiteness) (Indefinite)  {Number) (singular)))  baby ((Noun NIL (Number) (Singular)  (Predicate) {Baby)}}  girl {(Noun NIL {Number} (Singular}  (Predicate) {Girl)l}  handed ((Verb NIL (Tense) (Past)  (Predicate) (Hand)))  the ((DET NIL (Definiteness) {Definite)))  toys ((Noun NIL (Number) (Plural}  (Predicate) (Toys)))  Rules in SAIL are written using a def~ttle format where  all the fields appearing in the CGUs can be defined; in  addit ion two fields are devoted to the state definition  (STATUS field) and the rule type definition, that  is ff the  rule is a standard rule or a contextual  or a NOP rule  (CNTXTLOR", "acronym": "DET", "label": "Determiner", "ID": "2188"}, {"sentence": "DET the canonical function of these argu- ments (canonicalization).", "acronym": "DET", "label": "Determine", "ID": "2189"}, {"sentence": "Combining  Finite  State Automata and a Greedy Learning Algorithm  to DET the Syntactic Roles of Commas.", "acronym": "DET", "label": "Determine", "ID": "2190"}, {"sentence": "3 DETd by lexical information (question marks, dis/agreement indications and sentence length) 2.3.1 Relevance Decisions Our raw representation allows as many as six pre- vious turns to be relevant to the classification de- cision, however not all turns are indeed relevant, and even relevant turns may consist only of a handful of relevant sentences.", "acronym": "DET", "label": "Determine", "ID": "2191"}, {"sentence": "Query  Classi f icat ion DET what category of  entity the question is asking for.", "acronym": "DET", "label": "Determine", "ID": "2192"}, {"sentence": "AN EXAMPLE  The example concerns a simple fragment of a LFG  written in SAIL according to the CGU model, Our example  is taken f rom/Kap lan  1982/and/Winograd  1983/.   The lexical entries for this grammar in SAIL are the  following:  a {(DETr NIL (Definiteness) (Indefinite)  {Number) (singular)))  baby ((Noun NIL (Number) (Singular)  (Predicate) {Baby)}}  girl {(Noun NIL {Number} (Singular}  (Predicate) {Girl)l}  handed ((Verb NIL (Tense) (Past)  (Predicate) (Hand)))  the ((DETr NIL (Definiteness) {Definite)))  toys ((Noun NIL (Number) (Plural}  (Predicate) (Toys)))  Rules in SAIL are written using a def~ttle format wh", "acronym": "DET", "label": "Determine", "ID": "2193"}, {"sentence": "DETrs were inserted before nouns result- ing in queries of the type story/stories about and about the/a/0 war/wars for the compound war story.", "acronym": "DET", "label": "Determine", "ID": "2194"}, {"sentence": "1 In t roduct ion   We present a statistical method for DETmin-  ing pronoun anaphora.", "acronym": "DET", "label": "deter", "ID": "2195"}, {"sentence": "Finally we attempted a fully automatic di-  rect test of the accuracy of both pronoun meth-  ods for gender DETmination.", "acronym": "DET", "label": "deter", "ID": "2196"}, {"sentence": "A statistical approach is present in the dis-  course module only where it is used to DET-  mine the probability that a noun (verb) phrase  is the center of a sentence.", "acronym": "DET", "label": "deter", "ID": "2197"}, {"sentence": "One can judge the pro-  gram informally by simply examining the re-  sults and DETmining if the program's gender  decisions are correct (occasionally ooking at the  text for difficult cases).", "acronym": "DET", "label": "deter", "ID": "2198"}, {"sentence": "First, as one might expect given the al-  ready noted superior performance of the Hobbs  scheme over last-noun, Hobbs also performs bet-  ter at DETmining ender.", "acronym": "DET", "label": "deter", "ID": "2199"}, {"sentence": "This transducer is non-DETministic, producing more than  one sequence of nlarks Ior a given input.", "acronym": "DET", "label": "deter", "ID": "2200"}, {"sentence": "2.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al, 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al, 1994) 93.2 RRR Maximum Likelihood-based (Hindle and Rooth, 1993) 79.7 AP Maximum entropy, words (Ratnaparkhi et al, 1994) 77.7 RRR Maximum entropy, words & classes (Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR TBL (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and Nagao, 1997) 88.1 RRR WordNet Memory-based Learning (Zavrel et al, 1997) 84.4 RRR LexSpace Maximum entropy, unsupervised (Ratnaparkhi, 1998) 81.9 Maximum entropy, supervised (R", "acronym": "TBL", "label": "Transformation-Based Learning", "ID": "2201"}, {"sentence": "Computing Dialogue Acts from Features  with TBL.", "acronym": "TBL", "label": "Transformation-Based Learning", "ID": "2202"}, {"sentence": "Text Chunking Using TBL.", "acronym": "TBL", "label": "Transformation-Based Learning", "ID": "2203"}, {"sentence": "For instance, the general principle of  the TBL (Brill, 1993) is  to assign to each element its most f requent  category,  and then to learn transformation rules which cor-  rect its initial categorisation.", "acronym": "TBL", "label": "Transformation-Based Learning", "ID": "2204"}, {"sentence": "Text  Chunking Using TBL.", "acronym": "TBL", "label": "Transformation Based Learning", "ID": "2205"}, {"sentence": "TBL and Data- Driven Lexical Disambiguation: Syntactic and Semantic Ambiguity Resolution.", "acronym": "TBL", "label": "Transformation Based Learning", "ID": "2206"}, {"sentence": "Part-Of-Speech  Tagging and Chunking using Conditional Random  Fields and TBL.", "acronym": "TBL", "label": "Transformation Based Learning", "ID": "2207"}, {"sentence": "Avinesh PVS and Karthik G. Part-Of-Speech Tagging  and Chunking Using Conditional Random Fields and  TBL.", "acronym": "TBL", "label": "Transformation Based Learning", "ID": "2208"}, {"sentence": "Methods using both sources of information for tagging are: Hidden Markov Modeling, Maximum Entropy modeling, and TBL (Brill, 1995).", "acronym": "TBL", "label": "Transformation Based Learning", "ID": "2209"}, {"sentence": "Screening for PTSD using verbal features in self narratives: A text min- ing approach.", "acronym": "PTSD", "label": "posttraumatic stress disorder", "ID": "2210"}, {"sentence": "Measuring PTSD in Twitter.", "acronym": "PTSD", "label": "post traumatic stress disorder", "ID": "2211"}, {"sentence": "Measuring PTSD in twitter.", "acronym": "PTSD", "label": "post traumatic stress disorder", "ID": "2212"}, {"sentence": "Brown?s algorithm in 2) finds clusters such that PMI terms are maximized; in 3), we not only maximize the mutual information, but we also reduce the effective V by ensuring that each cluster (or state) specializes and represents as few words as possible.", "acronym": "PMI", "label": "pairwise mutual information", "ID": "2213"}, {"sentence": "\u000e62 \u0001-,52 0 \u000e \u0001 confidence , ,\u001a2 0 \u0001 PMI , \u0003 \u0005 \u0007 \t\f\u000b , \u0005 \u0003 \u0001-,\u001a2 .", "acronym": "PMI", "label": "pairwise mutual information", "ID": "2214"}, {"sentence": "Generating Object-Trouble Pairs To generate and rank object-trouble pairs we use a variant of PMI that scores an object- trouble pair ?", "acronym": "PMI", "label": "pairwise mutual information", "ID": "2215"}, {"sentence": "Other approaches for query similarity use sta- tistical translation models (Riezler et al, 2008), analysing search engine logs (Jones et al, 2006), looking for different anchor texts pointing to the same pages (Kraft and Zien, 2004), or replacing query words with other words that have the high- est PMI (Terra and Clarke, 2004).", "acronym": "PMI", "label": "pointwise mutual information", "ID": "2216"}, {"sentence": "We then convert the raw counts to positive PMI scores, which has been shown to improve word similarity correlation results (Turney and Pantel, 2010).", "acronym": "PMI", "label": "pointwise mutual information", "ID": "2217"}, {"sentence": "Espresso takes advantage of PMI (pmi) (Manning and Schu?tze, 1999) between instances and patterns to evaluate their reliability.", "acronym": "PMI", "label": "pointwise mutual information", "ID": "2218"}, {"sentence": "Medians and maxima of pairwise collocation statistics for tokens for a particular size of ngram motifs: we use the following statis- tics: PMI, Chi- square statistic, and conditional probability.", "acronym": "PMI", "label": "pointwise mutual information", "ID": "2219"}, {"sentence": "Stream- ing PMI.", "acronym": "PMI", "label": "pointwise mutual information", "ID": "2220"}, {"sentence": "The strength of co-occurrence (as measured by PMI) be- tween two contrasting word pairs is taken to be the degree of antonymy.", "acronym": "PMI", "label": "pointwise mutual information", "ID": "2221"}, {"sentence": "We also study the behavior of approxi- mate PMI and Log Likeli- hood Ratio for the sketches.", "acronym": "PMI", "label": "Pointwise Mutual Information", "ID": "2222"}, {"sentence": "2013), we use the top 10K most frequent content lemmas as context features, PMI as weight- ing method and we reduce the dimensionality of the data by both Non-negative Matrix Factoriza- tion (NMF, Lee and Seung (2000)) and Singular Value Decomposition (SVD).", "acronym": "PMI", "label": "Pointwise Mutual Information", "ID": "2223"}, {"sentence": "Streaming PMI.", "acronym": "PMI", "label": "Pointwise Mutual Information", "ID": "2224"}, {"sentence": "III, 2011a), computing approximate as- sociation scores like PMI (Li et al 2008; Van Durme and Lall, 2009b; Goyal and Daume?", "acronym": "PMI", "label": "Pointwise Mutual Information", "ID": "2225"}, {"sentence": "Sev- eral sources of information are used to identify the best label including PMI scores, WordNet hypernymy relations and distribu- tional similarity.", "acronym": "PMI", "label": "Pointwise Mutual Information", "ID": "2226"}, {"sentence": "Thus, the author computes  the PMI score between  seed words and new words on the basis of the  number of AltaVista hits returned when querying  the seed word and the word to be classified with  the ?", "acronym": "PMI", "label": "Pointwise Mutual Information", "ID": "2227"}, {"sentence": "Unsupervised Large Vocabulary WSDn with Graph-based Al- gorithms for Sequence Data Labeling, In Proc.", "acronym": "WSD", "label": "Word Sense Disambiguatio", "ID": "2228"}, {"sentence": "Combin- ing Lexical and Syntactic Features for Supervised WSDn. (", "acronym": "WSD", "label": "Word Sense Disambiguatio", "ID": "2229"}, {"sentence": "In Proceedings of the ACL Work- shop on WSDn: Recent Successes and Future Directions, pages 40?46, Philadelphia.", "acronym": "WSD", "label": "Word Sense Disambiguatio", "ID": "2230"}, {"sentence": "70  Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, page 1, Beijing, August 2010 WSDn and IR Pushpak Bhattacharya Department of Computer Science & Engineering, Indian Institute of Technology Bombay, Powai, Mumbai 400076, India pb@cse.iitb.ac.in 1                                                                 Edmonton, May-June 2003                                                                      Tutorials , pg.", "acronym": "WSD", "label": "Word Sense Disambiguatio", "ID": "2231"}, {"sentence": "c?2013 Association for Computational Linguistics DALE: A WSDn System for Biomedical Documents Trained using Automatically Labeled Examples Judita Preiss and Mark Stevenson Department of Computer Science, University of Sheffield Regent Court, 211 Portobello Sheffield S1 4DP, United Kingdom j.preiss,m.stevenson@dcs.shef.ac.uk Abstract Automatic interpretation of documents is hampered by the fact that language contains terms which have", "acronym": "WSD", "label": "Word Sense Disambiguatio", "ID": "2232"}, {"sentence": "Unsupervised Graph- based WSDn Using Measures of Word Semantic Similarity.", "acronym": "WSD", "label": "Word Sense Disambiguatio", "ID": "2233"}, {"sentence": "Unsupervised Large Vocabulary WSD with Graph-based Al- gorithms for Sequence Data Labeling, In Proc.", "acronym": "WSD", "label": "Word Sense Disambiguation", "ID": "2234"}, {"sentence": "Combin- ing Lexical and Syntactic Features for Supervised WSD. (", "acronym": "WSD", "label": "Word Sense Disambiguation", "ID": "2235"}, {"sentence": "In Proceedings of the ACL Work- shop on WSD: Recent Successes and Future Directions, pages 40?46, Philadelphia.", "acronym": "WSD", "label": "Word Sense Disambiguation", "ID": "2236"}, {"sentence": "70  Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, page 1, Beijing, August 2010 WSD and IR Pushpak Bhattacharya Department of Computer Science & Engineering, Indian Institute of Technology Bombay, Powai, Mumbai 400076, India pb@cse.iitb.ac.in 1                                                                 Edmonton, May-June 2003                                                                      Tutorials , pg.", "acronym": "WSD", "label": "Word Sense Disambiguation", "ID": "2237"}, {"sentence": "c?2013 Association for Computational Linguistics DALE: A WSD System for Biomedical Documents Trained using Automatically Labeled Examples Judita Preiss and Mark Stevenson Department of Computer Science, University of Sheffield Regent Court, 211 Portobello Sheffield S1 4DP, United Kingdom j.preiss,m.stevenson@dcs.shef.ac.uk Abstract Automatic interpretation of documents is hampered by the fact that language contains terms which have", "acronym": "WSD", "label": "Word Sense Disambiguation", "ID": "2238"}, {"sentence": "Unsupervised Graph- based WSD Using Measures of Word Semantic Similarity.", "acronym": "WSD", "label": "Word Sense Disambiguation", "ID": "2239"}, {"sentence": "WSD: A survey.", "acronym": "WSD", "label": "Word sense disambiguation", "ID": "2240"}, {"sentence": "WSD using statistical models of Roget's categories trained on large cor- pora.", "acronym": "WSD", "label": "Word sense disambiguation", "ID": "2241"}, {"sentence": "WSD: MWEs tend to be less polysemous than simple words.", "acronym": "WSD", "label": "Word sense disambiguation", "ID": "2242"}, {"sentence": "WSD using conceptual density.", "acronym": "WSD", "label": "Word sense disambiguation", "ID": "2243"}, {"sentence": "WSD using a second language monolingual corpus.", "acronym": "WSD", "label": "Word sense disambiguation", "ID": "2244"}, {"sentence": "A com- parative study of support vector machines applied  to the supervised WSD prob- lem in the medical domain.", "acronym": "WSD", "label": "word sense disambiguation", "ID": "2245"}, {"sentence": "Similarity-based meth- ods for WSD.", "acronym": "WSD", "label": "word sense disambiguation", "ID": "2246"}, {"sentence": "Graph- based WSD of biomedical docu- ments.", "acronym": "WSD", "label": "word sense disambiguation", "ID": "2247"}, {"sentence": "Artificial sense tagged corpora is used  to evaluate WSD algorithms and is  created by adding ambiguity to a corpus.", "acronym": "WSD", "label": "word sense disambiguation", "ID": "2248"}, {"sentence": "It makes sense: A wide- coverage WSD system for free text.", "acronym": "WSD", "label": "word sense disambiguation", "ID": "2249"}, {"sentence": "One has to realize that even though such a performance might be adequate for some tasks (such as WSD), for many other (such as parsing or translation) the implied sentence error rate at 50% or more is sim- ply too much to deal with.", "acronym": "WSD", "label": "word sense disambiguation", "ID": "2250"}, {"sentence": "We are not aware of any studies on the impact of WSD on restricted domains and certainly this area is wo", "acronym": "WSD", "label": "word-sense disambiguation", "ID": "2251"}, {"sentence": "A statistical model for parsing and WSD.", "acronym": "WSD", "label": "word-sense disambiguation", "ID": "2252"}, {"sentence": "Constru(:ting bayesian etworks from Word-  Net for WSD: I/.el)resenta-  tional and I)rocessing issues.", "acronym": "WSD", "label": "word-sense disambiguation", "ID": "2253"}, {"sentence": "We are not aware of any studies on the impact of WSD on restricted domains and certainly this area is worth exploring.", "acronym": "WSD", "label": "word-sense disambiguation", "ID": "2254"}, {"sentence": "This sense of print is not available in WordNet5 and therefore it is not possible to apply WSD techniques to find the appropriate sense.", "acronym": "WSD", "label": "word-sense disambiguation", "ID": "2255"}, {"sentence": "and Vicedo Question Answering in Restricted Domains: An Overview it should be noted that some words would still have several senses available and therefore WSD still plays a role.", "acronym": "WSD", "label": "word-sense disambiguation", "ID": "2256"}, {"sentence": "The impact of WSD is possibly reduced in RDQA, though 46 Molla?", "acronym": "WSD", "label": "word-sense disambiguation", "ID": "2257"}, {"sentence": "For the latter, we use seven types of information generally accepted to be personally II in- formation (McCallister, 2010), as listed in the left column of Table 2.", "acronym": "II", "label": "identifiable", "ID": "2258"}, {"sentence": "For this reason, such an item can always be referred to as  \"established,\" or \"recoverable,\" or \"II\" in the terminology of Halliday (1967) or Chafe (1976).", "acronym": "II", "label": "identifiable", "ID": "2259"}, {"sentence": "THE S-RELATORS OF IVIR EEl\" AL  In a study unfortunately ittle known, \\[Ivir et al1973\\]  categorize with uncommon completeness numerous \"S-  relators\" -- syntactic structures associated with II  semantic links among the meanings of sentences or clauses.", "acronym": "II", "label": "identifiable", "ID": "2260"}, {"sentence": "We will term this kind of composition, where the  same resource means different things in different  contexts, \"non-linear\" composition; this is in contrast to  \"linear\" composition, where each resource contributes an  II part of the whole and what it contributes i not context dependent.", "acronym": "II", "label": "identifiable", "ID": "2261"}, {"sentence": "Guide to protecting the confi- dentiality of personally II information.", "acronym": "II", "label": "identifiable", "ID": "2262"}, {"sentence": "Each Turker is II only through an anonymous ID like A23KO2TP7I4KK2.)", "acronym": "II", "label": "identifiable", "ID": "2263"}, {"sentence": "Engaged - it is committed a conversation The most obvious case of engagement is when the person and the machine are having a conversation - that is Listening and Talking to each other, how- ever even if the conversation II, the sys- tem may still want to keep the context of the recent discussion.", "acronym": "II", "label": "is finished", "ID": "2264"}, {"sentence": "When the root node is traversed, the trans- lating II.", "acronym": "II", "label": "is finished", "ID": "2265"}, {"sentence": "When the student II with the translation, she clicks on a ?", "acronym": "II", "label": "is finished", "ID": "2266"}, {"sentence": "Once the input II, it appears in text on the left side of the screen.", "acronym": "II", "label": "is finished", "ID": "2267"}, {"sentence": "Thus, by the time Stage 2  processing II, all information about the surface lin-  guistic form is gone.", "acronym": "II", "label": "is finished", "ID": "2268"}, {"sentence": "In (2) it is the case that the paper II, but it would be hard to claim that anything or anyone is up.", "acronym": "II", "label": "is finished", "ID": "2269"}, {"sentence": "It II that the TED accuracy (Tsarfaty et al 2011) for the lattices is 0.8305 which is ranked second.", "acronym": "II", "label": "is worth noting", "ID": "2270"}, {"sentence": "It II that the experiments with Mal- tOptimizer do not take so long.", "acronym": "II", "label": "is worth noting", "ID": "2271"}, {"sentence": "Finally it II that the TED accuracy 3DASHTAG comes from the original constituent data, when a DASHTAG was present in a head node label, this feature was kept in the Catib corpus.", "acronym": "II", "label": "is worth noting", "ID": "2272"}, {"sentence": "The MaltOptimizer process was sped up following heuristics derived from deep proven experience (Nivre and Hall, 2010), which means that there are several combinations that are untested; however, it II that these heuristics re- sulted in similar performance to more exhaustive search for a big set of languages (Ballesteros, 2013).", "acronym": "II", "label": "is worth noting", "ID": "2273"}, {"sentence": "It II that it can be applied to any treebank in CoNLL data format.1 The rest of the paper is organized as follows.", "acronym": "II", "label": "is worth noting", "ID": "2274"}, {"sentence": "It II that meaningless iteration of  words (especially, of function words) has less influ-  ence on the text similarity:  a(\"It is a dog.\",", "acronym": "II", "label": "is worth noting", "ID": "2275"}, {"sentence": "EL1 was  a semantically driven parser which maps English language  sentences into the CD \\[6\\]  representations of their meanings, it made extensive  use of the semantic properties of the words being  processed, but interacted only slightly with the rest of  the understanding processes it was a part of.", "acronym": "CD", "label": "Conceptual Dependency", "ID": "2276"}, {"sentence": "It 3 s  not apparent  t h a t  d e f i n i t i o n s  of  themes can be con t ro l l ed  i n  CD Theory, i n  t h a t  if  s t a t e d  i n  terms of  primitives, each a b s t r a c t  term could become ex-  tremelv l a rge .", "acronym": "CD", "label": "Conceptual Dependency", "ID": "2277"}, {"sentence": "Shank R.C. CD: a theory of natnral language understanding.", "acronym": "CD", "label": "Conceptual Dependency", "ID": "2278"}, {"sentence": "it would  pass o f f  a completed CD  representation of each sentence to SAM or PAM which  would try to incorporate it into an overall story  r", "acronym": "CD", "label": "Conceptual Dependency", "ID": "2279"}, {"sentence": "It seems to me not untrue historically to claim that RDF, the representational base of the SW, is a return of the level of representation that Schank (under the name CD, in Schank [1975]) and I (under the name Preference Semantics) developed in the late 1960s and early 1970s (Wilks 1975).", "acronym": "CD", "label": "Conceptual Dependency", "ID": "2280"}, {"sentence": "The story is generated using the notation  of CD and is fed to another program which  translates it into English.", "acronym": "CD", "label": "Conceptual Dependency", "ID": "2281"}, {"sentence": "At Stanford as a post-doc, I was on the same corridor asWinograd, just arrived from MIT; Schank, then starting to build his CD empire; and Colby and his large team building the PARRY dialogue system, which included Larry Tesler, later the Apple software architect.", "acronym": "CD", "label": "Conceptual Dependency", "ID": "2282"}, {"sentence": "one where all five systems were involved, the other where subsets of CDity 2 of these crowds were re-combined.10 4 Results As already described in Section 2.2, we performed two types of experiments.", "acronym": "CD", "label": "cardinal", "ID": "2283"}, {"sentence": "c?2007 Association for Computational Linguistics Real-Time Correction of Closed-Captions P. Cardinal, G. Boulianne, M. Comeau, M. Boisvert Centre de recherche Informatique de Montreal (CRIM) Montreal, Canada patrick.CD@crim.ca Abstract Live closed-captions for deaf and hard of hearing audiences are currently produced by stenographers, or by voice writers us- ing speech recognition.", "acronym": "CD", "label": "cardinal", "ID": "2284"}, {"sentence": "| denotes the CDity of the set, and ?", "acronym": "CD", "label": "cardinal", "ID": "2285"}, {"sentence": "It will be used to associate some approxi- mate CDity to the set of elements selected in the Loc- Geo, allowing to compute some ?", "acronym": "CD", "label": "cardinal", "ID": "2286"}, {"sentence": "The higher role CD- ity models were not possible to create because of the very high computational requirements.", "acronym": "CD", "label": "cardinal", "ID": "2287"}, {"sentence": "They are summarized here:  I name I class operations  cent ains/uses  1,2...N relation CDity  inherits  Figure 1: symboles used in the figures  1.2 Extens ions  to  LFG fo rmal i sm  Four types of equations are defined in classical LFG  (Bresnan and Kaplan, 1981):  1.", "acronym": "CD", "label": "cardinal", "ID": "2288"}, {"sentence": "EL1 was  a semantically driven parser which maps English language  sentences into the CDy \\[6\\]  representations of their meanings, it made extensive  use of the semantic properties of the words being  processed, but interacted only slightly with the rest of  the understanding processes it was a part of.", "acronym": "CD", "label": "Conceptual Dependenc", "ID": "2289"}, {"sentence": "It 3 s  not apparent  t h a t  d e f i n i t i o n s  of  themes can be con t ro l l ed  i n  CDy Theory, i n  t h a t  if  s t a t e d  i n  terms of  primitives, each a b s t r a c t  term could become ex-  tremelv l a rge .", "acronym": "CD", "label": "Conceptual Dependenc", "ID": "2290"}, {"sentence": "Shank R.C. CDy: a theory of natnral language understanding.", "acronym": "CD", "label": "Conceptual Dependenc", "ID": "2291"}, {"sentence": "it would  pass o f f  a completed CDy  representation of each sentence to SAM or PAM which  would try to incorporate it into an overall story  r", "acronym": "CD", "label": "Conceptual Dependenc", "ID": "2292"}, {"sentence": "It seems to me not untrue historically to claim that RDF, the representational base of the SW, is a return of the level of representation that Schank (under the name CDy, in Schank [1975]) and I (under the name Preference Semantics) developed in the late 1960s and early 1970s (Wilks 1975).", "acronym": "CD", "label": "Conceptual Dependenc", "ID": "2293"}, {"sentence": "The story is generated using the notation  of CDy and is fed to another program which  translates it into English.", "acronym": "CD", "label": "Conceptual Dependenc", "ID": "2294"}, {"sentence": "At Stanford as a post-doc, I was on the same corridor asWinograd, just arrived from MIT; Schank, then starting to build his CDy empire; and Colby and his large team building the PARRY dialogue system, which included Larry Tesler, later the Apple software architect.", "acronym": "CD", "label": "Conceptual Dependenc", "ID": "2295"}, {"sentence": "54  Word Sense Disambiguation  using CD  Eneko Agirre*  Lengoaia eta Sistema Informatikoak saila.", "acronym": "CD", "label": "Conceptual Density", "ID": "2296"}, {"sentence": "The automatic decision procedure for lexical  ambiguity resolution presented in this paper is based  on an elaboration of the conceptual distance among  concepts: CD \\[Agirre & Rigau 95\\].", "acronym": "CD", "label": "Conceptual Density", "ID": "2297"}, {"sentence": "Word  Sense Disambiguation using CD.", "acronym": "CD", "label": "Conceptual Density", "ID": "2298"}, {"sentence": "The method  relies on the use oil' the wide-coverage noun  taxonomy of WordNet and the notion of  conceptual distance among concepts, captured by  a CD formula developed for this  purpose.", "acronym": "CD", "label": "Conceptual Density", "ID": "2299"}, {"sentence": "3_(5  2 CD and Word  Sense Disambiguation  Conceptual distance tries to provide a basis for  measuring closeness in meaning among words, taking  as reference a structured hierarchical net.", "acronym": "CD", "label": "Conceptual Density", "ID": "2300"}, {"sentence": "Our system tries to resolve the lexical  ambiguity ot' nouns by finding the combination of  senses from a set of contiguous nouns that  maximises the CD among senses.", "acronym": "CD", "label": "Conceptual Density", "ID": "2301"}, {"sentence": "e conceptual distance among  concepts: CD \\[Agirre & Rigau 95\\].", "acronym": "CD", "label": "Conceptual Density", "ID": "2302"}, {"sentence": "Though exact inference with this class of models is infeasible we use an effi- cient approximation (Bengio and Delalleau, 2007), which can be regarded either as a mean-field approx- imation to the reconstruction error or a determinis- tic version of the CD sampling method (Hinton, 2002).", "acronym": "CD", "label": "Contrastive Divergence", "ID": "2303"}, {"sentence": "We use 1-step CD (Hinton, 2002) to update the parameters by performing gradient ascent: ?", "acronym": "CD", "label": "Contrastive Divergence", "ID": "2304"}, {"sentence": "Traditional learning strategies such as CD are very inef- ficient.", "acronym": "CD", "label": "Contrastive Divergence", "ID": "2305"}, {"sentence": "Usually many steps of Gibbs-Sampling are necessary to get an unbiased sample from the distribution, but in the CD algorithm only one step of sampling is performed (Hinton, 2002).", "acronym": "CD", "label": "Contrastive Divergence", "ID": "2306"}, {"sentence": "We also tried a CD based training procedure for TRBM instead of equation 7, but that resulted in about an absolute 10% lower LAS.", "acronym": "CD", "label": "Contrastive Divergence", "ID": "2307"}, {"sentence": "The training objective is to learn a set of weights that maximize the likelihood of training observations, and given the independences inherent, in the model it can be trained quickly and effectively via CD.", "acronym": "CD", "label": "Contrastive Divergence", "ID": "2308"}, {"sentence": "Noun phrases with certain words, such as non-article determiners (e.g., this car), possessive pronouns (e.g., his car), CDs (e.g., one car) or quantifiers (e.g., some cars), also fall into this category.", "acronym": "CD", "label": "cardinal number", "ID": "2309"}, {"sentence": "The quantification can  be by a CD, or by a more vague  expression, like several or many.", "acronym": "CD", "label": "cardinal number", "ID": "2310"}, {"sentence": "e Classifier (HP?Obj) High?Precision Subjective Sentence Classifier (HP?Subj) Unannotated Text Collection unlabeled sentences unlabeled sentences unlabeled sentences Pattern?based Subjective Sentence Classifier Extraction Pattern Learner subjective sentences subjective sentences objective sentences subjective patterns subjective patterns Figure 1: Bootstrapping Process jective class (examples are CDs (Wiebe et al.,", "acronym": "CD", "label": "cardinal number", "ID": "2311"}, {"sentence": "Some of the  prenominal elements coming before a noun head  are CDs, ordinal numbers, superla- tive adjectives, and indefinite determiners; post- nominal elements are nouns and noun phrases,  adjectives and adjectival phrases, adjectival  clauses with conjunctions, indefinite post- determiners, prepositional phrases, adverbs of  place and time, ordinal numbers, possessive ad- jectives, and Ezafeh.", "acronym": "CD", "label": "cardinal number", "ID": "2312"}, {"sentence": "\u0001 \u0001 CD except one \u0001 ?", "acronym": "CD", "label": "cardinal number", "ID": "2313"}, {"sentence": "e word form of the adjective that modifies the head noun ModByAdjTag The POS tag of the adjective that modifies the head noun ModByPrep Indicates that the head noun is modified by a preposition ModByPrepWord The word form of the preposition that modifies the head noun ModByPossesive Indicates that the head noun is modified by a possesive ModByCardinal Indicates that the head noun is modified by a CD ModByRelative Indicates that the head noun is modified by a relative clause Table 2: Feature templates for English determiner correction.", "acronym": "CD", "label": "cardinal number", "ID": "2314"}, {"sentence": "Lexicalisation is often decomposed into two different stages: lexical choice, which occurs dur- ing CD, where the lexical term chosen, which may still be underspecified, is depen- dent on reasoning procedures, the knowledge base contents, and grammatical constraints; and lexical variation which is the choice of a particular word or form among possible synonyms or paraphrases.", "acronym": "CD", "label": "content determination", "ID": "2315"}, {"sentence": "For instance, distinguish two levels of CD, ?", "acronym": "CD", "label": "content determination", "ID": "2316"}, {"sentence": "After the command is executed, the CD module constructs the se- mantic representation of the effects that were ap- plied, updates the player KB with it and passes it to the next module for its verbalization (so that the player knows what changed in the world).", "acronym": "CD", "label": "content determination", "ID": "2317"}, {"sentence": "nce\u0006 \u0011\u0002 E\u0002 Reiter and R\u0002 Dale\u0002 \f\t\t\t\u0002 Building Natural Language Generation Systems\u0002 Cambridge University Press\u0002 \u0004 http \u0000\u0001\u0001xml\u0002apache\u0002org\u0001xalan\u0003j\u0001index\u0002html H\u0002 Somers\u0002 \f\t\t\t\u0002 Machine translation\u0002 In R\u0002 Dale\u0006 H\u0002 Moisl\u0006 and H\u0002 Somers\u0006 editors\u0006 Handbook of Nat\u0000 ural Language Processing\u0006 chapter \u0005\u0004\u0006 pages \u0004\f\u0010\u000e\u0004  \u0002 Marcel Dekker\u0002 S\u0002 G\u0002 Sripada\u0006 E\u0002 Reiter\u0006 J\u0002 Hunter\u0006 and J\u0002 Yu\u0002 \f\t\t\u0005\u0002 A two\u0003stage model for CD\u0002 In Proc\u0003 of the \u0004th European Workshop on Natural Language Generation associated to ACL \u0005\u0006th Ann\u0003 Meeting and \u0007\bth Conf\u0003 of the European Chapter\u0006 pages \u0004\u000e\u0005\t\u0006 Toulouse\u0006 France\u0006 July  \u000e\u000f\u0002 A The Implementation Code in Document Planning Stage A\u0001\u0002 Code Fragment in Document Structuring if \u0000fxoBasket\u0001Open\u0000Host\u0002 \u0003\u0003 \u0004\u0002\u0005 if \u0000fxoBasket\u0001Login\u0000Alias\u0006 UserID\u0006 Password\u0002 \u0003\u0003 \u0004\u0002\u0005 if \u0000fxoBasket", "acronym": "CD", "label": "content determination", "ID": "2318"}, {"sentence": "This is a challenge for both CD and mi- croplanning.", "acronym": "CD", "label": "content determination", "ID": "2319"}, {"sentence": "Danlos's system  thus performs CD and lexical choice before discourse organization  and syntactic realization (a very original position).", "acronym": "CD", "label": "content determination", "ID": "2320"}, {"sentence": "14 Qiu et al Opinion Word Expansion and Target Extraction through DP 4.1 Propagation Rules Defined Based on Relations In our propagation, there are four subtasks: (1) extracting targets using opinion words; (2) extracting targets using the extracted targets; (3) extracting opinion words using the extracted targets; (4) extracting opinion words using both the given and the extracted opinion words.", "acronym": "DP", "label": "Double Propagation", "ID": "2321"}, {"sentence": "10 Qiu et al Opinion Word Expansion and Target Extraction through DP 2.1 Opinion Word Extraction Extensive work has been done on sentiment analysis at word, expression (Breck, Choi, and Cardie 2007; Takamura, Inui, and Okumura 2007), sentence (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and document (Pang, Lee, and Vaithyanathan 2002; Turney 2002) levels.", "acronym": "DP", "label": "Double Propagation", "ID": "2322"}, {"sentence": "16 Qiu et al Opinion Word Expansion and Target Extraction through DP Figure 3 The propagation algorithm.", "acronym": "DP", "label": "Double Propagation", "ID": "2323"}, {"sentence": "18 Qiu et al Opinion Word Expansion and Target Extraction through DP In this work, we filter non-targets based on frequency.", "acronym": "DP", "label": "Double Propagation", "ID": "2324"}, {"sentence": "Relation Identification As stated previously, identification of the relations between opinion words/targets and other opinion words/targets is the key to our opinion lexicon expansion and target 12 Qiu et al Opinion Word Expansion and Target Extraction through DP extraction methods.", "acronym": "DP", "label": "Double Propagation", "ID": "2325"}, {"sentence": "1510  Opinion Word Expansion and Target Extraction through DP Guang Qiu?", "acronym": "DP", "label": "Double Propagation", "ID": "2326"}, {"sentence": "Figure 4: DP estimation of q?.", "acronym": "DP", "label": "Dynamic programming", "ID": "2327"}, {"sentence": "1 Introduction DP algorithms, also known as tabular or chart-based algorithms, are at the core of many applications in natural language processing.", "acronym": "DP", "label": "Dynamic programming", "ID": "2328"}, {"sentence": "DP for linear-time incremental parsing.", "acronym": "DP", "label": "Dynamic programming", "ID": "2329"}, {"sentence": "1.1 DP as deduction The ?", "acronym": "DP", "label": "Dynamic programming", "ID": "2330"}, {"sentence": "DP search for continuous speech recognition.", "acronym": "DP", "label": "Dynamic programming", "ID": "2331"}, {"sentence": "Possible kinds of LEs are: word forms, parts-of-speech, DP, syntactic phrases, named entities, semantic roles, etc.", "acronym": "DP", "label": "dependency relationships", "ID": "2332"}, {"sentence": "Dependency Relations (DR) restrains word co- occurrence to words that are reachable from the multi-sense word via a syntactic parse composed of DP limited by some length (Pado?", "acronym": "DP", "label": "dependency relationships", "ID": "2333"}, {"sentence": "The logical metaf~nction is responsible for the con-  struction of composite semantic entities using the re-  sources of interdependency; it is manifested in grammar  by DP such as those that hold be-  tween the head of a phrase and its dependents and the  association of concepts to be expressed with particu-  lar heads in the sentence structure.", "acronym": "DP", "label": "dependency relationships", "ID": "2334"}, {"sentence": "The strategy only considers adjectives in successive sentences and does not use features or any DP.", "acronym": "DP", "label": "dependency relationships", "ID": "2335"}, {"sentence": "Next, by as- suming DP between the bunsetsus, candidate dependency trees are con- structed.", "acronym": "DP", "label": "dependency relationships", "ID": "2336"}, {"sentence": "To do this, we first identify nsubj and nsubjpass DP.", "acronym": "DP", "label": "dependency relationships", "ID": "2337"}, {"sentence": "To solve riddles, we utilize a DP algorithm to combine the identified metaphors based on the alignments and rules to obtain the candidate solutions.", "acronym": "DP", "label": "dynamic programming", "ID": "2338"}, {"sentence": "As with hidden Markov models (Rabiner, 1989), yw(x) can be com- puted efficiently for suitable feature functions using DP.", "acronym": "DP", "label": "dynamic programming", "ID": "2339"}, {"sentence": "A CKY style DP  chart parser is used to find the maximum probability  tree for each sentence (see figure 6).", "acronym": "DP", "label": "dynamic programming", "ID": "2340"}, {"sentence": "Then, in the solving phase, we utilize a DP method to combine the identified metaphors to obtain candidate solutions.", "acronym": "DP", "label": "dynamic programming", "ID": "2341"}, {"sentence": "A combination of these two types of methods was developed by Kikuchi et al (2003), where summarization is performed in two steps: first, sentence extraction is done through feature combination; second, compaction is done by scoring the words in each sentence and then a DP technique is applied to select the words that will remain in the sentence to be included in the summary.", "acronym": "DP", "label": "dynamic programming", "ID": "2342"}, {"sentence": "Second, in the solving phase, we utilize a DP algorithm on the basis of the alignments and rules to figure out the candidate solutions.", "acronym": "DP", "label": "dynamic programming", "ID": "2343"}, {"sentence": "This setting includes a  broad range of structured prediction problems such as semantic role labeling, named  entity and relation recognition, co-reference resolution, DP and  semantic parsing.", "acronym": "DP", "label": "dependency parsing", "ID": "2344"}, {"sentence": "Non-projective DP using spanning tree algorithms.", "acronym": "DP", "label": "dependency parsing", "ID": "2345"}, {"sentence": "The passive char- acters were identified via the following relations extracted by DP: nsubjpass (passive nominal subject) and pobj (object of a preposition).", "acronym": "DP", "label": "dependency parsing", "ID": "2346"}, {"sentence": "For the task of unsupervised DP, Smith and Eisner (2006) add a constraint of the form ?", "acronym": "DP", "label": "dependency parsing", "ID": "2347"}, {"sentence": "We show that the combination of data-driven and rule-based components can reduce the number of all parsing errors by 14% and raise the attachment accuracy for DP of German to an unprecedented 92%.", "acronym": "DP", "label": "dependency parsing", "ID": "2348"}, {"sentence": "This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) named entity recognition, (vi) DP, and (vii) co-reference analysis.", "acronym": "DP", "label": "dependency parsing", "ID": "2349"}, {"sentence": "DP Knowledge Resources Lexicon Resources Grammars Process Manager Tokenlist Legend Output Manager Source Document NLP/IE Processor(s)Tokenizer Tokenlist Lexicon Lookup  POS Tagging Named Entity Detection Shallow Parsing Deep Parsing Relationship Detection Document pool NE CE EP SVO Time Normalization Alias and Coreference Profile/Event Linking/Merging Abbreviations POS = Part of Sp", "acronym": "DP", "label": "Document Processor", "ID": "2350"}, {"sentence": "DP Knowledge Resources Lexicon Resources Grammars Process Manager Tokenlist Legend Output Manager Source Document Linguistic Processor(s)Tokenizer Tokenlist Lexicon Lookup Pragmatic Filtering  POS Tagging Named Entity Detection Shallow Parsing Deep Parsing Relationship Detection Document pool NE CE EP SVO Time Normalization Alias/Coreference Linking Profile/Event Linking Profile/Ev", "acronym": "DP", "label": "Document Processor", "ID": "2351"}, {"sentence": "DP Knowledge Resources Lexicon Resources Grammars Process Manager Tokenlist Legend Output Manager Source Document Linguistic Processor(s)Tokenizer Tokenlist Lexicon Lookup  POS Tagging NE Tagging Shallow Parsing Relationship Extraction Document pool NE CE EP SVO Time Normalization Profile/Event Consolidation Event Extraction Abbreviations NE = Named Entity CE = Correlated Entity EP", "acronym": "DP", "label": "Document Processor", "ID": "2352"}, {"sentence": "DPes, Chinese restaurant processes and all that.", "acronym": "DP", "label": "Dirichlet process", "ID": "2353"}, {"sentence": "Both the hierarchical DP and the hierarchi- cal Pitman-Yor process are examples of Bayesian nonparametric processes.", "acronym": "DP", "label": "Dirichlet process", "ID": "2354"}, {"sentence": "Hierarchical DPes.", "acronym": "DP", "label": "Dirichlet process", "ID": "2355"}, {"sentence": "The hierarchical Pitman-Yor process is a natural generalization of the recently proposed hierarchi- cal DP (Teh et al, 2006).", "acronym": "DP", "label": "Dirichlet process", "ID": "2356"}, {"sentence": "The hier- archical DP was proposed to solve a different problem?that of clustering, and it is interesting to note that such a direct generaliza- tion leads us to a good language model.", "acronym": "DP", "label": "Dirichlet process", "ID": "2357"}, {"sentence": "Three New Probabilistic Models for  DP: An Exploration.", "acronym": "DP", "label": "Dependency Parsing", "ID": "2358"}, {"sentence": "Exploring Automatic Feature Selection for Transition-Based DP.", "acronym": "DP", "label": "Dependency Parsing", "ID": "2359"}, {"sentence": "2.1 Syntactic DP We use a shift-reduce scheme to implement syn- tactic dependency parsing as in (Nivre, 2003).", "acronym": "DP", "label": "Dependency Parsing", "ID": "2360"}, {"sentence": "2.2 Semantic DP Assuming no predicates overtly known, we keep using a word-pair classifier to perform semantic parsing through a single-stage processing.", "acronym": "DP", "label": "Dependency Parsing", "ID": "2361"}, {"sentence": "Exploring Morphosyntactic Annotation Over a Spanish Corpus for DP .", "acronym": "DP", "label": "Dependency Parsing", "ID": "2362"}, {"sentence": "45  CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 233?237 Manchester, August 2008 Semantic DP using N-best Semantic Role Sequences and Roleset Information Joo-Young Lee, Han-Cheol Cho, and Hae-Chang Rim Natural Language Processing Lab.", "acronym": "DP", "label": "Dependency Parsing", "ID": "2363"}, {"sentence": "DP ?", "acronym": "DP", "label": "Discriminatory Power", "ID": "2364"}, {"sentence": "DP.", "acronym": "DP", "label": "Discriminatory Power", "ID": "2365"}, {"sentence": "DP We consider output of three types of MT system (Phrase-based, Syntax- based and Rule-based) to attempt to gain insight into the different types of semantic information preserved by the different systems.", "acronym": "DP", "label": "Discriminatory Power", "ID": "2366"}, {"sentence": "stands for DP adaptor (b = 0), PY stands for Pitman-Yor adaptor (b optimized), and PY+inc.", "acronym": "DP", "label": "Dirichlet Process", "ID": "2367"}, {"sentence": "Hierarchical DPes.?", "acronym": "DP", "label": "Dirichlet Process", "ID": "2368"}, {"sentence": "6.1 Model 0: Simple Mixture Model In our first model, based on Haghighi and Klein?s baseline DP model, each image i corresponds to the set of observed mentions wi from across its captions.", "acronym": "DP", "label": "Dirichlet Process", "ID": "2369"}, {"sentence": "con- trols the probability of branching via the per-node DP, and L is the fixed tree depth.", "acronym": "DP", "label": "Dirichlet Process", "ID": "2370"}, {"sentence": "Since our generative models are based on DP priors, they are de- signed to favor a small number of unique entities per image.", "acronym": "DP", "label": "Dirichlet Process", "ID": "2371"}, {"sentence": "t indicate the variance of corresponding entity dis- tribution respectively, and u r,m is the m-th com- ponent translation vector of relation r. Chinese Restaurant Process (CRP) is a DP and it can automatically detect semantic compo- nents.", "acronym": "DP", "label": "Dirichlet Process", "ID": "2372"}, {"sentence": "ICDM and Knowledge Discovery.", "acronym": "ICDM", "label": "In Data Mining", "ID": "2373"}, {"sentence": "ICDM and Knowledge Discovery, pages 259-275.", "acronym": "ICDM", "label": "In Data Mining", "ID": "2374"}, {"sentence": "ICDM and Knowledge Discovery for Big Data, pages 1?40.", "acronym": "ICDM", "label": "In Data Mining", "ID": "2375"}, {"sentence": "ICDM, Eighth IEEE International Con- ference on, pages 1025?1030.", "acronym": "ICDM", "label": "In Data Mining", "ID": "2376"}, {"sentence": "ICDM in Time Series Databases.", "acronym": "ICDM", "label": "In Data Mining", "ID": "2377"}, {"sentence": "ICDM for Security Ap- plications.", "acronym": "ICDM", "label": "In Data Mining", "ID": "2378"}, {"sentence": "of  the SIAM ICDM.", "acronym": "ICDM", "label": "International Conference on Data Mining", "ID": "2379"}, {"sentence": "In SIAM ICDM (SDM).", "acronym": "ICDM", "label": "International Conference on Data Mining", "ID": "2380"}, {"sentence": "In  SIAM ICDM.", "acronym": "ICDM", "label": "International Conference on Data Mining", "ID": "2381"}, {"sentence": "In proceedings of the Text Mining Workshop, at the 3rd SIAM ICDM.", "acronym": "ICDM", "label": "International Conference on Data Mining", "ID": "2382"}, {"sentence": "In Pro- ceedings of the ICDM, pages 629?632.", "acronym": "ICDM", "label": "International Conference on Data Mining", "ID": "2383"}, {"sentence": "In Proceedings of the Second SIAM ICDM, Arlington, VA, USA, April 11-13, 2002, pages 158?", "acronym": "ICDM", "label": "International Conference on Data Mining", "ID": "2384"}, {"sentence": "1 In t roduct ion   We present a statistical method for determin-  ICDM pronoun anaphora.", "acronym": "ICDM", "label": "ing", "ID": "2385"}, {"sentence": "We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governICDM head informa-  tion and noun phrase repetition.", "acronym": "ICDM", "label": "ing", "ID": "2386"}, {"sentence": "We combine  them into a sICDMle probability t", "acronym": "ICDM", "label": "ing", "ID": "2387"}, {"sentence": "We present some experiments il-  lustratICDM the accuracy of the method", "acronym": "ICDM", "label": "ing", "ID": "2388"}, {"sentence": "city of the  proposed antecedent, governICDM head informa-  tion and noun phrase repetition.", "acronym": "ICDM", "label": "ing", "ID": "2389"}, {"sentence": "We present some experiments il-  lustratICDM the accuracy of the method and note  that with this information added, our pronoun  resolution method achieves 84.2% accuracy.", "acronym": "ICDM", "label": "ing", "ID": "2390"}, {"sentence": "brown, edu  Abst ract   This paper presents an algorithm for identi-  fyICDM pronominal anaphora and two experi-  ments based upon this algorithm.", "acronym": "ICDM", "label": "ing", "ID": "2391"}, {"sentence": "The second  experiment investigates a method for unsuper-  vised learnICDM of gender/number/animaticity  inform", "acronym": "ICDM", "label": "ing", "ID": "2392"}, {"sentence": "This program differs  from earlier work in its almost complete lack of  hand-craftICDM, relyICDM instead on a very small  corpus of Penn Wall Street Journal Tree-bank  text (Marcus", "acronym": "ICDM", "label": "ing", "ID": "2393"}, {"sentence": "We combine  them into a sICDMle probability that enables us  to identify the referent.", "acronym": "ICDM", "label": "ing", "ID": "2394"}, {"sentence": "thm for identi-  fyICDM pronominal anaphora and two experi-  ments based upon this algorithm.", "acronym": "ICDM", "label": "ing", "ID": "2395"}, {"sentence": "This program differs  from earlier work in its almost complete lack of  hand-craftICDM, relyICDM instead on a very small  corpus of Penn Wall Street Journal Tree-bank  text (Marcus et al, 1993) that has been marked  with co-reference information.", "acronym": "ICDM", "label": "ing", "ID": "2396"}, {"sentence": "The second  experiment investigates a method for unsuper-  vised learnICDM of gender/number/animaticity  information.", "acronym": "ICDM", "label": "ing", "ID": "2397"}, {"sentence": "This program differs  from earlier work in its almost complete lack of  hand-craftICDM, relyi", "acronym": "ICDM", "label": "ing", "ID": "2398"}, {"sentence": "c?2014 Association for Computational Linguistics Word-Based Dialog State Tracking with RNNs Matthew Henderson, Blaise Thomson and Steve Young Department of Engineering, University of Cambridge, U.K. {mh521, brmt2, sjy}@eng.cam.ac.uk Abstract Recently discriminative methods for track- ing the state of a spoken dialog have been shown to outperform traditional generative models.", "acronym": "RNN", "label": "Recurrent Neural Network", "ID": "2399"}, {"sentence": "Specifically for RNNs, Lample et al (2016) and Huang et.", "acronym": "RNN", "label": "Recurrent Neural Network", "ID": "2400"}, {"sentence": "3 LSTM RNNs Our work is based on recurrent neural networks.", "acronym": "RNN", "label": "Recurrent Neural Network", "ID": "2401"}, {"sentence": "Exten- sions of RNN Language Model.", "acronym": "RNN", "label": "Recurrent Neural Network", "ID": "2402"}, {"sentence": "c?2016 Association for Computational Linguistics Keyphrase Extraction Using Deep RNNs on Twitter Qi Zhang, Yang Wang, Yeyun Gong, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University Shanghai, P.R. China {qz, ywang14, yygong12, xjhuang}@fudan.edu.cn Abstract Keyphrases can provide highly condensed and valuable information that allows users to quickly acquire the main ideas.", "acronym": "RNN", "label": "Recurrent Neural Network", "ID": "2403"}, {"sentence": "c?2014 Association for Computational Linguistics Translation Modeling with Bidirectional RNNs Martin Sundermeyer 1 , Tamer Alkhouli 1 , Joern Wuebker 1 , and Hermann Ney 1,2 1 Human Language Technology and Pattern Recognition Group RWTH Aachen University, Aachen, Germany 2 Spoken Language Processing Group Univ.", "acronym": "RNN", "label": "Recurrent Neural Network", "ID": "2404"}, {"sentence": "Better word representa- tions with RNNs for morphol- ogy.", "acronym": "RNN", "label": "recursive neural network", "ID": "2405"}, {"sentence": "Better word representations with RNNs for morphology.", "acronym": "RNN", "label": "recursive neural network", "ID": "2406"}, {"sentence": "2011), regression models by Guevara (2010), and RNN based solutions by Socher et al (2012) and Collobert et al (2011) have been proposed.", "acronym": "RNN", "label": "recursive neural network", "ID": "2407"}, {"sentence": "Transition-based dependency parsing using RNNs.", "acronym": "RNN", "label": "recursive neural network", "ID": "2408"}, {"sentence": "Parsing natural scenes and nat- ural language with RNNs.", "acronym": "RNN", "label": "recursive neural network", "ID": "2409"}, {"sentence": "Exten- sions of RNN language model.", "acronym": "RNN", "label": "recurrent neural network", "ID": "2410"}, {"sentence": "Chinese poetry generation with RNNs.", "acronym": "RNN", "label": "recurrent neural network", "ID": "2411"}, {"sentence": "A RNN language model (Mikolov et al, 2010) aims to esti- mate the probability of observing a word given its preceding context.", "acronym": "RNN", "label": "recurrent neural network", "ID": "2412"}, {"sentence": "RNN language modeling toolkit.", "acronym": "RNN", "label": "recurrent neural network", "ID": "2413"}, {"sentence": "Incremen- tal RNN dependency parser with search-based discriminative training.", "acronym": "RNN", "label": "recurrent neural network", "ID": "2414"}, {"sentence": "Incre- mental RNN dependency parser with search-based discriminative training.", "acronym": "RNN", "label": "recurrent neural network", "ID": "2415"}, {"sentence": "Since large sense-tagged corpora were not available, we simulated the baseline method with a modified version of the proposed method; namely, for each polysemous word, the sense that maximizes the sum of correlations with all clues was selected as the MFS.", "acronym": "MFS", "label": "most frequent sense", "ID": "2416"}, {"sentence": "Both sys- tems achieved comparable accuracy (0.851 and 0.857), which outperforms considerably the MFS baseline (0.787).", "acronym": "MFS", "label": "most frequent sense", "ID": "2417"}, {"sentence": "The decision trees learned by our system fall back on the MFS in case the identified features are unable to disambiguate the target word.", "acronym": "MFS", "label": "most frequent sense", "ID": "2418"}, {"sentence": "Automatic Annotation Results (fine-grained score) The baseline results were obtained running a sim- ple algorithm that assigned to the instances of the test set the MFS of each lemma in the training set.", "acronym": "MFS", "label": "most frequent sense", "ID": "2419"}, {"sentence": "Comparison with a baseline method, which selects the MFS of each polysemous word independently of contexts, was also done.", "acronym": "MFS", "label": "most frequent sense", "ID": "2420"}, {"sentence": "A majority classifier which always chooses the MFS of a word in the training data, achieves an accuracy of 56.5%.", "acronym": "MFS", "label": "most frequent sense", "ID": "2421"}, {"sentence": "We calculated a baseline of 0.788 for the Lexical Sample sub-task, using the MFS for each lemma in the training data.", "acronym": "MFS", "label": "Most Frequent Sense", "ID": "2422"}, {"sentence": "5.2 MFS WordNet keeps track of the frequency of each word meaning within a sense-annotated corpus.", "acronym": "MFS", "label": "Most Frequent Sense", "ID": "2423"}, {"sentence": "657 Metric Correlation MSRpar 0.591 MSRvid 0.726 SMTeuroparl 0.485 Table 2: Correlation scores across individual cor- pora using Path Distance and MFS.", "acronym": "MFS", "label": "Most Frequent Sense", "ID": "2424"}, {"sentence": "Comparison of word sense disambiguation strategies and semantic similarity measures on the training data showed that the best results were ob- tained using the Path Distance Measure combined 658 with the MFS approach (see Ta- bles 1 and 2) and these were used for the official run.", "acronym": "MFS", "label": "Most Frequent Sense", "ID": "2425"}, {"sentence": "System Accuracy MFS Baseline 7.7 Best SMT system 33.0 UMD unsupervised system 44.5 WSD naive Bayes 60.4 WSD KPCA 63.6 WSD Boosting 64.1 WSD Maximum Entropy 64.4 WSD Ensemble (current best Senseval-3 model) 66.2 6.1 The dedicated supervised WSD models all significantly outperform SMT Table 2 clearly shows that even the best of the SMT model considered performs significantly worse than any of the de", "acronym": "MFS", "label": "Most Frequent Sense", "ID": "2426"}, {"sentence": "However, this induction step has proven to be greatly chal- lenging, in the most recent shared tasks, induction systems either appear to perform poorly or fail to outperform the simple MFS baseline (Agirre and Soroa, 2007a; Manandhar et al, 2010).", "acronym": "MFS", "label": "Most Frequent Sense", "ID": "2427"}, {"sentence": "However, not so much attention has been paid on learning class-based classifiers from other available sense?groupings such as WordNet Domains (Magnini and Cavaglia`, 2000), SUMO labels (Niles and Pease, 2001), EWN Base Concepts (Vossen et al, 1998), Top Con- cept Ontology labels (Alvez et al, 2008) or Ba- sic Level Concepts (Izquierdo et al, 2007).", "acronym": "EWN", "label": "EuroWordNet", "ID": "2428"}, {"sentence": "For this purpose, an architecture similar to the one used by EWN (Vossen, 2000) was implemented, in order to obtain knowledge databases for the different languages, but all of them connected though a unit denominated TER-ILI or Temporal Expression Rules Interlingua Index.", "acronym": "EWN", "label": "EuroWordNet", "ID": "2429"}, {"sentence": "The success of Word- Net led to the adoption of its model by lexical re- sources in different languages, such as the ones in the EWN project (Vossen, 1997), or WordNet.", "acronym": "EWN", "label": "EuroWordNet", "ID": "2430"}, {"sentence": "EWN: Building a Multilin- gual Database with WordNets in 8 European Lan- guages.", "acronym": "EWN", "label": "EuroWordNet", "ID": "2431"}, {"sentence": "P. Vossen, and W. Peters, 1997 Multilingual design  of EWN, Proceedings of the Delos  workshop on Cross-language Information  Retrieval.", "acronym": "EWN", "label": "EuroWordNet", "ID": "2432"}, {"sentence": "We can find ontologies ranging from general world knowledge resources, such as WordNet (Fellbaum 1998), EWN (Vossen 1998), Cyc (Lenat and Guha 1990), and FrameNet (Johnson and Fillmore 2000, to very specific domain knowledge, such as the medical domain (Lindberg, Humphreys, and McCray 1993) or the chemistry domain (Barker et al 2004).", "acronym": "EWN", "label": "EuroWordNet", "ID": "2433"}, {"sentence": "The result of this phase is the ex-  tension of the EWN with the Italian synsets.", "acronym": "EWN", "label": "English WordNet", "ID": "2434"}, {"sentence": "We used a con-  straint satisfaction algorithm (relaxation label-  ing) to select -among all the candidate trans-  lations proposed by a bilingual dictionary- the  right EWN synset for each sense in  a taxonomy automatically derived from a Span-  ish monolingua\\] dictionary.", "acronym": "EWN", "label": "English WordNet", "ID": "2435"}, {"sentence": "To remove the  ambiguities we develop new word sense  disambiguation heuristics and automatic mapping  method to construct Korean WordNet based on  the existing EWN.", "acronym": "EWN", "label": "English WordNet", "ID": "2436"}, {"sentence": "A set of  automatic WSD techniques i described for  linking Korean words collected from a  bilingual MRD to EWN synsets.", "acronym": "EWN", "label": "English WordNet", "ID": "2437"}, {"sentence": "The method has been applied to  link Korean words from a bilingual dictionary to  EWN synsets.", "acronym": "EWN", "label": "English WordNet", "ID": "2438"}, {"sentence": "Miller et al, 1994) found that automatic  I We use EWN version 1.6  - L  143  assignment of polysemous words in Brown  Corpus to senses in WordNet was 58% correct  with a heuristic of most frequently occurring  sense.", "acronym": "EWN", "label": "English WordNet", "ID": "2439"}, {"sentence": "it changes to a NNS", "acronym": "NNS", "label": "noun ?", "ID": "2440"}, {"sentence": "in which the head NNS", "acronym": "NNS", "label": "noun ?", "ID": "2441"}, {"sentence": "against, about, versus) with the NNS", "acronym": "NNS", "label": "noun ?", "ID": "2442"}, {"sentence": "attaches to the NNS", "acronym": "NNS", "label": "noun ?", "ID": "2443"}, {"sentence": "6  Table 2: POS Groups  ProNNS", "acronym": "NNS", "label": "noun ?", "ID": "2444"}, {"sentence": "cannot be detected even if whether the NNS", "acronym": "NNS", "label": "noun ?", "ID": "2445"}, {"sentence": "Delete a right paren to the left of a NNS.", "acronym": "NNS", "label": "plural noun", "ID": "2446"}, {"sentence": "cannot refer to an NP headed by a NNS.", "acronym": "NNS", "label": "plural noun", "ID": "2447"}, {"sentence": "NNSs do not take an indefinite article?,", "acronym": "NNS", "label": "plural noun", "ID": "2448"}, {"sentence": "Delete a left paren to the left of a NNS.", "acronym": "NNS", "label": "plural noun", "ID": "2449"}, {"sentence": "6-grams : [SP]-[O]-[SP]-[O]-[S]-[JK], where S and P correspond to singular or NNSs, O to the prepositions (also in combination with the article), and J and K to singular or plural adjectives (e.g. ?", "acronym": "NNS", "label": "plural noun", "ID": "2450"}, {"sentence": "The parsing  stage also includes classification of NNSs.", "acronym": "NNS", "label": "plural noun", "ID": "2451"}, {"sentence": "ISOQuest?s NetOwl and IBM?s Textract attempted  to determine whether multiple NEs refer to  the same entity but neither had the ability to distinguish  different entities with the same name.", "acronym": "NE", "label": "named entitie", "ID": "2452"}, {"sentence": "are a disguise of multiple NEs  such as ?", "acronym": "NE", "label": "named entitie", "ID": "2453"}, {"sentence": "Evaluation       Given a collection of NEs from  documents, the coreferencing task is to put them into  equivalence classes, where every mention in the same  class refers to the same entity (person, location,  organization, and so on).", "acronym": "NE", "label": "named entitie", "ID": "2454"}, {"sentence": "A clear candidate is the use of NEs, but the creation of templates has also been tried in open domains (Srihari and Li 2000) and restricted domains (Weischedel, Xu, and Licuanan 2004).", "acronym": "NE", "label": "named entitie", "ID": "2455"}, {"sentence": "Nowadays it is possible to parse the entire corpus used in the QA track of TREC and to extract all its NEs.", "acronym": "NE", "label": "named entitie", "ID": "2456"}, {"sentence": "It is therefore feasible to parse and extract the NEs of corpora of restricted domains.", "acronym": "NE", "label": "named entitie", "ID": "2457"}, {"sentence": "Documents, paragraphs, sentences and tokens are uniquely identified and lemmata, part-of-speech (POS), NE information and morphological analyses have been automatically annotated in the data.", "acronym": "NE", "label": "named entity", "ID": "2458"}, {"sentence": "We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) NE recognition was used for identifying proper names, e.g., ?", "acronym": "NE", "label": "named entity", "ID": "2459"}, {"sentence": "Two broad approaches for the iden- tification of story characters were followed: (i) NE recognition, and (ii) identification of character nominals, e.g., ?", "acronym": "NE", "label": "named entity", "ID": "2460"}, {"sentence": "This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) NE recognition, (vi) dependency parsing, and (vii) co-reference analysis.", "acronym": "NE", "label": "named entity", "ID": "2461"}, {"sentence": "Then, we ran the  documents through Identifinder, a NE  extraction system developed by BBN, to tag the named  entities in the documents.", "acronym": "NE", "label": "named entity", "ID": "2462"}, {"sentence": "F 14 NE, especially a protein name.", "acronym": "NE", "label": "named entity", "ID": "2463"}, {"sentence": "2 Japanese NE   Recogn i t ion   2.1 Task of the IREX Workshop  The task of named entity recognition of the  IREX workshop is to recognize ight named en-  tity types in Table 1 (IREX Conmfittee, 1999).", "acronym": "NE", "label": "Named Ent i ty", "ID": "2464"}, {"sentence": "Unsurprisingly, the Miscellaneous and Other  NE  categories are problematic; unfortu-  nately, they are also rather frequent.", "acronym": "NE", "label": "Named Ent i ty", "ID": "2465"}, {"sentence": "... Mm(<3 )  1\" (2)  (Current Position)  4 Superv ised Learning for Japanese  NE  Recogn i t ion   This section describes how to apply tile deci-  sion list learning method to chunking/tagging  named entities.", "acronym": "NE", "label": "Named Ent i ty", "ID": "2466"}, {"sentence": "J apanese  NE  Ext rac t ion  Eva luat ion   - Ana lys i s  o f  Resu l t s  -  Satosh i  Sek ine   Computer  Science Depar tment   New York University  715 Broadway, 7th floor  New York, NY 10003, USA  sekine@cs, nyu.", "acronym": "NE", "label": "Named Ent i ty", "ID": "2467"}, {"sentence": "M I M M \\] M  0 0RG_I 0 L0C_I LOC_I LOC_I LOC_B 0  0 0RG_U 0 LOC_S L0C_C LOC_E LOC_U 0  V Mo i - l )hemes  to 1 NE \\ ]   <ORGANIZATION>  .... Roshia gun -..   ( S<,s,~i,.", "acronym": "NE", "label": "Named Ent i ty", "ID": "2468"}, {"sentence": "NE extraction from noisy input: speech and OCR.", "acronym": "NE", "label": "Named entity", "ID": "2469"}, {"sentence": "NE recognition through classifier combination.", "acronym": "NE", "label": "Named entity", "ID": "2470"}, {"sentence": "NE recognition in tweets: an experimental study.", "acronym": "NE", "label": "Named entity", "ID": "2471"}, {"sentence": "NE recognition through classifier combina- tion.", "acronym": "NE", "label": "Named entity", "ID": "2472"}, {"sentence": "NE disam- biguation by leveraging Wikipedia semantic knowl- edge.", "acronym": "NE", "label": "Named entity", "ID": "2473"}, {"sentence": "NE extraction from broadcast news.", "acronym": "NE", "label": "Named entity", "ID": "2474"}, {"sentence": "NEy extraction from noisy input: speech and OCR.", "acronym": "NE", "label": "Named entit", "ID": "2475"}, {"sentence": "NEy recognition through classifier combination.", "acronym": "NE", "label": "Named entit", "ID": "2476"}, {"sentence": "NEy recognition in tweets: an experimental study.", "acronym": "NE", "label": "Named entit", "ID": "2477"}, {"sentence": "NEy recognition through classifier combina- tion.", "acronym": "NE", "label": "Named entit", "ID": "2478"}, {"sentence": "NEy disam- biguation by leveraging Wikipedia semantic knowl- edge.", "acronym": "NE", "label": "Named entit", "ID": "2479"}, {"sentence": "NEy extraction from broadcast news.", "acronym": "NE", "label": "Named entit", "ID": "2480"}, {"sentence": "We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) NEd entity recognition was used for identifying proper NEs, e.g., ?", "acronym": "NE", "label": "name", "ID": "2481"}, {"sentence": "2) a set of part-of-speech patterns was used for the extraction of human and non-human characters that were not represented by proper NEs, e.g., ?", "acronym": "NE", "label": "name", "ID": "2482"}, {"sentence": "To that end, we  devised a more objective test, useful only for  scoring the subset of referents that are NEs  of people.", "acronym": "NE", "label": "name", "ID": "2483"}, {"sentence": "This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) NEd entity recognition, (vi) dependency parsing, and (vii) co-reference analysis.", "acronym": "NE", "label": "name", "ID": "2484"}, {"sentence": "Two broad approaches for the iden- tification of story characters were followed: (i) NEd entity recognition, and (ii) identification of character nominals, e.g., ?", "acronym": "NE", "label": "name", "ID": "2485"}, {"sentence": "The utilization of available resources containing associations between person NEs and gender was followed in (Elson and 41 McKeown, 2010).", "acronym": "NE", "label": "name", "ID": "2486"}, {"sentence": "We first ex- tract named entities from scattered opinions DT using Stanford NNE Recognizer (Finkel et al, 2005).", "acronym": "NE", "label": "amed Entity", "ID": "2487"}, {"sentence": "Workshop on NNE Recognition.", "acronym": "NE", "label": "amed Entity", "ID": "2488"}, {"sentence": "We are studying to combine WordNet with a NNE Recogniser to produce generalised rules.", "acronym": "NE", "label": "amed Entity", "ID": "2489"}, {"sentence": "The Third International Chi- nese Language Processing Bakeoff: Word Seg- mentation and NNE Recognition, In  Proceedings of SIGHAN5 the 3rd International  Chinese Language Processing Bakeoff at Col- ing/ACL 2006, July, Sydney, Australia, 108-117.", "acronym": "NE", "label": "amed Entity", "ID": "2490"}, {"sentence": "NNE recogni- tion using a HMM-based chunk tagger.", "acronym": "NE", "label": "amed Entity", "ID": "2491"}, {"sentence": "3) We employed IE methods (including pattern sets  and NNE Recognition) as initial extraction  steps.", "acronym": "NE", "label": "amed Entity", "ID": "2492"}, {"sentence": "RST: A framework for the analysis of texts.", "acronym": "RST", "label": "Rhetorical structure theory", "ID": "2493"}, {"sentence": "RST: Toward a func-  tional theory of text organization.", "acronym": "RST", "label": "Rhetorical structure theory", "ID": "2494"}, {"sentence": "RST: Toward a functional theory of text organization.", "acronym": "RST", "label": "Rhetorical structure theory", "ID": "2495"}, {"sentence": "RST: Toward a functional the- ory of text organization.", "acronym": "RST", "label": "Rhetorical structure theory", "ID": "2496"}, {"sentence": "RST: Toward a functional the-  ory of text organization.", "acronym": "RST", "label": "Rhetorical structure theory", "ID": "2497"}, {"sentence": "RST: Toward a Functional  Theory of Text Organization.", "acronym": "RST", "label": "Rhetorical Structure Theory", "ID": "2498"}, {"sentence": "Mann W. C. and Thompson S. A (1988)  \"RST: Towards a  Functional Theory of Text Organization.\"", "acronym": "RST", "label": "Rhetorical Structure Theory", "ID": "2499"}, {"sentence": "In  both respects, our proposals will contrast with those of  RST \\[Mann and Thompson\\].", "acronym": "RST", "label": "Rhetorical Structure Theory", "ID": "2500"}, {"sentence": "RST: Toward a Functional Theory of Text Organization.", "acronym": "RST", "label": "Rhetorical Structure Theory", "ID": "2501"}, {"sentence": "In Olle ma.jor theory of discourse  structure, RST (Mann &: Thompson 1988; Imrea.l'ter simply RS'T), the  smallest possible linguistic units that can lmrtMl)ate in a rhetorical rela.tion a,re called units,  and \"are essentially clauses, except that clausal subjects a.nd complenlents a.nd restricte(l rel-  ative clauses are considered as parts of their host clause units rather than as sepa, rate units\"  \\[p.", "acronym": "RST", "label": "Rhetorical Structure Theory", "ID": "2502"}, {"sentence": "RST : A Theory of Text  Organization.", "acronym": "RST", "label": "Rhetorical Structure Theory", "ID": "2503"}, {"sentence": "The Task Trajectory and ED features are based on computing a token-level edit distance from a student?s program with respect to that student?s final correct solution.", "acronym": "ED", "label": "Edit Distance", "ID": "2504"}, {"sentence": "Answer Extraction as Se- quence Tagging with Tree ED.", "acronym": "ED", "label": "Edit Distance", "ID": "2505"}, {"sentence": "In addition, both feature sets include a feature related to the task progress of the student: Task Trajectory for ECR and ED for SR.", "acronym": "ED", "label": "Edit Distance", "ID": "2506"}, {"sentence": "SMO has been de- 1 The pseudocode for SMO may be found in the appendix of Platt (1999) ED (e ?", "acronym": "ED", "label": "Edit Distance", "ID": "2507"}, {"sentence": "Iteration Initial-State ECR Feature Ordering Mean SR  Feature Ordering 1 Last Action Number of Tutor Moves 2 Task Trajectory ED 3 Current Action Last Action 4 Elapsed Idle Time Current Action 5 Number of Tutor Moves Elapsed Idle Time 6 ED Task Trajectory  Table 2.", "acronym": "ED", "label": "Edit Distance", "ID": "2508"}, {"sentence": "The fourth dataset was collected in a clinical con- trolled trial at Cincinnati Children?s Hospital Med- ical Center ED.", "acronym": "ED", "label": "Emergency Department", "ID": "2509"}, {"sentence": "ED  reports and Discharge Summaries contain similar  proportions of historical conditions (17% and 19%  respectively), which might be explained by the fact  that both reports describe a patient?s temporal pro- gression throughout the stay in the Emergency De- partment or the hospital.", "acronym": "ED", "label": "Emergency Department", "ID": "2510"}, {"sentence": "Examples of report sections include Review of Sys- tems (ED), Findings (Opera- tive Gastrointestinal and Radiology) and  Discharge Diagnosis (ED and  Discharge Summary).", "acronym": "ED", "label": "Emergency Department", "ID": "2511"}, {"sentence": "4 Methods  4.1 Dataset Generation  We randomly selected seven reports from each of  six genres of clinical reports dictated at the Univer- sity of Pittsburgh Medical Center during 2007  These included Discharge Summaries, Surgical  Pathology, Radiology, Echocardiograms, Opera- tive Gastrointestinal, and ED  reports.", "acronym": "ED", "label": "Emergency Department", "ID": "2512"}, {"sentence": "status post indi- cates the reason for the transfer as an inpatient  from the ED and the condition  is recent.", "acronym": "ED", "label": "Emergency Department", "ID": "2513"}, {"sentence": "Answers with the getByCategory function 1709 Model k P@1(5) P@1(10) RG - 20.0 10.0 NBOW 50 63.9 47.6 single LSTM 50 68.2 53.9 parallel LSTMs 50 66.9 52.1 Attention LSTMs 50 73.5 62.0 Attention LSTMs (w-by-w) 50 75.1 64.0 LC-LSTMs (Single Direction) 50 75.4 63.0 LC-LSTMs 50 76.1 64.1 three stacked LC-LSTMs 50 78.5 66.2 TC-LSTMs (Single Direction) 50 74.3 62.4 TC-LSTMs 50 74.9 62.9 three stacked TC-LSTMs 50 77.0 65.3 Table 4: Results on Yahoo question-answ", "acronym": "RG", "label": "Random Guess", "ID": "2514"}, {"sentence": "The logistic regression model signifi- cantly outperforms the baselines, but underperforms 2389 Exact Match F1 Dev Test Dev Test RG 1.1% 1.3% 4.1% 4.3% Sliding Window 13.2% 12.5% 20.2% 19.7% Sliding Win.", "acronym": "RG", "label": "Random Guess", "ID": "2515"}, {"sentence": "query to retrieval top 1, 000 re- 1040 Female  gymnast  warm  up  before  a  competition  Gymnast  get  ready  for  a  competition (a) A   female  gymnast  in  black   and  red  being  coach ed  on  bar  s k il l s   T h e  female  gymnast  is  tr a ining (b) Figure 5: Examples of external memory positions attended when encoding the next word pair (bold and marked by a box) Model k P@1(5) P@1(10) RG - 20.0 10.0 NBOW 50 63.9 47.6 single LSTM 50 68.2 53.9 parallel LSTMs 50 66.9 52.1 Attention LSTMs 50 73.5 62.0 Attention(w-by-w) LSTMs 50 75.1 64.0 DF-LSTMs 50 76.5 65.0 Table 4: Results of our proposed model against other neural models on Yahoo!", "acronym": "RG", "label": "Random Guess", "ID": "2516"}, {"sentence": "Madnani et al (2012) used a combination of three basic MT metrics (BLEU, NIST and TER) and five complex MT met- rics (TERp, METEOR, BADGER, MAXISIM, model acc RG 20.00 DeepMatch 34.17 WordEmbed 38.28 SENMLP 34.57 SENNA+MLP 42.09 URAE+MLP 27.41 ARC-I 45.04 ARC-II 50.18 MultiGranCNN 56.27 Table 1: Performance on clause coherence test set.", "acronym": "RG", "label": "Random Guess", "ID": "2517"}, {"sentence": "Incorporating time features further improves the relative accuracy by 104 Model Overall Accuracy RG 10.0% Unigram NLLR 24.1% Filtered NLLR 29.1% MaxEnt Unigram 45.1% MaxEnt Time 48.3% MaxEnt Time+NER 51.4% Joint 53.4% Table 3: Performance as measured by accuracy.", "acronym": "RG", "label": "Random Guess", "ID": "2518"}, {"sentence": "First, the topic of email RG in the context of customer care has been investigated by (Coch, 1996; Lapalme and Kosseim, 2003; Zukerman and Marom, 2007).", "acronym": "RG", "label": "response generation", "ID": "2519"}, {"sentence": "The typed input is sent to NUBEE which queries the domain reasoner, BEER, and dialogue history to help build a logical form which it sends to the dialogue manager of the system, the central component of BEETLE?s RG.", "acronym": "RG", "label": "response generation", "ID": "2520"}, {"sentence": "In this paper we focus on just two aspects of  RG that rely on our model of the  learner's generation process.", "acronym": "RG", "label": "response generation", "ID": "2521"}, {"sentence": "Keywords  tailoring RG, user modeling,  computer-assisted language learning  1 Introduction  Our long-term goal is to develop a computer-  assisted language learning (CALL) tool to help  deaf students leam written English.", "acronym": "RG", "label": "response generation", "ID": "2522"}, {"sentence": "We argue that  these two models can be used to \"explain\" the stu-  dent's sentence generation capabilities and should  affect the system's RG as well.", "acronym": "RG", "label": "response generation", "ID": "2523"}, {"sentence": "In contrast, in this paper, due to constraints from the deployment environment, we rely on a template-based approach to RG.", "acronym": "RG", "label": "response generation", "ID": "2524"}, {"sentence": "These word pairs are a subset of 65 word pairs used by (RG, 1965), in a similar study almost 25 years earlier.", "acronym": "RG", "label": "Rubenstein and Goodenough", "ID": "2525"}, {"sentence": "Human similarity ratings: We use the dataset of RG (1965), consist- ing of averages of subject similarity ratings for 65 noun pairs.", "acronym": "RG", "label": "Rubenstein and Goodenough", "ID": "2526"}, {"sentence": "the Miller and Charles study as well as the RG research.", "acronym": "RG", "label": "Rubenstein and Goodenough", "ID": "2527"}, {"sentence": "License details: http: //creativecommons.org/licenses/by/4.0/ of RG (1965) measure similarity between word pairs, while the data sets of Navigli (2006) and Kilgarriff (2001) offer a bi- nary similar-dissimilar distinction between senses.", "acronym": "RG", "label": "Rubenstein and Goodenough", "ID": "2528"}, {"sentence": "Second, there are two gold standards for the Miller and Charles (1991) set: one has the scores assigned during the original experiment run by RG (1965), the other has the scores assigned during Miller and Charles (1991)?s own experiment.", "acronym": "RG", "label": "Rubenstein and Goodenough", "ID": "2529"}, {"sentence": "Gurevych (2005) conducted experiments with two datasets: i) a German translation of the English dataset by RG (1965) (Gur65), and ii) a larger dataset containing 350 word pairs (Gur350).", "acronym": "RG", "label": "Rubenstein and Goodenough", "ID": "2530"}, {"sentence": "They used the test data consisting of 10 En- glish and Japanese verbs taken from RG?s The- saurus and BGH (Bunrui Goi Hyo) (BGH, 1989).", "acronym": "RG", "label": "Roget", "ID": "2531"}, {"sentence": "2.2 A Thesaurus -based  Approach   Morris and Hirst \\[1991\\] used RG's thesaurus as  knowledge base for determining whether or not two  words are semantically related.", "acronym": "RG", "label": "Roget", "ID": "2532"}, {"sentence": "cats(n2) P(t1|p)P(t2|p)(7) Here, t1 and t2 represent concepts in RG?s thesaurus.", "acronym": "RG", "label": "Roget", "ID": "2533"}, {"sentence": "Fre- quencies for the two models were obtained from the same corpus and from RG?s thesaurus (version 1911) by counting pairs of nouns that are either strictly adjacent or co-occur within a window of a fixed size (e.g., two, three, fifty, or hundred words).", "acronym": "RG", "label": "Roget", "ID": "2534"}, {"sentence": "An alternative to manual classification is  using on-line thesaura, such as RG's and  Wordnet categories in English 3.", "acronym": "RG", "label": "Roget", "ID": "2535"}, {"sentence": "The choices could come from various inventories of near-synonyms or similar words, for example the RG thesaurus (RG, 1852), dictionaries of syn- onyms (Hayakawa, 1994), or clusters acquired from corpora (Lin, 1998).", "acronym": "RG", "label": "Roget", "ID": "2536"}, {"sentence": "By collaps- ing monotone gap and RG into neutral, it can be thought of as a local reordering model sim- ilar to the block orientation bigram (Tillmann and Zhang, 2005).", "acronym": "RG", "label": "reverse gap", "ID": "2537"}, {"sentence": "Since non-local reorderings such as monotone gap and RG are more frequent in Japanese to English translations, they are worth modeling ex- plicitly in this reordering model.", "acronym": "RG", "label": "reverse gap", "ID": "2538"}, {"sentence": "Combination of  FSA and Neural Network for  Spoken Language Understanding.", "acronym": "FSA", "label": "Finite State Automata", "ID": "2539"}, {"sentence": "FSA The fsa module defines a data type for encod- ing finite state automata; and an interface for creating automata from regular expressions.", "acronym": "FSA", "label": "Finite State Automata", "ID": "2540"}, {"sentence": "From Signatures to  FSA.", "acronym": "FSA", "label": "Finite State Automata", "ID": "2541"}, {"sentence": "Language Independent Text Correction us- ing FSA.", "acronym": "FSA", "label": "Finite State Automata", "ID": "2542"}, {"sentence": "Language Independent Text Correction using FSA Ahmed Hassan?", "acronym": "FSA", "label": "Finite State Automata", "ID": "2543"}, {"sentence": "007 Mixed Initiative in Dialogue: An Investigation into Discourse Segmentation (ACL90), M. Walker, S. Whittaker 63 9504017 A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances (ACL95), D. Marcu, G. Hirst 64 9504024 A Morphographemic Model for Error Correction in Nonconcatenative Strings (ACL95), T. Bowden, G. Kiraz 65 9504026 The Intersection of FSA and Definite Clause Grammars (ACL95), G. van Noord 66 9504027 An Efficient Generation Algorithm for Lexicalist MT (ACL95), V. Poznanski et al 67 9504030 Statistical Decision-Tree Models for Parsing (ACL95), D. Magerman 68 9504033 Corpus Statistics Meet the Noun Compound: Some Empirical Results (ACL95), M. Lauer 79 9504034 Bayesian Grammar Induction for Language Modeling (ACL9", "acronym": "FSA", "label": "Finite State Automata", "ID": "2544"}, {"sentence": "These FSA are then converted into transducers by modifying the arcs: split the labels of each arc, x:y, making x the input label for that arc, and y the output label.", "acronym": "FSA", "label": "finite state acceptors", "ID": "2545"}, {"sentence": "The  output associated by v to an input string, z, is obtained by concatenating the output strings  of the edges of r that are used to parse the successive symbols of z.  One problem of using Finite State Transducers in our framework is that the problem of  learning of general Finite State Transducers i at least as hard as the problem of learning  a general FSAn, which is well known to be probably intractable.", "acronym": "FSA", "label": "Finite State Automato", "ID": "2546"}, {"sentence": "Hobbs, Douglas Appelt, Mabry Tyson, John Bear, and David Israel SRI International Menlo Park, California 94025 hobbs?ai .sri .com (415) 859-222 9 INTRODUCTIO N FASTUS is a (slightly permuted) acronym for FSAn Text Understanding System .", "acronym": "FSA", "label": "Finite State Automato", "ID": "2547"}, {"sentence": "E-Systems' engineers address the overall  military message traffic processing problem, while  SRI Interation~l's computational linguists adapt  their FSAn Text Understanding  System (FASTUS) technology to the domain of mil-  itary message free-text.", "acronym": "FSA", "label": "Finite State Automato", "ID": "2548"}, {"sentence": "An example of FSAn to recognize open series compounds.", "acronym": "FSA", "label": "Finite State Automato", "ID": "2549"}, {"sentence": "Since each position set can be adjacent to at most four other position sets, such a DS has size O(|p|).", "acronym": "DS", "label": "data structure", "ID": "2550"}, {"sentence": "We further adopted an efficient DS, spectral bloom filter, to estimate the gradients for the candidate features without generating them.", "acronym": "DS", "label": "data structure", "ID": "2551"}, {"sentence": "The main advantage of this approach  is the extreme simplicity both of its DSs  and of their interpretation.", "acronym": "DS", "label": "data structure", "ID": "2552"}, {"sentence": "For example, the  lexical atoms extracted by this process from the  CACM corpus (about 1 MB) include \"operating  system\", \"DS\", \"decision table\", \"data  base\", \"real time\", \"natural anguage\", \"on line\",  \"least squares\", \"numerical integration\", and \"fi-  nite state automaton\", among others.", "acronym": "DS", "label": "data structure", "ID": "2553"}, {"sentence": "An example of 4 blocks obtained from the training data is shown in 1To apply the restrictions exhaustively, we have imple- mented tree-based DSs to store the 23 million blocks with phrases of up to length 8 in about 1.6 gigabyte of RAM.", "acronym": "DS", "label": "data structure", "ID": "2554"}, {"sentence": "in our DSs Ve and Va is then per- formed in constant time, as described above.", "acronym": "DS", "label": "data structure", "ID": "2555"}, {"sentence": "Since players are en- couraged to produce as many terms per image, the dataset?s increased coverage is at the expense of accuracy in the word-to-image mapping: a dog in a field with a house in the background might be a golden retriever in ImageNet and could have tags dog, golden retriever, grass, field, house, door in the ESP DS.", "acronym": "DS", "label": "Dataset", "ID": "2556"}, {"sentence": "5 Experimental Study 5.1 DS We crawl 77,308 character riddles including riddle descriptions with its solution from the Web.", "acronym": "DS", "label": "Dataset", "ID": "2557"}, {"sentence": "URL: http: //ebiquity.umbc.edu/resource/html/ id/212/Splog-Blog-DS.", "acronym": "DS", "label": "Dataset", "ID": "2558"}, {"sentence": "For evaluation, 10-fold cross valida- DS Relaxed Exact lex pos lex pos Baseline 0.625 0.250 QUOTES1 0.869 0.883 0.445 0.373 QUOTES2 0.877 0.831 0.450 0.435 BOTH 0.886 0.858 0.464 0.383 Table 4: Age estimation: average accuracy.", "acronym": "DS", "label": "Dataset", "ID": "2559"}, {"sentence": "4.1 DSs Used The datasets used for our experiments along with the related tasks are presented in Table 2.", "acronym": "DS", "label": "Dataset", "ID": "2560"}, {"sentence": "3.2 DSs  To construct our corpus, we collected news arti- cles from news clusters on the World Wide Web.", "acronym": "DS", "label": "Dataset", "ID": "2561"}, {"sentence": "Measures of DS.", "acronym": "DS", "label": "Distributional Similarity", "ID": "2562"}, {"sentence": "4.4 DS Distributional similarity is based on the distribu- tional hypothesis that similar terms appear in simi- 3Consider only those pairs in which one word appears in the seed list and the other word appears in the test set.", "acronym": "DS", "label": "Distributional Similarity", "ID": "2563"}, {"sentence": "3.2 DS Here, we present the computation of the distribu- tional similarity between terms using three graphs.", "acronym": "DS", "label": "Distributional Similarity", "ID": "2564"}, {"sentence": "Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) 0.74 (reimplemented in (Yeh et al 2009)) 0.71 Compact Hierarchical ESA (Liberman and Markovitch, 2009) 0.71 Hyperlink Graph (Milne and Witten, 2008) 0.69 Graph Traversal (Agirre et al 2009)) 0.66 DS (Agirre et al 2009)) 0.65 Latent Semantic Analysis (Finkelstein et al 2002) 0.56 Random Graph Walk (Hughes and Ramage, 2007) 0.55 Normalized Path-length (lch) (Strube and Ponzetto, 2006) 0.55 cPMId(? :", "acronym": "DS", "label": "Distributional Similarity", "ID": "2565"}, {"sentence": "Improving DS with Lessons Learned from Word Embeddings.", "acronym": "DS", "label": "Distributional Similarity", "ID": "2566"}, {"sentence": "4 DS Various similarity measures have been proposed and used for NLP tasks (Korhonen, 2002).", "acronym": "DS", "label": "Distributional Similarity", "ID": "2567"}, {"sentence": "Attention, intentions, and the structure of  DS.", "acronym": "DS", "label": "discourse", "ID": "2568"}, {"sentence": "References  by pronouns are closely related to the topic or  the center of the DS.", "acronym": "DS", "label": "discourse", "ID": "2569"}, {"sentence": "The method is a very simple mechanism  for harvesting the kind of gender information  present in DS fragments like \"Kim slept.", "acronym": "DS", "label": "discourse", "ID": "2570"}, {"sentence": "They use Japanese newspa-  per articles tagged with DS information  as training examples for a machine-learning al- gorithm which is the C4.5 decision-tree algo-  rithm by Quinlan (1993).", "acronym": "DS", "label": "discourse", "ID": "2571"}, {"sentence": "However, any syntax-only pronoun resolution  strategy will be wrong some of the time - these  methods know nothing about DS bound-  aries, intentions, or real-world knowledge.", "acronym": "DS", "label": "discourse", "ID": "2572"}, {"sentence": "The system  is not entirely \"statistical\" in that it consists of  various types of rule-based knowledge -- syn-  tactic, semantic, domain, DS, and heuris-  tic.", "acronym": "DS", "label": "discourse", "ID": "2573"}, {"sentence": "Examples of report sections include Review of Sys- tems (Emergency Department), Findings (Opera- tive Gastrointestinal and Radiology) and  Discharge Diagnosis (Emergency Department and  DS).", "acronym": "DS", "label": "Discharge Summary", "ID": "2574"}, {"sentence": "Excerpt  of  DS 55   the patient noted that he had a recurrence of this  56   vague chest discomfort as he was sitting and  57   talking to friends.", "acronym": "DS", "label": "Discharge Summary", "ID": "2575"}, {"sentence": "un ruhe 'anyway'   shouxian 'first', qici \"next\"  huan ju hua shuo 'in other words'  zhengru 'just as'  nandao ('does it mean... ')   kexi 'unfortunately'  and Associated Rhetorical Relations in Chinese  It may be noted that our analysis of Chinese  has yielded about 150 discourse markers, and that  on the average, argumentative t xt (e.g. editorials)  in Chinese shows more than one third of the  DSs to contain discourse markers.", "acronym": "DS", "label": "discourse segment", "ID": "2576"}, {"sentence": "be a linear sequence of clauses and sentences, it  has \"long been recognized by linguists that these  clauses and sentences tend to cluster together into  units, called DSs, that are related  pragmatically toform a hierarchical structure.", "acronym": "DS", "label": "discourse segment", "ID": "2577"}, {"sentence": "From the above tagging, we can obtain the  following discourse structure with embedding  relations:  A dversativity ( &F (14 ),  Sufficiency(F rontClause(15),  BackClause(15)))  where &F(n) denotes the Front DS  of an inter-sentence rhetorical relation whose  sequence number is n. We can define &B(n)  similarly.", "acronym": "DS", "label": "discourse segment", "ID": "2578"}, {"sentence": "The application of dot-  plotting to DSation can be performed ei-  ther manually, by examining a graph, or automatically,  using an optimization algorithm.", "acronym": "DS", "label": "discourse segment", "ID": "2579"}, {"sentence": "The function of discourse analysis is to  divide a text into DSs, and to  recognize and re-construct the discourse structure  of the text as intended by its author.", "acronym": "DS", "label": "discourse segment", "ID": "2580"}, {"sentence": "cognized by linguists that these  clauses and sentences tend to cluster together into  units, called DSs, that are related  pragmatically toform a hierarchical structure.", "acronym": "DS", "label": "discourse segment", "ID": "2581"}, {"sentence": "The function of discourse analysis is to  divide a text into DSs, and to  recognize and re-construct the discourse structure  of the text as intend", "acronym": "DS", "label": "discourse segment", "ID": "2582"}, {"sentence": "We do not argue that DSs can never take on a  hierarchical form.", "acronym": "DS", "label": "discourse segment", "ID": "2583"}, {"sentence": "Springer Verlag, Berlin, Heidelberg, New York, AI edition, 2000.", "acronym": "AI", "label": "artificial Intelligence", "ID": "2584"}, {"sentence": "artificial AI,?", "acronym": "AI", "label": "artificial intelligence", "ID": "2585"}, {"sentence": "There is hardly a  literature to cite, unless it be that unruly assemblage we  have come to call \"AI.\"", "acronym": "AI", "label": "artificial intelligence", "ID": "2586"}, {"sentence": "Having discovered this venue, many researchers in the fields of AI and machine learning see MTurk as a valuable and effective source of annotations, labels, and data, namel", "acronym": "AI", "label": "artificial intelligence", "ID": "2587"}, {"sentence": "They were selected  on the criteria that they (1) have demonstrated problem-solving skills by having suc-  cessfully completed a computer science course and having enrolled in another, (2) not  have excessive familiarity with AI or natural language processing as  would occur, for example, if they had had a course on one of these topics, and (3)  not be an electrical engineering major (in which case they could probably repair the  circuits without aid).", "acronym": "AI", "label": "artificial intelligence", "ID": "2588"}, {"sentence": "Some philosophical problems from the standpoint of AI.", "acronym": "AI", "label": "artificial intelligence", "ID": "2589"}, {"sentence": "Having discovered this venue, many researchers in the fields of AI and machine learning see MTurk as a valuable and effective source of annotations, labels, and data, namely the kind requiring human knowledge.", "acronym": "AI", "label": "artificial intelligence", "ID": "2590"}, {"sentence": "Discourse in computational linguistics and AI.", "acronym": "AI", "label": "artificial intelligence", "ID": "2591"}, {"sentence": "In Proceedings,  5th National Conference ofthe American  AAAI.", "acronym": "AAAI", "label": "Association for Artificial Intelligence", "ID": "2592"}, {"sentence": "of the Italian AAAI, Reggio Emilia, Italy.", "acronym": "AAAI", "label": "Association for Artificial Intelligence", "ID": "2593"}, {"sentence": "In Proceedings of the American AAAI, September.", "acronym": "AAAI", "label": "Association for Artificial Intelligence", "ID": "2594"}, {"sentence": "In Proceedings of EVALITA Workshop, 11th Congress of Italian AAAI, Reggie Emilia.", "acronym": "AAAI", "label": "Association for Artificial Intelligence", "ID": "2595"}, {"sentence": "of the American AAAI, pages 984-989.", "acronym": "AAAI", "label": "Association for Artificial Intelligence", "ID": "2596"}, {"sentence": "Of the 17th Annual Confer- ence of the American AAAI.", "acronym": "AAAI", "label": "Association for Artificial Intelligence", "ID": "2597"}, {"sentence": "Erom a psychological point of view, however, there  are strong reasons for maintaining the distinction  between these two kinds of knowledge (Anderson,  1976:116-119):  - the DK seems possessed in  all-or-none manner whereas i t  is possible to  possess procedural knowledge only partial ly;  - the DK is acquired suddenly  by being told whereas the procedural knowledge can  be acquired only gradually by performing a skit1;  - i t  is possible to communicate verbally the  declarative but not the procedural knowledge.", "acronym": "DK", "label": "declarative knowledge", "ID": "2598"}, {"sentence": "Procedural and DK are  inter-related, and are processed by an inference engine  based on a modified \"recognize-acts\" cycle, which includes  matching, conflict resolution, and execution phases.", "acronym": "DK", "label": "declarative knowledge", "ID": "2599"}, {"sentence": "We will look at  ways in which Constrained Conditional Models can be used to augment  probabilistic models with DK based constraints and how these  support expressive global decisions.", "acronym": "DK", "label": "declarative knowledge", "ID": "2600"}, {"sentence": "However they rely on hand-coded DK bases.", "acronym": "DK", "label": "declarative knowledge", "ID": "2601"}, {"sentence": "e DK seems possessed in  all-or-none manner whereas i t  is possible to  possess procedural knowledge only partial ly;  - the DK is acquired suddenly  by being told whereas the procedural knowledge can  be acquired only gradually by performing a skit1;  - i t  is possible to communicate verbally the  declarative but not the procedural knowledge.", "acronym": "DK", "label": "declarative knowledge", "ID": "2602"}, {"sentence": "In ALICE the DK is  constituted by the information that the system is  able to derive from the texts.", "acronym": "DK", "label": "declarative knowledge", "ID": "2603"}, {"sentence": "7.6%  + Stemming 26.1% 59.5% 36.3%     + Proper Nouns 36.5% 56.8% 44.4%  Named Entities 48.4% 49.1% 48.7%  All Combined 21.1% 65.0% 31.9%  Manual Scoring 67.0% 75.0% 70.8%      Single word SVM 19.0% 30.0% 23.3%  + Stemming 22.0% 30.2% 25.5%     + Proper Nouns 46.3% 54.0% 49.9%  Named Entities 60.1% 41.5% 49.1%  All Combined 20.3% 65.7% 31.0%  Manual Scoring 47.0% 62.0% 53.5%  Figure 1: Results on DK (top) and Red State  (bottom) data.", "acronym": "DK", "label": "Daily Kos", "ID": "2604"}, {"sentence": "Red State is a conserva- tive political blog whereas DK is a liberal  political blog.", "acronym": "DK", "label": "Daily Kos", "ID": "2605"}, {"sentence": "We collected a  total of 100,000 blog posts from DK and  70,000 blog posts from Red State and a total of  787,780 tags across both blogs (an average of 4.63  tags per post).", "acronym": "DK", "label": "Daily Kos", "ID": "2606"}, {"sentence": "ajor political blogs,  DK (www.dailykos.com) and Red State  (www.redstate.com).", "acronym": "DK", "label": "Daily Kos", "ID": "2607"}, {"sentence": "For instance, in the top- ics induced from DK, a very liberal leaning blog, we see that the most negative topic (i.e. the topic that contributes the most to potential nega- tive comments) talks about the Bush adminstration and Vice President Cheney, which was and remains quite unpopular with people from the left.", "acronym": "DK", "label": "Daily Kos", "ID": "2608"}, {"sentence": "5 Results and Evaluation  For evaluating our methods, we used 2,681 posts  from DK and 571 posts from Red State.", "acronym": "DK", "label": "Daily Kos", "ID": "2609"}, {"sentence": "3 Data  We collected data from two major political blogs,  DK (www.dailykos.com) and Red State  (www.redstate.com).", "acronym": "DK", "label": "Daily Kos", "ID": "2610"}, {"sentence": "We collected a  total of 100,000 blog posts from DK and  70,000 b", "acronym": "DK", "label": "Daily Kos", "ID": "2611"}, {"sentence": "7.6%  + Stemming 26.1% 59.5% 36.3%     + Proper Nouns 36.5% 56.8% 44.4%  Named Entities 48.4% 49.1% 48.7%  All Combined 21.1% 65.0% 31.9%  Manual Scoring 67.0% 75.0% 70.8%      Single word SVM 19.0% 30.0% 23.3%  + Stemming 22.0% 30.2% 25.5%     + Proper Nouns 46.3% 54.0% 49.9%  Named Entities 60.1% 41.5% 49.1%  All Combined 20.3% 65.7% 31.0%  Manual Scoring 47.0% 62.0% 53.5%  Figure 1: Results on DKs (top) and Red State  (bottom) data.", "acronym": "DK", "label": "Daily Ko", "ID": "2612"}, {"sentence": "Red State is a conserva- tive political blog whereas DKs is a liberal  political blog.", "acronym": "DK", "label": "Daily Ko", "ID": "2613"}, {"sentence": "We collected a  total of 100,000 blog posts from DKs and  70,000 blog posts from Red State and a total of  787,780 tags across both blogs (an average of 4.63  tags per post).", "acronym": "DK", "label": "Daily Ko", "ID": "2614"}, {"sentence": "ajor political blogs,  DKs (www.dailykos.com) and Red State  (www.redstate.com).", "acronym": "DK", "label": "Daily Ko", "ID": "2615"}, {"sentence": "For instance, in the top- ics induced from DKs, a very liberal leaning blog, we see that the most negative topic (i.e. the topic that contributes the most to potential nega- tive comments) talks about the Bush adminstration and Vice President Cheney, which was and remains quite unpopular with people from the left.", "acronym": "DK", "label": "Daily Ko", "ID": "2616"}, {"sentence": "5 Results and Evaluation  For evaluating our methods, we used 2,681 posts  from DKs and 571 posts from Red State.", "acronym": "DK", "label": "Daily Ko", "ID": "2617"}, {"sentence": "3 Data  We collected data from two major political blogs,  DKs (www.dailykos.com) and Red State  (www.redstate.com).", "acronym": "DK", "label": "Daily Ko", "ID": "2618"}, {"sentence": "We collected a  total of 100,000 blog posts from DKs and  70,000 b", "acronym": "DK", "label": "Daily Ko", "ID": "2619"}, {"sentence": "Another, and perhaps more serious example concerns so-called -nie/-cie VBGs, i.e., substan- tiva verbalia (Puzynina, 1969) such as pi?c::picie ?", "acronym": "VBG", "label": "gerund", "ID": "2620"}, {"sentence": "on a particular understanding of the notion of lex- eme, various deverbal forms, such as participles and VBGs.", "acronym": "VBG", "label": "gerund", "ID": "2621"}, {"sentence": "For example, a typical Polish verbal lexeme contains a number of personal forms, a number of impersonal forms, as well as, depending on a particular understanding of the notion of lex- eme, various deverbal forms, such as participles and VBGs.", "acronym": "VBG", "label": "gerund", "ID": "2622"}, {"sentence": "it is often not clear whether VBGs are ?", "acronym": "VBG", "label": "gerund", "ID": "2623"}, {"sentence": "ts of such lexemes consisting of those forms which have the same inflectional proper- ties: all verbal forms of given lexeme with the inflectional category of person and number are grouped into one flexeme, other forms belong- ing to this lexeme, but with adjectival inflectional properties, are grouped into another flexeme, those forms, which inflect for case but not for gender are grouped into a VBGial flexeme, etc.", "acronym": "VBG", "label": "gerund", "ID": "2624"}, {"sentence": "We selected all adjectives in the cor- pus with more than 50 occurences (2283 lemmata), including some VBGs and participles with a pre- dominant modifying function (for more details on the selection criteria, cf.", "acronym": "VBG", "label": "gerund", "ID": "2625"}, {"sentence": "These forms have very different morphosyntactic properties: finite non-past tense forms have the inflectional categories of person and number, adjectival participles have the inflec- tional properties of non-gradable adjectives and, additionally, inflect for negation and have aspect, VBGs inflect for case and, at least potentially, for number, but not for person, etc.", "acronym": "VBG", "label": "gerund", "ID": "2626"}, {"sentence": "Following Rhetorical Structure Theory (RST; Mann and Thompson 1988), they represent documents by trees whose leaves correspond to EDUs (edus) and whose nodes specify how these and larger units (e.g., multi-sentence segments) are linked to each other by rhetorical relations (e.g., Contrast, Elaboration).", "acronym": "EDUs", "label": "elementary discourse units", "ID": "2627"}, {"sentence": "We stud- ied how preferences are linguistically expressed in EDUs on two different cor- pus genres: one already available, the Verbmobil corpus and the Booking corpus purposely built for this project.", "acronym": "EDUs", "label": "elementary discourse units", "ID": "2628"}, {"sentence": "Each discourse analy-  sis yielded an average of 52 EDUs.", "acronym": "EDUs", "label": "elementary discourse units", "ID": "2629"}, {"sentence": "Table 1 displays average kappa  statistics that reflect the reliability of the annota-  tion of EDUs, k~,, hierarchical  discourse spans, ks, hierarchical nuclearity assign-  ments, k,~, and hierarchical rhetorical relation as-  signments, k~. Kappa figures higher than 0.8 corre-  spond to good agreement; kappa figures higher than  0.6 correspond to acceptable agreement.", "acronym": "EDUs", "label": "elementary discourse units", "ID": "2630"}, {"sentence": "4 This is the decision-based parser described in Marcu (2000); it achieves an F1 of 38.2 for the identification of EDUs, 50.0 for hierarchical spans, 39.9 for nuclearity, and 23.4 for relation assignment.", "acronym": "EDUs", "label": "elementary discourse units", "ID": "2631"}, {"sentence": "Our Dis- course Parser uses a symbolic approach and pro- duces discourse trees, which include nuclearity,  but lacking rhetorical relation names: intermedi- ate nodes in the discourse tree have no name and  terminal nodes are EDUs,  mainly clauses.", "acronym": "EDUs", "label": "elementary discourse units", "ID": "2632"}, {"sentence": "1In the TRAINING portion of the RST Treebank, we found 17213 EDUs (EDU?s).", "acronym": "EDUs", "label": "Elementary Discourse Units", "ID": "2633"}, {"sentence": "143  From EDUs to Complex Ones  Holger  Schauer   Computational Linguistics Division  Freiburg University  D-79085 Freiburg, Germany  s c\\]~auer@coling, uni-freiburg, de  Abst ract   Coherence relations have usually  been taken to link clauses and larger  units.", "acronym": "EDUs", "label": "Elementary Discourse Units", "ID": "2634"}, {"sentence": "The steps carried out for the annotation of the corpora were the following: A. EDUs segmentation.", "acronym": "EDUs", "label": "Elementary Discourse Units", "ID": "2635"}, {"sentence": "In particular, we split all reasons into several reason units by simple preprocessing (splitting us- ing Stanford CoreNLP (Manning et al, 2014), seg- mentation into EDUs by RST tools (Surdeanu et al, 2015)) and identified the ref- erenced arguments (A1 or A2) by pattern matching and dependency parsing.", "acronym": "EDUs", "label": "Elementary Discourse Units", "ID": "2636"}, {"sentence": "However in the actual situation of  NLU process, complete  knowlegde cannot be given, but only partial  knowledge is available.", "acronym": "NLU", "label": "natural language understanding", "ID": "2637"}, {"sentence": "1 In t roduct ion   This paper describes a NLU sys-  tem for the domain of naive thermodynamics.", "acronym": "NLU", "label": "natural language understanding", "ID": "2638"}, {"sentence": "Conceptual dependency: A the- ory of NLU.", "acronym": "NLU", "label": "natural language understanding", "ID": "2639"}, {"sentence": "Differentiat- ing between implicative and non-implicative verbs is therefore an essential component of NLU, relevant to applications such as textual entailment and summarization.", "acronym": "NLU", "label": "natural language understanding", "ID": "2640"}, {"sentence": "The system core is the Dialogue Manager, which processes the information coming from the different input modality agents by means of a NLU module and provides output in the appropriate modality.", "acronym": "NLU", "label": "natural language understanding", "ID": "2641"}, {"sentence": "Nevertheless, they are key to various NLP applications, including those benefiting from deep NLU (e.g., textual inference (Bobrow et al, 2007)), generation of well- formed output (e.g., natural language weather alert systems (Lareau and Wanner, 2007)) or both (as in machine translation (Oepen et al, 2007)).", "acronym": "NLU", "label": "natural language understanding", "ID": "2642"}, {"sentence": "Hirst G. (1981) \"Discourse Oriented Anaphoral  Resolution in NLU:  A Review.\"", "acronym": "NLU", "label": "Natural Language Understanding", "ID": "2643"}, {"sentence": "Sabah G~rard 1997, The fundamental role of  pragmatics in NLU  and its implications for modular, cognitively mo-  tivated architectures, Studies in Computational  Pragmatics: Abduction, Belief, and Context, Uni-  versity College Press, to appear, London.", "acronym": "NLU", "label": "Natural Language Understanding", "ID": "2644"}, {"sentence": "44  Domain Dependent NLU  Klaus Heje Munch  Dept.", "acronym": "NLU", "label": "Natural Language Understanding", "ID": "2645"}, {"sentence": "Small, S. 1980 Word Expert Parsing: a Theory of Distributed Word-  Based NLU.", "acronym": "NLU", "label": "Natural Language Understanding", "ID": "2646"}, {"sentence": "Pages  12-20 in Recent Developments and Applications of  NLU, edited by Jeremy  Peckham.", "acronym": "NLU", "label": "Natural Language Understanding", "ID": "2647"}, {"sentence": "But there is no clear process for identifying potential tasks (other than consensus by a sufficient num- ber of researchers), nor for quantifying their po- tential contribution to existing NLP tasks, let alne to NLU.", "acronym": "NLU", "label": "Natural Language Understanding", "ID": "2648"}, {"sentence": "We show how such a DM can be used for topic identification of unseen calls.", "acronym": "DM", "label": "domain model", "ID": "2649"}, {"sentence": "The DM is comprised of pri- marily a topic taxonomy where every node is char- acterized by topic(s), typical Questions-Answers (Q&As), typical actions and call statistics.", "acronym": "DM", "label": "domain model", "ID": "2650"}, {"sentence": "We note that full parsing is applied in all systems, with the specific choice of the parser of Charniak and Johnson (2005) with the biomedical DM of McClosky (2009) and conversion into the Stan- ford Dependency representation (de Marneffe et al, 2006) being adopted by five participants.", "acronym": "DM", "label": "domain model", "ID": "2651"}, {"sentence": "Each such domain generally has a DM which is essential to handle customer complaints.", "acronym": "DM", "label": "domain model", "ID": "2652"}, {"sentence": "Towards this, we propose an unsupervised technique to generate DMs automati- cally from call transcriptions.", "acronym": "DM", "label": "domain model", "ID": "2653"}, {"sentence": "The DM is comprised of pri- marily a topic taxonomy where ever", "acronym": "DM", "label": "domain model", "ID": "2654"}, {"sentence": "The links actually shown in tilt; figure are based  on the, procedural relations in the DM.", "acronym": "DM", "label": "domain model", "ID": "2655"}, {"sentence": "This, however, appears to run  counter to what we expect from results reported in  prior work on discourse(Kurohashi and Nagao, 1994;  Litman and Passonneau, 1995; Grosz and Sidner,  1986; Marcu, 1997), where the notion of clues or  cue phrases forms an important part of identifying  a structure of discourse7  Table 4 shows how the confidence value (CF) af-  fects the performance of DMs.", "acronym": "DM", "label": "discourse model", "ID": "2656"}, {"sentence": "We anchor a referring expression  like ref()~D(named(D, Marg)&card(D, 1))) in  tile DM by proving the existence of  an entity in the model which satisfies the prop-  erties specified by the referring expression, in  this case aD(na,~ed(D, Mary)~ea,'d(D, 1)) 2.", "acronym": "DM", "label": "discourse model", "ID": "2657"}, {"sentence": "Models and  DMs, dournal of Language and  Computation, 1(2):159-174.", "acronym": "DM", "label": "discourse model", "ID": "2658"}, {"sentence": "63  Semantic analyses can account for these cases if  nominalizations are assumed to evoke event repre-  sentations into the DM.", "acronym": "DM", "label": "discourse model", "ID": "2659"}, {"sentence": "vx((ho,, e(x) v car(X))  qY (of (Y, AZ(door( Z) ), X)~eard(Y, 1)))  This means that, having used utterance (1)  above to update the DM, we have  the fbllowing amongst the facts in Discourse  State 1:  Discourse State 1  seel(#138)  0(#138, agent, #94)  0(#138, object, #139)  card(#139, 1) house( #139)  ends_be for'e(#4(1), #sat )   door(#46(#139))  entrance( #46( #139 ) )  of(#46(#139),  ~d(door'(A)), #139)  card(#46(#139),  1   aspect(simple, #137, #138)  In updating utterance (2), the bridging descrip-  tion whi", "acronym": "DM", "label": "discourse model", "ID": "2660"}, {"sentence": "S&H claim that the ellipsis resolution pro-  cess obtains referents from propositional represen-  tations, whereas what they term Model Interpre-  tive Anaphora (MIA) (e.g., 'do it' anaphora) ob-  tains referents from the DM.", "acronym": "DM", "label": "discourse model", "ID": "2661"}, {"sentence": "1 Introduction Determining when to make a DM is a topic of growing importance in dialogue systems.", "acronym": "DM", "label": "dialogue move", "ID": "2662"}, {"sentence": "Because turn boundaries are not clearly defined or enforced, we apply RL to the problem of when to make a DM, rather than what type of DM to make.", "acronym": "DM", "label": "dialogue move", "ID": "2663"}, {"sentence": "To date, RL has been applied to learn the most effective DM to make, but has not been applied to learning the timings of these moves, although the related con-cept of when to release a turn has been explored (English and Heeman 2005).", "acronym": "DM", "label": "dialogue move", "ID": "2664"}, {"sentence": "It consists of 66 textual dia-logues between human tutors and students, with an average of 90 tutor DMs and 36 student DMs.", "acronym": "DM", "label": "dialogue move", "ID": "2665"}, {"sentence": "This strategy assumes that each individual input can be considered as an independent DM.", "acronym": "DM", "label": "dialogue move", "ID": "2666"}, {"sentence": "Our overall approach is also related to many other DM approaches, including those that construct dialog graphs from dialog data via clustering (Lee et al, 2009), learn information state updates using discriminative classification models (Hakkani-Tur et al, 2012; Mairesse et al, 2009), optimize dialog strategy using reinforce- ment learning (RL) (Scheffler and Young, 2002; Rieser and Lemon, 2008), or combine RL with information state", "acronym": "DM", "label": "dialog management", "ID": "2667"}, {"sentence": "However, such systems are challenging to build, currently requiring manual engineering of substantial domain-specific task knowl- edge and DM strategies.", "acronym": "DM", "label": "dialog management", "ID": "2668"}, {"sentence": "3.4 Dialogue Management We have developed servers for DM and to control the application interface for database query.", "acronym": "DM", "label": "dialog management", "ID": "2669"}, {"sentence": "However, such systems are challenging to build, currently requiring expensive, expert engineering of significant domain-specific task knowledge and DM strategies.", "acronym": "DM", "label": "dialog management", "ID": "2670"}, {"sentence": "Williams J, Young S (2003) Using Wizard-of-Oz  simulations to bootstrap Reinforcement-Learning- based DM systems.", "acronym": "DM", "label": "dialog management", "ID": "2671"}, {"sentence": "Re- inforcement learning for spoken DM using least-squares policy iteration and fast feature selection.", "acronym": "DM", "label": "dialog management", "ID": "2672"}, {"sentence": "5.2 DM of geographically significant local linguistic contexts We currently use data mining on tagged corpora to learn the contexts in which geographic and non-geographic ref- erences occur, the words and phrases leading up to and trailing the name n. The tagged corpora were obtained using the Alembic tagger (Day et al, 1997).", "acronym": "DM", "label": "Data mining", "ID": "2673"}, {"sentence": "In Proceedings of the  2004 ACM SIGKDD international conference on  Knowledge Discovery and DM.", "acronym": "DM", "label": "Data mining", "ID": "2674"}, {"sentence": "DM techniques proved  not to be very transparent when digging up  justifications for scores.", "acronym": "DM", "label": "Data mining", "ID": "2675"}, {"sentence": "The elements of statistical learning:  DM, inference, and prediction.", "acronym": "DM", "label": "Data mining", "ID": "2676"}, {"sentence": "DM tools for biological se- quences.", "acronym": "DM", "label": "Data mining", "ID": "2677"}, {"sentence": "In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and DM, pages 430?", "acronym": "DM", "label": "Data mining", "ID": "2678"}, {"sentence": "Asterisks (*) indicate transitions not in the baseline DM.", "acronym": "DM", "label": "dialog manager", "ID": "2679"}, {"sentence": "The key benefit of the POMDP approach is that the DM can exploit the belief state to make better progress in the face of low-confidence or even nonsensical replies, without sacrificing over- all task completion.", "acronym": "DM", "label": "dialog manager", "ID": "2680"}, {"sentence": "A text plan is a set of communicative goals which is assumed to be output by a DM of a spoken dialog system.", "acronym": "DM", "label": "dialog manager", "ID": "2681"}, {"sentence": "We created a state-based DM by hand (called HC) which broadly reflects the agents?", "acronym": "DM", "label": "dialog manager", "ID": "2682"}, {"sentence": "After each step HC checks if the con- nection is working by asking if the network light is green, pinging the modem, then asking the user 6 POMDP HC HC(0) CER 30% 30% 0% N 500 500 500 TCR 96.1% 78.0% 88.6% Length 19.9 76.5 48.5 Return 73.3 8.13 48.8 Table 2: Results for the POMDP and hand-crafted DMs.", "acronym": "DM", "label": "dialog manager", "ID": "2683"}, {"sentence": "... ... ... ... Table 3: Fragment of a conversation with the POMDP DM.", "acronym": "DM", "label": "dialog manager", "ID": "2684"}, {"sentence": "MIMUS follows the Information State Up- date approach to DM, and supports English, German and Spanish, with the possibility of changing language on?the?", "acronym": "DM", "label": "dialogue management", "ID": "2685"}, {"sentence": "3 ISU?based Dialogue Management in MIMUS As pointed out above, MIMUS follows the ISU approach to DM (Larsson and Traum, 2000).", "acronym": "DM", "label": "dialogue management", "ID": "2686"}, {"sentence": "Information State and DM in the TRINDI Dia- logue Move Engine Toolkit.", "acronym": "DM", "label": "dialogue management", "ID": "2687"}, {"sentence": "In language processing, reinforcement learning has been ap- plied to a DM system that con- verses with a human user by taking actions that generate natural language (Scheffler and Young, 2002; Young et al, 2013).", "acronym": "DM", "label": "dialogue management", "ID": "2688"}, {"sentence": "MIMUS follows the Information State Update approach to DM, and has been developed under the EU?funded TALK project (Talk Project, 2004).", "acronym": "DM", "label": "dialogue management", "ID": "2689"}, {"sentence": "Partially observable Markov decision processes with continu- ous observations for DM.", "acronym": "DM", "label": "dialogue management", "ID": "2690"}, {"sentence": "The DAFs are pushed and popped on a single stack, and that simple virtual machine is the DM, where DAFs being pushed, popped, or reentered at a lower stack point are intended to capture the exits from, and returns to, abandoned topics and the movement of conversational initiative between the system and the user.", "acronym": "DM", "label": "Dialogue Manager", "ID": "2691"}, {"sentence": "Integrating OWL Ontologies with a DM.", "acronym": "DM", "label": "Dialogue Manager", "ID": "2692"}, {"sentence": "In our framework, the NLG component must achieve a high-level Communicative Goal from the DM (e.g. to present a number of items) through planning a sequence of lower- level generation steps or actions, for example first to summarize all the items and then to recommend the highest ranking one.", "acronym": "DM", "label": "Dialogue Manager", "ID": "2693"}, {"sentence": "The DM checks the input pool regularly to retrieve the corresponding input.", "acronym": "DM", "label": "Dialogue Manager", "ID": "2694"}, {"sentence": "The in- put to the module is a Communicative Goal supplied by the DM.", "acronym": "DM", "label": "Dialogue Manager", "ID": "2695"}, {"sentence": "The system core is the DM, which processes the information coming from the different input modality agents by means of a natural language understanding module and provides output in the appropriate modality.", "acronym": "DM", "label": "Dialogue Manager", "ID": "2696"}, {"sentence": "This database is  CMPed by an intelligent search procedure  able to handle multimedia representations of  knowledge.", "acronym": "CMP", "label": "complement", "ID": "2697"}, {"sentence": "Subordinate clauses: clausal CMPs of verbs like ?", "acronym": "CMP", "label": "complement", "ID": "2698"}, {"sentence": "The clustering clearly recognizes a majority of objects bearing no CMP and a minority having a regular CMP.", "acronym": "CMP", "label": "complement", "ID": "2699"}, {"sentence": "in the surface structure, as follows: The auxiliary ngiya is subject to the constraints in (2), meaning that it combines with a verb as its first CMP and then the verb?s CMPs as its remaining CMPs.9 The auxiliary can combine with its CMPs in any order, thanks to a series of head- CMP rules which realize the nth element of 6The grammar in fact finds 42 parses for this example.", "acronym": "CMP", "label": "complement", "ID": "2700"}, {"sentence": "Any other mod- ifiers or CMPs (PPs, other adjectives, etc.)", "acronym": "CMP", "label": "complement", "ID": "2701"}, {"sentence": "The PDT method in- troduced in our work is an explicit attempt to build individual translation systems that meet this crite- ria, while being less computationally demanding than the diversity generating techniques explored by Macherey and Och (2007).", "acronym": "PDT", "label": "Positive Diversity Tuning", "ID": "2702"}, {"sentence": "The PDT method explored in this work can be used to tune individual systems for any ensemble in which individual models can be fit to multiple extrinsic loss functions.", "acronym": "PDT", "label": "Positive Diversity Tuning", "ID": "2703"}, {"sentence": "Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 320?328, Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics PDT for Machine Translation System Combination Daniel Cer, Christopher D. Manning and Daniel Jurafsky Stanford University Stanford, CA 94305, USA {danielcer,manning,jurafsky}@stanford.edu Abstract We present PDT, a newmethod for tuningmachine translation models specifically for improved perfor- mance during system combination.", "acronym": "PDT", "label": "Positive Diversity Tuning", "ID": "2704"}, {"sentence": "PDT for Machine Translation System Combination.", "acronym": "PDT", "label": "Positive Diversity Tuning", "ID": "2705"}, {"sentence": "Lexical cat-  egories include noun, verb, pronoun, propernoun,  adjective, adverb, preposition, particle, conjunction,  determiner, cardinal, ordinal, PDT, noun  modifier, and month.", "acronym": "PDT", "label": "predeterminer", "ID": "2706"}, {"sentence": "It surpassed the syntactic model, which did not  provide for a PDT such as \"all\" preceding  another determiner, such as \"your\" or \"the\".", "acronym": "PDT", "label": "predeterminer", "ID": "2707"}, {"sentence": "1 would give: the/DT alien/NN would/MD not/RB use/VB my/PRP$ spaceship/NN but/CC the/DT hers/PRP ./. Finally, words are replaced with their correspond- ing POS tags; for the following words, word to- kens are used as their corresponding POS tags: coordinating conjunctions, determiners, preposi- tions, modals, PDTs, possessives, pro- nouns, question adverbs.", "acronym": "PDT", "label": "predeterminer", "ID": "2708"}, {"sentence": "Thus we first look for all possible PDTs to.g, 'all')  within the available gal) before the corresponding header word,  Again the succe.", "acronym": "PDT", "label": "predeterminer", "ID": "2709"}, {"sentence": "interjection D determiner P pre- or postposition, or subordinating conjunction & coordinating conjunction T verb particle X existential there, PDTs Y X + verbal # hashtag (indicates topic/category for tweet) @ at-mention (indicates a user as a recipient of a tweet) ~ discourse marker, indications of continuation across multiple tweets U URL or email address E emoticon $ numeral , punctuation G other abbreviations, foreign words, possessive endings, symbols, garbage Table 6: POS tagset from Gimpel et al(2011) used in this paper,", "acronym": "PDT", "label": "predeterminer", "ID": "2710"}, {"sentence": "The only change we have made is to turn all ncmod GRs with of as the modifier into iobj GRs (unless the ncmod is a par- titive PDT).", "acronym": "PDT", "label": "predeterminer", "ID": "2711"}, {"sentence": "Turney (2002) suggested comparing the frequency  of  phrase  co-occurrences  with  words  PDT mined  by  the  sentiment  lexicon.", "acronym": "PDT", "label": "predeter-", "ID": "2712"}, {"sentence": "One such method  is the Linguistic Inquiry and Word Count (LIWC)  tool developed by Pennebaker et al, (2001), which  looks for words that fall into specific, PDT mined categories such as COGNITIVE MECHANISMS and POSITIVE EMOTIONS, then reports the percent  of words in the document that fall into that catego- ry.", "acronym": "PDT", "label": "predeter-", "ID": "2713"}, {"sentence": "If the NL tool fails, the Wizard may revise the request  and try again until it is understood, or until a PDT  mined time limit expires (usually around a minute).", "acronym": "PDT", "label": "predeter-", "ID": "2714"}, {"sentence": "This result is then compared with a set of PDT mined textual answers (for that given question, of course) in order to verify the correctness of the pa- tient?s input.", "acronym": "PDT", "label": "predeter-", "ID": "2715"}, {"sentence": "Firstly, it matches the input to a PDT mined raw emoticon database (with over ten thou- sand emoticons).", "acronym": "PDT", "label": "predeter-", "ID": "2716"}, {"sentence": "-best documents: One could choose a PDT mined number \u0001 of the best matching English doc- uments for each Mandarin story.", "acronym": "PDT", "label": "predeter-", "ID": "2717"}, {"sentence": "While phoneme error rates are generally higher than LER, it should be noted that the reference baseforms for the names contain only one or two alternate pronunciations for each name.", "acronym": "LER", "label": "letter error rates", "ID": "2718"}, {"sentence": "Figure 2 shows the number of letters in the ref- erence (top), number of letter errors (middle), and LER (bottom) for each group.", "acronym": "LER", "label": "letter error rate", "ID": "2719"}, {"sentence": "While phoneme error rates are generally higher than LERs, it should be noted that the reference baseforms for the names contain only one or two alternate pronunciations for each name.", "acronym": "LER", "label": "letter error rate", "ID": "2720"}, {"sentence": "Number of letters in reference (top), number of letter er- rors (middle), and LER (bottom) partitioned according to word frequencies.", "acronym": "LER", "label": "letter error rate", "ID": "2721"}, {"sentence": "Testing on a disjoint cor-  pus of 30 embedded and end-point detected words (place  and ship names) gave a 39.3% LER and 21.1%  word accuracy.", "acronym": "LER", "label": "letter error rate", "ID": "2722"}, {"sentence": "Choice of data For CCM, we found that if the full dataset (all SLs) is used in train- ing, then performance degrades when evaluating on sentences of length ?", "acronym": "SL", "label": "sentence length", "ID": "2723"}, {"sentence": "For actions described by natural language text strings, the action space is inherently discrete and potentially unbounded due to the exponential complexity of language with re- spect to SL.", "acronym": "SL", "label": "sentence length", "ID": "2724"}, {"sentence": "`, where ` is the maximal SL being evaluated.", "acronym": "SL", "label": "sentence length", "ID": "2725"}, {"sentence": "These had the effect of performing normali- zation for SL and other factors.", "acronym": "SL", "label": "sentence length", "ID": "2726"}, {"sentence": "This test set has an average SL of 17.7 words, and from previous experiments we estimate that it is comparable in difficulty to the NEGRA corpus to within 1% of accuracy.", "acronym": "SL", "label": "sentence length", "ID": "2727"}, {"sentence": "J} corresponds to a sequence of source word positions, where J is the source SL, and with null representing un- aligned target words.", "acronym": "SL", "label": "sentence length", "ID": "2728"}, {"sentence": "They  may also be useful for encoding SL  information, such as whether or not a gene was attested  in training data or whether it is present in a particular  language model or other local resource.", "acronym": "SL", "label": "strictly local", "ID": "2729"}, {"sentence": "Notably, it is SL (it does not consider the context of the analysed ex- pression), while the localisation can be ambiguous: ?", "acronym": "SL", "label": "strictly local", "ID": "2730"}, {"sentence": "V, {A, V }} which essentially says that the dislocated A immedi- ately precedes the matrix verb V and precedes (not necessarily immediately) the in-situ B and C. 5.2 Local case To begin with, let us see a case where dependency between the preverbal constituent and the matrix verb is SL, taking (1) as an example.", "acronym": "SL", "label": "strictly local", "ID": "2731"}, {"sentence": "case where dependency between the preverbal constituent and the matrix verb is SL, e.g: 1 Ein Buch geben die Eltern dem Sohn.", "acronym": "SL", "label": "strictly local", "ID": "2732"}, {"sentence": "Such anaphora resolution cannot be done without relevant context, which is not available in SL paradigms of sentence compression.", "acronym": "SL", "label": "strictly local", "ID": "2733"}, {"sentence": "The factuality status of a given event cannot be determined from the SL modality and polarity operators scoping over that event alone; rather, if present, other non-local markers must be considered as well to obtain the adequate interpretation.", "acronym": "SL", "label": "strictly local", "ID": "2734"}, {"sentence": "Extracting Terms and Terminological  Collocations from the ELAN SL-English Par- allel Corpus.", "acronym": "SL", "label": "Slovene", "ID": "2735"}, {"sentence": "Second, it is not clear that vari- ous grammatical categories and their values have the same interpretation in each language; for ex- ample, it is rather surprising that only the Roma- nian tagset explicitly mentions strong and weak pronominal forms, it is not clear whether negative pronouns in Romanian, SL, Czech and Bul- garian are negative in the same sense of participat- ing in Negative Concord, it is not clear why Roma- nian has negative adverbs while, say, Czech lacks them, etc.", "acronym": "SL", "label": "Slovene", "ID": "2736"}, {"sentence": "Such correspondences are not exceptional, e.g., the at least three masculine gen- ders of Polish (Man?czak, 1956; Saloni, 1976) are mapped into the single masculine gender of many other languages, the dual and the plural numbers of some languages (SL, Czech) are mapped to plural of other languages, etc.", "acronym": "SL", "label": "Slovene", "ID": "2737"}, {"sentence": "Morphosyntactic Tagging of SL: Eval- uating PoS Taggers and Tagsets.", "acronym": "SL", "label": "Slovene", "ID": "2738"}, {"sentence": "For ex- ample, Vintar (2000) presented two methods for  extraction of terminological collocations in order  to assist the translation process in SL.", "acronym": "SL", "label": "Slovene", "ID": "2739"}, {"sentence": "One admirable standardization effort in the field of Slavic part of speech (POS) tagging has been the Multext-East project (Erjavec, 2001), one of whose aims was to construct mutually compati- ble tagsets for 8 European languages, including 4 Slavic languages (originally Bulgarian, Czech and SL, later extended to Croatian); additionally, a Multext-East-style tagset for Russian was con- structed at the University of T?bingen (http: //www.sfb441.uni-tuebingen.de/c1/ tagset.html).", "acronym": "SL", "label": "Slovene", "ID": "2740"}, {"sentence": "One  1 approach utilizes multiple translations of a sin- gle SLe text, where the source lan- guage text guarantees semantic equivalence in  the target language texts (e.g., Barzilay &  McKeown, 2001; Pang et al, 2003).", "acronym": "SL", "label": "source languag", "ID": "2741"}, {"sentence": "We consider a parse tree on the SLe as a set of dependency edges to be transferred.", "acronym": "SL", "label": "source languag", "ID": "2742"}, {"sentence": "Background A word alignment for a parallel sentence pair represents the correspondence between words in a SLe and their translations in a target language (Brown et al 1993b).", "acronym": "SL", "label": "source languag", "ID": "2743"}, {"sentence": "This system uses a word-aligned corpus and a parser for a resource-rich language (SLe) in order to create a parser for a resource-poor language (target language).", "acronym": "SL", "label": "source languag", "ID": "2744"}, {"sentence": "A rare word in the SLe links to many words in the target language that we would ideally like to see unaligned, or aligned to other words in the sentence.", "acronym": "SL", "label": "source languag", "ID": "2745"}, {"sentence": "Figure 4 shows precision/recall curves for the different models on the En-Fr corpus using English as the SLe (left), and on the En-Pt corpus using Portuguese as the source.", "acronym": "SL", "label": "source languag", "ID": "2746"}, {"sentence": "level Introductory course in  Indian SL?", "acronym": "SL", "label": "Sign Language", "ID": "2747"}, {"sentence": "Place names map in Japanese SL in Japan (in Japanese, ???????????)", "acronym": "SL", "label": "Sign Language", "ID": "2748"}, {"sentence": "Spanish SL (LSE) translation.", "acronym": "SL", "label": "Sign Language", "ID": "2749"}, {"sentence": "A Survey and Critique of  American SL Natural Language Genera- tion and Machine Translation Systems.", "acronym": "SL", "label": "Sign Language", "ID": "2750"}, {"sentence": "British SL cor- pus project: open access archives and the observer?s paradox.", "acronym": "SL", "label": "Sign Language", "ID": "2751"}, {"sentence": "c?2014 Association for Computational Linguistics Proper Name Machine Translation from Japanese to Japanese SL Taro Miyazaki, Naoto Kato, Seiki Inoue, Shuichi Umeda, Makiko Azuma, Nobuyuki Hiruma NHK Science & Technology Research Laboratories Tokyo, Japan {miyazaki.t-jw, katou.n-ga, inoue.s-li, umeda.s-hg, azuma.m-ia, hiruma.n-dy}@nhk.or.jp Yuji Nagashima Faculty of Information, Kogakuin University Tokyo, japan nagasima@cc.kogakuin.ac.jp Abstract This paper describes machine transla- tion of", "acronym": "SL", "label": "Sign Language", "ID": "2752"}, {"sentence": "CON-based  CCG  of  Japanese  Quantifiers.", "acronym": "CON", "label": "Continuation", "ID": "2753"}, {"sentence": "3CON counts are used for the lower layers.", "acronym": "CON", "label": "Continuation", "ID": "2754"}, {"sentence": "CONs and clarifications are handled as  standard mechanisms of our interpreter.", "acronym": "CON", "label": "Continuation", "ID": "2755"}, {"sentence": "CON class  V_Rootl  V.Root2  V_Root3  V_Root4  V_Root5  V_Root6  V_Root7  V_Root8  Applicable rules  none  +ed  +s  +s, +ed  +ing  +ing, +ed  +ing, +s  +in~, +s, +ed  Figure 2: CON classes for verbs  Examples of lexical entries for verbs follow:  admire V~oot8 \"V(admire)\"  dyeing V_Roo1:1 \"V(dye) PROG\"  dye V_~oot4 \"V(dye)\"  zigza~ing V-Root I \"V(zigzag) PROG\"  z igzagged V-Root1", "acronym": "CON", "label": "Continuation", "ID": "2756"}, {"sentence": "CON class  V_Rootl  V.Root2  V_Root3  V_Root4  V_Root5  V_Root6  V_Root7  V_Root8  Applicable rules  none  +ed  +s  +s, +ed  +ing  +ing, +ed  +ing, +s  +in~, +s, +ed  Figure 2: CON classes for verbs  Examples of lexical entries for verbs follow:  admire V~oot8 \"V(admire)\"  dyeing V_Roo1:1 \"V(dye) PROG\"  dye V_~oot4 \"V(dye)\"  zigza~ing V-Root I \"V(zigzag) PROG\"  z igzagged V-Root1 \"V(zigzag) PAST WE\"  zigzagged V_Rootl \"V(zigzag) PPART WE\"  z igzag V_Root3 \"V(zigzag)\"  tangoes V_P.oot;1 \"V(tango) 3SG PRES\"  t;amgo V_Root6 \"V(tango)\"  taught V_Rootl \"V(teaeh) PAST", "acronym": "CON", "label": "Continuation", "ID": "2757"}, {"sentence": "CON of this research in the near fnture  consists of the design of an adaptive variant of this  mode\\[, one that learns a grammar from examples in  an unsupervised fashion.", "acronym": "CON", "label": "Continuation", "ID": "2758"}, {"sentence": "5 CONs and future work In this paper we have pursued a line of research that seeks to induce semantic classes for adjectives from distributional evidence.", "acronym": "CON", "label": "Conclusion", "ID": "2759"}, {"sentence": "5 CONs and Future Directions In this paper, we described the development of a multi-step system aimed for story analysis with particular emphasis on analyzing children?s sto- ries.", "acronym": "CON", "label": "Conclusion", "ID": "2760"}, {"sentence": "5 CON This paper has presented a precision, hand-built grammar for the Australian language Wambaya, and through that grammar a case study evaluation of the LinGO Grammar Matrix.", "acronym": "CON", "label": "Conclusion", "ID": "2761"}, {"sentence": "5 CON  In this paper we have presented an approach to  automatic extraction of terminology in a morpho- logically rich language, such as Serbian.", "acronym": "CON", "label": "Conclusion", "ID": "2762"}, {"sentence": "7 CON  We described measurement of semantic similarity be-  tween words.", "acronym": "CON", "label": "Conclusion", "ID": "2763"}, {"sentence": "Perceptron TP FP FN P (%) R (%) F1 (%) 671 128 119 83.98 84.94 84.46 Conditional Random Fields TP FP FN P (%) R (%) F1 (%) 643 78 147 89.18 81.39 85.11 Bayes Point Machines TP FP FN P (%) R (%) F1 (%) 647 79 143 89.12 81.90 85.36 Table 4: Performance of different optimization strategies 6 CON To tackle the hedge cue detection problem posed by the CoNLL-2010 shared task, we utilized a classifier for sequential labeling following previ- ous work (Morante and Daelemans, 2009).", "acronym": "CON", "label": "Conclusion", "ID": "2764"}, {"sentence": "Although transferring CONal net- work features is not a new idea (Driancourt and Bottou, 1990), the simultaneous availability of large datasets and cheap GPU co-processors has contributed to th", "acronym": "CON", "label": "convolution", "ID": "2765"}, {"sentence": "eon Bottou Microsoft Research New York leon@bottou.org Abstract We construct multi-modal concept repre- sentations by concatenating a skip-gram linguistic representation vector with a vi- sual concept representation vector com- puted using the feature extraction layers of a deep CONal neural network (CNN) trained on a large labeled object recognition dataset.", "acronym": "CON", "label": "convolution", "ID": "2766"}, {"sentence": "SIFT and HOG descriptors pro- duced big performance gains a decade ago, and now deep CONal features are providing a similar", "acronym": "CON", "label": "convolution", "ID": "2767"}, {"sentence": "2015) and shown to achieve human level per- formance by applying CONal neural net- works to the raw image pixels.", "acronym": "CON", "label": "convolution", "ID": "2768"}, {"sentence": "The CONal layers are then used as mid-level feature extractors on a variety of computer vi- sion tasks (Oquab et al.,", "acronym": "CON", "label": "convolution", "ID": "2769"}, {"sentence": "Imagenet classification with deep CONal neu- ral networks.", "acronym": "CON", "label": "convolution", "ID": "2770"}, {"sentence": "An  unusually large reduction in 5D suggests the opti-  inal clustering has been obtained 3 (see n = 10 in  3In practice, CON (mask {1,2,4,8,4,2,1}) is first  aI)plied to 5D to smooth out sharp local changes  Rank matrix Step 2  Step 1  cut  Step 3  Figure 5: A working example of the divisive eluster-  ing algorithm.", "acronym": "CON", "label": "convolution", "ID": "2771"}, {"sentence": "Although transferring CONal net- work features is not a new idea (Driancourt and Bottou, 1990), the simultaneous availability of large datasets and cheap GPU co-processors has contributed to the achievement of considerable performance gains on a variety computer vision benchmarks: ?", "acronym": "CON", "label": "convolution", "ID": "2772"}, {"sentence": "ATR tools have been developed for English  (Frantzi et al, 2000), FR (Jacquemin, 2001),  Japanese (Nakagawa and Mori, 2000), etc.", "acronym": "FR", "label": "French", "ID": "2773"}, {"sentence": "The top row of Figure 1 shows two word alignments between an English?FR sentence pair.", "acronym": "FR", "label": "French", "ID": "2774"}, {"sentence": "There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, auxiliary verbs used in one lan- guage but not the other (e.g., English He walked and FR Il est alle?),", "acronym": "FR", "label": "French", "ID": "2775"}, {"sentence": "483 Computational Linguistics Volume 36, Number 3 Table 1 Test corpora statistics: English?FR, English?Spanish, English?Portuguese, Portuguese?Spanish, Portuguese?FR, and Spanish?FR.", "acronym": "FR", "label": "French", "ID": "2776"}, {"sentence": "The document data consisted of the  Wall Street Journal, San Jose Mercury News, AP  Newswire, Information from Computer Selected disks,  FR, U.S. Patents and short abstracts from the  Department of Energy.", "acronym": "FR", "label": "Federal Register", "ID": "2777"}, {"sentence": "In TREC-5, the test data was actual OCR output  of scanned images of the 1994 FR.", "acronym": "FR", "label": "Federal Register", "ID": "2778"}, {"sentence": "The  10 subcollections were defined corresponding to the  various dates of the data, i.e., the three different years  of the Wall Street Journal, the two different years of  the AP newswire, the two sets of Ziff documents (one  on each disk), and the three single subcollections (the  FR, the San Jose Mercury News, and the  U.S. Patents).", "acronym": "FR", "label": "Federal Register", "ID": "2779"}, {"sentence": "The evaluation corpus used in the HARD track consists of 372,219 documents, and includes three newswire corpora (New York Times,  Associated Press Worldstream and Xinghua English) and two governmental corpora (The Congressional Record and FR).", "acronym": "FR", "label": "Federal Register", "ID": "2780"}, {"sentence": "TREC-3 was forced to re-use some of the training  data, and TREC-4 performed routing tests using the  FR (with new data) for 25 of the topics,  and using training data and \"net trash\" for testing  the other 25 topics.", "acronym": "FR", "label": "Federal Register", "ID": "2781"}, {"sentence": "U.S. Govenltnent Ala~zzlol, 19 73- 74, office of the FR, National Archives and  Records Service, General Services Administration, Washington, D.C. 20408, 1974.", "acronym": "FR", "label": "Federal Register", "ID": "2782"}, {"sentence": "IG is an  information theoretic concept measuring how  much the probability distributions for a feature dif- fer among the different classes.", "acronym": "IG", "label": "Information gain", "ID": "2783"}, {"sentence": "We first separately select eight words before  and after the target word in a sentence to  constitute the context, expressed as the  following form:  <wd?8, wd?7, wd?6, wd?5, wd?4, wd?3, wd?2, wd?1  , focus-word,  wd+1, wd+2, wd+3, wd+4, wd+5, wd+6, wd+7, wd+8>  Table 1 IG of every position of  context  Left context Right context  Position Information  gain  Position Information  gain  wd?1 3.979 875 wd+1 4.005 737  wd?2 2.800 943 wd+2 2.931 834  wd?3 2.183 287 wd+3 2.287 020  wd?4 1.709 504 wd+4 1.810 530  wd?5 1.361 637 wd+5 1.437 952  wd?6 1.074 606 wd+6 1.137 979  wd?7 0.304 546 wd+7 0.821 330  wd?8 0.298 992 wd+8 0.419 472    The amount of info", "acronym": "IG", "label": "Information gain", "ID": "2784"}, {"sentence": "0.27 25.32 Table 2: Evaluation results Feature Weight part-of-speech pattern 0.1935 CI shortterm 0.1744 Wikipedia keyphraseness 0.1731 CI maxscore 0.1689 CI shortterm normalized 0.1379 ChiInformativeness 0.1122 document frequency (df) 0.1031 tf.idf 0.0870 ChiPhraseness 0.0660 length of phrase 0.0416 named entity heuristic 0.0279 within document frequency 0.0227 term frequency (tf) 0.0209 Table 4: IG feature weight Table 4 shows the weight associated with each feature.", "acronym": "IG", "label": "Information gain", "ID": "2785"}, {"sentence": "We used two measures, both of which in- dicate a similar trend, to calculate feature effec- tiveness: IG (Kullback-Leibler di- vergence) and the chi-square statistic.", "acronym": "IG", "label": "Information gain", "ID": "2786"}, {"sentence": "IG is used to select target words, in this case resulting in 849 words.", "acronym": "IG", "label": "Information gain", "ID": "2787"}, {"sentence": "The  expression D\\[f=v\\] refers to those patterns in the database that have value v for feature  f; V is the set of possible values for feature f.  H(D~\\]) = ~ H(D\\[f=v,\\]) ID~ vdI (5)  viEV  IG is then obtained by equation (6) and scaled to be used as a weight  for the feature during similarity matching.", "acronym": "IG", "label": "Information gain", "ID": "2788"}, {"sentence": "One limitation of the IG based feature selection used in langid.py is that each feature is scored inde- pendently, and each language receives a binarized score.", "acronym": "IG", "label": "Information Gain", "ID": "2789"}, {"sentence": "8.2 43.2 N.NN 22.2 15.3 N.. 16.4 31.7 HV 41.5 46.4 .NN 29.9 22.9 .P 52.2 68.3 NN 86.3 83.0 XN 0.4 0.4 P. 6.6 16.8 H 61.8 65.9 NNNN 6.2 3.2 D. 4.4 12.6 R 61.5 65.5 D 99.2 99.5 ..$ 0.0 0.0 RR 7.2 9.4 NNN 28.3 18.6 J.. 5.0 12.6 NNNN 21.7 18.5 .NNN 6.7 4.0 ..VV 0.9 5.2 .C 15.8 18.8 N.D 58.6 47.8 DN.. 4.2 11.0 ... 0.8 0.3 NX 0.8 0.5 .PD 24.5 36.3 N.C 11.3 13.6 Table 5: Top 10 POS features per-group by IG, along with percentage of sentences in each language in which the feature appears.", "acronym": "IG", "label": "Information Gain", "ID": "2790"}, {"sentence": "For the per-group classifiers, runs AO1 and AO2 use a naive Bayes model on a word-level representation, with feature selection by IG.", "acronym": "IG", "label": "Information Gain", "ID": "2791"}, {"sentence": "Averbuch et al (2004) develop an IG algorithm for learning negative context patterns in discharge summaries and measure the effect of context identification on the performance of medical information retrieval.", "acronym": "IG", "label": "Information Gain", "ID": "2792"}, {"sentence": "using IG (Hall et al.,", "acronym": "IG", "label": "Information Gain", "ID": "2793"}, {"sentence": "If we look at the IG Ratio, of all features the rank difference of the verbs in the training data seems to be the strongest feature, and of the external features the frequency difference of the entire phrase containing the NC and the verb.", "acronym": "IG", "label": "Information Gain", "ID": "2794"}, {"sentence": "Multiset-valued  linear IGs.", "acronym": "IG", "label": "index grammar", "ID": "2795"}, {"sentence": "Multiset-valued linear IGs: Imposing dominance constraints on derivations.", "acronym": "IG", "label": "index grammar", "ID": "2796"}, {"sentence": "The best test for each node is selected with the standard IG criterion.", "acronym": "IG", "label": "information gain", "ID": "2797"}, {"sentence": "This test, which maximizes the IG 1 wrt.", "acronym": "IG", "label": "information gain", "ID": "2798"}, {"sentence": "The recursive tree building process ter- minates if the IG is 0.", "acronym": "IG", "label": "information gain", "ID": "2799"}, {"sentence": "A node is pruned if the IG of the best test mul- tiplied by the size of the data subsample is below a given threshold.", "acronym": "IG", "label": "information gain", "ID": "2800"}, {"sentence": "the class, is 1 The IG measures how much the test de- creases the uncertainty about the class.", "acronym": "IG", "label": "information gain", "ID": "2801"}, {"sentence": "The IG is therefore 0.92 ?", "acronym": "IG", "label": "information gain", "ID": "2802"}, {"sentence": "Granada: European  LREC Association.", "acronym": "LREC", "label": "Language Resources", "ID": "2803"}, {"sentence": "on LREC and  Evaluation, p. 567-572.", "acronym": "LREC", "label": "Language Resources", "ID": "2804"}, {"sentence": "In Proceedings of the 3rd Work- shop on Asian LREC and International Standardization, COLING 19, Taipei, Taiwan.", "acronym": "LREC", "label": "Language Resources", "ID": "2805"}, {"sentence": "In First International Conference on  LREC 8J Evaluation: Workshop  on Linguistic Coreference.", "acronym": "LREC", "label": "Language Resources", "ID": "2806"}, {"sentence": "LREC and Evaluation, 39(4):267?285.", "acronym": "LREC", "label": "Language Resources", "ID": "2807"}, {"sentence": "Granada: European  LRECs Association.", "acronym": "LREC", "label": "Language Resource", "ID": "2808"}, {"sentence": "on LRECs and  Evaluation, p. 567-572.", "acronym": "LREC", "label": "Language Resource", "ID": "2809"}, {"sentence": "In Proceedings of the 3rd Work- shop on Asian LRECs and International Standardization, COLING 19, Taipei, Taiwan.", "acronym": "LREC", "label": "Language Resource", "ID": "2810"}, {"sentence": "In First International Conference on  LRECs 8J Evaluation: Workshop  on Linguistic Coreference.", "acronym": "LREC", "label": "Language Resource", "ID": "2811"}, {"sentence": "LRECs and Evaluation, 39(4):267?285.", "acronym": "LREC", "label": "Language Resource", "ID": "2812"}, {"sentence": "In Proceedings of the Third LREC Conference.", "acronym": "LREC", "label": "Language Resources and Evaluation", "ID": "2813"}, {"sentence": "In Proceedings of Fifth International Conference on LREC, pages 449?", "acronym": "LREC", "label": "Language Resources and Evaluation", "ID": "2814"}, {"sentence": "In Proceedings of the 1st Interna- tional Conference on LREC, Granada, Spain.", "acronym": "LREC", "label": "Language Resources and Evaluation", "ID": "2815"}, {"sentence": "In Proceedings of the Fifth International Conference on LREC, pages 449?454, Genoa, Italy.", "acronym": "LREC", "label": "Language Resources and Evaluation", "ID": "2816"}, {"sentence": "LREC, (in press).", "acronym": "LREC", "label": "Language Resources and Evaluation", "ID": "2817"}, {"sentence": "LREC.", "acronym": "LREC", "label": "Language Resources and Evaluation", "ID": "2818"}, {"sentence": "Using IC to Eval- uate Semantic Similarity in a Taxonomy.", "acronym": "IC", "label": "Information Content", "ID": "2819"}, {"sentence": "The score for each n-gram is computed as its IC (Cover and Thomas, 1991), ie. ?", "acronym": "IC", "label": "Information Content", "ID": "2820"}, {"sentence": "Using IC to Evaluate Semantic Similarity in a Taxonomy.", "acronym": "IC", "label": "Information Content", "ID": "2821"}, {"sentence": "Using IC to  Evaluate Semantic Similarity in a Taxonomy.", "acronym": "IC", "label": "Information Content", "ID": "2822"}, {"sentence": "The word counts required for computing IC were obtained from Google Books Ngrams.3 The DKPro system (Ba?r et al 2012) obtained first place in STS?12 with the second run.", "acronym": "IC", "label": "Information Content", "ID": "2823"}, {"sentence": "Using IC to Evalu- ate Semantic Similarity in a Taxonomy.", "acronym": "IC", "label": "Information Content", "ID": "2824"}, {"sentence": "IC for selected word pairs  Ix, y\\] ;C(x, Ix, y\\]) ;C(y, \\[x,y \\]) fxa n, d~,  \\[system,parallel\\] 2 910 322  \\[system,computation\\] 78 910 322  \\[path,parallel\\] 1 19 14  \\[class,grammar\\] 5 128 86  \\[define,grammar\\] 3 131 80  \\[class,language\\] 1 128 86  \\[define,language\\] 9 131 80  0.0016 57 24  0.0634 740 201  0.0313 57 24  0.0235 47 34  0.0143 47 34  0.0047 295 116", "acronym": "IC", "label": "Informational Contribution", "ID": "2825"}, {"sentence": "In this way, a more uniform rate of information transfer is achieved, because the higher ICt of the un- expected word is stretched over a slightly longer time.", "acronym": "IC", "label": "information conten", "ID": "2826"}, {"sentence": "The  source grammar essentially guides the  elicitation of the sentence semantic structure  into its corresponding UNL structure, by  determining RLs and ALs, always giving  priority to ICt.", "acronym": "IC", "label": "information conten", "ID": "2827"}, {"sentence": "In order to evaluate the relative contribution of WordNet concepts to the ICt of a text as a whole, a node specificity metric was derived based on an empirical analysis of the distribution of topological features of WordNet such as inheritance, hierarchy depth, clustering coefficients and node de- gree and how these features map onto human judg- ments of concept specificity or informativity.", "acronym": "IC", "label": "information conten", "ID": "2828"}, {"sentence": "The various syntactic  frameworks that we examine below can be seen to share  a great deal of their underlying substantive claims about  the ICt of the category label of a  constituent.", "acronym": "IC", "label": "information conten", "ID": "2829"}, {"sentence": "Using ICt to evaluate semantic similarity in a taxonomy.", "acronym": "IC", "label": "information conten", "ID": "2830"}, {"sentence": "Using ICt to  evaluate semantic similarity in a taxonomy.\"", "acronym": "IC", "label": "information conten", "ID": "2831"}, {"sentence": "(5) The continuous CLs should be cut into  several ICs.", "acronym": "IC", "label": "independent CL", "ID": "2832"}, {"sentence": "In our approach towards language-ICIA system, we have developed context aware query translation using Wikipedia.", "acronym": "IC", "label": "independent CL", "ID": "2833"}, {"sentence": "In this way, a more uniform rate of information transfer is achieved, because the higher IC of the un- expected word is stretched over a slightly longer time.", "acronym": "IC", "label": "information content", "ID": "2834"}, {"sentence": "The  source grammar essentially guides the  elicitation of the sentence semantic structure  into its corresponding UNL structure, by  determining RLs and ALs, always giving  priority to IC.", "acronym": "IC", "label": "information content", "ID": "2835"}, {"sentence": "In order to evaluate the relative contribution of WordNet concepts to the IC of a text as a whole, a node specificity metric was derived based on an empirical analysis of the distribution of topological features of WordNet such as inheritance, hierarchy depth, clustering coefficients and node de- gree and how these features map onto human judg- ments of concept specificity or informativity.", "acronym": "IC", "label": "information content", "ID": "2836"}, {"sentence": "The various syntactic  frameworks that we examine below can be seen to share  a great deal of their underlying substantive claims about  the IC of the category label of a  constituent.", "acronym": "IC", "label": "information content", "ID": "2837"}, {"sentence": "Using IC to evaluate semantic similarity in a taxonomy.", "acronym": "IC", "label": "information content", "ID": "2838"}, {"sentence": "Using IC to  evaluate semantic similarity in a taxonomy.\"", "acronym": "IC", "label": "information content", "ID": "2839"}, {"sentence": "This multiple-pivot idea is similar to IC in that multiple pivots are re- quired, but using multiple pivot languages frees it from the dependency on rich input lexicons that contain a variety of synonyms.", "acronym": "IC", "label": "Inverse Consultation", "ID": "2840"}, {"sentence": "For example, let V  denote the semantic valuation function (with a particu-  lar interpretation and possible world understood) and  let  V(P) = {<a,b,c>, <a,b ,d>,  <e,f ,g>},  V(x) = a, V(y) = b, and V(z) = d,  where P is a triadic predicate symbol, x, y, and z are  ICs or variables, and a, b . . . . .", "acronym": "IC", "label": "individual constant", "ID": "2841"}, {"sentence": "The UM part of BGP-MS  consists of all partitions except SB and SW, plus all  representations i  SB (and probably SW) in which an  IC occurs denoting the user (the rest of  SB corresponds to Sparck Jones's world model).", "acronym": "IC", "label": "individual constant", "ID": "2842"}, {"sentence": "We can expand  this by defining new predicates, even higher-order  predicates that express, for instance, properties of  or relations between sets, and in doing so we can use  monadic predicates and ICs freely  since we can interpret hese as existentially bound  variables.", "acronym": "IC", "label": "individual constant", "ID": "2843"}, {"sentence": "Ain as unary predicate symbols and at1, ...,  at, as ICs; the symbols \"^\"  and \"~ \" are the conjunc-  tion and implication sign and the function symbol . . . . .", "acronym": "IC", "label": "individual constant", "ID": "2844"}, {"sentence": "2 L~,p - -The  Monad ic  Second-Order   Language o f  T rees   L2K,p is the monadic second-order language over  the signature including a set of ICs  (K), a set of monadic predicates (P), and binary  predicates for immediate domination (,~), domina-  tion (,~*), linear precedence (-~) and equality (..~).", "acronym": "IC", "label": "individual constant", "ID": "2845"}, {"sentence": "For instance, the ICs  even the variables are only substantives while verbs are always  predicats one-.", "acronym": "IC", "label": "individual constant", "ID": "2846"}, {"sentence": "In Matuszek et al (2014), W was represented as a distribution over properties of tangible objects and U was a CCG parse.", "acronym": "CCG", "label": "Combinatory Categorical Grammar", "ID": "2847"}, {"sentence": "Efficient realization of coordinate structures in CCG.", "acronym": "CCG", "label": "Combinatory Categorial Grammar", "ID": "2848"}, {"sentence": "The framework is based on  CCGs and it  uses the morpheme as the basic building  block of the categorial lexicon.", "acronym": "CCG", "label": "Combinatory Categorial Grammar", "ID": "2849"}, {"sentence": "Natural Language Parsing with  CCGs in a Graph-  Unification-Based Formalism.", "acronym": "CCG", "label": "Combinatory Categorial Grammar", "ID": "2850"}, {"sentence": "For instance, Callaway (2003) re- 2CCG ports that the implementation of such a proces- sor for the SURGE realiser was the most time- consuming part of the evaluation with the result- ing component containing 4000 lines of code and 900 rules.", "acronym": "CCG", "label": "Combinatory Categorial Grammar", "ID": "2851"}, {"sentence": "Hoffman, in her thesis (Hoffman, 1995a,  Hoffman, 1995b), has used the Multiset-  CCG formalism  (Hoffman, 1992), an extension of Combinatory  Categorial Grammar to handle free word or-  der languages, to develop a generator for Turk-  ish.", "acronym": "CCG", "label": "Combinatory Categorial Grammar", "ID": "2852"}, {"sentence": "I integrated  a level of information structure with a unification-based  version of CCGs,  adapted  for free word order languages.", "acronym": "CCG", "label": "Combinatory Categorial Grammar", "ID": "2853"}, {"sentence": "Polynomial time parsing of CCGs.", "acronym": "CCG", "label": "combinatory categorial grammar", "ID": "2854"}, {"sentence": "Generat- ing with discourse CCG.", "acronym": "CCG", "label": "combinatory categorial grammar", "ID": "2855"}, {"sentence": "A left-to-right parser that simultaneously produces sentence-level syntactic and semantic analyses already exists for CCG (Steedman 1996, 583 Webber et al Anaphora and Discourse Structure 2000b; Hockenmaier, Bierner, and Baldridge, forthcoming), and it would seem straight- forward to extend such a parser to computing discourse-level syntax and semantics as well.", "acronym": "CCG", "label": "combinatory categorial grammar", "ID": "2856"}, {"sentence": "Multimodal CCG?.", "acronym": "CCG", "label": "combinatory categorial grammar", "ID": "2857"}, {"sentence": "Efficient normal-form parsing for CCG.", "acronym": "CCG", "label": "combinatory categorial grammar", "ID": "2858"}, {"sentence": "Such nonstandard constituents are also found with coordination, which was one motivation for CCG (Steedman 1996).", "acronym": "CCG", "label": "combinatory categorial grammar", "ID": "2859"}, {"sentence": "Szabolcsi, A. 1987 'On CCG.'", "acronym": "CCG", "label": "Combinatory Categorial grammar", "ID": "2860"}, {"sentence": "{ assert : PROP presup : PROP* 3.2 Alternative Sets The concept of alternative sets plays an impor- tant role in the semantics of alternative phrases.", "acronym": "PROP", "label": "proposition", "ID": "2861"}, {"sentence": "If D1 and D2 are adjacent, our PROP is true.", "acronym": "PROP", "label": "proposition", "ID": "2862"}, {"sentence": "concerns), since the PROP is clearly that the concerns pertain to the merger.", "acronym": "PROP", "label": "proposition", "ID": "2863"}, {"sentence": "The MRS in Fig- ure 1 is assigned to the example in (1).6 It in- cludes the basic PROPal structure: a situation of ?", "acronym": "PROP", "label": "proposition", "ID": "2864"}, {"sentence": "An alternative set is a set of PROPs which differ with respect to how one or more argu- ments are filled.", "acronym": "PROP", "label": "proposition", "ID": "2865"}, {"sentence": "We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named entity recognition was used for identifying PROP names, e.g., ?", "acronym": "PROP", "label": "proper", "ID": "2866"}, {"sentence": "Thus, unary adjectives denote PROPties and binary adjec- tives denote relations.", "acronym": "PROP", "label": "proper", "ID": "2867"}, {"sentence": "2) a set of part-of-speech patterns was used for the extraction of human and non-human characters that were not represented by PROP names, e.g., ?", "acronym": "PROP", "label": "proper", "ID": "2868"}, {"sentence": "1 Introduction The main hypothesis underlying the tasks in Lex- ical Acquisition is that it is possible to infer lexi- cal PROPties from distributional evidence, taken as a generalisation of a word?s linguistic behaviour in corpora.", "acronym": "PROP", "label": "proper", "ID": "2869"}, {"sentence": "Thus, we intend to induce semantic PROPties from syntactic distribution.", "acronym": "PROP", "label": "proper", "ID": "2870"}, {"sentence": "Once we have the trees in the PROP form  (to the degree this is possible) we run Hobbs'  algorithm repeatedly for each pronoun until it  has proposed n (= 15 in our experiment) can-  didates.", "acronym": "PROP", "label": "proper", "ID": "2871"}, {"sentence": "They presented a simple method for retrieving bigram counts from the web by querying a search engine and demon- strated that web counts (a) correlate with frequencies ob- tained from a carefully edited, balanced corpus such as the 100M words British National Corpus (BNC), (b) cor- relate with frequencies recreated using smoothing meth- ods in the case of unseen bigrams, (c) Rel predict hu- man plausibility judgments, and (d) yield state-of-the-art performance on pseudo-disambiguation tasks.", "acronym": "Rel", "label": "reliably", "ID": "2872"}, {"sentence": "1 In t roduct ion :  Core ference   Annotat ion   Various practical tasks requiring language tech-  nology including, for example, information ex-  traction and text summarization, can be done  more Rel if it is possible to automatically  find parts of the text containing information  about a given topic.", "acronym": "Rel", "label": "reliably", "ID": "2873"}, {"sentence": "Arbisi-Kelm (2010), for example, suggest that disfluent repetitions can be identified Rel through the use of pitch, dura- tion, and pause detection (with precision up to 93% (Nakatani, 1993)).", "acronym": "Rel", "label": "reliably", "ID": "2874"}, {"sentence": "ohashi, 441?8580, Japan Abstract In the framework of bilingual lexicon acquisition from cross-lingually relevant news articles on the Web, it is relatively harder to Rel estimate bilin- gual term correspondences for low frequency terms.", "acronym": "Rel", "label": "reliably", "ID": "2875"}, {"sentence": "Considering such a situation, this paper proposes to complementarily use much larger monolingual Web documents collected by search engines, as a resource for Rel re-estimating bilingual term correspon- dences.", "acronym": "Rel", "label": "reliably", "ID": "2876"}, {"sentence": "Department of Information and Computer Sciences, Toyohashi University of Technology Tenpaku-cho, Toyohashi, 441?8580, Japan Abstract In the framework of bilingual lexicon acquisition from cross-lingually relevant news articles on the Web, it is relatively harder to Rel estimate bilin- gual term correspondences for low frequency terms.", "acronym": "Rel", "label": "reliably", "ID": "2877"}, {"sentence": "y indicators We only consider credibility indicators that avoid making use of the searcher?s or blogger?s identity (i.e., excluding 1a, 1c, 1e, 1f, 2e from Rubin and Liddy?s list), that can be estimated automatically from available test collections only so as to facilitate repeatability of our experiments (ruling out 3e, 4a, 4c, 4d, 4e), that are textual in nature (ruling out 2d), and that can be Rel estimated with state-of-the- art language technology (ruling out 2a, 2b, 2c, 2g).", "acronym": "Rel", "label": "reliably", "ID": "2878"}, {"sentence": "As for the features that were most Rel for each cluster, listed in Table 6, they confirm the anal- ysis just made and again match the hypotheses dis- cussed in Section 2.", "acronym": "Rel", "label": "relevant", "ID": "2879"}, {"sentence": "cl high values low values 0 -1cn+1co, -1cn+1cd -1aj+1pe, -1ve+1pe 1 -1ve+1pe, -1co+1pe -1cn+1aj Table 5: Unary/binary: most Rel features (rep- resented as in examples 1 and 2).", "acronym": "Rel", "label": "relevant", "ID": "2880"}, {"sentence": "3.1 The  gender /an imat ic i ty  stat ist ics   After we have identified the correct antecedents  it is a simple counting procedure to compute  P(p\\[wa) where wa is in the correct antecedent  for the pronoun p (Note the pronouns are  grouped by their gender):  \\[ wain the antecedent for p \\[  P(pl o) =  When there are multiple Rel words in the  antecedent we apply the likelihood test designed  by Dunning (1993) on all the words in the candi-  date NP.", "acronym": "Rel", "label": "relevant", "ID": "2881"}, {"sentence": "3.1 Feature representation Although we already had some hypotheses with re- spect to what features could be Rel, as dis- cussed in Section 2, we wanted to proceed as empir- ically as possible.", "acronym": "Rel", "label": "relevant", "ID": "2882"}, {"sentence": "Indeed, the most Rel features for each cluster matched very closely the hypotheses discussed in Section 2.", "acronym": "Rel", "label": "relevant", "ID": "2883"}, {"sentence": "When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is Rel o the particular choice of an-  tecedent.", "acronym": "Rel", "label": "relevant", "ID": "2884"}, {"sentence": "Firstly, The TSAs are constructed in an unsuper- vised learning manner, and optimized by the trans- lation model during the FD process,  without using any statistics and linguistic heuristics  or syntactic constraints.", "acronym": "FD", "label": "forced decoding", "ID": "2885"}, {"sentence": "Figure 2 depicts the TSA generation algorithm  in which a phrase-based FD tech- nique is adopted to produce the TSA of each sen- tence pair.", "acronym": "FD", "label": "forced decoding", "ID": "2886"}, {"sentence": "Max- violation perceptron and FD for scalable MT training.", "acronym": "FD", "label": "forced decoding", "ID": "2887"}, {"sentence": "In this work, we do not apply syntax- based FD (e.g., tree-to-string) because  phrase-based models can achieve the state-of-the- art translation quality with a large amount of train- ing data, and are not limited by any constituent  boundary based constraints for decoding.", "acronym": "FD", "label": "forced decoding", "ID": "2888"}, {"sentence": "Apply M to implement phrase-based FD  on each training sentence pair (c, e), and output its  best derivation d* that can transform c into e.   3.", "acronym": "FD", "label": "forced decoding", "ID": "2889"}, {"sentence": "Unlike their efforts, this paper presents a simple  approach that automatically builds the translation  span alignment (TSA) of a sentence pair by utiliz- ing a phrase-based FD technique, and  then improves syntactic rule extraction by deleting  spurious links and adding new valuable links based  on bilingual translation span correspondences.", "acronym": "FD", "label": "forced decoding", "ID": "2890"}, {"sentence": "Formally, given a sentence pair (c, e), the  phrase-based FD technique aims to  search for the best derivation d* among all consis- tent deriv", "acronym": "FD", "label": "forced decoding", "ID": "2891"}, {"sentence": "Here we select the MaxEnt aligner because it has 935 Precision Recall F-score F-content F-function HMM 62.65 48.57 54.72 62.10 34.39 BM 72.76 54.82 62.53 68.64 43.93 ME 72.66 66.17 69.26 72.52 61.41 Link-Select 69.19 72.49 70.81 74.31 60.26 Intersection-Union-Refine 63.34 66.07 64.68 70.15 49.72 Table 2: Link Selection and Combination Results the highest FD among the three aligners, although the algorithm described below can be ap- plied to any aligner.", "acronym": "FD", "label": "F-measure", "ID": "2892"}, {"sentence": "When looking into the alignment links which are removed during the alignment link fil- tering process, we found that 80% of the removed links (1320 out of 1661 links) are incorrect align- ments, For A-E alignment, it increased the pre- cision by 3 points while reducing recall by 0.5 points, and the alignment FD is increased by about 1.5 points absolute,", "acronym": "FD", "label": "F-measure", "ID": "2893"}, {"sentence": "Tables 3 and 4 show the improvement of C-E and A-E alignment FDs with the confidence-based alignment link filtering (ALF).", "acronym": "FD", "label": "F-measure", "ID": "2894"}, {"sentence": "Figure 5 shows the FD graph.", "acronym": "FD", "label": "F-measure", "ID": "2895"}, {"sentence": "Figure 3 provides another comparison of the three  approaches by highlighting how the FD varies   0  10  20  30  40  50  60  70  80  90  100  0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1 Pe rc e n ta ge Threshold precision recall F-Measure Figure 1: Results of cross-document coreferencing  on the John Smith corpus using the incremental  vector space approach.", "acronym": "FD", "label": "F-measure", "ID": "2896"}, {"sentence": "As a result, 82% of selected alignments have higher F- scores, and the FD of the combined align- ments is increased over the best aligner (the Max- Ent aligner) by 0.8.", "acronym": "FD", "label": "F-measure", "ID": "2897"}, {"sentence": "For C-E alignment, removing low confidence alignment links increased alignment precision by 5.5 point, while decreased recall by 1.8 point, and the overall alignment FD is increased by 1.3 point.", "acronym": "FD", "label": "F-measure", "ID": "2898"}, {"sentence": "As a first step, we have compiled a preliminary list of  the 4000 most frequent German words from an existing  FD (Meier (1967)) and news texts.", "acronym": "FD", "label": "frequency dictionary", "ID": "2899"}, {"sentence": "The icelandic FD.", "acronym": "FD", "label": "frequency dictionary", "ID": "2900"}, {"sentence": "On the basis of the above collected word li- brary, we discriminate manually the positive and  negative meaning, PSAA, NSAA, and inverse  words in 50,000 Chinese words according to  Richard Xiao?s Top 50,000 Chinese Word Fre- quency List, which collects the frequency of the  top 50000 Chinese words covered in the just  published FD of Mandarin  Chinese based on a balanced corpus of ca.", "acronym": "FD", "label": "frequency dictionary", "ID": "2901"}, {"sentence": "If it is recalled, we weight it with its  frequency from the FD.", "acronym": "FD", "label": "frequency dictionary", "ID": "2902"}, {"sentence": "+ = rel rel rel F tsmfactorcc tsscorec tsscore , ,max , 32 1  (3)  2.4 Evaluation  We generated a Japanese-Hungarian dictionary  using selection methods A, B and F; with C, D  and E contributing indirectly through F.  (a) Recall evaluation  We used a Japanese FD that we  generated from the Japanese EDR corpus (Isa- hara, 2007) to weight each Japanese entry.", "acronym": "FD", "label": "frequency dictionary", "ID": "2903"}, {"sentence": "The Icelandic FD.", "acronym": "FD", "label": "frequency dictionary", "ID": "2904"}, {"sentence": "rel rel F tsmfactorcc tsscorec tsscore , ,max , 32 1  (3)  2.4 Evaluation  We generated a Japanese-Hungarian dictionary  using selection methods A, B and F; with C, D  and E contributing indirectly through F.  (a) Recall evaluation  We used a Japanese FD that we  generated from the Japanese EDR corpus (Isa- hara, 2007) to weight each Japanese entry.", "acronym": "FD", "label": "frequency dictionary", "ID": "2905"}, {"sentence": "Set- ting the standard to the FD (its  recall value being 100), we automatically search  each entry from the FD, verify- ing whether or not it is included in the bilingual  dictionary.", "acronym": "FD", "label": "frequency dictionary", "ID": "2906"}, {"sentence": "3.2 GS Recall that we could not use any previously well- established classification.", "acronym": "GS", "label": "Gold Standard", "ID": "2907"}, {"sentence": "In the testing phase, we used a different subset with 80 adjectives as GS against which we could compare the clustering results (see Section 3.2 for details on the manual annotation process).", "acronym": "GS", "label": "Gold Standard", "ID": "2908"}, {"sentence": "Also, as the present analysis is based on a small sample of manually annotated adjectives, we intend to obtain a larger GS, in order to establish statistically more reliable results.", "acronym": "GS", "label": "Gold Standard", "ID": "2909"}, {"sentence": "4.2 Experiments on Testset 2  GS: Two humans 8  annotated 100  sentences randomly selected from news media  texts.", "acronym": "GS", "label": "Gold Standard", "ID": "2910"}, {"sentence": "4.1 Experiments on Testset 1  GS: In total, Testset 1 contains 2028  annotated sentences collected from FrameNet  data set. (", "acronym": "GS", "label": "Gold Standard", "ID": "2911"}, {"sentence": "We therefore built our own GS, as has been mentioned at the beginning of this section.", "acronym": "GS", "label": "Gold Standard", "ID": "2912"}, {"sentence": "The first one is based on GS.", "acronym": "GS", "label": "Gibbs sampling", "ID": "2913"}, {"sentence": "They apply GS or the expectation max- imization algorithm to discover the word classes that are most probable in the context of surround- ing word classes.", "acronym": "GS", "label": "Gibbs sampling", "ID": "2914"}, {"sentence": "Clearly, for the sentence s there is at most Ns(1+Ns) 2 parent changes1.To estimate the parameters of LTLM we apply GS and gradually sample ?", "acronym": "GS", "label": "Gibbs sampling", "ID": "2915"}, {"sentence": "3.4 Inference For posterior inference of SDTM, we use col- lapsed GS which integrates out la- tent random variables ?,", "acronym": "GS", "label": "Gibbs sampling", "ID": "2916"}, {"sentence": "It is orders of magnitude faster than previous MCMC algorithms like GS, making efficient sampling pos- sible on a scale that was previously out of reach.", "acronym": "GS", "label": "Gibbs sampling", "ID": "2917"}, {"sentence": "Note that in the GS equation, we assume that the Dirichlet parameters ?", "acronym": "GS", "label": "Gibbs sampling", "ID": "2918"}, {"sentence": "We created the GS data, verb classes, using IPAL.", "acronym": "GS", "label": "gold standard", "ID": "2919"}, {"sentence": "We collected seman- tic classes from IPAL Japanese dictionary (IPAL, 1987), and used them as a GS data.", "acronym": "GS", "label": "gold standard", "ID": "2920"}, {"sentence": "Sense id Pattern Synonyms 1 kare(he) ga(nominative) soba(noodles) wo(accusative) kuu (eat) 2 kare (he) ga(nominative) fukugyo(a part-time job) de(accusative) kurasu (live) Table 2: Examples of test verbs and their polysemic GS senses Id Sense Verb Classes Id Sense Verb Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4 ask {negau, tanomu} 14 accept {uketoru, ukeru, morau, osameru} 5 leave {saru, hanareru} 15 take {uketoru, toru, kaisyakusuru, miru} 6 move {saru, utsuru} 1", "acronym": "GS", "label": "gold standard", "ID": "2921"}, {"sentence": "in the GS, 1285 are attached correctly by the parser, while 607 receive an incorrect regent.", "acronym": "GS", "label": "gold standard", "ID": "2922"}, {"sentence": "These annotated corpora are  then used as a 'GS' against which  the program's achievements can be compared.", "acronym": "GS", "label": "gold standard", "ID": "2923"}, {"sentence": "Toward a task-based GS for evaluation of NP chunks and tech- nical terms.", "acronym": "GS", "label": "gold standard", "ID": "2924"}, {"sentence": "Many of the so- lutions they seem to apply (such as providing con- straints over a GS so that it generates only ?", "acronym": "GS", "label": "generative system", "ID": "2925"}, {"sentence": "solutions) are intended to avoid problems arising from the large amount of memory that would be required to consider all possible solu- tions provided by the GS.", "acronym": "GS", "label": "generative system", "ID": "2926"}, {"sentence": "SEQUITUR (SEQ), a GS based on the joint n-gram approach (Bisani and Ney, 2008).", "acronym": "GS", "label": "generative system", "ID": "2927"}, {"sentence": "Most similar to our entity-level approach is the system of Haghighi and Klein (2010), which also uses approximate global inference; however, theirs is an unsupervised, GS and they attempt to directly model multinomials over words in each mention.", "acronym": "GS", "label": "generative system", "ID": "2928"}, {"sentence": "Rather, the lexicon can  be seen as a GS, where word senses are related by logical operations  defined by the well-formedness rules of the semantics.", "acronym": "GS", "label": "generative system", "ID": "2929"}, {"sentence": "This is simi- lar to asking what one gains from a formal descrip- tion of language as a GS.", "acronym": "GS", "label": "generative system", "ID": "2930"}, {"sentence": "Tagger Forward FEEmbedding Layer Hidden Layer w0=fox, w-1=the, ?", "acronym": "FE", "label": "Feature Extractor", "ID": "2931"}, {"sentence": "FE: For each entity page candidate, we extract a set of context and URL features.", "acronym": "FE", "label": "Feature Extractor", "ID": "2932"}, {"sentence": "CDM FE: For those  untagged CDMs, the classification is  carried out through C4.5.", "acronym": "FE", "label": "Feature Extractor", "ID": "2933"}, {"sentence": "Opensmile: The Munich Versatile and Fast Open-Source Audio FE.", "acronym": "FE", "label": "Feature Extractor", "ID": "2934"}, {"sentence": ".5 75.4 438.7 53.8 434.8 57.7 t(ReLIE) t(CRF) 0.067 0.009 0.144 0.244 0.168 0.143 0.091 0.019 Table 2: Average Training/Testing Time (sec)(with 40% data for training) Task(Extra Feature) Data for Training 10% 40% 80% CRF C+RL CRF C+RL CRF C+RL CourseNumberTask(Negative Dictionary) 0.553 0.624 0.644 0.764 0.854 0.845 PhoneNumberTask(Quantifier) 0.695 0.893 0.820 0.937 0.821 0.964 Table 3: ReLIE as FE (C+RL is CRF enhanced with features learned by ReLIE).", "acronym": "FE", "label": "Feature Extractor", "ID": "2935"}, {"sentence": "5.4 ReLIE as FE for CRF To understand the effect of incorporating ReLIE- identified features into CRF, we chose the two tasks (CourseNumberTask and PhoneNumberTask) with the least F-measure in our experiments to determine raw extraction quality.", "acronym": "FE", "label": "Feature Extractor", "ID": "2936"}, {"sentence": "Features  FE identification is a binary classifica- tion problem in which each constituent in a parse  tree is described by a feature vector and, based on  that vector, tagged as either a frame element or not.", "acronym": "FE", "label": "Frame element", "ID": "2937"}, {"sentence": "FrameNet II contains                                                    5 http://www.wjh.harvard.edu/~inquirer/homecat.htm  Table 1: Example of opinion related frames  and lexical units  Frame  name Lexical units FEs  Desiring want, wish, hope,  eager, desire,  interested,  Event,  Experiencer,  Location_of_event Emotion _directed agitated, amused,  anguish, ashamed,  angry, annoyed,  Event, Topic  Experiencer,  Expressor,  Mental  _property absurd, brilliant,  careless, crazy,  cunning, foolish  Behavior,  Protagonist,  Domain, Degree  Subject  _stimulus delightful, amazing,  annoying, amusing,", "acronym": "FE", "label": "Frame element", "ID": "2938"}, {"sentence": "Previous role (r_n): FEs do not  occur in isolation, but rather, depend very  much on the other elements in a sentence.", "acronym": "FE", "label": "Frame element", "ID": "2939"}, {"sentence": "A Subcategoriza-tionFE represents the syntactic valence of a word sense.", "acronym": "FE", "label": "Frame element", "ID": "2940"}, {"sentence": "FEs that have been so designated for a particular sentence appear to be Core frame elements, but not all core frame elements missing from a sentence have designated as null instantiations.", "acronym": "FE", "label": "Frame element", "ID": "2941"}, {"sentence": "3.3 Frame Elements  FE entries are preceded with a ?%?,", "acronym": "FE", "label": "Frame element", "ID": "2942"}, {"sentence": "In this paper, we consider only predicting polarity (positive and neg- ative FE).", "acronym": "FE", "label": "feeling", "ID": "2943"}, {"sentence": "2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 4 Early Academic Life My overwhelming emotion on getting this honor was, after surprise, a FE of in- adequacy in measuring up to previous honorees, but nonetheless, I want to grasp at this moment of autobiography, or at what in his own acceptance paper Martin Kay called: ?", "acronym": "FE", "label": "feeling", "ID": "2944"}, {"sentence": "In the free-text parts of the reviews, the author describes his subjective FEs and views towards the product, and in the sections Pros and cons and Bottomline he summa- rizes the advantages and disadvantages of the prod- uct, usually by providing some keyphrases or short sentences.", "acronym": "FE", "label": "feeling", "ID": "2945"}, {"sentence": "This is a FE peculiar to mathematics, I think, because the talent range is so much wider than in most subjects, even at the top level.", "acronym": "FE", "label": "feeling", "ID": "2946"}, {"sentence": "The resolution of this prob- lem can lead to a complete, realistic and coher- ent analysis of the natural language, therefore ma- jor attention is drawn to the opinion, sentiment and emotion analysis, and to the identification of be- liefs, thoughts, FEs and judgments (Quirk et al, 1985), (Wilson and Wiebe, 2005).", "acronym": "FE", "label": "feeling", "ID": "2947"}, {"sentence": "As the sentence proceeds  after the object Roma, the new word a causes things to  change again and we go back with a FE of surprise  to the first hypothesis.", "acronym": "FE", "label": "feeling", "ID": "2948"}, {"sentence": "In our study, we build a semantic role  labeling system as an intermediate step to label  opinion holders and topics by training it on opin- ion-bearing frames and their FE in  FrameNet.", "acronym": "FE", "label": "frame elements", "ID": "2949"}, {"sentence": "Expanding the Lexicon  In order to expand our lexicon for each of the  FE, we used the human-built knowledge  base (WordNet (Fellbaum 1998)) and its rich  hierarchical structure.", "acronym": "FE", "label": "frame elements", "ID": "2950"}, {"sentence": "We assume that the FE are  conditionally independent from each other, given the  frame.", "acronym": "FE", "label": "frame elements", "ID": "2951"}, {"sentence": "For  this task, we assume that that the frame is a latent  class variable (whose domain is the set of lexical  units) and the FE are variables whose  domain is the expanded lexicon (FrameNet +  WordNet).", "acronym": "FE", "label": "frame elements", "ID": "2952"}, {"sentence": "schematic  representations of situations involving various  FE such as participants, props, and  other conceptual roles.?", "acronym": "FE", "label": "frame elements", "ID": "2953"}, {"sentence": "The result is a database that contains a set  of frames (related through hierarchy and  composition), a set of FE for each frame,  and a set of frame annotated sentences that covers the  different patterns of usage for lexical units in the  frame.", "acronym": "FE", "label": "frame elements", "ID": "2954"}, {"sentence": "We filtered out all of the non-relevant terms in  all FE lexicons.", "acronym": "FE", "label": "frame element", "ID": "2955"}, {"sentence": "Figure 1  We then used the sum-product algorithm (Frey  1998) for statistical inference on the FE  induced graph (such as in Figure 1).", "acronym": "FE", "label": "frame element", "ID": "2956"}, {"sentence": "Expanding the Lexicon  In order to expand our lexicon for each of the  FEs, we used the human-built knowledge  base (WordNet (Fellbaum 1998)) and its rich  hierarchical structure.", "acronym": "FE", "label": "frame element", "ID": "2957"}, {"sentence": "This gave us a graphical model of the form  shown in Figure 1 for the FrameNet FE  Suspect and WordNet category Thief.", "acronym": "FE", "label": "frame element", "ID": "2958"}, {"sentence": "For our initial experiments, we built  a graph whose leaf was the enclosing category of the  FrameNet annotated FE.", "acronym": "FE", "label": "frame element", "ID": "2959"}, {"sentence": "=   4.2 Relevance of a WordNet Nodes  Throughout our experiments with the training  data, we discovered that some infrequent tail terms in  the FE lexicon that might not be filtered  out by the statistical inference algorithm but still are  frequently used in relevant text.", "acronym": "FE", "label": "frame element", "ID": "2960"}, {"sentence": "n), a set of FEs for each frame,  and a set of frame annotated sentences that covers the  different patterns of usage for lexical units in the  frame.", "acronym": "FE", "label": "frame element", "ID": "2961"}, {"sentence": "We filtered out all of the non-relevant terms in  all FE", "acronym": "FE", "label": "frame element", "ID": "2962"}, {"sentence": "on  In order to expand our lexicon for each of the  FEs, we used the human-built knowledge  base (WordNet (Fellbaum 1998)) and its rich  hierarchical structure.", "acronym": "FE", "label": "frame element", "ID": "2963"}, {"sentence": "The result is a database that contains a set  of frames (related through hierarchy and  composition), a set of FEs for each frame,  and a set of frame annotated sentences that covers the  different patterns of usage for lexical units in the  frame.", "acronym": "FE", "label": "frame element", "ID": "2964"}, {"sentence": "We com- bined MT systems for FEh and German - English using provided data only.", "acronym": "FE", "label": "French - Englis", "ID": "2965"}, {"sentence": "Our experience on choosing pivot languages is that: (1) a pivot language should be a language whose translation quality can be well guaranteed by the MT engines; (2) it is better to choose a pivot language similar to the source lan- guage (e.g., FEh), which is easier to translate; (3) the translation quality of a pivot lan- guage should not vary a lot among the MT en- gines.", "acronym": "FE", "label": "French - Englis", "ID": "2966"}, {"sentence": "The 1992 Evaluation tested three research MT systems:  CANDIDE (IBM, FEh) uses a statistical  language modeling technique based on speech recognition  algorithms (see Brown et al, 1990).", "acronym": "FE", "label": "French - Englis", "ID": "2967"}, {"sentence": "Spanish-English 976 Number of Words Language Pair Foreign English German - English 42 M 45 M Czech - English 56 M 65 M Spanish - English 232 M 210 M FEh 962 M 827 M Table 3: Training data statistics.", "acronym": "FE", "label": "French - Englis", "ID": "2968"}, {"sentence": "Aligning Sentences in Bilingual  Texts FEh and French - Arabic.", "acronym": "FE", "label": "French - Englis", "ID": "2969"}, {"sentence": "SYSTRAN Translation Systems Inc. provided output from  a FEh production system and a Spanish -  English pilot prototype.", "acronym": "FE", "label": "French - Englis", "ID": "2970"}, {"sentence": "Of the ten prolific work- ers, one was located in the United States, one in the GB, and eight in India.", "acronym": "GB", "label": "United Kingdom", "ID": "2971"}, {"sentence": "In 22nd COLING, pages 51?54, Manchester, GB.", "acronym": "GB", "label": "United Kingdom", "ID": "2972"}, {"sentence": "c?2010 Association for Computational Linguistics Combining Manual Rules and Supervised Learning for Hedge Cue and Scope Detection Marek Rei Computer Laboratory University of Cambridge GB Marek.Rei@cl.cam.ac.uk Ted Briscoe Computer Laboratory University of Cambridge GB Ted.Briscoe@cl.cam.ac.uk Abstract Hedge cues were detected using a super- vised Conditional Random Field (CRF) classifier exploiting features from the RASP parser.", "acronym": "GB", "label": "United Kingdom", "ID": "2973"}, {"sentence": "c?2013 Association for Computational Linguistics DALE: A Word Sense Disambiguation System for Biomedical Documents Trained using Automatically Labeled Examples Judita Preiss and Mark Stevenson Department of Computer Science, University of Sheffield Regent Court, 211 Portobello Sheffield S1 4DP, GB j.preiss,m.stevenson@dcs.shef.ac.uk Abstract Automatic interpretation of documents is hampered by the fact that language contains terms which have multiple meanings.", "acronym": "GB", "label": "United Kingdom", "ID": "2974"}, {"sentence": "Automatic Feature Selection for Agenda-Based Dependency Parsing Miguel Ballesteros Natural Language Processing Group Universitat Pompeu Fabra Barcelona, Spain miguel.ballesteros@upf.edu Bernd Bohnet School of Computer Science University of Birmingham Birmingham, GB bohnetb@cs.bham.ac.uk Abstract In this paper we present an in-depth study on automatic feature selection for beam-search depen- dency parsers.", "acronym": "GB", "label": "United Kingdom", "ID": "2975"}, {"sentence": "GB?)", "acronym": "GB", "label": "Great Britain", "ID": "2976"}, {"sentence": "Phonetics, Pelican Books, GB.", "acronym": "GB", "label": "Great Britain", "ID": "2977"}, {"sentence": "In: L. J. Cahill & ll,iehard  Coates \\[eds.\\]: Sussex Papers in General and Com-  putational Linguistics: Presented to the Linguis-  tic Association of GB Conference at  Brighton Polytechnic, 6th-8th April 1992.", "acronym": "GB", "label": "Great Britain", "ID": "2978"}, {"sentence": "There is, in fact, also an informal  personalized style which may be  illustrated by the following quotes  from a British newspaper (the  European):  Players in the Rugby League test  match between GB and  Australia on Saturday may need  longer studs and safe hands to  tackle the tricky conditions at  London 's  Wembley  stadium.", "acronym": "GB", "label": "Great Britain", "ID": "2979"}, {"sentence": "Estimation of  conditional Probabilities with Decision Trees and  an Application to Fine-Grained POS tagging,  COLING 2008, Manchester, GB.", "acronym": "GB", "label": "Great Britain", "ID": "2980"}, {"sentence": "In Proceedings of the 12th Con- ference on Computational Natural Language Learning (CoNLL-2008), Manchester, GB.", "acronym": "GB", "label": "Great Britain", "ID": "2981"}, {"sentence": "9 : Horoscope GB 9.11?", "acronym": "GB", "label": "Gambling", "ID": "2982"}, {"sentence": "Animals Fishing Plants  Baseball Flight Race  Body Football Religion  Botany GB Sick  Boundary Grasp Size  Chess Health Sound  Color Height Sports  Combustion Light Taste  Cooking Liquid Temperature  Courtship Machine Texture  Cut Maritime Theater  Directional force Money Time of day  Dogs Motion Toxicity  Drug use Mythology Vehicle  Electricity Natural disasters War  Energy source Nuclear Weaponry  Entry Odor Weather", "acronym": "GB", "label": "Gambling", "ID": "2983"}, {"sentence": "For instance, the polarity for GB increases the number of crimes is ?", "acronym": "GB", "label": "Gambling", "ID": "2984"}, {"sentence": "In this paper, to reduce computational cost, we restrict that adjacent SIBs in the subtrees must be adjacent in the original tree.", "acronym": "SIB", "label": "sibling", "ID": "2985"}, {"sentence": "Starting with all the synsets containing a term in 11 a triple, the term is assigned to the synset with higher similarity to the contexts from where the triple was extracted, computed based on the terms in the synset, SIB synsets and direct hyponym synsets.", "acronym": "SIB", "label": "sibling", "ID": "2986"}, {"sentence": "Thus, we only need to consider two types of words: (i) the children of the rightmost leaf of S r , (ii) the adjacent right SIB of the any node in S r , as shown in Figure 2.", "acronym": "SIB", "label": "sibling", "ID": "2987"}, {"sentence": "Its possible values include parent, SIB, child, uncle, grand parent etc.", "acronym": "SIB", "label": "sibling", "ID": "2988"}, {"sentence": "today is not a subtree because He and today are not adjacent SIBs.", "acronym": "SIB", "label": "sibling", "ID": "2989"}, {"sentence": "For argument, a dependency version of the prun- ing algorithm in (Xue and Palmer, 2004) is used to find, in an iterative way, the current syntactic head and its SIBs in a parse tree in a constituent- based representation.", "acronym": "SIB", "label": "sibling", "ID": "2990"}, {"sentence": "The total number of subtrees is no greater than L 3 , because the level of a subtree is less than L, and for the children of each node, there are at most L 2 subsequences of SIBs.", "acronym": "SIB", "label": "sibling", "ID": "2991"}, {"sentence": "University of Geneva  3SIBs, Geneva  4SIM, University Hospital of Geneva  {imad.tbahriti;christine.chichester;frederique.lisacek}@genebio.com - patrick.ruch@epfl.ch   Abstract  The aim of this study is to investigate the  relationships between citations and the  scientific argumentation found in the abstract.", "acronym": "SIB", "label": "Swiss Institute of Bioinformatic", "ID": "2992"}, {"sentence": "The MRR An- swer Rank (MRAR) is used to compute the over- all performance of the systems participating in the TREC evaluation efZ*[gZ^] _ bihkj b ) _ `La;bdc Qml In ad- dition, TREC-9 imposed the constraint that an an- swer is considered correct only when the textual context from the document that contains it can account for it.", "acronym": "MRR", "label": "Mean Reciprocal", "ID": "2993"}, {"sentence": "For eval- uation we employed the averaged top-1 accuracy and MRR Rank (MMR) at finding the real comparable document in the other language.", "acronym": "MRR", "label": "Mean Reciprocal", "ID": "2994"}, {"sentence": "As one measure of success, we show that a MRR Rank near 0.5 can be achieved when more than one relevant response exists; this corresponds to a relevant response appearing in the second position of a ranked list, on average (by the harmonic mean).", "acronym": "MRR", "label": "Mean Reciprocal", "ID": "2995"}, {"sentence": "5 Evaluation and Experimental Results 5.1 Evaluation Measures We use the following evaluation measures for evalu- ating the results: MRR Rank For a question, the recip- rocal rank RR is", "acronym": "MRR", "label": "Mean Reciprocal", "ID": "2996"}, {"sentence": "6http://lucene.apache.org/java/docs/ 48 T -SW T -SW +SC S -SW S -SW +SC L -SW L -SW Preprocessing 50 60 70 80 90 100 Ac cu ra cy T -SW T -SW +SC S -SW S -SW +SC L -SW L -SW Preprocessing 0.5 0.6 0.7 0.8 0.9 1.0 M RR Matching coefficient Overlap coefficient Normalised edit distance Ngram Overlap Term vector similarity Lucene Figure 1: Accuracy (%) and MRR Rank obtained for different question similarity measures and pre- processing strategies: tokens (T), stemming (S), lemmatisation (L), stop words removal (-SW), spelling correction (+SC).", "acronym": "MRR", "label": "Mean Reciprocal", "ID": "2997"}, {"sentence": "In order to evaluate Qanda's performance on the  five tests we decided to use the MRR  Answer Rank (MRAR) technique which was used  for evaluating question-answering systems at TREC-  8 (Singhal, 1999).", "acronym": "MRR", "label": "Mean Reciprocal", "ID": "2998"}, {"sentence": "We measure the Top-1 accu- racy (i.e., whether the true comparable is the clos- est in the test set), and the MRR Rank of the true comparable, and report the average per- formance over the two retrieval directions.", "acronym": "MRR", "label": "Mean Reciprocal", "ID": "2999"}, {"sentence": "RESULTS We entered the TREC-9 short form QA track, and received an overall MRR score of 0.318, which put Webclopedia in essentially tied second place with two others.", "acronym": "MRR", "label": "Mean Reciprocal Rank", "ID": "3000"}, {"sentence": "For eval- uation we employed the averaged top-1 accuracy and MRR (MMR) at finding the real comparable document in the other language.", "acronym": "MRR", "label": "Mean Reciprocal Rank", "ID": "3001"}, {"sentence": "The MRR score for TFIDF and LDA1 are more than 80%.", "acronym": "MRR", "label": "Mean Reciprocal Rank", "ID": "3002"}, {"sentence": "As one measure of success, we show that a MRR near 0.5 can be achieved when more than one relevant response exists; this corresponds to a relevant response appearing in the second position of a ranked list, on average (by the harmonic mean).", "acronym": "MRR", "label": "Mean Reciprocal Rank", "ID": "3003"}, {"sentence": "5 Evaluation and Experimental Results 5.1 Evaluation Measures We use the following evaluation measures for evalu- ating the results: MRR For a question, the recip- rocal rank RR is", "acronym": "MRR", "label": "Mean Reciprocal Rank", "ID": "3004"}, {"sentence": "6http://lucene.apache.org/java/docs/ 48 T -SW T -SW +SC S -SW S -SW +SC L -SW L -SW Preprocessing 50 60 70 80 90 100 Ac cu ra cy T -SW T -SW +SC S -SW S -SW +SC L -SW L -SW Preprocessing 0.5 0.6 0.7 0.8 0.9 1.0 M RR Matching coefficient Overlap coefficient Normalised edit distance Ngram Overlap Term vector similarity Lucene Figure 1: Accuracy (%) and MRR obtained for different question similarity measures and pre- processing strategies: tokens (T), stemming (S), lemmatisation (L), stop words removal (-SW), spelling correction (+SC).", "acronym": "MRR", "label": "Mean Reciprocal Rank", "ID": "3005"}, {"sentence": "We measure the Top-1 accu- racy (i.e., whether the true comparable is the clos- est in the test set), and the MRR of the true comparable, and report the average per- formance over the two retrieval directions.", "acronym": "MRR", "label": "Mean Reciprocal Rank", "ID": "3006"}, {"sentence": "We took the MRR (the inverse of the harmonic mean) as a figure of merit for each configuration, and used a paired two-tailed \u0000 - test (with p \u0001 0.05) to assess the statistical significance of observed differences.", "acronym": "MRR", "label": "mean reciprocal rank", "ID": "3007"}, {"sentence": "The accuracy of a list C(w) for an attribute w is measured by a scoring metric that corresponds to a modification (Pas?ca and Alfonseca, 2009) of the MRR score (Voorhees and Tice, 2000): DRR = max 1 rank(c)? (", "acronym": "MRR", "label": "mean reciprocal rank", "ID": "3008"}, {"sentence": "Experimental results show that EMR can select seed sets that pro- vide significantly higher MRR on realistic data than existing naive selection methods or random seed sets.", "acronym": "MRR", "label": "mean reciprocal rank", "ID": "3009"}, {"sentence": "Consid- ering L2, accuracy, average probability, equal er- ror rate, log probability and MRR across all components of the the dialog state, these give a total of 318 metrics.", "acronym": "MRR", "label": "mean reciprocal rank", "ID": "3010"}, {"sentence": "These accuracies correspond to MRRs of 0.47?0.51, and the random baseline on WORDSPLEND and MAC-CONF in terms of this mea- sure is 0.03?0.07.", "acronym": "MRR", "label": "mean reciprocal rank", "ID": "3011"}, {"sentence": "5 Similar results are obtained with MRR.", "acronym": "MRR", "label": "mean reciprocal rank", "ID": "3012"}, {"sentence": "One of them consists in using pre-processing components that can detect the LV at stake.", "acronym": "LV", "label": "language variety", "ID": "3013"}, {"sentence": "These genres vary considerably in terms of their functions and the LV employed.", "acronym": "LV", "label": "language variety", "ID": "3014"}, {"sentence": "Starting with the vocabulary extracted from a corpus of European Spanish newspaper texts, we expect our bilingual dictionary to be biased with respect to the LV, register and genre.", "acronym": "LV", "label": "language variety", "ID": "3015"}, {"sentence": "By a vocabulary profile, it is meant that the fre- quency of each vocabulary item is calculated from a reference corpus covering the LV of the target situation.", "acronym": "LV", "label": "language variety", "ID": "3016"}, {"sentence": "Such lexicons may not be available for a given LV.", "acronym": "LV", "label": "language variety", "ID": "3017"}, {"sentence": "4.4 Dialectal Arabic Features Dialect: We apply the two gold LV features, {MSA, DA}, on the Twitter data set to rep- resent whether the tweet is in MSA or in a dialect.", "acronym": "LV", "label": "language variety", "ID": "3018"}, {"sentence": "These include verbs of becoming or seeming (e.g., trans- form, appear), LVs, auxiliaries, and aspec- tual verbs.", "acronym": "LV", "label": "light verb", "ID": "3019"}, {"sentence": "at Boulder, Boulder CO 80309  2Department of Computer Science, Brandeis University, Waltham MA 02453  3Department of Linguistics, University of Illinois at Urbana-Champaign, Urbana IL 61801  {hwangd,claire.bonial,aous.mansouri,ashwini.vaidya,martha.palmer}  @colorado.edu, bhatia@illinois.edu, xuen@brandeis.edu         Abstract  In this paper, we have addressed the task  of PropBank annotation of LV  constructions, which like multi-word  expressions pose special problems.", "acronym": "LV", "label": "light verb", "ID": "3020"}, {"sentence": "ha.palmer}  @colorado.edu, bhatia@illinois.edu, xuen@brandeis.edu         Abstract  In this paper, we have addressed the task  of PropBank annotation of LV  constructions, which like multi-word  expressions pose special problems.", "acronym": "LV", "label": "light verb", "ID": "3021"}, {"sentence": "andeis.edu         Abstract  In this paper, we have addressed the task  of PropBank annotation of LV  constructions, which like multi-word  expressions pose special problems.", "acronym": "LV", "label": "light verb", "ID": "3022"}, {"sentence": "We also discuss how in  various languages the LV  constructions are identified and can be  distinguished from the non-LV  word groupings.", "acronym": "LV", "label": "light verb", "ID": "3023"}, {"sentence": "The final method involves three passes:  (1) manual identification of a LV  construction, (2) annotation based on the  LV construction?s Frame File, and  (3) a deterministic merging of the first  two passes.", "acronym": "LV", "label": "light verb", "ID": "3024"}, {"sentence": "Figure 2: The LDA Model (left) and our SBT Document Model (right).", "acronym": "LDA", "label": "Latent Dirichlet Allocation", "ID": "3025"}, {"sentence": "LDA.", "acronym": "LDA", "label": "Latent Dirichlet Allocation", "ID": "3026"}, {"sentence": "night day cold long sleep dream days ... 9 mind world lose nothing left gonna shake ... 10 time life long live day end die ... 11 god lord man hell king heaven jesus ... 12 hear play call people good talk heard ... 13 sky eyes fire fly blue high sea ... 14 heart inside pain soul break broken deep ... 15 go back home again time gonna coming ... Table 2: Example words from a topic model induced by LDA on the lyrics corpus.", "acronym": "LDA", "label": "Latent Dirichlet Allocation", "ID": "3027"}, {"sentence": "Our expe- riments with LDA corrobo- rate a recent proposal that appending training  data with perceptible feature or property infor- mation for a subset of concrete nouns can signif- icantly improve the quality of the model?s lexical  representations.", "acronym": "LDA", "label": "Latent Dirichlet Allocation", "ID": "3028"}, {"sentence": "Gibbs sampling in the generative  model of LDA.", "acronym": "LDA", "label": "Latent Dirichlet Allocation", "ID": "3029"}, {"sentence": "As a point of comparison, the original approach for LyriCloud was to employ a topic model such as LDA (Blei et al, 2003).", "acronym": "LDA", "label": "Latent Dirichlet Allocation", "ID": "3030"}, {"sentence": "Word features for LDAn.", "acronym": "LDA", "label": "latent Dirichlet alocatio", "ID": "3031"}, {"sentence": "Online inference of topics with LDAn.", "acronym": "LDA", "label": "latent Dirichlet alocatio", "ID": "3032"}, {"sentence": "Holistic sentiment analysis across languages: Multilingual supervised LDAn.", "acronym": "LDA", "label": "latent Dirichlet alocatio", "ID": "3033"}, {"sentence": "A spectral algorithm for LDAn.", "acronym": "LDA", "label": "latent Dirichlet alocatio", "ID": "3034"}, {"sentence": "Online learning for LDAn.", "acronym": "LDA", "label": "latent Dirichlet alocatio", "ID": "3035"}, {"sentence": "Holistic sentiment analysis across languages: Multilingual su- pervised LDAn.", "acronym": "LDA", "label": "latent Dirichlet alocatio", "ID": "3036"}, {"sentence": "Word features for latent DirichLDA.", "acronym": "LDA", "label": "let alocation", "ID": "3037"}, {"sentence": "Latent dirichLDA.", "acronym": "LDA", "label": "let alocation", "ID": "3038"}, {"sentence": "Latent Dirich- LDA.", "acronym": "LDA", "label": "let alocation", "ID": "3039"}, {"sentence": "Latent DirichLDA.", "acronym": "LDA", "label": "let alocation", "ID": "3040"}, {"sentence": "Word features for LDA.", "acronym": "LDA", "label": "latent Dirichlet alocation", "ID": "3041"}, {"sentence": "Online inference of topics with LDA.", "acronym": "LDA", "label": "latent Dirichlet alocation", "ID": "3042"}, {"sentence": "Holistic sentiment analysis across languages: Multilingual supervised LDA.", "acronym": "LDA", "label": "latent Dirichlet alocation", "ID": "3043"}, {"sentence": "A spectral algorithm for LDA.", "acronym": "LDA", "label": "latent Dirichlet alocation", "ID": "3044"}, {"sentence": "Online learning for LDA.", "acronym": "LDA", "label": "latent Dirichlet alocation", "ID": "3045"}, {"sentence": "Holistic sentiment analysis across languages: Multilingual su- pervised LDA.", "acronym": "LDA", "label": "latent Dirichlet alocation", "ID": "3046"}, {"sentence": "Our expe- riments with Latent DirichLDA corrobo- rate a recent proposal that appending training  data with perceptible feature or property infor- mation for a subset of concrete nouns can signif- icantly improve the quality of the model?s lexical  representations.", "acronym": "LDA", "label": "let Allocation", "ID": "3047"}, {"sentence": "Gibbs sampling in the generative  model of Latent DirichLDA.", "acronym": "LDA", "label": "let Allocation", "ID": "3048"}, {"sentence": "As a point of comparison, the original approach for LyriCloud was to employ a topic model such as Latent DirichLDA (Blei et al, 2003).", "acronym": "LDA", "label": "let Allocation", "ID": "3049"}, {"sentence": "ld long sleep dream days ... 9 mind world lose nothing left gonna shake ... 10 time life long live day end die ... 11 god lord man hell king heaven jesus ... 12 hear play call people good talk heard ... 13 sky eyes fire fly blue high sea ... 14 heart inside pain soul break broken deep ... 15 go back home again time gonna coming ... Table 2: Example words from a topic model induced by Latent DirichLDA on the lyrics corpus.", "acronym": "LDA", "label": "let Allocation", "ID": "3050"}, {"sentence": "Latent Di- richLDA.", "acronym": "LDA", "label": "let Allocation", "ID": "3051"}, {"sentence": "Dur- ing learning, TC is calculated online according to the noise model, penalising all slots which are filled but not confirmed.", "acronym": "TC", "label": "Task Completion", "ID": "3052"}, {"sentence": "31  TC The first column of Table 4 shows  that the subjects were able to provide an answer in allthe  scenarios when the system was in robust mode, whereas  only 83% of the scenarios were completed in non-robust  mode.", "acronym": "TC", "label": "Task Completion", "ID": "3053"}, {"sentence": "TC Time The task completion time is  summarized in the third column of Table 4.", "acronym": "TC", "label": "Task Completion", "ID": "3054"}, {"sentence": "Actual TC?", "acronym": "TC", "label": "Task Completion", "ID": "3055"}, {"sentence": "Partial TC statistics System Succ. (", "acronym": "TC", "label": "Task Completion", "ID": "3056"}, {"sentence": "Full TC statistics System Succ. (", "acronym": "TC", "label": "Task Completion", "ID": "3057"}, {"sentence": "Perceived TC?", "acronym": "TC", "label": "Task Completion", "ID": "3058"}, {"sentence": "Fea- ture Generation for TC Using World Knowledge.", "acronym": "TC", "label": "Text Categorization", "ID": "3059"}, {"sentence": "TC with  Support Vector Machines: Learning with Many  Relevant Features.", "acronym": "TC", "label": "Text Categorization", "ID": "3060"}, {"sentence": "In Proceedings of AAAI-98 Workshop on Learning for TC.", "acronym": "TC", "label": "Text Categorization", "ID": "3061"}, {"sentence": "Overcoming the Brittleness Bottleneck using Wikipedia: Enhancing TC with Encyclopedic Knowledge.", "acronym": "TC", "label": "Text Categorization", "ID": "3062"}, {"sentence": "1 Introduction The subjective analysis of a text is becoming impor- tant for many Natural Language Processing (NLP) applications such as Question Answering, Informa- tion Extraction, TC among others (Shanahan et al, 2006).", "acronym": "TC", "label": "Text Categorization", "ID": "3063"}, {"sentence": "RCV1: A New Benchmark Col- lection for TC Research.", "acronym": "TC", "label": "Text Categorization", "ID": "3064"}, {"sentence": "Some properties of surface syntactic structure can be ex- pressed only in terms of both dependency (or its TC called dominance) and prece- dence.", "acronym": "TC", "label": "transitive closure", "ID": "3065"}, {"sentence": "To compensate for this, Shaw and Hatzivassiloglou (1999) propose to compute the TC of the ordering relation: if a ?", "acronym": "TC", "label": "transitive closure", "ID": "3066"}, {"sentence": "A single LIG derivation step  will be notated with ~ and its reflexive TC with 3* .", "acronym": "TC", "label": "transitive closure", "ID": "3067"}, {"sentence": "The reflex-  ive and TC of \\[-c, is denoted with  H~. The state projection of Hc is the binary  relation   (Ho) = {(r, r')l  /3x: (p,/3) (p',/3x)}.", "acronym": "TC", "label": "transitive closure", "ID": "3068"}, {"sentence": "After all the mentions have been processed, the links are used to generate a TC that corresponds to the recognized entities in the document.", "acronym": "TC", "label": "transitive closure", "ID": "3069"}, {"sentence": "(b) 73 is the reflexive TC of  79, and 73 is antisymmetric.", "acronym": "TC", "label": "transitive closure", "ID": "3070"}, {"sentence": "The set of rela-  tions used in TGs come from three  sources.", "acronym": "TG", "label": "temporary graph", "ID": "3071"}, {"sentence": "syntactic pattern  s:np\\[A\\],vp\\[B\\]  John eats  vp:vp\\[A\\],inf_vp\\[B\\]  eat to grow  TG  \\[B\\]- >(instrument )- > \\[A\\]  \\[A\\]-> (part-of)-> \\[B\\]  \\[Bl->(loc)->\\[A\\]  TG  \\[B\\]-> (agent)-> \\[h\\]  \\[eat\\]- > (agent)- >\\[John\\]  \\[A\\]-> (goal)-> \\[B\\]  \\[e at\\]- > ( goal)- > \\[grow\\]  Table 1: Examples of relations found in sentences  and their corresponding TGs  3 Knowledge integration  This section describes how given a trigger word,  we per", "acronym": "TG", "label": "temporary graph", "ID": "3072"}, {"sentence": "syntactic pattern  s:np\\[A\\],vp\\[B\\]  John eats  vp:vp\\[A\\],inf_vp\\[B\\]  eat to grow  TG  \\[B\\]- >(instrument )- > \\[A\\]  \\[A\\]-> (part-of)-> \\[B\\]  \\[Bl->(loc)->\\[A\\]  TG  \\[B\\]-> (agent)-> \\[h\\]  \\[eat\\]- > (agent)- >\\[John\\]  \\[A\\]-> (goal)-> \\[B\\]  \\[e at\\]- > ( goal)- > \\[grow\\]  Table 1: Examples of relations found in sentences  and their corresponding TGs  3 Knowledge integration  This section describes how given a trigger word,  we perform a series of forward and backward  searches in the dictionary to build a CCKG con-  taining", "acronym": "TG", "label": "temporary graph", "ID": "3073"}, {"sentence": "By keeping  the preposition itself within the TG,  we delay the ambiguity resolution process until  we have gathered more information and we even  hopefully avoid the decision process as the ambi-  guity might later be resolved by the integration  process itself.", "acronym": "TG", "label": "temporary graph", "ID": "3074"}, {"sentence": "Our processing of the definitions results in the  construction of a special type of conceptual graph  which we call a TG.", "acronym": "TG", "label": "temporary graph", "ID": "3075"}, {"sentence": "syntactic pattern  s:np\\[A\\],vp\\[B\\]  John eats  vp:vp\\[A\\],inf_vp\\[B\\]  eat to grow  TG  \\[B\\]- >(instrument )- > \\[A\\]  \\[A\\]-> (part-of)-> \\[B\\]  \\[Bl->(loc)->\\[A\\]  TG  \\[B\\]-> (agent)-> \\[h\\]  \\[eat\\]- > (agent)- >\\[John\\]  \\[A\\]-> (goal)-> \\[B\\]  \\[e at\\]- > ( goal)- > \\[grow\\]  Table 1: Examples of relations found in sentences  and their corresponding TGs  3 Knowledge integration  This section describes how given a trigger word,  we perform a series of forward and backward  searches in the dictionary to build a CCKG con-  taining useful information pertaining to the trig-  ger word and to closely related words.", "acronym": "TG", "label": "temporary graph", "ID": "3076"}, {"sentence": "Precision Overgeneratio n TST3 8 90 T5T4 10 87 Table 3 : Overall precision scores (all templates row) The effects of this TG strategy on our precision scores were fairly dramatic .", "acronym": "TG", "label": "template generation", "ID": "3077"}, {"sentence": "There are three major steps in LSI TG : 1) the first pass, in which templates for specifie d events and the related entity templates are generated ; 2) the second pass, in which entity templates are generated for MUC-5 relevant words that are not treated in the first pass ; and 3) template linking, in which co-reference relations are determined .", "acronym": "TG", "label": "template generation", "ID": "3078"}, {"sentence": "In the LSI TG for the walkthrough text, during the first pass two critical EME domai n verbs are identified : \"use\" and \"sell\", Event templates are generated for them with a set of theta role slot s (agent, patient, etc .)", "acronym": "TG", "label": "template generation", "ID": "3079"}, {"sentence": "AIML is an XML language that provides a pat- tern/TG model mainly used for chatbot systems.", "acronym": "TG", "label": "template generation", "ID": "3080"}, {"sentence": "Less than 10 percent of the current code is dedicated to MUC4-like processing, 90 percent of whic h consists of TG and merging as dictated by MUC4 template guidelines.", "acronym": "TG", "label": "template generation", "ID": "3081"}, {"sentence": "ISSUES WITH PRECISIO N Our precision error rate is largely accounted for by overly eager TG .", "acronym": "TG", "label": "template generation", "ID": "3082"}, {"sentence": "TG As mentioned, we rely on the inference mechanism to perform the bulk of the work of data extraction.", "acronym": "TG", "label": "Template Generation", "ID": "3083"}, {"sentence": "3.7 TG  The template generator takes the DDOs produced by  discourse processing and fills out the application-specific  templates.", "acronym": "TG", "label": "Template Generation", "ID": "3084"}, {"sentence": "--------------------------------------------------------------------- - IN-AND-OUT-PERSON-OF: \"Dooner\" (score= 0 ) \"McCann\" (score= 4 ) \"official\" (score= 4 ) \"James\" (score= 4) <=> \t \"Robert L. James \" \"One\" (score= 6) <=> \t \"Kevin Goldman \" IN-AND-OUT-NEW-STATUS-OF: POSITION-STATUS-GEN-IN (score= 0) POSITION-STATUS-GEN-OUT (score= 4) TG The template generator takes the DDOs produced by discourse processing and fills out the application-specifi c templates .", "acronym": "TG", "label": "Template Generation", "ID": "3085"}, {"sentence": "3.1.4 TG.", "acronym": "TG", "label": "Template Generation", "ID": "3086"}, {"sentence": "TG The task of the template generation component is to take the results of the abductive proofs in pragmatics , and put them into a template form according to the specifications of the task .", "acronym": "TG", "label": "Template Generation", "ID": "3087"}, {"sentence": "TG The template generator takes the DDOs produced by discourse processing and fills out the application-specifi c templates .", "acronym": "TG", "label": "Template Generation", "ID": "3088"}, {"sentence": "46 but the first two WN senses were considered.", "acronym": "WN", "label": "WordNet", "ID": "3089"}, {"sentence": "a subset of the STORIES dataset that included 10 stories, the following schemes were used for filter- ing of candidate speakers: (i) Scheme 1: all speak- ers linked with speech verbs, (ii) Scheme 2: speak- ers, who are persons or animals or spiritual entities according to their first WN sense, linked with speech verbs , and (iii) Scheme 3: as Scheme 2, 5SU+/?", "acronym": "WN", "label": "WordNet", "ID": "3090"}, {"sentence": "A character was retained if any of its hypernyms was found to fall into certain types of WN concepts: person, animal, plant, artifact, spiritual being, physical entity.", "acronym": "WN", "label": "WordNet", "ID": "3091"}, {"sentence": "Our approach demonstrates the  use of two human built knowledge bases  (WN and FrameNet) for the task of  semantic extraction.", "acronym": "WN", "label": "WordNet", "ID": "3092"}, {"sentence": "In this paper we report results obtained from  combining IE and graphical modeling techniques,  with semantic resources (WN and FrameNet)  for automatic Semantic Extraction.", "acronym": "WN", "label": "WordNet", "ID": "3093"}, {"sentence": "ORIES dataset that included 10 stories, the following schemes were used for filter- ing of candidate speakers: (i) Scheme 1: all speak- ers linked with speech verbs, (ii) Scheme 2: speak- ers, who are persons or animals or spiritual entities according to their first WN sense, linked with speech verbs , and (iii) Scheme 3: as Scheme 2, 5SU+/?", "acronym": "WN", "label": "WordNet", "ID": "3094"}, {"sentence": "The ongoing development of public  knowledge bases such as WN, FrameNet, CYC,  etc.", "acronym": "WN", "label": "WordNet", "ID": "3095"}, {"sentence": "l , 951 the following holds: max w1,w2,...,WN ,w0 [ N?", "acronym": "WN", "label": "wN", "ID": "3096"}, {"sentence": "WN ) = N?", "acronym": "WN", "label": "wN", "ID": "3097"}, {"sentence": "i||wi||2 We first prove the following relation: Lemma 2.1 Assume (w?0, ...,w?N ) = arg maxw1,...,WN ,w0 [ N?", "acronym": "WN", "label": "wN", "ID": "3098"}, {"sentence": "HoWNet is an on-line common- sense knowledge base providing a universal lexi- cal concept representation mechanism.", "acronym": "WN", "label": "wN", "ID": "3099"}, {"sentence": "i||wi||2 )] = max w1,w2,...,WN [ N?", "acronym": "WN", "label": "wN", "ID": "3100"}, {"sentence": "The semantic  information can be extracted from a Chinese- English bilingual semantic resource: HoWNet  (Dong, 2000).", "acronym": "WN", "label": "wN", "ID": "3101"}, {"sentence": "Even though Wambaya is in some respects very different from the well-studied lan- guages on which the Matrix is based, the existing machinery otherwise worked quite well, providing a significant JUMP-start to the grammar development process.", "acronym": "JUMP", "label": "jump", "ID": "3102"}, {"sentence": "The input consists of a rich set of fea- 1559 The        fox JUMPs     .stack buffer det (a) Parser configuration c. Figure 4: A schematic for the PARSER stack-propagation update.", "acronym": "JUMP", "label": "jump", "ID": "3103"}, {"sentence": "w0=JUMPs, w-1=fox, ?", "acronym": "JUMP", "label": "jump", "ID": "3104"}, {"sentence": "Despite large typological differ- ences between Wambaya and the languages on which the development of the resource was based, the Grammar Matrix is found to pro- vide a significant JUMP-start in the creation of the grammar for Wambaya: With less than 5.5 person-weeks of development, the Wambaya grammar was able to assign correct seman- tic representations to 76% of the sentences in a naturally occurring text.", "acronym": "JUMP", "label": "jump", "ID": "3105"}, {"sentence": "This paper aims to evaluate both the utility of the Grammar Matrix in JUMP-starting precision gram- mar development and the current state of its cross- linguistic hypotheses through a case study of a 977 language typologically very different from any of the languages above: the non-Pama-Nyungan Aus- tralian language Wambaya (Nordlinger, 1998).", "acronym": "JUMP", "label": "jump", "ID": "3106"}, {"sentence": "JUMP?", "acronym": "JUMP", "label": "jump", "ID": "3107"}, {"sentence": "In or- der to improve the transition model in HMM, we  extend the transition probabilities to be word de- pendent so that the probability of JUMP from  state aj_to aj not only depends on aj_, but also de- pends on the English word at position aj_.", "acronym": "JUMP", "label": "jumping", "ID": "3108"}, {"sentence": "For each source word e, we not only model  its self-transition probability, but also the probabil- ity of JUMP from word e to a different word", "acronym": "JUMP", "label": "jumping", "ID": "3109"}, {"sentence": "Important knowledge of  JUMP from e to another position, e.g., JUMP  80 forward (monotonic alignment) or JUMP back- ward (non-monotonic alignment), is not modeled.", "acronym": "JUMP", "label": "jumping", "ID": "3110"}, {"sentence": "%     state i=0 denotes the state of a null word at the  English side, and p0 is the probability of JUMP  to state 0, which is estimated from hold-out data.", "acronym": "JUMP", "label": "jumping", "ID": "3111"}, {"sentence": "For each source word e, we not only model  its self-transition probability, but also the probabil- ity of JUMP from word e to a different word.", "acronym": "JUMP", "label": "jumping", "ID": "3112"}, {"sentence": "In this paper, we initially present a method for collecting time-series data from students (e.g. marks, LA) and use ex- ample feedback from lecturers in a data- driven approach to content selection.", "acronym": "LA", "label": "lectures attended", "ID": "3113"}, {"sentence": "For in- stance, a summary that talks about the student?s performance, the number of lectures that he/she attended, potential health problems and revision done can be translated into the following ngram: start, marks, LA, health issues, re- vision, end.", "acronym": "LA", "label": "lectures attended", "ID": "3114"}, {"sentence": "It was found that the most frequent or- dering is: start, marks, hours studied, understand- ability, difficulty, deadlines, health issues, per- sonal issues, LA, revision, end.", "acronym": "LA", "label": "lectures attended", "ID": "3115"}, {"sentence": "138 Raw Data factors week 2 week 3 ... week 10 marks 5 4 ... 5 hours studied 1 2 ... 3 ... ... ... ... ... Trends from Data factors trend (1) marks trend other (2) hours studied trend increasing (3) understandability trend decreasing (4) difficulty trend decreasing (5) deadlines trend increasing (6) health issues trend other (7) personal issues trend decreasing (8) LA trend other (9) revision trend decreasing Summary Your overall performance was excellent during the semester.", "acronym": "LA", "label": "lectures attended", "ID": "3116"}, {"sentence": "factor trend way it is mentioned (1) marks stable average (2) hours studied decreasing trend (3) health issues decreasing weeks (4) LA stable average (5) personal issues increasing trend Table 2: The top 5 features out of the 18 selected through PCR analysis.", "acronym": "LA", "label": "lectures attended", "ID": "3117"}, {"sentence": "An example of the initial state of a student can be: <marks increased, LA stable, hours studied increased, understandability stable, difficulty increased, health issues stable, per- sonal issues stable, revision increased, 0> The agent explores the state space by selecting a factor and then by deciding whether to talk about it or not.", "acronym": "LA", "label": "lectures attended", "ID": "3118"}, {"sentence": "This value reflects the bias for LA: the ma- jority of relat", "acronym": "LA", "label": "low attachment", "ID": "3119"}, {"sentence": "subj?link 0 System?subj?link 0 +system?subj?link +currency?obj?link 0 system?subj?link 1.2 +company?subj?link +currency?obj?link 0 company?subj?link -1.1 empty 3 Table 3: Queries for computing high attach- ment (above) and LA (below) for Example 1.", "acronym": "LA", "label": "low attachment", "ID": "3120"}, {"sentence": "Parser X tends to produce LA for PPs while the language tends to have high attachment is a claim about structural bias, which is related to parser errors.", "acronym": "LA", "label": "low attachment", "ID": "3121"}, {"sentence": "The highest values are 12.2 for high attachment (mechanism) and 3 for LA (System).", "acronym": "LA", "label": "low attachment", "ID": "3122"}, {"sentence": "Other high-frequency word classes with relatively LA accuracy are preposi- tions (80%), adverbs (82%) and subordinating con- junctions (80%), for a total of another 23% of the test corpus.", "acronym": "LA", "label": "low attachment", "ID": "3123"}, {"sentence": "The value 3 for LA is the de- fault value for the empty context.", "acronym": "LA", "label": "low attachment", "ID": "3124"}, {"sentence": "The arc above the NP can go either left (for high attachment (a) of the coordinated phrase) or right (for LA (b) of the coordinated phrase).", "acronym": "LA", "label": "low attachment", "ID": "3125"}, {"sentence": "bj?link +currency?obj?link 10.2 mechanism?subj?link 3.4 +European Monetary System?subj?link 0 +currency?obj?link +System?subj?link +currency?obj?link 0 European Monetary System?subj?link 0 System?subj?link 0 +system?subj?link +currency?obj?link 0 system?subj?link 1.2 +company?subj?link +currency?obj?link 0 company?subj?link -1.1 empty 3 Table 3: Queries for computing high attach- ment (above) and LA (below) for Example 1.", "acronym": "LA", "label": "low attachment", "ID": "3126"}, {"sentence": "By contrast, Turkish (Oflazer et al, 2003; Atalay et al, 2003) exhibits high root accu- racy but consistently LA scores (about 88% for length 1 and 68% for length 2).", "acronym": "LA", "label": "low attachment", "ID": "3127"}, {"sentence": "This value reflects the bias for LA: the ma- jority of relative clauses are attached low.", "acronym": "LA", "label": "low attachment", "ID": "3128"}, {"sentence": "887 CONFLICT LOW MED HIGH 0.4 0.6 0.8 1.0  0  5 10 15  0  5 10 15  0  5 10 15  0  5 10 15 Number of annotated instances x 1,000 Ac cu ra cy algorithm MomResp LogResp Majority 20 Newsgroups CONFLICT LOW MED HIGH 0.4 0.6 0.8 1.0  0  5 10 15  0  5 10 15  0  5 10 15  0  5 10 15 Number of annotated instances x 1,000 Te st  Ac cu ra cy algorithm MomResp LogResp 20 Newsgroups Figure 3: Top row: Inferred LA on three-deep annotations.", "acronym": "LA", "label": "label accuracy", "ID": "3129"}, {"sentence": "Improving dependency LA using statistical post-editing: A cross-framework study.", "acronym": "LA", "label": "label accuracy", "ID": "3130"}, {"sentence": "Inferred LA on items that have been annotated is the primary task of crowdsourc- ing; we track this measure accordingly.", "acronym": "LA", "label": "label accuracy", "ID": "3131"}, {"sentence": "The SRL systems compared in (Ruppenhofer et al, 2010) all achieved precision in the mid 60% range, but SEMAFOR achieved substantially higher re- call, F 1 , and LA on this subtask. (", "acronym": "LA", "label": "label accuracy", "ID": "3132"}, {"sentence": "Table 2 reports the percent of the dataset that must be annotated three-deep before LOGRESP?s in- ferred LA surpasses that of MOMRESP.", "acronym": "LA", "label": "label accuracy", "ID": "3133"}, {"sentence": "LA?,", "acronym": "LA", "label": "label accuracy", "ID": "3134"}, {"sentence": "Figure 2 shows the degree of LA (i.e., the distance between UL and LL) over time for a control and a dysarthric speaker repeating the se- quence /ah p iy/. Here, the dysarthric speech is no- tably slower and has more excessive movement.", "acronym": "LA", "label": "lip aperture", "ID": "3135"}, {"sentence": "Figure 1: Two roughly equivalent Arabic sentences, one in MSA and one in LA, translated by the same MT system into English.", "acronym": "LA", "label": "Levantine Arabic", "ID": "3136"}, {"sentence": "The variations are a miniature version of the variations in LA in general.", "acronym": "LA", "label": "Levantine Arabic", "ID": "3137"}, {"sentence": "Darwish (2013)'s data is more focused on  Egyptian and LA and code switch- ing with English.", "acronym": "LA", "label": "Levantine Arabic", "ID": "3138"}, {"sentence": "For more information on PAL and LA in general, see (Rice and Sa?id, 1960; Cowell, 1964; Bateson, 1967; Brustad, 2000; Halloun, 2000; Holes, 2004; Elihai, 2004).", "acronym": "LA", "label": "Levantine Arabic", "ID": "3139"}, {"sentence": "Sawaf (2010), Salloum and Habash (2011) and Salloum and Habash (2013) used morpholog- ical analysis and morphosyntactic transformation rules for processing EGY and LA.", "acronym": "LA", "label": "Levantine Arabic", "ID": "3140"}, {"sentence": "PAL is part of the South LA dialect subgroup (of which Jordanian Arabic is another dialect).", "acronym": "LA", "label": "Levantine Arabic", "ID": "3141"}, {"sentence": "For example, LR means the source side site only allows LA trees and the target side site only allows right adjoining trees.", "acronym": "LA", "label": "left adjoining", "ID": "3142"}, {"sentence": "if node is substitution site or word then append site or word to y else append LA sites to y in outside-to-inside order foreach child c of node do append result of get-yield(c) to y append right adjoining sites to y in inside-to-outside order return y end function add-states(ruld-id, node-list) foreach substitution or adjunction site s i and in node-list do if s i is substitution site then state = concat(?q?,", "acronym": "LA", "label": "left adjoining", "ID": "3143"}, {"sentence": "On each side of the rule, we traverse the tree in a top- down, left-to-right order, recording words, substi- tution sites, and adjoining sites in the order en- countered (LAs before the node?s chil- dren and right adjoinings after).", "acronym": "LA", "label": "left adjoining", "ID": "3144"}, {"sentence": "We attach all foreign-side adjoining sites to be LA, except on the right side of the right-hand child.", "acronym": "LA", "label": "left adjoining", "ID": "3145"}, {"sentence": "For example, in the alignment of  LA factum with Spanish hecho, the affricate \\[if\\]  should be linked with both \\[k\\] and \\[t\\] rather than  with just one of them, because it originates from the  merger of the two consonants.", "acronym": "LA", "label": "Latin", "ID": "3146"}, {"sentence": "For  example, at morphological level, foreign suffixes,  mostly originating from LA and Greek, are of- ten ?", "acronym": "LA", "label": "Latin", "ID": "3147"}, {"sentence": "occurrence of bi-grams or tri-grams typical  for other languages (especially LA and  English), and  ?", "acronym": "LA", "label": "Latin", "ID": "3148"}, {"sentence": "Note that taking a se-  g r ~e s  g r ~ m e n  II g r s II  II g r a m II en  II g r II s  II g r a II men  Table 5: Three alignments of English grass and  LA gramen obtained with global, semiglobal, and  local comparison.", "acronym": "LA", "label": "Latin", "ID": "3149"}, {"sentence": "glish grass with LA gramen, it is importa~nt to  match only the first three segments in each word;  the remaining segments are unrelated.", "acronym": "LA", "label": "Latin", "ID": "3150"}, {"sentence": "Note that taking a se-  g r ~e s  g r ~ m e n  II g r s II  II g r a m II en  II g r II s  II g r a II men  Table 5: Three alignments of English grass and  LA gramen obtained with global, semiglobal, a", "acronym": "LA", "label": "Latin", "ID": "3151"}, {"sentence": "The remaining average  feature distances were calculated using a set of most  frequent phonemes represented by 25 letters of the  LA alphabet (all but q).", "acronym": "LA", "label": "Latin", "ID": "3152"}, {"sentence": "Zernik, U. LA.", "acronym": "LA", "label": "Lexical Acquisition", "ID": "3153"}, {"sentence": "A Graph Model for Unsupervised LA.", "acronym": "LA", "label": "Lexical Acquisition", "ID": "3154"}, {"sentence": "LA: Ex-  ploiting On-Line Resou~ves to Build a Lex-  icon.", "acronym": "LA", "label": "Lexical Acquisition", "ID": "3155"}, {"sentence": "Carter, D.M. (1989) \"LA in  the Core Language Engine\", Proceedings  of the Fourth Conference of the European  Chapter of the Association for Computa-  tional Linguistics, pp 137-144.", "acronym": "LA", "label": "Lexical Acquisition", "ID": "3156"}, {"sentence": "Miller A. G., C. Fellbaum and D. Gross 1989,  WORDNET a Lexical Database Organised on  Psycholinguistic Principles, Proceedings IJCAI,  First International LA Workshop,  Detroit.", "acronym": "LA", "label": "Lexical Acquisition", "ID": "3157"}, {"sentence": "We only use EP along with two sections of the Gigaword corpus: Agence France Press English Service(afe) and The New York Times NW Service (nyt).", "acronym": "NW", "label": "Newswire", "ID": "3158"}, {"sentence": "The tail documents typically have lower phrase coverage, thus incor- rect phrase translation pairs derived from incorrect 937 # phrase pairs Average Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 934206 60.74 28.05 16.35 69.02 17.83 25.60 ALF 797685 60.33 28.52 15.91 68.31 19.27 24.52 Table 5: Improved Chinese-English NW Translation with Alignment Link Filtering # phrase pairs Average Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 934206 62.87 25.08 18.89 66.55 18.80 23.88 ALF 797685 62.30 24.89 18.70 65.97 19.25 23.36 Table 6: Improved Chinese-English Web-Blog Translation with Alignment Link Filtering alignment links are more likely to be selected.", "acronym": "NW", "label": "Newswire", "ID": "3159"}, {"sentence": "The first is the sentence alignment confidence measure, based on which the best whole sentence alignment is se- 938 # phrase pairs Average Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 939911 43.53 50.51 -3.49 53.14 40.60 6.27 ALF 618179 43.11 50.24 -3.56 51.75 42.05 4.85 Table 7: Improved Arabic-English NW Translation with Alignment Link Filtering # phrase pairs Average Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 598721 49.91 39.90 5.00 57.30 30.98 13.16 ALF 383561 48.94 40.00 4.42 55.99 31.92 12.04 Table 8: Improved Arabic-English Web-Blog Translation with Alignment Link Filtering lected among multiple alignments and it obtained 0.8 F-measure improvement over the single best", "acronym": "NW", "label": "Newswire", "ID": "3160"}, {"sentence": "Associated Press NW, June 1988.", "acronym": "NW", "label": "Newswire", "ID": "3161"}, {"sentence": "Mani, I., et al (1993) \"Identifying Unknown Proper  Names in NW Text.\"", "acronym": "NW", "label": "Newswire", "ID": "3162"}, {"sentence": "The system F-score performance is 81.73%, 75.67%, 58.11% on ACE2005 Broadcast News, NW, and Web blogs respectively.", "acronym": "NW", "label": "Newswire", "ID": "3163"}, {"sentence": "To estimate the timeliness and semantic cred- ibility indicators, we use AQUAINT-2, a set of NW articles (2.5 GB, about 907K documents) that are roughly contemporaneous with the TREC Blog06 collection (AQUAINT-2, 2007).", "acronym": "NW", "label": "newswire", "ID": "3164"}, {"sentence": "On the whole test set the difference is smaller, 0.07 for the NW translation and 0.58 for the web-blog translation.", "acronym": "NW", "label": "newswire", "ID": "3165"}, {"sentence": "For NW, the translation quality is improved by 0.44 on the whole test set and 1.1 on the tail documents, as measured by (TER-BLEU)/2.", "acronym": "NW", "label": "newswire", "ID": "3166"}, {"sentence": "Ta- bles 5 and 6 show the NW and web-blog translation scores as well as the number of phrase translation pairs obtained from each alignment.", "acronym": "NW", "label": "newswire", "ID": "3167"}, {"sentence": "These filters  were used for ATR from NW corpora and in biomedi- cine (Frantzi et al, 2000; Nenadi?", "acronym": "NW", "label": "newswire", "ID": "3168"}, {"sentence": "The domains are broadcast conversations (bc), broadcast news (bn), conversa- tional telephone speech (cts), NW (nw), usenet (un) and weblog (wl).", "acronym": "NW", "label": "newswire", "ID": "3169"}, {"sentence": "1 Introduction  An overwhelming amount of textual information  presented in NW, scientific literature, legal  texts, etc.,", "acronym": "NW", "label": "newswire", "ID": "3170"}, {"sentence": "We tested 6 metrics: Re?nyi, Vari- ational (L1), Euclidean, Cosine, Kullback-Leibler, and the BC.", "acronym": "BC", "label": "Bhattacharyya coefficient", "ID": "3171"}, {"sentence": "248 2.2 BC When the context profiles are probability distribu- tions, we usually utilize the measures on probabil- ity distributions such as the Jensen-Shannon (JS) divergence to calculate similarities (Dagan et al, 1994; Dagan et al, 1997).", "acronym": "BC", "label": "Bhattacharyya coefficient", "ID": "3172"}, {"sentence": "When the context pro- files are multinomial distributions, the pri- ors are Dirichlet, and the base measure is the BC, we can de- rive an analytical form that allows efficient calculation.", "acronym": "BC", "label": "Bhattacharyya coefficient", "ID": "3173"}, {"sentence": "LIPN: Introducing a new geographical con- text similarity measure and a statistical similarity measure based on the BC.", "acronym": "BC", "label": "Bhattacharyya coefficient", "ID": "3174"}, {"sentence": "In Section 2, we briefly introduce the Bayesian esti- mation and the BC.", "acronym": "BC", "label": "Bhattacharyya coefficient", "ID": "3175"}, {"sentence": "3.2 Similarity metrics To measure the difference between two corpora we implemented six similarity metrics: Re?nyi2 (Re?nyi, 1961), Variational (L1) (Lee, 2001), Euclidean (Lee, 2001), Cosine (Lee, 2001), Kullback-Leibler (Kullback and Leibler, 1951) and BC (Comaniciu et al, 2003; Bhattacharyya, 1943).", "acronym": "BC", "label": "Bhattacharyya coefficient", "ID": "3176"}, {"sentence": "ME Adapted ME English Broadcast News 147K 290 255 243 230 27.2 26.3 25.9 292K 286 250 236 223 26.7 25.8 25.6 591K 280 243 228 215 26.6 25.9 25.6 1119K 272 232 217 204 26.2 25.6 24.9 Estonian BC 104K 237 197 200 169 40.5 38.9 37.4 17.7 17.3 16.6 Table 2: Perplexity, WER and LER results comparing pooled and interpolated N -gram models and interpolated and adapted ME models, with changing amount of available in-domain data.", "acronym": "BC", "label": "Broadcast Conversations", "ID": "3177"}, {"sentence": "The 2005 ACE data comes from 5 domains: Broad- cast News (bn), BC (bc), Newswire (nw), Weblog (wl), Usenet (un) and Converstaional Telephone Speech (cts).", "acronym": "BC", "label": "Broadcast Conversations", "ID": "3178"}, {"sentence": "2 ID English Broadcast News 147K 2e8 3e5 5e7 2e7 2e6 292K 2e8 5e5 5e7 2e7 2e6 591K 2e8 1e6 5e7 2e7 2e6 1119K 2e8 2e6 5e7 2e7 5e6 Estonian BC 104K 5e8 3e5 5e7 1e7 2e6 Table 1: The unnormalized values of Gaus- sian prior variances for interpolated out-of-domain (OD) and in-domain (ID) ME models, and hierar- chically adapted global (*), out-of-odomain (OD) and in-domain (ID) models that were used in the experiments.", "acronym": "BC", "label": "Broadcast Conversations", "ID": "3179"}, {"sentence": "2006): 1996 CSR Hub4 Language Model data, EARS BN03 closed captions, GALE Phase 2 Distillation GNG Evaluation Supplemental Mul- tilingual data, Hub4 acoustic model training tran- scripts, TDT4 closed captions, TDT4 newswire, and GALE BC and GALE Broad- cast News.", "acronym": "BC", "label": "Broadcast Conversations", "ID": "3180"}, {"sentence": "Task 2: Estonian BC.", "acronym": "BC", "label": "Broadcast Conversations", "ID": "3181"}, {"sentence": "c?2011 Association for Computational Linguistics Detection of Agreement and Disagreement in BC Wen Wang1 Sibel Yaman2y\u0003 Kristin Precoda1 Colleen Richey1 Geoffrey Raymond3 1SRI International, 333 Ravenswood Avenue, Menlo Park, CA 94025, USA 2IBM T. J. Watson Research Center P.O.Box 218, Yorktown Heights, NY 10598, USA 3University of California, Santa Barbara, CA, USA fwwang,precoda,colleeng@speech.sri.com, syaman@us.ibm.com, graymond@soc.ucsb.edu Abstract We present C", "acronym": "BC", "label": "Broadcast Conversations", "ID": "3182"}, {"sentence": "Using the technique described above and the  lexicon derived frora the BC we ex-  tracted prefix morphological rules (no alter-  ations), suffix morphological rules without alter-  ations and ending guessing rules, exactly as it was  done in (Mikheev, 1996).", "acronym": "BC", "label": "Brown Corpus", "ID": "3183"}, {"sentence": "We performed a rule-induction experiment us-  ing the lexicon and word-frequencies derived  from the BC (Prancis&Kucera, 1982).", "acronym": "BC", "label": "Brown Corpus", "ID": "3184"}, {"sentence": "BC which is useflll for  comparison and eval", "acronym": "BC", "label": "Brown Corpus", "ID": "3185"}, {"sentence": "BC which is useflll for  comparison and evaluation.", "acronym": "BC", "label": "Brown Corpus", "ID": "3186"}, {"sentence": "sky  teacher  plant  school  the BC, texts from the Wall Street Your-  hal, Grolier's Electronic Encyclopedia nd scientific  abstracts from different fields.", "acronym": "BC", "label": "Brown Corpus", "ID": "3187"}, {"sentence": "ng the lexicon and word-frequencies derived  from the BC (Prancis&Kucera, 1982).", "acronym": "BC", "label": "Brown Corpus", "ID": "3188"}, {"sentence": "The most im-  portant ones are that the BC provides  a model of general multi-domain language use,  so general language regularities carl be induced  h'om it;, and second, many taggers come with data  trained on the.", "acronym": "BC", "label": "Brown Corpus", "ID": "3189"}, {"sentence": "There are a number of reasons tbr choosing tile  BC data for training.", "acronym": "BC", "label": "Brown Corpus", "ID": "3190"}, {"sentence": "The aspect of social communication most ex- plored so far is the detection of participant role, particularly in spoken genres such as broadcast news, BC, and meetings.", "acronym": "BC", "label": "broadcast conversations", "ID": "3191"}, {"sentence": "Wang et al (2011) presented a condi- tional random field based approach for detecting agreement/disagreement between speakers in En- glish BC.", "acronym": "BC", "label": "broadcast conversations", "ID": "3192"}, {"sentence": "The English lan- guage portion comprises roughly 1.7M words and Chinese language portion comprises roughly 1M words of newswire, magazine articles, broadcast news, BC, web data and con- versational speech data3.", "acronym": "BC", "label": "broadcast conversations", "ID": "3193"}, {"sentence": "Both the dev and the test set are composed of a mixture of broadcast news and BC crawled from the web and have two references.", "acronym": "BC", "label": "broadcast conversations", "ID": "3194"}, {"sentence": "The domains are BC (bc), broadcast news (bn), conversa- tional telephone speech (cts), newswire (nw), usenet (un) and weblog (wl).", "acronym": "BC", "label": "broadcast conversations", "ID": "3195"}, {"sentence": "195 from newswire, broadcast news, weblogs, usenet  newsgroups/discussion forum, conversational  telephone speech and BC.", "acronym": "BC", "label": "broadcast conversations", "ID": "3196"}, {"sentence": "More recently, the OntoNotes project (Pradhan et al, 2007) released a one mil- lion word English corpus of newswire, broadcast news, and BC that is annotated for Penn Treebank syntax, PropBank predicate ar- gument structures, coreference, and named enti- ties.", "acronym": "BC", "label": "broadcast conversation", "ID": "3197"}, {"sentence": "especially the ones in the BC, web data, 5doc/propbank/english-propbank.pdf 6As we will see later these are not used during the task.", "acronym": "BC", "label": "broadcast conversation", "ID": "3198"}, {"sentence": "The English lan- guage portion comprises roughly 1.7M words and Chinese language portion comprises roughly 1M words of newswire, magazine articles, broadcast news, BCs, web data and con- versational speech data3.", "acronym": "BC", "label": "broadcast conversation", "ID": "3199"}, {"sentence": "More recently under DARPA GALE funding it has been expanded to include broadcast news, BC, news groups and web log data.", "acronym": "BC", "label": "broadcast conversation", "ID": "3200"}, {"sentence": "The domains are BCs (bc), broadcast news (bn), conversa- tional telephone speech (cts), newswire (nw), usenet (un) and weblog (wl).", "acronym": "BC", "label": "broadcast conversation", "ID": "3201"}, {"sentence": "The training corpora in- clude a mixed genre of newswire, weblog, broad- cast news, BC, discussion fo- rums and comes from various sources such as LDC, HK Law, HK Hansard and UN data.", "acronym": "BC", "label": "broadcast conversation", "ID": "3202"}, {"sentence": "Based on  this resource, we also intend to analyze  the syntactic correlations of prosodic  phrase in BN speech, and  compare the phonetic and prosodic fea- tures in movie dialogues among several  same-name movies in different histori- cal eras.", "acronym": "BN", "label": "broadcasting news", "ID": "3203"}, {"sentence": "One reason for this may be due to OntoNotes  corpus is from BN domain.", "acronym": "BN", "label": "broadcasting news", "ID": "3204"}, {"sentence": "1 Introduction In the recent years, the microblogging service Twit- ter has become a very popular tool for express- ing opinions, BN, and simply com- municating with friends.", "acronym": "BN", "label": "broadcasting news", "ID": "3205"}, {"sentence": "However, in BN, these sentences  maybe simply joined by conjunction word ?", "acronym": "BN", "label": "broadcasting news", "ID": "3206"}, {"sentence": "In particular, statistical machine translation systems (Koehn et al, 2007; Bach et al, 2007; Shen et al, 2008) have advanced to a state that the transla- tion quality for certain language pairs (e.g. Spanish- English, French-English, Iraqi-English) in certain do- mains (e.g. BN, force-protection, travel) is acceptable to users.", "acronym": "BN", "label": "broadcasting news", "ID": "3207"}, {"sentence": "1 Introduction In the recent years, the microblogging service Twit- ter has become a popular tool for expressing opin- ions, BN, and simply communicating with friends.", "acronym": "BN", "label": "broadcasting news", "ID": "3208"}, {"sentence": "BN Corpus Consortium, Ox- ford University Computing Service.", "acronym": "BN", "label": "British National", "ID": "3209"}, {"sentence": "We use the data set from our previous work, which contain sentences from the first half of the BN Corpus, with near- synonyms from the eleven groups from Table 4.", "acronym": "BN", "label": "British National", "ID": "3210"}, {"sentence": "100 million words of English: the BN Corpus.", "acronym": "BN", "label": "British National", "ID": "3211"}, {"sentence": "2013) trained on a corpus consisting of the 400M word Text8 corpus of Wikipedia text 2 together with the 100M word BN Corpus (Leech et al.,", "acronym": "BN", "label": "British National", "ID": "3212"}, {"sentence": "3.2.2 Language Model For our language model, we use a Kneser-Ney smoothed trigram model learned from a version of the BN Corpus modified to use Americanized spellings (Chen and Goodman, 1996; Burnard, 1995).", "acronym": "BN", "label": "British National", "ID": "3213"}, {"sentence": "Claws4: the tagging of the BN Corpus.", "acronym": "BN", "label": "British National", "ID": "3214"}, {"sentence": "News substantiate this relevance that is also sup- ported by the spoken language scenario, where most speech summarization systems concentrate on BN (McKeown et al, 2005).", "acronym": "BN", "label": "broadcast news", "ID": "3215"}, {"sentence": "We use BN as a case study and news stories from online newspapers provide the background information.", "acronym": "BN", "label": "broadcast news", "ID": "3216"}, {"sentence": "Feature-based methods combine several types of features: current work uses lexical, acoustic/prosodic, structural, and dis- course features to summarize documents from do- mains like BN or meetings (Maskey and Hirschberg, 2005; Murray et al, 2006; Ribeiro and de Matos, 2007).", "acronym": "BN", "label": "broadcast news", "ID": "3217"}, {"sentence": "In fact, the previously enumerated problems that make speech summarization such a difficult task constrain the applicability of text summarization techniques to speech summariza- tion (although in the presence of planned speech, as it partly happens in the BN domain, that portability is more feasible (Christensen et al, 2003)).", "acronym": "BN", "label": "broadcast news", "ID": "3218"}, {"sentence": "The domains are broadcast conversations (bc), BN (bn), conversa- tional telephone speech (cts), newswire (nw), usenet (un) and weblog (wl).", "acronym": "BN", "label": "broadcast news", "ID": "3219"}, {"sentence": "Even though this heuristic may appear naif, we believe it is adequate as a rough approach, considering the target material (BN).", "acronym": "BN", "label": "broadcast news", "ID": "3220"}, {"sentence": "5 A Case Study Using Broadcast News 5.1 Media Monitoring System SSNT (Amaral et al, 2007) is a system for selec- tive dissemination of multimedia contents, work- ing primarily with Portuguese BN ser- vices.", "acronym": "BN", "label": "broadcast news", "ID": "3221"}, {"sentence": "We use graph Gs as a representation of the BN with random variables Es and rs.", "acronym": "BN", "label": "Bayesian network", "ID": "3222"}, {"sentence": "The contrast classifier is also competitive with the best case result in (Galley et al, 2004) (last entry), which adds speaker change, segment duration, and adjacency pair sequence dependency features using a dynamic BN.", "acronym": "BN", "label": "Bayesian network", "ID": "3223"}, {"sentence": "Identifying agreement and  disagreement in conversational speech: Use of  BNs to model pragmatic  dependencies.", "acronym": "BN", "label": "Bayesian network", "ID": "3224"}, {"sentence": "Identifying agreement and disagreement in con- versational speech: use of BNs to modeldependencies.", "acronym": "BN", "label": "Bayesian network", "ID": "3225"}, {"sentence": "More recently, Abend et al (2009) propose an unsupervised algorithm for argument identifica- tion that relies only on part-of-speech annotations, whereas Grenager and Manning (2006) focus on role induction which they formalize as probabilis- tic inference in a BN.", "acronym": "BN", "label": "Bayesian network", "ID": "3226"}, {"sentence": "rh(; ,,s(, of  BNs for WSI) has I)een l)rop()sed by  others such as Wiebe eL al (1.998), but a different  fl)rmulation is used in this mod('l.", "acronym": "BN", "label": "Bayesian network", "ID": "3227"}, {"sentence": "We could not find any publica- tion on BN stemmer following rule-based ap- proach.", "acronym": "BN", "label": "Bengali", "ID": "3228"}, {"sentence": "c?2008 Asian Federation of Natural Language Processing Design of a Rule-based Stemmer for Natural Language Text in BN Sandipan Sarkar  IBM India  sandipan.sarkar@in.ibm.com,  sandipansarkar@gmail.com  Sivaji Bandyopadhyay  Computer Science and Engineering Department  Jadavpur University, Kolkata  sbandyopadhyay@cse.jdvu.ac.in      Abstract  This paper presents a rule-based approach  for finding out the stems from text in Ben- gali, a resource-poor language.", "acronym": "BN", "label": "Bengali", "ID": "3229"}, {"sentence": "Our approach in this work is to identify and  formalize rules in BN to build a stemming sys- tem with acceptable accuracy.", "acronym": "BN", "label": "Bengali", "ID": "3230"}, {"sentence": "2006)  accepted the absence of rule-based stemmer in  BN and proposed a statistical clustering-based  approach to discover equivalence classes of root  words from electronic texts in different languages  including BN.", "acronym": "BN", "label": "Bengali", "ID": "3231"}, {"sentence": "This paper deals  with design of such a system to stem BN  words tokens tagged with their respective parts of  speech (POS).", "acronym": "BN", "label": "Bengali", "ID": "3232"}, {"sentence": "2005) proposed a  clustering-based approach to identify stem from  BN image documents.", "acronym": "BN", "label": "Bengali", "ID": "3233"}, {"sentence": "It starts by  introducing the concept of orthographic  syllable, the basic orthographic unit of  BN.", "acronym": "BN", "label": "Bengali", "ID": "3234"}, {"sentence": "c  syllable, the basic orthographic unit of  BN.", "acronym": "BN", "label": "Bengali", "ID": "3235"}, {"sentence": "These concepts are  applied in the design and implementation  of an extensible architecture of a stemmer  system for BN text.", "acronym": "BN", "label": "Bengali", "ID": "3236"}, {"sentence": "In  Proceedings of the DARPA BN Tran-  scription and Understanding Workshop.", "acronym": "BN", "label": "Broadcast News", "ID": "3237"}, {"sentence": "Extractive Sum- marization of BN: Comparing Strate- gies for European Portuguese.", "acronym": "BN", "label": "Broadcast News", "ID": "3238"}, {"sentence": "Are Extractive Text Summarisation Tech- niques Portable To BN?", "acronym": "BN", "label": "Broadcast News", "ID": "3239"}, {"sentence": "Automatic vs. Manual Topic Seg- mentation and Indexation in BN.", "acronym": "BN", "label": "Broadcast News", "ID": "3240"}, {"sentence": "A Prototype System for Selective Dissemination of BN in European Por- tuguese.", "acronym": "BN", "label": "Broadcast News", "ID": "3241"}, {"sentence": "5 A Case Study Using BN 5.1 Media Monitoring System SSNT (Amaral et al, 2007) is a system for selec- tive dissemination of multimedia contents, work- ing primarily with Portuguese broadcast news ser- vices.", "acronym": "BN", "label": "Broadcast News", "ID": "3242"}, {"sentence": "UN (15%) and Conversational Telephone Speech (15%).", "acronym": "UN", "label": "Usenet Newsgroups", "ID": "3243"}, {"sentence": "The 20-Newsgroups (20NG) corpus is a collection of newsgroup postings gathered from twenty different categories from the UN hierarchy4.", "acronym": "UN", "label": "Usenet Newsgroups", "ID": "3244"}, {"sentence": "Four of the clusters are from  UN newsgroups.", "acronym": "UN", "label": "Usenet", "ID": "3245"}, {"sentence": "Our data is a corpus of articles from 20 different UN newsgroups released by Mitchell (1999).", "acronym": "UN", "label": "Usenet", "ID": "3246"}, {"sentence": "Subjects were recruited via postings to local UN newsgroups.", "acronym": "UN", "label": "Usenet", "ID": "3247"}, {"sentence": "The most similar work to our own is that of Wang & Rose (2010) who analyzed UN fo- rum quote/response structures.", "acronym": "UN", "label": "Usenet", "ID": "3248"}, {"sentence": "Hassan et al use a supervised Markov model, part of speech, and dependency patterns to identify attitudinal polarities in threads posted to UN discussion posts (Hassan et al, 2010).", "acronym": "UN", "label": "Usenet", "ID": "3249"}, {"sentence": "uracy produced for the two feature sets\u0004 The results show that there is no signi\u0003cant increase by either algorithm by the addition of semantic features\u0004 The original motivation behind our work on question terminology was to improve the re\u0001 trieval accuracy of our system called FAQFinder \tBurke et al\u0002 \f  \u000e\u000f Lytinen and Tomuro\u0002 \u0010\u0007\u0007\u0010 \u0004 FAQFinder is a web\u0001based\u0002 natural language Q\u0012A system which uses UN Frequently Asked Questions \tFAQ  \u0003les to answer users\u0011 questions\u0004 Figures \f and \u0010 show an example ses\u0001 sion with FAQFinder\u0004 First\u0002 the user enters a question in natural language\u0004 The system then searches the FAQ \u0003les for questions that are similar to the user\u0011s\u0004 Based on the results of the search\u0002 FAQFinder displays a maximum of \u0006 FAQ questions which are ranked the highest by the system\u0011s s", "acronym": "UN", "label": "Usenet", "ID": "3250"}, {"sentence": "In International Conference for WLs and Social Media.", "acronym": "WL", "label": "Webblog", "ID": "3251"}, {"sentence": "5.1 Newswire Data vs. WL Data Considering the big gap in accuracy between newswire and webblog data in our baseline results, we delve deeper into the data and found several major distinctions between these two domains that might have contributed to the rather significant dif- ference in performance on tense inference.", "acronym": "WL", "label": "Webblog", "ID": "3252"}, {"sentence": "We use the ACE 2005 coreference cor- pus as released by the LDC, which consists of the 599 training documents used in the official ACE evaluation.3 To ensure diversity, the corpus was created by selecting documents from six different sources: Broadcast News (bn), Broadcast Con- versations (bc), Newswire (nw), WL (wb), Usenet (un), and conversational telephone speech (cts).", "acronym": "WL", "label": "Webblog", "ID": "3253"}, {"sentence": "In Conference for WLs and Social Media.", "acronym": "WL", "label": "Webblog", "ID": "3254"}, {"sentence": "The ACE05 corpus is drawn from six sources (Newswire, Broadcast News, Broadcast Conversations, Conversational Telephone Speech, WLs, and Usenet).", "acronym": "WL", "label": "Webblog", "ID": "3255"}, {"sentence": "Sentence level and word level  annotation    TypeCraft provides a set of glosses for  syntactic and semantic coding and a set of  parameters along which sentences may be  classified that allow for standardized  annotation and cross linguistic comparison as  illustrated in figure1:        figure1: Word and sentence level annotation     3.1 WL    WL annotation allows for analysis of  predicates in terms of syntactic and semantic  properties including information about the  subcategorization properties and argument  structure of predicates.", "acronym": "WL", "label": "Word level", "ID": "3256"}, {"sentence": "WL  precision and recall with F1-measure is evaluated.", "acronym": "WL", "label": "Word level", "ID": "3257"}, {"sentence": "WL precision and recall with F1-measure  are employed as evaluation standard.", "acronym": "WL", "label": "Word level", "ID": "3258"}, {"sentence": "WL agreement between the annotators was calculated as the per- cent of the decisions on which they agreed, and found to be 97.1%.", "acronym": "WL", "label": "Word level", "ID": "3259"}, {"sentence": "2.3 Sentence Vector Representations WL representation often cannot properly capture more complex linguistic phenomena in a sentence or multi-word phrase.", "acronym": "WL", "label": "Word level", "ID": "3260"}, {"sentence": "In the algorithm, the opinion WL O and review data R about a product are provided as the input.", "acronym": "WL", "label": "word lexicon", "ID": "3261"}, {"sentence": "For the English?French task (Section 3.1), we train translation mod- els on different training data sets and augment the phrase-based system with a hierarchical re- ordering model, a word class language model, a discriminative WL and a insertion and deletion model.", "acronym": "WL", "label": "word lexicon", "ID": "3262"}, {"sentence": "Further, we applied the hierar- chical reordering model, the word class language model, the discriminative WL, and the insertion and deletion model.", "acronym": "WL", "label": "word lexicon", "ID": "3263"}, {"sentence": "The 2~g0-WL for the  Michael Reese stroke database takes up  about a third of a megabyte, so this  approach would work on a mainframe or a  large minicomputer such as our Vax 75g,  but could not readily be ported to a  smaller machine; nor could we handle a  much larger vocabulary such as we plan to  create with the automatic lexicon builder.", "acronym": "WL", "label": "word lexicon", "ID": "3264"}, {"sentence": "Nu- merous approaches emerged over the years that try to induce bilingual WLs on the basis of distributional information.", "acronym": "WL", "label": "word lexicon", "ID": "3265"}, {"sentence": "classified as a positive word by Gen- eral Inquirer5 , a sentiment WL, as a  positive opinion indicator.", "acronym": "WL", "label": "word lexicon", "ID": "3266"}, {"sentence": "In Emre Kiciman, Nicole B. Ellison, Bernie Hogan, Paul Resnick, and Ian Soboroff, editors, ICWSM?13: Proceedings of the 7th International AAAI Conference on WL and Social Media.", "acronym": "WL", "label": "Weblogs", "ID": "3267"}, {"sentence": "In John G. Breslin, Nicole B. Ellison, James G. Shana- han, and Zeynep Tufekci, editors, ICWSM?12: Pro- ceedings of the 6th International AAAI Conference on WL and Social Media.", "acronym": "WL", "label": "Weblogs", "ID": "3268"}, {"sentence": "of AAAI Spring Symposium: Compu- tational Approaches to Analyzing WL.", "acronym": "WL", "label": "Weblogs", "ID": "3269"}, {"sentence": "WL: Credibility and collaboration in an online world.", "acronym": "WL", "label": "Weblogs", "ID": "3270"}, {"sentence": "In AAAI Conference on WL and Social Media.", "acronym": "WL", "label": "Weblogs", "ID": "3271"}, {"sentence": "In John G. Breslin, Nicole B. Ellison, James G. Shanahan, and Zeynep Tufekci, editors, ICWSM?12: Proceedings of the 6th International AAAI Conference on WL and So- cial Media.", "acronym": "WL", "label": "Weblogs", "ID": "3272"}, {"sentence": "3.2 Extracting IAs We define implicit attitudes as the semantic sim- ilarity between texts comprising discussant utter- ances or posts in a thread.", "acronym": "IA", "label": "Implicit Attitude", "ID": "3273"}, {"sentence": "c?2012 Association for Computational Linguistics Genre Independent Subgroup Detection in Online Discussion Threads: A Pilot Study of IA using Latent Textual Semantics Pradeep Dasigi pd2359@columbia.edu Weiwei Guo weiwei@cs.columbia.edu Center for Computational Learning Systems, Columbia University Mona Diab mdiab@ccls.columbia.edu Abstract We describe an unsupervised approach to the problem of automatically detecting sub- groups of people holding similar opinions in a discussion thread.", "acronym": "IA", "label": "Implicit Attitude", "ID": "3274"}, {"sentence": "Alfred was good in suspense and all, but his work is not as deep as Kubrick?s Table 1: Example of Agreement based on IA WIKI CD Median No.", "acronym": "IA", "label": "Implicit Attitude", "ID": "3275"}, {"sentence": "Algorithm 1 The Basic IA Require: T = target object; D = set of distractor objects.", "acronym": "IA", "label": "Incremental Algorithm", "ID": "3276"}, {"sentence": "The IA.", "acronym": "IA", "label": "Incremental Algorithm", "ID": "3277"}, {"sentence": "Algorithm 2 The Locative IA DESC = Basic-Incremental-Algorithm(T,D) if DESC !", "acronym": "IA", "label": "Incremental Algorithm", "ID": "3278"}, {"sentence": "For the IA (Dale and Re- iter, 1995), this could be achieved by augmenting the list of preferred attributes with a list of ?", "acronym": "IA", "label": "Incremental Algorithm", "ID": "3279"}, {"sentence": "the IA (Dale and Reiter, 1995), one of the most widely used REG algorithms, assumes that certain at- tributes are preferred over others, partly based on evidence provided by Pechmann (1989); a chair would first be described in terms of its color, and only if this does not result in a unique charac- terization, other, less preferred attributes such as orientation are tried.", "acronym": "IA", "label": "Incremental Algorithm", "ID": "3280"}, {"sentence": "For example, the IA (Dale and Reiter, 1995), one of the most widely used REG algorithms, assumes that certain at- tributes are preferred over others, partly based on evidence provided by Pechmann (1989); a chair would first be described in terms of its color, and only if this does not result in a unique charac- terization, other, less preferred attributes such as orientation are tried.", "acronym": "IA", "label": "Incremental Algorithm", "ID": "3281"}, {"sentence": "The IA is arguably unique in assuming a complete pref- erence order of attributes, but other REG algo- rithms rely on similar distinctions.", "acronym": "IA", "label": "Incremental Algorithm", "ID": "3282"}, {"sentence": "14 2.3 IA and Input Interpreter The IA is designed as both domain and dialogue context independent.", "acronym": "IA", "label": "Input Analyzer", "ID": "3283"}, {"sentence": "erpreter The IA is designed as both domain and dialogue context independent.", "acronym": "IA", "label": "Input Analyzer", "ID": "3284"}, {"sentence": "First of all, the user's inputs are interpreted  through the IA.", "acronym": "IA", "label": "Input Analyzer", "ID": "3285"}, {"sentence": "In contrast to the IA, the Input In- terpreter analyzes the input with respect to the context of the dialogue.", "acronym": "IA", "label": "Input Analyzer", "ID": "3286"}, {"sentence": "6.4 Ranking Models RR (see Section 4.1) is per- formed using the implementation of the extremely randomized trees algorithm (Geurts et al, 2006) provided by the Scikit-learn package (Pedregosa et al, 2011).", "acronym": "RR", "label": "Ranking by regression", "ID": "3287"}, {"sentence": "Coarse-to-fine n-best parsing and maxent dis- criminative RR.", "acronym": "RR", "label": "reranking", "ID": "3288"}, {"sentence": "Then we learn a RR model using these candidate trees.", "acronym": "RR", "label": "reranking", "ID": "3289"}, {"sentence": "We use two parses per sentence: the outputs of a self-trained RR parser Charniak and Johnson (2005); McClosky and Charniak (2008) and a CCG parser (Clark and Curran, 2007), provided as part of the shared task dataset.", "acronym": "RR", "label": "reranking", "ID": "3290"}, {"sentence": "For comparison, we implement the following RR models: ?", "acronym": "RR", "label": "reranking", "ID": "3291"}, {"sentence": "Word lattice RR for Chinese word segmentation and part-of-speech tagging.", "acronym": "RR", "label": "reranking", "ID": "3292"}, {"sentence": "score O3 (T ) and score rerank (T ) are the outputs of the third order parser and the RR classifier respectively.", "acronym": "RR", "label": "reranking", "ID": "3293"}, {"sentence": "While a sim- ilar effect may be possible with RR archi- tectures, we believe that in terms of implemen- tation efforts our approach is at least as simple.", "acronym": "RR", "label": "reranking", "ID": "3294"}, {"sentence": "In  Type  Total noun phrases  Articles  Left Modifiers of Nouns  Navy  339  27  72  4  \\[ Medical  532  38  Adjectival Modifiers:  Adj  Adj + Adj  Possessive N 138  34  4 0  Noun Modifiers:  Noun 99 76  N+N 25 4  Verb 7 0  Table I: Left Modifier Statistics  Right Modifiers of Nouns  Type \\[ Navy \\[ Medical  Prepositional Phrases 95 107  Relative Clauses 1 5  Adverb 4 0  RR Clauses 7 9  Table 2: Right Modifier Statistics  506  the sublanguage of Navy messages, unmarked verb  modifiers of nouns also occur.", "acronym": "RR", "label": "Reduced Relative", "ID": "3295"}, {"sentence": "Seen that RR Clauses are headed by the past participle verb form, and that Participial Adjuncts may be attached to any NP head nouns quite consistently; and seen also that is very hard to apply strict subcategorization tests for participial SUBJect - or deep OBJect in case of passives - with good enough confidence we assume that such tests will only be performed in case the parser is at the co", "acronym": "RR", "label": "Reduced Relative", "ID": "3296"}, {"sentence": "1.59946 5.87759 Figure 4: Mean 10.5 the banker told about the buy-back resigned 1 2 3 4 5 6 Log[ previous prefix current prefix ] RR Clause 0.798547 1.59946 0.622262 1.3212 0.", "acronym": "RR", "label": "Reduced Relative", "ID": "3297"}, {"sentence": "35  PP > RR Clause > Relative Clause  A method frequently proposed to account for this  distinction is to use, as a measure of the cost of  raising, a count of the number of nodes in the syn-  tactic structure over which the quantifier is raised.", "acronym": "RR", "label": "Reduced Relative", "ID": "3298"}, {"sentence": "f 9 , rank inform(s, i, v) = the sum of all the RR of the scores assigned by the SLU to the user informing the value of slot s is v, or 0 if informing v cannot be found in the SLU n-best list.", "acronym": "RR", "label": "reciprocal rank", "ID": "3299"}, {"sentence": "Experimental results show that EMR can select seed sets that pro- vide significantly higher mean RR on realistic data than existing naive selection methods or random seed sets.", "acronym": "RR", "label": "reciprocal rank", "ID": "3300"}, {"sentence": "Consid- ering L2, accuracy, average probability, equal er- ror rate, log probability and mean RR across all components of the the dialog state, these give a total of 318 metrics.", "acronym": "RR", "label": "reciprocal rank", "ID": "3301"}, {"sentence": "f 12 , rank deny(s, i, v) = the sum of all the RR of the scores assigned by the SLU to the user denying the value of slot s is v, or 0 if denying v cannot", "acronym": "RR", "label": "reciprocal rank", "ID": "3302"}, {"sentence": "Finally, we also have two meta features: (1) is the person answering the question the one who asked it; (2) RR of the comment in the thread.", "acronym": "RR", "label": "reciprocal rank", "ID": "3303"}, {"sentence": "f 10 , rank affirm(s, i, v) = the sum of all the RR of the scores assigned by the SLU to the user affirming the value of slot s is v, or 0 if affirming v cannot be found in the SLU n-best list.", "acronym": "RR", "label": "reciprocal rank", "ID": "3304"}, {"sentence": "Not surprisingly, the most important turn out to be the TASK FEATURES (contributing over five MAP points) as they handle important informa- tion sources that are not available to the system from other feature groups, e.g., the RR alone contributes about two points.", "acronym": "RR", "label": "reciprocal rank", "ID": "3305"}, {"sentence": "We develop the RR approach to pars- ing in which we untangle the projection of grammatical functions and their means of realization to allow for phrase-structure variability and morphological-syntactic in- teraction.", "acronym": "RR", "label": "Relational-Realizational", "ID": "3306"}, {"sentence": "Through a quantitative and quali- tative analysis we illustrate the advantages of the RR approach and its poten- tial promise for parsing other ?", "acronym": "RR", "label": "Relational-Realizational", "ID": "3307"}, {"sentence": "Our RR parsing pro- posal, strongly inspired by Relational Grammar (Perlmutter, 1982), takes grammatical relations such as ?", "acronym": "RR", "label": "Relational-Realizational", "ID": "3308"}, {"sentence": "179  Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 889?896 Manchester, August 2008 RR Parsing Reut Tsarfaty and Khalil Sima?an Institute for Logic, Language and Computation, University of Amsterdam Plantage Muidergracht 24, 1018TV, Amsterdam, The Netherlands {rtsarfat,simaan}@science.uva.nl Abstract State-of-the-art statistical parsing models applied to free word-order languages tend to underperform compared to, e.g., pars- ing English.", "acronym": "RR", "label": "Relational-Realizational", "ID": "3309"}, {"sentence": "RR Parsing.", "acronym": "RR", "label": "Relational-Realizational", "ID": "3310"}, {"sentence": "The awareness of the model- ing challenges gave rise to new lines of work on top- ics such as joint morpho-syntactic processing (Gold- berg and Tsarfaty, 2008), RR Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold- berg, 2011), PLCFRS parsing (Kallmeyer and Maier, 2013), the use of factored lexica (Green et al 2013), the use of bilingual data (Fraser et al 2013), and more developments that are currently under way.", "acronym": "RR", "label": "Relational-Realizational", "ID": "3311"}, {"sentence": "D. A. DaM and C. N. Ball, \"RR in PUN-  DIT,\" in P. Saint-Dizier and S. Szpakowicz, eds., (", "acronym": "RR", "label": "Reference resolution", "ID": "3312"}, {"sentence": "2.3 Reference Resolution RR identifies the most prob- able antecedent for each anaphor within a text (Hirschman and Chinchor, 1997).", "acronym": "RR", "label": "Reference resolution", "ID": "3313"}, {"sentence": "RR, which in-  volves carrying out a proof that a retbrring ex-  pression denotes, is implemented as part of the  update step.", "acronym": "RR", "label": "Reference resolution", "ID": "3314"}, {"sentence": "RR is now performed using a discourse focus list .", "acronym": "RR", "label": "Reference resolution", "ID": "3315"}, {"sentence": "RR within the framework of cog- nitive grammar.", "acronym": "RR", "label": "Reference resolution", "ID": "3316"}, {"sentence": "RR within IE, however,  cannot operate only on those parts that describe  target information because anaphoric expressions  within target linguistic patterns may have an-  tecedents outside of the target, and those that oc-  cur in an apparently irrelevant pattern may ac-  tually resolve to target entities.", "acronym": "RR", "label": "Reference resolution", "ID": "3317"}, {"sentence": "solution for SVM training and  classification - LIBSVM6 (Fan et al, 2005) with  6 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ the default parameters (C-SVC classification and RBF).", "acronym": "RBF", "label": "radial basis kernel function", "ID": "3318"}, {"sentence": "6.3 Learning Parameters  For SVR, the RBF is em- ployed and the optimal values for parameters C,  v and g (for the kernel) are found using the gri- dregression.py tool provided by LibSVM  (Chang and Lin, 2001) with a 5-fold cross vali- dation on the training set.", "acronym": "RBF", "label": "radial basis kernel function", "ID": "3319"}, {"sentence": "We have thus far presented results using  mixture Gaussian models, but are now experimenting with  other types of models and discriminators including multi-  layer perceptrons and RBF.", "acronym": "RBF", "label": "radial basis functions", "ID": "3320"}, {"sentence": "Ney \\[11\\] has found strong  similarities between RBF and mixture  densities using Gaussians with diagonal covariances.", "acronym": "RBF", "label": "radial basis functions", "ID": "3321"}, {"sentence": "5.1 Method We adopted the method of Georgescul (2010): Sup- port Vector Machine classification based on a Gaus- sian RBF (Vapnik, 1998; Fan et al, 2005), employing n-grams from annotated cue phrases as features, as described in more detail be- low.", "acronym": "RBF", "label": "Radial Basis kernel function", "ID": "3322"}, {"sentence": "The gain is the same for 3https://github.com/lspecia/QualityEstimation/blob/master/ test set.tar.gz 4available at https://github.com/lspecia/QualityEstimation/ blob/master/test set.likert 117 the optimisation of the RBF param- eters.", "acronym": "RBF", "label": "radial basis function", "ID": "3323"}, {"sentence": "SVM (Chang and Lin, 2001): cache size = 40; cost = 1; degree = 3; eps = 0.001; loss = 0.1; kernel type = RBF; nu = 0.5; seed = 1.", "acronym": "RBF", "label": "radial basis function", "ID": "3324"}, {"sentence": "RBF or Gaussian), k(x,x?)", "acronym": "RBF", "label": "radial basis function", "ID": "3325"}, {"sentence": "Baselines: The baselines use the SVM regres- sion algorithm with RBF kernel and parameters ?,", "acronym": "RBF", "label": "radial basis function", "ID": "3326"}, {"sentence": "We experimented two ker- nels: linear, and RBF.", "acronym": "RBF", "label": "radial basis function", "ID": "3327"}, {"sentence": "We experiment with two kernels: linear and RBF.", "acronym": "RBF", "label": "radial basis function", "ID": "3328"}, {"sentence": "The recognition of ORG was much better in this article: R 64%, P 80%.", "acronym": "ORG", "label": "organisations", "ID": "3329"}, {"sentence": "The best performing subtask was the entity slot, particularly where the task required the extraction of ORG and persons.", "acronym": "ORG", "label": "organisations", "ID": "3330"}, {"sentence": "The training corpora are processed with a part- of-speech tagger and a module for Named Entity Recognition and Classification (NERC) that anno- tates people, ORG, locations, dates, rela- tive temporal expressions and numbers (Alfonseca et al, 2006b), so this information can be included in the patterns.", "acronym": "ORG", "label": "organisations", "ID": "3331"}, {"sentence": "Views expressed in this publication cannot be ascribed to any of these funding ORG.", "acronym": "ORG", "label": "organisations", "ID": "3332"}, {"sentence": "The additions include names of major USA institutions and ORG (e.g., government departments), names of newspapers, names of major geographical locations in the USA, US states abbreviations and names of countries and nationalities of the world.", "acronym": "ORG", "label": "organisations", "ID": "3333"}, {"sentence": "The UN and Europe were pan national ORG CHUNK the honeymoon is over VARIANT the honey moon period is over The shortest post-election honeymoon is over.", "acronym": "ORG", "label": "organisations", "ID": "3334"}, {"sentence": "N Table 3: Syllable ORG for the       Logistic Regression Model 4.1 Logistic Regression Model for Combining Syllables The model to combine syllables is built upon Binary Logistic Regression whose answers are either combine or not combine.", "acronym": "ORG", "label": "Organization", "ID": "3335"}, {"sentence": "I would like a higher optical zoom, the W200 does a great digital zoom translation... Table 3: Opinion ORG Result for Sony Cybershot DSC-W200 Camera listed in Freebase, but we can find it in people?s online discussion using mutual information.", "acronym": "ORG", "label": "Organization", "ID": "3336"}, {"sentence": "Database Center for Life Science, Research ORG of Information and System, Japan ?", "acronym": "ORG", "label": "Organization", "ID": "3337"}, {"sentence": "Opinion ORG: Table 2 and Table 3 present sample results for President Ronald Rea- gan and Sony Cybershot DSC-W200 camera re- spectively5.", "acronym": "ORG", "label": "Organization", "ID": "3338"}, {"sentence": "Table 2: Opinion ORG Result for President Ronald Reagan FreeBase Aspects Supt Representative Opinion Sentences Format: 13 Quality pictures in a compact package.", "acronym": "ORG", "label": "Organization", "ID": "3339"}, {"sentence": "Topic Detection and Tracking: Event- based Information ORG, Kluwer Academic  Publishers, 2002.", "acronym": "ORG", "label": "Organization", "ID": "3340"}, {"sentence": "in news or  transcripts (BBN 2001), or for other information  ORGn tasks that might benefit from precise  knowledge of how names occur, such as Topic  Detection and Tracking (Allan 2002).", "acronym": "ORG", "label": "organizatio", "ID": "3341"}, {"sentence": "Help desk systems in large ORGns: Help desk staff in large ORGns need to quickly satisfy the customer?s need for information.", "acronym": "ORG", "label": "organizatio", "ID": "3342"}, {"sentence": "Evaluation       Given a collection of named entities from  documents, the coreferencing task is to put them into  equivalence classes, where every mention in the same  class refers to the same entity (person, location,  ORGn, and so on).", "acronym": "ORG", "label": "organizatio", "ID": "3343"}, {"sentence": "A third category contains texts with uncertain provenance, e.g. the texts from various European Union ORGns or from Wikipedia.", "acronym": "ORG", "label": "organizatio", "ID": "3344"}, {"sentence": "In providing the fundamental ORGn of the gram- 983 mar, to the extent that that ORGn is consistent with the language modeled, these types significantly ease the path to creating a working grammar.", "acronym": "ORG", "label": "organizatio", "ID": "3345"}, {"sentence": "in news or  transcripts (BBN 2001), or for other information  ORG tasks that might benefit from precise  knowledge of how names occur, such as Topic  Detection and Tracking (Allan 2002).", "acronym": "ORG", "label": "organization", "ID": "3346"}, {"sentence": "Help desk systems in large ORGs: Help desk staff in large ORGs need to quickly satisfy the customer?s need for information.", "acronym": "ORG", "label": "organization", "ID": "3347"}, {"sentence": "Evaluation       Given a collection of named entities from  documents, the coreferencing task is to put them into  equivalence classes, where every mention in the same  class refers to the same entity (person, location,  ORG, and so on).", "acronym": "ORG", "label": "organization", "ID": "3348"}, {"sentence": "A third category contains texts with uncertain provenance, e.g. the texts from various European Union ORGs or from Wikipedia.", "acronym": "ORG", "label": "organization", "ID": "3349"}, {"sentence": "In providing the fundamental ORG of the gram- 983 mar, to the extent that that ORG is consistent with the language modeled, these types significantly ease the path to creating a working grammar.", "acronym": "ORG", "label": "organization", "ID": "3350"}, {"sentence": "Specifically, we show how Discourse Representation Structures can be combined with distribu- tional models for word meaning inside a MLN and used to successfully perform inferences that take advantage of logical concepts such as factivity as well as probabilistic informa- tion on word meaning in context.", "acronym": "MLN", "label": "Markov Logic Network", "ID": "3351"}, {"sentence": "2 MLNs and Related Work Markov logic [14, 4] is a recently developed theoretically sound framework for combining first-order logic and probabilistic graphical models.", "acronym": "MLN", "label": "Markov Logic Network", "ID": "3352"}, {"sentence": "Coref (x , y) Table 2: Rules used in the MLN noted that, because we are assuming an order on the arguments of Coref (x , y), we need three formulae to capture transivity relationships.", "acronym": "MLN", "label": "Markov Logic Network", "ID": "3353"}, {"sentence": "Subse- quently, we will describe Markov Logic (section 4) and our MLN for event ex- !\"# !\"", "acronym": "MLN", "label": "Markov Logic Network", "ID": "3354"}, {"sentence": "Compared to other structured prediction frameworks such as MLNs (Poon and Vanderwende, 2010), SEARN provides high modeling flexibility but it does not requiring task- dependent approximate inference.", "acronym": "MLN", "label": "Markov Logic Network", "ID": "3355"}, {"sentence": "Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical repre- sentation, and MLNs are used to perform probabilistic logical inference.", "acronym": "MLN", "label": "Markov Logic Network", "ID": "3356"}, {"sentence": "The systems use a combination of deep semantic parsing, MLN and Conditional Random Field classifiers.", "acronym": "MLN", "label": "Markov Logic Networks", "ID": "3357"}, {"sentence": "2 MLN and Related Work Markov logic [14, 4] is a recently developed theoretically sound framework for combining first-order logic and probabilistic graphical models.", "acronym": "MLN", "label": "Markov Logic Networks", "ID": "3358"}, {"sentence": "To model the reciprocal relationship be- tween scope assignment and disambiguation, we propose a latent variables based approach using MLN that allows us to learn the parameters for the scope assignment and the disambiguation tasks jointly and enables us to per- form joint inference.", "acronym": "MLN", "label": "Markov Logic Networks", "ID": "3359"}, {"sentence": "There has also been a revival in using weighted logical forms in structured relational learning, such as MLN (Domingos and Kok, 2005), and this is related to the scoring of facts used by the current system in merging texts.", "acronym": "MLN", "label": "Markov Logic Networks", "ID": "3360"}, {"sentence": "Compared to other structured prediction frameworks such as MLN (Poon and Vanderwende, 2010), SEARN provides high modeling flexibility but it does not requiring task- dependent approximate inference.", "acronym": "MLN", "label": "Markov Logic Networks", "ID": "3361"}, {"sentence": "Distributional similarity between pairs of words is converted into weighted inference rules that are added to the logical repre- sentation, and MLN are used to perform probabilistic logical inference.", "acronym": "MLN", "label": "Markov Logic Networks", "ID": "3362"}, {"sentence": "MLN.", "acronym": "MLN", "label": "Markov logic networks", "ID": "3363"}, {"sentence": "Systems have also tried to take advantage of more global in- formation to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks such as integer linear programming and MLN (Bramsen et al, 2006; Cham- bers and Jurafsky, 2008; Yoshikawa et al, 2009; Uz- Zaman and Allen, 2010).", "acronym": "MLN", "label": "Markov logic networks", "ID": "3364"}, {"sentence": "The bilingual parallel corpus that we used was distributed as part of the 2008 NIST OpenMT Workshop.4 The training set contained 88,108 Urdu?English sentence pairs, and a bilingual dictionary with 113,911 entries.", "acronym": "OpenMT", "label": "Open Machine Translation Evaluation", "ID": "3365"}, {"sentence": "In NIST OpenMT Meeting.", "acronym": "OpenMT", "label": "Open Machine Translation Evaluation", "ID": "3366"}, {"sentence": "52008 NIST OpenMT: Chi- nese to English Task.", "acronym": "OpenMT", "label": "Open Machine Translation Evaluation", "ID": "3367"}, {"sentence": "The bilingual parallel corpus that we used was distributed as part of the 2008 NIST OpenMT Evaluation Workshop.4 The training set contained 88,108 Urdu?English sentence pairs, and a bilingual dictionary with 113,911 entries.", "acronym": "OpenMT", "label": "Open Machine Translation", "ID": "3368"}, {"sentence": "The 2008 NIST OpenMT  Evaluation.", "acronym": "OpenMT", "label": "Open Machine Translation", "ID": "3369"}, {"sentence": "In NIST OpenMT Evaluation Meeting.", "acronym": "OpenMT", "label": "Open Machine Translation", "ID": "3370"}, {"sentence": "Pro- ceedings of the 2008 NIST OpenMT Evaluation Workshop.", "acronym": "OpenMT", "label": "Open Machine Translation", "ID": "3371"}, {"sentence": "NIST OpenMT 2009 Evaluation (MT09).", "acronym": "OpenMT", "label": "Open Machine Translation", "ID": "3372"}, {"sentence": "OpenMT 2008 Evalua- tion.", "acronym": "OpenMT", "label": "Open Machine Translation", "ID": "3373"}, {"sentence": "For the work that we are reporting, we have adopted the InterlinguaPlus approach using the Carabao OpenMT framework (Berman, 2012).", "acronym": "OpenMT", "label": "open machine translation", "ID": "3374"}, {"sentence": "The next section discusses the implementation of the system using Carabao?s OpenMT framework and the results obtained.", "acronym": "OpenMT", "label": "open machine translation", "ID": "3375"}, {"sentence": "FL Function which describes how  an utterance constraints the future beliefs and  actions of the participants, and affects the dis-  course, and  ?", "acronym": "FL", "label": "Forward Looking", "ID": "3376"}, {"sentence": "The Main Topic may be regarded the FL Center in the centering terminology or the Current Focus.", "acronym": "FL", "label": "Forward Looking", "ID": "3377"}, {"sentence": "Backward Looking/FL Fea- tures.", "acronym": "FL", "label": "Forward Looking", "ID": "3378"}, {"sentence": "FL such semantic mismatches between verbs and arguments is the task of preference vio- lation detection.", "acronym": "FL", "label": "Flagging", "ID": "3379"}, {"sentence": "FL individual elements Previous work scored only entire rules, but some dependencies are problematic and others are not.", "acronym": "FL", "label": "Flagging", "ID": "3380"}, {"sentence": "The Dev-5k and Dev columns report labeled AS on the development sets.", "acronym": "AS", "label": "attachment score", "ID": "3381"}, {"sentence": "5 75.73 75.73 77.67 77.67 Table 1: Labeled AS per phase compared to default settings for all training sets from the Shared Task on PMRLs in the gold scenario on the held-out test set for optimization.", "acronym": "AS", "label": "attachment score", "ID": "3382"}, {"sentence": "The unlabeled ASs of the converted dependencies are shown as the accuracies in Table 5, since most bunsetsu-based dependency parsers out- put only unlabeled structure.", "acronym": "AS", "label": "attachment score", "ID": "3383"}, {"sentence": "79.98 83.59 86.96 Hebrew 76.78 76.80 79.37 80.17 3.39 79.83 79.83 76.61 76.61 80.03 80.03 Hungarian 70.37 71.11 71.98 81.91 11.54 80.69 80.74 71.27 72.34 82.37 83.14 Korean 87.22 87.22 87.22 88.94 1.72 86.52 90.20 81.69 88.43 83.74 89.39 Polish 75.52 75.58 79.28 80.27 4.75 81.58 81.91 76.64 77.70 79.79 80.49 Swedish 76.75 76.75 78.91 79.76 3.01 74.85 74.85 75.73 75.73 77.67 77.67 Table 1: Labeled AS per phase compared to default settings for all training sets from the Shared Task on PMRLs in the gold scenario on the held-out test set for optimization.", "acronym": "AS", "label": "attachment score", "ID": "3384"}, {"sentence": "Benefiting from the rich features selected in the tree kernel space, our model achieved the best reported unlabeled AS of 93.72 without using any additional resource.", "acronym": "AS", "label": "attachment score", "ID": "3385"}, {"sentence": "77.81 79.22 82.75 Hebrew 76.29 76.31 79.01 79.67 3.38 73.40 73.40 69.97 69.97 73.01 73.01 Hungarian 68.26 69.12 69.96 78.71 10.45 76.82 77.62 69.08 70.15 79.00 79.63 Korean 80.08 80.08 80.08 81.63 1.55 77.96 83.02 74.87 82.06 75.90 82.65 Polish 74.43 74.49 76.93 78.41 3.98 80.61 80.83 75.29 75.63 79.50 80.49 Swedish 74.53 74.53 76.51 77.66 3.13 72.90 72.90 73.21 73.21 75.82 75.82 Table 2: Labeled AS per phase compared to default settings for all training sets from the Shared Task on PMRLs in the predicted scenario on the held-out test set for optimization.", "acronym": "AS", "label": "attachment score", "ID": "3386"}, {"sentence": "The result is promising: these features significantly improved a state-of-the-art third order dependency parser, yielding the best reported unlabeled AS of 93.72 without using any additional resource.", "acronym": "AS", "label": "attachment score", "ID": "3387"}, {"sentence": "4  Proceedings of the Workshop on AS for Different Genres, Media, and Languages, pages 1?7, Portland, Oregon, June 23, 2011.", "acronym": "AS", "label": "Automatic Summarization", "ID": "3388"}, {"sentence": "Of the Workshop on AS (including DUC 2002), pp 27- 36.", "acronym": "AS", "label": "Automatic Summarization", "ID": "3389"}, {"sentence": "AS.", "acronym": "AS", "label": "Automatic Summarization", "ID": "3390"}, {"sentence": "In Proceedings of the NAACL Workshop on AS, Pitts- burgh, PA.", "acronym": "AS", "label": "Automatic Summarization", "ID": "3391"}, {"sentence": "Of the Workshop on AS (including DUC 2002), pp 1-8.", "acronym": "AS", "label": "Automatic Summarization", "ID": "3392"}, {"sentence": "Chinese NLP resources     Part 2: Text Processing  2.1 Lexical processing   a. Segmentation   b. Disambiguation   c. Unknown word detection   d. Named Entity Recognition  2.2 Syntactic processing   a. Issues in PoS tagging   b. Hidden Markov Models  2.3 NLP Applications  References   AS Balance Corpus of Mandarin Chi- nese.", "acronym": "AS", "label": "Academia Sinica", "ID": "3393"}, {"sentence": "from the AS Balanced Corpus.", "acronym": "AS", "label": "Academia Sinica", "ID": "3394"}, {"sentence": "AS?),", "acronym": "AS", "label": "Academia Sinica", "ID": "3395"}, {"sentence": "Technical Report, Taiwan,  Taipei, AS.", "acronym": "AS", "label": "Academia Sinica", "ID": "3396"}, {"sentence": "279  Conceptual Metaphors: Ontology-based representation and corpora driven Mapping Principles Kathleen Ahrens National Taiwan University kathleenahrens@yahoo.com Siaw Fong Chung National Taiwan University claricefong6376@hotmail.com Chu-Ren Huang AS churen@sinica.edu.tw Abstract The goal of this paper is to integrate the Conceptual Mapping Model with an on- tology-based knowledge representation (i.e. Suggested Upper Merged Ontology (SUMO)) in order to demonstrate that conceptual metaphor analysis can be re- stricted and eventually, automated.", "acronym": "AS", "label": "Academia Sinica", "ID": "3397"}, {"sentence": "Institute of Information Science,  AS.", "acronym": "AS", "label": "Academia Sinica", "ID": "3398"}, {"sentence": "Some Distributional Properties  of Mandarin Chinese-A Study Based on the  AS.", "acronym": "AS", "label": "Academia Sinica Corpus", "ID": "3399"}, {"sentence": "2 Related Work 2.1 AS Work on automatic summarisation dates back more than 50 years, with a focus on the English language (Luhn, 1958).", "acronym": "AS", "label": "Automatic Summarisation", "ID": "3400"}, {"sentence": "In Proceedings of the NAACL Workshop on AS, Association for Computational Linguistics, pages 41?49.", "acronym": "AS", "label": "Automatic Summarisation", "ID": "3401"}, {"sentence": "in AS: ANLP/NAACL  2000 Workshop, New Brunswick, New Jersey.", "acronym": "AS", "label": "Automatic Summarisation", "ID": "3402"}, {"sentence": "2 AS 2.1 Background Much of the previous NLP work in the legal domain con- cerns Information Retrieval (IR) and the computation of simple features such as word frequency.", "acronym": "AS", "label": "Automatic Summarisation", "ID": "3403"}, {"sentence": "In Proceedings of the NAAACL Workshop on AS, Pittsburgh, USA, June.", "acronym": "AS", "label": "Automatic Summarisation", "ID": "3404"}, {"sentence": "How can we use the discourse information for the evaluation of AS and Readability Assessment?", "acronym": "AS", "label": "Automatic Summarisation", "ID": "3405"}, {"sentence": "and ACC object such as ?", "acronym": "ACC", "label": "accusative", "ID": "3406"}, {"sentence": "Sense id Pattern Synonyms 1 kare(he) ga(nominative) soba(noodles) wo(ACC) kuu (eat) 2 kare (he) ga(nominative) fukugyo(a part-time job) de(ACC) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense Verb Classes Id Sense Verb Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4 ask {negau, tanomu} 14 accept {uketoru, ukeru, morau, osameru} 5 leave {saru,", "acronym": "ACC", "label": "accusative", "ID": "3407"}, {"sentence": "refers to the co- occurrence pattern between a verb and a noun 33 [Sentence pattern] <word1> ga <word2> wo taberu (eat) [Sense relation] agent object [Case particle] ga (nominative) wo (ACC) [Sense identifier] 30f6b0 (human);30f6bf (animal) 30f6bf(animal);30f6ca(plants); 30f6e5(parts of plants); 3f9639(food and drink); 3f963a(feed) Figure 1: An example of a verb ?", "acronym": "ACC", "label": "accusative", "ID": "3408"}, {"sentence": "Sense id Pattern Synonyms 1 kare(he) ga(nominative) soba(noodles) wo(ACC) kuu (eat) 2 kare (he) ga(nominative) fukugyo(a part-time job) de(ACC) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense Verb Classes Id Sense Verb Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4", "acronym": "ACC", "label": "accusative", "ID": "3409"}, {"sentence": "Merlo and Stevenson (2001) report inter-judge \u000b values of 0.53 to 0.66 for a task we consider to be comparable to ours, that of classifying verbs into unergative, unACC and object-drop, and argue that Car- letta?s ?", "acronym": "ACC", "label": "accusative", "ID": "3410"}, {"sentence": "One is to use several methods 38 [Sentence pattern] <word1> ga <word2> wo ukeireru / yurusu (forgive) [Concept relation] agent object [Case particle] ga (nominative) wo (ACC) [Sense identifier] 0ee0de; 0f58b4; 0f98ee 0f0157; 30f6b0 0ee0de: the part of a something written that makes reference to a particular matter 0f58b4: a generally-held opinion 0f98ee: the people who citizens of a nation 0f0157: a human being 30f6b0: human Figure 4: Extracted Verb frames of ?", "acronym": "ACC", "label": "accusative", "ID": "3411"}, {"sentence": "and ACCive object such as ?", "acronym": "ACC", "label": "accusat", "ID": "3412"}, {"sentence": "Sense id Pattern Synonyms 1 kare(he) ga(nominative) soba(noodles) wo(ACCive) kuu (eat) 2 kare (he) ga(nominative) fukugyo(a part-time job) de(ACCive) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense Verb Classes Id Sense Verb Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4 ask {negau, tanomu} 14 accept {uketoru, ukeru, morau, osameru} 5 leave {saru,", "acronym": "ACC", "label": "accusat", "ID": "3413"}, {"sentence": "refers to the co- occurrence pattern between a verb and a noun 33 [Sentence pattern] <word1> ga <word2> wo taberu (eat) [Sense relation] agent object [Case particle] ga (nominative) wo (ACCive) [Sense identifier] 30f6b0 (human);30f6bf (animal) 30f6bf(animal);30f6ca(plants); 30f6e5(parts of plants); 3f9639(food and drink); 3f963a(feed) Figure 1: An example of a verb ?", "acronym": "ACC", "label": "accusat", "ID": "3414"}, {"sentence": "Sense id Pattern Synonyms 1 kare(he) ga(nominative) soba(noodles) wo(ACCive) kuu (eat) 2 kare (he) ga(nominative) fukugyo(a part-time job) de(ACCive) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense Verb Classes Id Sense Verb Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4", "acronym": "ACC", "label": "accusat", "ID": "3415"}, {"sentence": "Merlo and Stevenson (2001) report inter-judge \u000b values of 0.53 to 0.66 for a task we consider to be comparable to ours, that of classifying verbs into unergative, unACCive and object-drop, and argue that Car- letta?s ?", "acronym": "ACC", "label": "accusat", "ID": "3416"}, {"sentence": "One is to use several methods 38 [Sentence pattern] <word1> ga <word2> wo ukeireru / yurusu (forgive) [Concept relation] agent object [Case particle] ga (nominative) wo (ACCive) [Sense identifier] 0ee0de; 0f58b4; 0f98ee 0f0157; 30f6b0 0ee0de: the part of a something written that makes reference to a particular matter 0f58b4: a generally-held opinion 0f98ee: the people who citizens of a nation 0f0157: a human being 30f6b0: human Figure 4: Extracted Verb frames of ?", "acronym": "ACC", "label": "accusat", "ID": "3417"}, {"sentence": "in the GEN case  (g), while the case (z1) in the first pair is ?", "acronym": "GEN", "label": "genitive", "ID": "3418"}, {"sentence": "Namely,  as indicated previously (see Table 2), the major- ity of nested terms in Serbian are in GEN case,  which means that the termhood for a term candi- date in GEN case would differ significantly  from its counterparts in other cases.", "acronym": "GEN", "label": "genitive", "ID": "3419"}, {"sentence": "preposition + noun  113  e.g. /bar/ + /as?s-e/   on   +   basis  /e/ an obligatory GEN ending,  2.", "acronym": "GEN", "label": "genitive", "ID": "3420"}, {"sentence": "Some Polish numerals have forms that agree in case with noun (marked congr), as well as forms that require a noun in GEN case (marked rec): (2) Przyszli came dwaj two-nom.congr ch?opcy.", "acronym": "GEN", "label": "genitive", "ID": "3421"}, {"sentence": "As a rule, the fro- zen part is in GEN.", "acronym": "GEN", "label": "genitive", "ID": "3422"}, {"sentence": "We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, GEN/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.", "acronym": "GEN", "label": "gender", "ID": "3423"}, {"sentence": "In particular, the scheme infers the GEN of a  referent from the GEN of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.", "acronym": "GEN", "label": "gender", "ID": "3424"}, {"sentence": "The second half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal GEN of English words, information that is  itself used in the pronoun resolution program.", "acronym": "GEN", "label": "gender", "ID": "3425"}, {"sentence": "The second  experiment investigates a method for unsuper-  vised learning of GEN/number/animaticity  information.", "acronym": "GEN", "label": "gender", "ID": "3426"}, {"sentence": "Next, the actual words in a proposed noun-  phrase antecedent give us information regarding  the GEN, number, and animaticity of the pro-  posed referent.", "acronym": "GEN", "label": "gender", "ID": "3427"}, {"sentence": "The first set (ANERGaz) pro- posed by (Benajiba and Rosso, 2008), which 80 Feature Feature Values Aspect Verb aspect: Command, Imperfective, Perfective, Not applicable Case Grammatical case: Nominative, Accusative, GEN, Not applicable, Undefined Gender Nominal Gender: Feminine, Masculine, Not applicable Mood Grammatical mood: Indicative, Jussive, Subjunctive, Not applicable, Undefined Number Grammatical number: Singular, Plural, Dual, Not applicable, Undefined Person Person Information: 1st, 2nd, 3rd, Not applicable State Grammatical state: Indefinite, Definite, Construct/Poss/Idafa, Not applicable, Und", "acronym": "GEN", "label": "Genitive", "ID": "3428"}, {"sentence": "(i) Nuori pitk~ vieh~tt~v~ tytt6  Young tall charming girl  GEN attributes, themselves nominals~ im~dify  head nominals recursively, as in the phrase (2).", "acronym": "GEN", "label": "Genitive", "ID": "3429"}, {"sentence": "On the other hand, for such  a noun as professor 'professor' GEN con.", "acronym": "GEN", "label": "Genitive", "ID": "3430"}, {"sentence": "Example  2 makes use of the domain f)red  icate: it is the predicate implied by a lexico-  gr~l)hic definition of a noun th;Lt deterinine, ill very inany eases, the exact interpretation f the  GEN construction with a concrete noun ~  a heard.", "acronym": "GEN", "label": "Genitive", "ID": "3431"}, {"sentence": "\u0019 Z  (someone) `\u0019 Z Z (which)   Adverbial kaf pro  (AKP)  \u0003}$\u0019 (where) |\u0019 Z  (when) \u0007?\u0004\u0019 Z (how)    GEN reflexive  (GR)  \u0007>?\u0002 (my)    GENs (G)  \u0002\u0003\u0004\u0005 (my) \u0002\u0006\u0007\u0015\t  Z (your)  Z \u0002\u0006\u0007\t\u000b (our) \u0002\u0003\u0004  Z (your)   Verb (VB)  \u0007>\u0015?\" (", "acronym": "GEN", "label": "Genitive", "ID": "3432"}, {"sentence": "The forms of the GEN case of the two words are g6da and g6ta:  syllabically go-da, go-ta, morphologically god-a, got-a.", "acronym": "GEN", "label": "Genitive", "ID": "3433"}, {"sentence": "Unlike in phyloGEN and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own parse tree.", "acronym": "GEN", "label": "genetics", "ID": "3434"}, {"sentence": "In particular we leverage the con- cept of additive tree metrics (Buneman, 1971; Buneman, 1974) in phyloGEN and machine learning that can create a special distance met- ric among the observed variables as a function of the underlying spectral dependencies (Choi et al.,", "acronym": "GEN", "label": "genetics", "ID": "3435"}, {"sentence": "in phyloGEN).", "acronym": "GEN", "label": "genetics", "ID": "3436"}, {"sentence": "These ten  were chosen as recent documents as of early March  2012 and which contained the text word maize and  discussed GEN.", "acronym": "GEN", "label": "genetics", "ID": "3437"}, {"sentence": "It is an adaptive heuristic search algorithm based on the evolutionary ideas of natural selec- tion and GEN.", "acronym": "GEN", "label": "genetics", "ID": "3438"}, {"sentence": "Unlike in phyloGEN and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own par", "acronym": "GEN", "label": "genetics", "ID": "3439"}, {"sentence": "This implicit reporting of results expressed in terms of negative experimental outcomes is very common in molecular biology and GEN.", "acronym": "GEN", "label": "genetics", "ID": "3440"}, {"sentence": "PCFG with latent annotations.", "acronym": "PCFG", "label": "Probabilistic CFG", "ID": "3441"}, {"sentence": "Simple PCFGs dictate that, given this information,  \"determiner noun\" should be the most likely interpretation of a  noun phrase.", "acronym": "PCFG", "label": "probabilistic CFG", "ID": "3442"}, {"sentence": "Supervised training for PCFGs requires parsed cor-  pora.,", "acronym": "PCFG", "label": "probabilistic CFG", "ID": "3443"}, {"sentence": "of Scores 657 60%  Figure 3: Search Space Reduction and Accuracy for 1,bur Probabilistic  Models  a simple PCFG model, the parser produced a much  lower accuracy rate (35%).", "acronym": "PCFG", "label": "probabilistic CFG", "ID": "3444"}, {"sentence": "They parse raw text into LFG f-structures by first parsing with a PCFG parser to choose the most proba- ble c-structure.", "acronym": "PCFG", "label": "probabilistic CFG", "ID": "3445"}, {"sentence": "For example, probabilities were de- fined over grammar rules in PCFG (Collins, 1999; Klein and Manning, 2003; Char- niak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure gram- mar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005).", "acronym": "PCFG", "label": "probabilistic CFG", "ID": "3446"}, {"sentence": "The search space for CFG with CSP was 4 to 5  times lower than the simple PCFG.", "acronym": "PCFG", "label": "probabilistic CFG", "ID": "3447"}, {"sentence": "Simple PCFGs provide generalinformation about  how likely a construct is going to appear anywhere in a sam-  ple of a language.", "acronym": "PCFG", "label": "probabilistic CFG", "ID": "3448"}, {"sentence": "ONYX contains a number of innovative ideas including a novel adaptation of Kay's (1980) parse algorithm; a symbolic language extended to include probabilis-tic and procedural elements; an integration of syn-tax and semantics that includes a semantically weighted PCFG and interpretation based both on a semantic network and a semantic grammar.", "acronym": "PCFG", "label": "probabilistic context free grammar", "ID": "3449"}, {"sentence": "In our implementation, we can use either a PCFG, or a lexicalized context free grammar which condi- tions rules on parent category and parent lexical head, and conditions the heads of non-head chil- dren on child category, parent category, and par- ent head (Eisner, 1997; Charniak, 1995; Carroll and Rooth, 1998).", "acronym": "PCFG", "label": "probabilistic context free grammar", "ID": "3450"}, {"sentence": "While these models are well suited for the effective handling of highly divergent sentential word orders, the above frameworks have a lim- itation shared with PCFGs that the preferred ordering of subtrees is insufficiently constrained by their embedding context, which is espe- cially problematic for very deep syntactic parses.", "acronym": "PCFG", "label": "probabilistic context free grammar", "ID": "3451"}, {"sentence": "To generate such a  semantic frame, words in the utterance are usually  aligned to a semantic tree by a parsing algorithm such  as a PCFG or a recursive  network whose nodes represent semantic symbols of  the words and arcs consist of transition probabilities.", "acronym": "PCFG", "label": "probabilistic context free grammar", "ID": "3452"}, {"sentence": "Basic methods  of PCFGs.\"", "acronym": "PCFG", "label": "probabilistic context free grammar", "ID": "3453"}, {"sentence": "Ba- sic methods of PCFGs.", "acronym": "PCFG", "label": "probabilistic context free grammar", "ID": "3454"}, {"sentence": "Coreference as the  Foundations for Link Analysis Over Free Text  DBs.", "acronym": "DB", "label": "Database", "ID": "3455"}, {"sentence": "In Christiane Fellbaum, editor, WordNet: An Electronic Lexical DB, Language, Speech, and Communication.", "acronym": "DB", "label": "Database", "ID": "3456"}, {"sentence": "DB Center for Life Science, Research Organization of Information and System, Japan ?", "acronym": "DB", "label": "Database", "ID": "3457"}, {"sentence": "Responses from a Portable Natural  Language DB Qury System, in  Brady,M.,Berwick,R.C.(eds): Computational Models of  Discourse, MIT Press(1983).", "acronym": "DB", "label": "Database", "ID": "3458"}, {"sentence": "Fellbaum C., WordNet: an Electionic Lexical  DB, Cambridge, MA, The MIT Press.", "acronym": "DB", "label": "Database", "ID": "3459"}, {"sentence": "Euro WordNet: A Multilingual DB with Lexical Semantic Networks.", "acronym": "DB", "label": "Database", "ID": "3460"}, {"sentence": "This introduces an addi-  tional data type with its own management principles for  DB, retrieval and update.", "acronym": "DB", "label": "data storage", "ID": "3461"}, {"sentence": "As regards DB, all recorded information is permanently stored in a MySQL database.", "acronym": "DB", "label": "data storage", "ID": "3462"}, {"sentence": "We have modularized the feature gen- eration, instance representation, DB for- mats, and algorithm implementations; this allows users to make seamless transitions along any of these dimensions with minimal effort.", "acronym": "DB", "label": "data storage", "ID": "3463"}, {"sentence": "The system is designed to separate cleanly low-level tasks such as DB, data visualisation, location and loading of com- ponents and execution of processes from the data structures and algorithms that actu- ally process human language.", "acronym": "DB", "label": "data storage", "ID": "3464"}, {"sentence": "When considering time-critical BKB  applications, uch as the BKB within a machine trans-  lation system, it is clear that efficient DB  techniques arc needed.", "acronym": "DB", "label": "data storage", "ID": "3465"}, {"sentence": "--9--  b. The DB for each entry of the glossary  is of variable length, since the lists of dependents,  governing probabilities, hypernyms and semantic classes  associated with the entries are of variable length.", "acronym": "DB", "label": "data storage", "ID": "3466"}, {"sentence": "For com- puting the counts of positive and negative words (Feature 15 and 16) we used the General Inquirer DB (Stone et al.,", "acronym": "DB", "label": "database", "ID": "3467"}, {"sentence": "The story-independent resources that we used are: (a) the U.S. Social Security Administration baby name DB (Security, 2014), in which person names are linked with gender and (b) a large name-gender association list developed us- ing a corpus-based bootstrapping approach, which even included the estimated gender for non-person entities (Bergsma and Lin, 2006).", "acronym": "DB", "label": "database", "ID": "3468"}, {"sentence": "U.S. social security adminis- tration baby name DB.", "acronym": "DB", "label": "database", "ID": "3469"}, {"sentence": "This DB is  complemented by an intelligent search procedure  able to handle multimedia representations of  knowledge.", "acronym": "DB", "label": "database", "ID": "3470"}, {"sentence": "In order to accomplish this goal, we suggest a  multimedia DB with powerful indexing and  classification functions. (", "acronym": "DB", "label": "database", "ID": "3471"}, {"sentence": "The result is a DB that contains a set  of frames (related through hierarchy and  composition), a set of frame elements for each frame,  and a set of frame annotated sentences that covers the  different patterns of usage for lexical units in the  frame.", "acronym": "DB", "label": "database", "ID": "3472"}, {"sentence": "ning of spatial language, we next discuss two agents that play the game: ListenerBot (Section 4) makes decisions us- ing a single-agent POMDP that does not take into account the beliefs or actions of its partner, whereas DB (Section 5) maintains a model of its part- ner?s beliefs.", "acronym": "DB", "label": "DialogBot", "ID": "3473"}, {"sentence": "At each time step, DB marginal- izes out the possible observations o?", "acronym": "DB", "label": "DialogBot", "ID": "3474"}, {"sentence": "To decide when and how to speak, DB maintains a dis- tribution over its partner?s beliefs and reasons about the effects his utterances will have on those beliefs.", "acronym": "DB", "label": "DialogBot", "ID": "3475"}, {"sentence": "5 DB We now introduce DB, a Cards agent which is capable of producing linguistic advice.", "acronym": "DB", "label": "DialogBot", "ID": "3476"}, {"sentence": "(c) DB POMDP Figure 3: The decision diagram for the ListenerBot POMDP, the full Dec-POMDP, and the DB ap- proximation POMDP.", "acronym": "DB", "label": "DialogBot", "ID": "3477"}, {"sentence": "Us- ing this task and a model of the meaning of spatial language, we next discuss two agents that play the game: ListenerBot (Section 4) makes decisions us- ing a single-agent POMDP that does not take into account the beliefs or actions of its partner, whereas DB (Section 5) maintains a model of its part- ner?s beliefs.", "acronym": "DB", "label": "DialogBot", "ID": "3478"}, {"sentence": "To handle these complexities, DB models the world as a Decentralized Partially Observable Markov Decision Process (Dec-POM", "acronym": "DB", "label": "DialogBot", "ID": "3479"}, {"sentence": "As a result of the cooperative structure of the underlying model and the effects of commu- nication within it, DB?s contributions are rel- evant, truthful, and informative, which leads to sig- nificantly improved task performance.", "acronym": "DB", "label": "DialogBot", "ID": "3480"}, {"sentence": "ML estimate of the probability distributions, i.e. the relative frequency in the training data.", "acronym": "ML", "label": "Maximum Likelihood", "ID": "3481"}, {"sentence": "2.2.2 ML Used in earlier models (Och and Ney, 2002), the likelihood criterion is defined as the likelihood of an oracle hypothesis e(t)k? ,", "acronym": "ML", "label": "Maximum Likelihood", "ID": "3482"}, {"sentence": "In training, the CRF model is built with labeled  data and by means of an iterative algorithm based  on ML Estimation.", "acronym": "ML", "label": "Maximum Likelihood", "ID": "3483"}, {"sentence": "ML from Incomplete Data via  the EM Algorithm?,", "acronym": "ML", "label": "Maximum Likelihood", "ID": "3484"}, {"sentence": "Bahl, L. R., Jelinek, E, and Mercer, R. A ML  Approach to Continuous Speech Recognition.", "acronym": "ML", "label": "Maximum Likelihood", "ID": "3485"}, {"sentence": "VM model applied to RRR dataset (RRR-basic experiment) and the same experiment applied to TB2 dataset (TB2- 278 Description Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al, 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al, 1994) 93.2 RRR ML-based (Hindle and Rooth, 1993) 79.7 AP Maximum entropy, words (Ratnaparkhi et al, 1994) 77.7 RRR Maximum entropy, words & classes (Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and", "acronym": "ML", "label": "Maximum Likelihood", "ID": "3486"}, {"sentence": "In each case 7 the final estimate is  e----Ale1 + (1 - &l)(A2e2 + (1 - &2)ea)  where ex, e2 and e3 are MLd esti-  mates with the context at levels 1, 2 and 3 in the  table, and ,kl, ,k2 and )~3 are smoothing parameters  where 0 _< ,ki _< 1.", "acronym": "ML", "label": "maximum likelihoo", "ID": "3487"}, {"sentence": "15) We use the expectation maximization algorithm (Dempster et al, 1977) for the MLd estimate of ?", "acronym": "ML", "label": "maximum likelihoo", "ID": "3488"}, {"sentence": "This is effectively a comparison of the MLd estimates of/)(pll ,  nl ) and P(PI(}, v), a  different measure from the backed-off estimate which gives i5(lIv,p , nl).", "acronym": "ML", "label": "maximum likelihoo", "ID": "3489"}, {"sentence": "Spectral conversion based on MLd es- timation considering global variance of converted pa- rameter.", "acronym": "ML", "label": "maximum likelihoo", "ID": "3490"}, {"sentence": "Most probability models for DOP use the relative frequency estimator to estimate fragment probabilities, although Bod (2000b) trains fragment probabilities by a MLd reestimation procedure belonging to the class of expectation-maximization algorithms.", "acronym": "ML", "label": "maximum likelihoo", "ID": "3491"}, {"sentence": "Counts of lower order tuples can also  be made-  for example f(1, P = from) is the number of times (P = from) is seen with noun attach-  ment in training data, f (V  = is, N2 = research) is the number of times (V = is, N2 = research)  is seen with either attachment and any value of N1 and P.  29  3.2 Max imum L ike l ihood  Es t imat ion   A MLd method would use the training data to give the following estimation for the  conditional probability:  l~(l\\[v, nl,p, n2)= f(1,v, nl,p, n2)  f(v, nl, p, n2)  Unfortunately sparse data problems make this estimate useless.", "acronym": "ML", "label": "maximum likelihoo", "ID": "3492"}, {"sentence": "He has developed several ML based natural language  processing systems that are widely used in the computational linguistics community and  in industry and has presented invited talks and tutorials in several major conferences.", "acronym": "ML", "label": "machine learning", "ID": "3493"}, {"sentence": "2013) use a su- pervised ML approach to address the same problem, though many of their preliminary steps and input features are similar to those used in (Elson and McKeown, 2010).", "acronym": "ML", "label": "machine learning", "ID": "3494"}, {"sentence": "Roth has published broadly in ML, natural language processing,  knowledge representation and reasoning and received several paper, teaching and  research awards.", "acronym": "ML", "label": "machine learning", "ID": "3495"}, {"sentence": "As these expressions are sparsely scattered throughout the texts, it is not easy to gen- eralize results of ML from a training set to a test set.", "acronym": "ML", "label": "machine learning", "ID": "3496"}, {"sentence": "The proposed methodology has a strong hybrid char- acter in that it employs different approaches that range from pattern-based to ML- based to the incorporation of external knowledge resources.", "acronym": "ML", "label": "machine learning", "ID": "3497"}, {"sentence": "He published several papers in natural language  processing, ML and semantic interpretation.", "acronym": "ML", "label": "machine learning", "ID": "3498"}, {"sentence": "In this task, we use a dataset consisting of a subset of the Wall Street Journal (WSJ) corpus, in which the ML of a text is 20 sentences, and the average length is 41 sentences.", "acronym": "ML", "label": "minimum length", "ID": "3499"}, {"sentence": "Minimum length of component: Specifies the ML of potential compound elements;  shorter substrings are excluded to avoid accidental matching of very short words.", "acronym": "ML", "label": "minimum length", "ID": "3500"}, {"sentence": "The risk of false positives is managed by making incorporating a fingerprint of the n-gram, and by making bit vectors longer than the ML required to store values.", "acronym": "ML", "label": "minimum length", "ID": "3501"}, {"sentence": "During processing we filtered out documents whose top domain scores were below a previously set minimum threshold and those whose document length was below a ML.", "acronym": "ML", "label": "minimum length", "ID": "3502"}, {"sentence": "This is because the ML of a Chi- nese common string is 2 characters and each  has 16 binary bits.", "acronym": "ML", "label": "minimum length", "ID": "3503"}, {"sentence": "When  the ML path between two concepts is  discovered the propositions standing on i t  are  returned as being relevant to the current input.", "acronym": "ML", "label": "minimum length", "ID": "3504"}, {"sentence": "Minimum length of compound: The ML of string that should be subject to  decompounding; short strings are unlikely to be compounds, so for efficiency reasons, they are not  decompounded.", "acronym": "ML", "label": "minimum length", "ID": "3505"}, {"sentence": "Genetic Algorithms in Search, Optimization, and ML.", "acronym": "ML", "label": "Machine Learning", "ID": "3506"}, {"sentence": "He  has worked on ML in the context of Natural Language Processing and  has published papers in several conferences.", "acronym": "ML", "label": "Machine Learning", "ID": "3507"}, {"sentence": "Journal of ML Re- search 3:951\u0015991.", "acronym": "ML", "label": "Machine Learning", "ID": "3508"}, {"sentence": "In ML and Cybernetics, 2007 Interna- tional Conference on, volume 7, pages 3997?4002.", "acronym": "ML", "label": "Machine Learning", "ID": "3509"}, {"sentence": "In Proceedings of AAAI Spring Symposium  on Applying ML to Discourse  ?", "acronym": "ML", "label": "Machine Learning", "ID": "3510"}, {"sentence": "Disambiguating Proteins, Genes, and RNA in Text:  A ML Approach.", "acronym": "ML", "label": "Machine Learning", "ID": "3511"}, {"sentence": "Following their  lead, the research presented here uses Priority  MaxiMLE modified from  the backoff combination as follows:  P? (", "acronym": "MLE", "label": "mum Likelihood Estimation", "ID": "3512"}, {"sentence": "In training, the CRF model is built with labeled  data and by means of an iterative algorithm based  on MaxiMLE.", "acronym": "MLE", "label": "mum Likelihood Estimation", "ID": "3513"}, {"sentence": "The n-gram LMs are word-based backoff  models, where the n-gram probabilities are esti- mated using MaxiMLE  with smoothing.", "acronym": "MLE", "label": "mum Likelihood Estimation", "ID": "3514"}, {"sentence": "Language model (LM)  A character-based trigram language model with  Katz back-off is constructed from the training  data to estimate the language model p(S) using  MaxiMLE.", "acronym": "MLE", "label": "mum Likelihood Estimation", "ID": "3515"}, {"sentence": "Intercept -0.136  Imposter -2.180**  Time (Question) -0.134*  *p<.05; **p<.01; Fit by MaxiMLE.", "acronym": "MLE", "label": "mum Likelihood Estimation", "ID": "3516"}, {"sentence": "1 1 1 1 11 )()( ),(log),()( n i n i n ij ji jin ij ji spsp sspssISMI  (6)  The parameters p(si,sj), p(si) and p(sj) are esti- mated using MaxiMLE on  the same training data as for training PTM.", "acronym": "MLE", "label": "mum Likelihood Estimation", "ID": "3517"}, {"sentence": "Due to the power-law nature of language (Zipf, 1949), the MLE mas- sively overestimates the probability of rare events and assigns zero probability to legitimate word se- quences that happen not to have been observed in the training data (Manning and Sch?utze, 1999).", "acronym": "MLE", "label": "maximum likelihood estimator", "ID": "3518"}, {"sentence": "P. The MLE for n-grams is derived from frequency counts for sequence X and symbol c, PML(c|X) = count(Xc)/extCount(X), where count(X) is the number of times the sequence X was observed in the training data and extCount(X) is the number of single-symbol extensions of X observed: extCount(X) = ?", "acronym": "MLE", "label": "maximum likelihood estimator", "ID": "3519"}, {"sentence": "Then the MLE for this model can be obtained by maximizing the log likelihood function: LL(?)", "acronym": "MLE", "label": "maximum likelihood estimator", "ID": "3520"}, {"sentence": "Among the estimators, the MLE  provides the best results for the training set, but it is the worst on the test set.", "acronym": "MLE", "label": "maximum likelihood estimator", "ID": "3521"}, {"sentence": "Then we calculated five different association measures for each candidate: MLE (mle), pointwise mutual information (pmi), Student's t-test (t), Dice's coefficient (dice), and log-likelihood (ll).", "acronym": "MLE", "label": "maximum likelihood estimator", "ID": "3522"}, {"sentence": "Expectation maximization is an iterative procedure for computing the MLE of a parameter set when only a subset of data is available.", "acronym": "MLE", "label": "maximum likelihood estimator", "ID": "3523"}, {"sentence": "Following their  lead, the research presented here uses Priority  MLE modified from  the backoff combination as follows:  P? (", "acronym": "MLE", "label": "Maximum Likelihood Estimation", "ID": "3524"}, {"sentence": "In training, the CRF model is built with labeled  data and by means of an iterative algorithm based  on MLE.", "acronym": "MLE", "label": "Maximum Likelihood Estimation", "ID": "3525"}, {"sentence": "The n-gram LMs are word-based backoff  models, where the n-gram probabilities are esti- mated using MLE  with smoothing.", "acronym": "MLE", "label": "Maximum Likelihood Estimation", "ID": "3526"}, {"sentence": "Language model (LM)  A character-based trigram language model with  Katz back-off is constructed from the training  data to estimate the language model p(S) using  MLE.", "acronym": "MLE", "label": "Maximum Likelihood Estimation", "ID": "3527"}, {"sentence": "Intercept -0.136  Imposter -2.180**  Time (Question) -0.134*  *p<.05; **p<.01; Fit by MLE.", "acronym": "MLE", "label": "Maximum Likelihood Estimation", "ID": "3528"}, {"sentence": "1 1 1 1 11 )()( ),(log),()( n i n i n ij ji jin ij ji spsp sspssISMI  (6)  The parameters p(si,sj), p(si) and p(sj) are esti- mated using MLE on  the same training data as for training PTM.", "acronym": "MLE", "label": "Maximum Likelihood Estimation", "ID": "3529"}, {"sentence": "The empirical probability for each sentence pair is estimated by MLE over the training data (Brown et al, 1993).", "acronym": "MLE", "label": "maximum likelihood estimation", "ID": "3530"}, {"sentence": "Secondly, MLE is  usually used, which is only loosely related to minimum sen-  tence error.", "acronym": "MLE", "label": "maximum likelihood estimation", "ID": "3531"}, {"sentence": "The structure of our Bayesian classifiers were derived from the K2 al- gorithm6, and their parameters were derived from MLE.", "acronym": "MLE", "label": "maximum likelihood estimation", "ID": "3532"}, {"sentence": "k that the logistic regression model estimates (from train- ing data, using MLE) 30 Coeff.", "acronym": "MLE", "label": "maximum likelihood estimation", "ID": "3533"}, {"sentence": "So the main problem is to estimate Pe(t), the  probability of a term t appearing in the context of  the entity e.  Using the annotated name mention data set M,  we can get the MLE of  Pe(t) as follows:  _ ( )( ) ( ) e e ML e t Count tP t Count t??", "acronym": "MLE", "label": "maximum likelihood estimation", "ID": "3534"}, {"sentence": "In spite of the simplifying n-gram assump- tion, MLE remains un- reliable and tends to underestimate the proba- bility of very rare n-grams.", "acronym": "MLE", "label": "maximum likelihood estimation", "ID": "3535"}, {"sentence": "is the probability distribution function whose parameters are MLE from the training set.", "acronym": "MLE", "label": "maximum likelihood estimates", "ID": "3536"}, {"sentence": "P (ti|ti?1) HMMs can be trained directly from labeled data by calculating MLE or from incomplete data using Expectation Maximization (EM) (Dempster et al1977).", "acronym": "MLE", "label": "maximum likelihood estimates", "ID": "3537"}, {"sentence": ", z,n) (2) 2 The word frequency distribution does not impact the in- ferred topics (because words are always observed), and in our experiments we simply use MLE for ?", "acronym": "MLE", "label": "maximum likelihood estimates", "ID": "3538"}, {"sentence": "i ) can be easily calculated us-ing the MLE of the prob- abilities (i.e., the estimate of each probability is the corresponding relative frequency).", "acronym": "MLE", "label": "maximum likelihood estimates", "ID": "3539"}, {"sentence": "Unfortunately, using multiple paraphrased ver- sions of the same sentence changes the word fre- quencies in the training bi-text, thus causing worse MLE, which results in bad system performance.", "acronym": "MLE", "label": "maximum likelihood estimates", "ID": "3540"}, {"sentence": "This is effectively a comparison of the MLE of/)(pll ,  nl ) and P(PI(}, v), a  different measure from the backed-off estimate which gives i5(lIv,p , nl).", "acronym": "MLE", "label": "maximum likelihood estimates", "ID": "3541"}, {"sentence": "Discriminative training meth- ods for hidden MMs: Theory and exper- iments with perceptron algorithms.", "acronym": "MM", "label": "Markov model", "ID": "3542"}, {"sentence": "2 Sequential Labeling We discriminatively train a MM us- ing Bayes Point Machines (BPM).", "acronym": "MM", "label": "Markov model", "ID": "3543"}, {"sentence": "As with hidden MMs (Rabiner, 1989), yw(x) can be com- puted efficiently for suitable feature functions using dynamic programming.", "acronym": "MM", "label": "Markov model", "ID": "3544"}, {"sentence": "We then prepared features, and fed the training data to a sequential labeling system, a discriminative MM much like Conditional Random Fields (CRF), with the difference being that the model parameters are tuned using Bayes Point Machines (BPM), and then compared our model against an equivalent CRF model.", "acronym": "MM", "label": "Markov model", "ID": "3545"}, {"sentence": "We focus on the simple and tractable hidden MM, and present an efficient learning algorithm for incorporating approximate bijectivity and symmetry constraints.", "acronym": "MM", "label": "Markov model", "ID": "3546"}, {"sentence": "It is not imMMtely obvious that the only paths which  can be formed by the remaining arcs are optimal.", "acronym": "MM", "label": "media", "ID": "3547"}, {"sentence": "2006) for gender identification, applied to the identification of users in social MM.", "acronym": "MM", "label": "media", "ID": "3548"}, {"sentence": "Associations are almost  always multiMMl, i.e., we associate texts,  sounds, pictures, movement e c.  The structure of an interactive, multiMM  encyclopedia that is based on associations includes  1) a knowledge space/domain,  2) an associative search procedure,  3) a function for storing associative traces.", "acronym": "MM", "label": "media", "ID": "3549"}, {"sentence": "Interactive MultiMM Navigation  Prof. Dr. Dr. Mihai NADIN  Computational Design, University of Wuppertal  Hofaue 35-39  D-42103 Wuppertal, Germany  nadin @ code.uni-wuppertal.de  Dipl.", "acronym": "MM", "label": "media", "ID": "3550"}, {"sentence": "Interactive MultiMM Navigation  Until now, problems of navigation have in the main  been reduced to the research of i", "acronym": "MM", "label": "media", "ID": "3551"}, {"sentence": "Clitic pronouns were tagged as verbs, for they always imMMtely precede or follow a verb.", "acronym": "MM", "label": "media", "ID": "3552"}, {"sentence": "When M is the initial state I, the result follows imMMtely  from line 3 which sets hartnony(I) to zero.", "acronym": "MM", "label": "media", "ID": "3553"}, {"sentence": "our approach 2 , SG model is picked as the baseline.", "acronym": "SG", "label": "Skip Gram", "ID": "3554"}, {"sentence": "For instance, the SG likelihood aims to maximize the follow- ing conditional: L(?,?,", "acronym": "SG", "label": "Skip Gram", "ID": "3555"}, {"sentence": "Finally, we also compare to shallow representation learning net- works such as SG and Continuous Bag of Words (CBoW) (Mikolov et al, 2013a), competitive state of the art window based baselines.", "acronym": "SG", "label": "Skip Gram", "ID": "3556"}, {"sentence": "In the regression the input x is trans- formed to directly predict y. The SG model, however, transforms both the context x and the tar- get y, and can therefore be seen as a generalization of the MLR.", "acronym": "SG", "label": "Skip Gram", "ID": "3557"}, {"sentence": "3.2 Sensitivity Analysis In order to test the sensitivity of our model and base- line SG to variations in the training set, we perform two sensitivity analyses.", "acronym": "SG", "label": "Skip Gram", "ID": "3558"}, {"sentence": "6.2 SG Vectors In the RNN model (?", "acronym": "SG", "label": "Skip Gram", "ID": "3559"}, {"sentence": "M. C. Mc('ord, \"SG,\" American Journal of  Computational Linguistics, Vol.", "acronym": "SG", "label": "Slot Grammars", "ID": "3560"}, {"sentence": "SG.", "acronym": "SG", "label": "Slot Grammars", "ID": "3561"}, {"sentence": "McCord, M. C. (1980) \"SG,\" Com-  putational Linguistics, vol.", "acronym": "SG", "label": "Slot Grammars", "ID": "3562"}, {"sentence": "McCord, M.C., \"SG\", Computationa/  Linguistics, vol 6, 31-43 (1980)  13.", "acronym": "SG", "label": "Slot Grammars", "ID": "3563"}, {"sentence": "SG\" Com-  putational Linguistlcs, Vol 6:31-43.", "acronym": "SG", "label": "Slot Grammars", "ID": "3564"}, {"sentence": "2.1 Document-Level Analysis  Document-level analysis is performed by component  PE's named Text Analysis Engines  (TAEs).", "acronym": "PE's", "label": "processing elements", "ID": "3565"}, {"sentence": "2 A Plug and Play Scenario In this section we present example dialogues from our current demonstrator and brie y outline the main PE's.", "acronym": "PE's", "label": "processing elements", "ID": "3566"}, {"sentence": "The PE's include: \u0019 Document tokenisation \u0019 Sentence splitting \u0019 Parts-of-speech tagging \u0019 Named Entity Recognition using a gazetteer lookup module and regular expressions \u0019 Named entity coreference using an ortho- graphic name matcher Named entities of type person, organization, ad- dress, date, and location are considered relevant document terms and stored in a special named en- tity", "acronym": "PE's", "label": "processing elements", "ID": "3567"}, {"sentence": "However, instead of  swapping the memories, we activate a sec-  ond set of PE's that are con-  nected to the memories in the right way.", "acronym": "PE's", "label": "processing elements", "ID": "3568"}, {"sentence": "1 In the rest of this paper, we begin by detail- ing our concrete Plug and Play scenario - device control in the home - with an example dialogue from our demonstrator and an outline of the main dialogue PE's.", "acronym": "PE's", "label": "processing elements", "ID": "3569"}, {"sentence": "Additionally, Reconcile can be eas- ily reconfigured to use different algorithms, fea- tures, prePE's, evaluation settings and metrics.", "acronym": "PE's", "label": "processing elements", "ID": "3570"}, {"sentence": "Table 3 Rules for Assigning Syntactic Roles to PE's  Pattern to be Scanned New Pattern to be Generated  TOGOV~ + OBJ  *: focus, - - :  not mentioned, ~: empty, \\[...\\]: optional  Table 4 Rules for Constructing Clausal Elements  Pattern to be Scanned New Element to be Generated  I*  \\[ SENT |  162  He saw a bird with a ribbon.", "acronym": "PE's", "label": "Phrasal Elements", "ID": "3571"}, {"sentence": "Unit completion for a CAT typing system.", "acronym": "CAT", "label": "computer-aided translation", "ID": "3572"}, {"sentence": "INTRODUCTION  Automatic language processing makes a current use of multilingual  databases as a component of the computer environment of machine  translation projects or CAT~ and as a means  of coordinating terminological standardization across several  lar~cages.", "acronym": "CAT", "label": "computer-aided translation", "ID": "3573"}, {"sentence": "Transtype: a CAT typing system.", "acronym": "CAT", "label": "computer-aided translation", "ID": "3574"}, {"sentence": "These linguistic resources  are integrated in a CAT  environment used by technical writers.", "acronym": "CAT", "label": "computer-aided translation", "ID": "3575"}, {"sentence": "FUNCTIONS A~ND USERS OF THE SYSTEM TERMSERVICE  The system TEk~VICE is designed to be used in the following  environments:  - in the CAT environment the terminological  database can be used by human translators and specialists in  scientific and technical fields as a computer-aided multilingual  dictionary;  - as far as the terminological environment is concerned, the data-  base provides sufficient linguistic information to conduct research  on terminology and to standardize terms, abbreviations, acronym", "acronym": "CAT", "label": "computer-aided translation", "ID": "3576"}, {"sentence": "Unit com- pletion for a CAT typing system.", "acronym": "CAT", "label": "computer-aided translation", "ID": "3577"}, {"sentence": "In this section, we define a standard automatic evaluation protocol, akin to the ones used in Computer-Aided Trans- lation and CAText Recognition.", "acronym": "CAT", "label": "Computer Aided T", "ID": "3578"}, {"sentence": "\"An Approacn t o  CATranslation,\" i n  IEEE Transactions  on Engineering W r i t i n g  and Speech, Vol.", "acronym": "CAT", "label": "Computer Aided T", "ID": "3579"}, {"sentence": "CATranslation: Theory and Practice.", "acronym": "CAT", "label": "Computer Aided T", "ID": "3580"}, {"sentence": "c?2014 Association for Computational Linguistics Human Effort and Machine Learnability in CATranslation Spence Green, Sida Wang, Jason Chuang, * Jeffrey Heer, * Sebastian Schuster, and Christopher D. Manning Computer Science Department, Stanford University {spenceg,sidaw,sebschu,manning}@stanford.edu * Computer Science Department, University of Washington {jcchuang,jheer}@uw.edu Abstract Analyses of computer aided translation typi- cally focus on either frontend interfaces", "acronym": "CAT", "label": "Computer Aided T", "ID": "3581"}, {"sentence": "Lippman, Erhard  O., and Plath,  W.J.  1970 \"Time Sharing and CATranslation,\" i n  The F i n i t e   Str ing,  .Vole 7 ,  No.", "acronym": "CAT", "label": "Computer Aided T", "ID": "3582"}, {"sentence": "c?2009 ACL and AFNLP A Web-Based Interactive CATranslation Tool Philipp Koehn School of Informatics University of Edinburgh pkoehn@inf.ed.ac.uk Abstract We developed caitra, a novel tool that aids human translators by (a) making sugges- tions for sentence completion in an inter- active machine translation setting, (b) pro- viding alternative word and phrase trans- lations, and (c) allowing them to post- edit machine translation", "acronym": "CAT", "label": "Computer Aided T", "ID": "3583"}, {"sentence": "Approacn t o  CATranslation,\" i n  IEEE Transactions  on Engineering W r i t i n g  and Speech, Vol.", "acronym": "CAT", "label": "Computer Aided T", "ID": "3584"}, {"sentence": "The first part of the following theorem follows from the existence of a GNF for ECFG (Albert et al, 1999).", "acronym": "GNF", "label": "Greibach Normal Form", "ID": "3585"}, {"sentence": "Watanabe et al (2006) introduce an Early- style top-down parser based on binary-branching GNF.", "acronym": "GNF", "label": "Greibach Normal Form", "ID": "3586"}, {"sentence": "It is known that a context free grammar can be con- verted to GNF, where each pro- duction will have the form:  *aVA ? ,", "acronym": "GNF", "label": "Greibach Normal Form", "ID": "3587"}, {"sentence": "Since GcFc is in GNF,  it is easy to make one-to-one correspondence between  a word in a sentence and a rule application in a  phrase-structure t e. The details of the proof are  given in Maruyama (1990).", "acronym": "GNF", "label": "Greibach Normal Form", "ID": "3588"}, {"sentence": "This can  be proved by constructing a constraint dependency  grammar GCDG from an arbitrary context-free gram-  mar GCFG in GNF, and by show-  ing that the two grammars generate exactly the same  language.", "acronym": "GNF", "label": "Greibach Normal Form", "ID": "3589"}, {"sentence": "as a result, their framework requires gram- mar transformation into the binary-branching GNF (which is not always possible) so that the resulting grammar always contain at least one Chinese word in each rule in order for a prediction step to always make progress.", "acronym": "GNF", "label": "Greibach Normal Form", "ID": "3590"}, {"sentence": "Watanabe et al (2006) further reduces the grammar?s size by enforcing all rules to comply with GNF.", "acronym": "GNF", "label": "Greibach Normal Form", "ID": "3591"}, {"sentence": "We have also presented a descrip-  tion method of such predictions as grammar ules which is based  on GNF, mad recognition mechanisms that are  specified by these rules, realized by using three stacks: the predic-  tion stack, the and stack, and the NP stack.", "acronym": "GNF", "label": "Greibach normal form", "ID": "3592"}, {"sentence": "We make use of  three kinds of stacks whose behavior is specified by grammar rules  in an extended version of GNF.", "acronym": "GNF", "label": "Greibach normal form", "ID": "3593"}, {"sentence": "GNF pro-  vides a well-known example of such a lexical-  ized context-free formalism.", "acronym": "GNF", "label": "Greibach normal form", "ID": "3594"}, {"sentence": "A  predictive a n a l y z e r  i s  a top-down parser  for c o n t e s t - f r e e   g rammars  written in GNF; t h i s  formulation of  t h e  grammar w a s  adopted from e a r l i e r  work by Ida Rhodes for  h e r  R u s s i a n - E n g l i s h  t r a n s l a t i o n  p r o j e c t .", "acronym": "GNF", "label": "Greibach normal form", "ID": "3595"}, {"sentence": "For example, a spec ia l  v e r s i o n   o f  t h i s  algorithm (for GNF grammars) was u s e d  i n   t h e  original Harvard p r e d i c t i v e  Analyzer [ICuno 19621.", "acronym": "GNF", "label": "Greibach normal form", "ID": "3596"}, {"sentence": "The analyzer makes use of the sim-  ple stack mechanism whose behavior is specified by rules described  in GNF.", "acronym": "GNF", "label": "Greibach normal form", "ID": "3597"}, {"sentence": "In Proceedings of 9th AMTA.", "acronym": "AMTA", "label": "Conference of the Association for Machine Translation in the Americas", "ID": "3598"}, {"sentence": "In AMTA.", "acronym": "AMTA", "label": "Conference of the Association for Machine Translation in the Americas", "ID": "3599"}, {"sentence": "of the 3rd AMTA, pp.", "acronym": "AMTA", "label": "Conference of the Association for Machine Translation in the Americas", "ID": "3600"}, {"sentence": "In Machine Translation: From Research to Real Users (Proceedings, 5th AMTA, Tiburon, California), Springer-Verlag, Heidelberg, Germany, 135-244  Simard, M., Foster, G., and Isabelle, P. 1992.", "acronym": "AMTA", "label": "Conference of the Association for Machine Translation in the Americas", "ID": "3601"}, {"sentence": "of the AMTA.", "acronym": "AMTA", "label": "Association for Machine Translation in the Americas", "ID": "3602"}, {"sentence": "In Proceedings of the AMTA, pages 223?231.", "acronym": "AMTA", "label": "Association for Machine Translation in the Americas", "ID": "3603"}, {"sentence": "In Proceedings of 9th Conference of the AMTA.", "acronym": "AMTA", "label": "Association for Machine Translation in the Americas", "ID": "3604"}, {"sentence": "In Proceedings of  AMTA.", "acronym": "AMTA", "label": "Association for Machine Translation in the Americas", "ID": "3605"}, {"sentence": "In Proceedings of the 7th Conference of the AMTA, pages 223?231, Cambridge, Massachusetts, USA, Au- gust.", "acronym": "AMTA", "label": "Association for Machine Translation in the Americas", "ID": "3606"}, {"sentence": "In The Ninth Conference of the AMTA, Denver,Colorado.", "acronym": "AMTA", "label": "Association for Machine Translation in the Americas", "ID": "3607"}, {"sentence": "For the shared task, we have CTs for six lan- guage pairs including English, German, French and Spanish.", "acronym": "CT", "label": "computed translation", "ID": "3608"}, {"sentence": "So far, we always CTs to single source words.", "acronym": "CT", "label": "computed translation", "ID": "3609"}, {"sentence": "For this purpose, we ran- domly selected 100 of the German multiword  expressions with an occurrence frequency above  nine, and verified their CTs  (i.e. the top ranked item for each) manually.", "acronym": "CT", "label": "computed translation", "ID": "3610"}, {"sentence": "2014) used continuous vector to represent the source language or target language of each phrase, and then CT probability using vector distance.", "acronym": "CT", "label": "computed translation", "ID": "3611"}, {"sentence": "For accessing them during decoding, we simply store them in the decoder?s data struc- ture, rather than storing pre-CT model features.", "acronym": "CT", "label": "computed translation", "ID": "3612"}, {"sentence": "System ID Correlation  E09 0.385  E11 0.299  E12 0.278  E14 0.307  E15 0.306  E17 0.385  E22 0.355  Average 0.331    Table 3: Correlation between METEOR Scores and  Human Assessments for the Chinese Dataset  3.5 Comparison with Other Metrics  We CT by translation correla- tions between human assessments and other met- rics besides the METEOR score, namely precision,  recall and Fmean.", "acronym": "CT", "label": "computed translation", "ID": "3613"}, {"sentence": "Corpus CT Setting 1 Setting 2 Setting 3 Setting 4 CITYU test 67,689 N1 1,428,763 609,921 463,756 406,312 N2 1,308,964 491,328 363,120 312,151 PKU test 172,733 N1 4,431,621 1,640,688 1,248,317 1,093,046 N2 3,953,008 1,214,480 896,741 769,674 MSR test 184,355 N1 3,910,003 1,665,511 1,313,351 1,269,858 N2 3,462,762 1,227,640 946,220 907,226 AS test 197,681 N1 1,840,266 1,353,924 1,305,937 1,210,", "acronym": "CT", "label": "Character Type", "ID": "3614"}, {"sentence": "Table 3:CT  2.2 Post-Processing  Two methods are used in post-processing to opti- mize the results obtained from basic segmenter.", "acronym": "CT", "label": "Character Type", "ID": "3615"}, {"sentence": "System Overview  NE recognized text (local code)  Language X Plain text (local code)  Lexical Analysis Rule  Character Code Converter (local code to Unicode) Character Code Converter (Unicode to local code) Lexical Analyzer  Word Candidates JP    CN   Statistical   Language Model (Dictionaries)  KR   EN NE   Recognizer  Morph  AnalyzerN-best  Word  Sequence  Search  Analytical Engine 2.2.1 CT and Word Length  Table 1 shows the varieties of character  types in each language.", "acronym": "CT", "label": "Character Type", "ID": "3616"}, {"sentence": "Table 3:CT  2.2 Post-Processing  Two methods are used in", "acronym": "CT", "label": "Character Type", "ID": "3617"}, {"sentence": "e 1:6-tag Set    Table 2: Feature Templates in Close Test    CT Example  Chinese Character ? ?", "acronym": "CT", "label": "Character Type", "ID": "3618"}, {"sentence": "CTs                2.2.2 Orthography and Spacing   There is an obvious difference in  orthography between each language, that is,  European languages put a space between  words while Japanese and Chinese do not.", "acronym": "CT", "label": "Character Type", "ID": "3619"}, {"sentence": "It is apparent that the  efficiency of word candidate generation  improves dramatically compared to the case  of generating all character strings as  CT  kanji  hiragana  katakana  alphabet  number  symbol  kanji ?", "acronym": "CT", "label": "Character Type", "ID": "3620"}, {"sentence": "Figure 1: Flow Chat    Status Tag  begin B  2nd B2  3rd B3  middle M  end E  single S  Table 1:6-tag Set    Table 2: Feature Templates in Close Test    CT Example  Chinese Character ? ?", "acronym": "CT", "label": "Character Type", "ID": "3621"}, {"sentence": "The four compo-  nents (Kirkpatrick et al, 1983) of a simulated anneal-  ing algorithm are (1) a specification of conf igurat ion,   (2) a random move generator  for rearrangements  of the elements in a configuration, (3) a CT(:-  l i on  for evaluating a configuration, (4) an annea l ing   s( 'hedule that specifies time and duration to decrease  the control parameter (or temperature).", "acronym": "CT", "label": "cost tim", "ID": "3622"}, {"sentence": "Under a time-sharing environment, which is the only prac-  tical environment for on-line systems of this kind, every inter-  ruption and interaction will CTe, and the total effect will make  the system so slow and cumbersome to make it impractical.", "acronym": "CT", "label": "cost tim", "ID": "3623"}, {"sentence": "In Fig- ure 1, solid lines indicate parent-child relationships in the CT, and dotted lines represent the linkage.", "acronym": "CT", "label": "constituent tree", "ID": "3624"}, {"sentence": "A dependency tree is then extracted from the resulting lexicalized CFG-style tree, as is commonly done for converting CTs into dependency trees after the application of a head- percolation table (Collins, 1999).", "acronym": "CT", "label": "constituent tree", "ID": "3625"}, {"sentence": "The output of the parser is twofold: it produces a CT as well as a linkage that shows the dependencies between words.", "acronym": "CT", "label": "constituent tree", "ID": "3626"}, {"sentence": "Along with negation focus annotation,  this corpus also contains other annotations, such  as POS tag, named entity, chunk, CT,  dependency tree, and semantic role.", "acronym": "CT", "label": "constituent tree", "ID": "3627"}, {"sentence": "In contrast, Lexical-Functional Grammar (Kaplan &  Bresnan 1982; Kaplan 1989), which assigns  representations consisting of a surface CT  enriched with a corresponding functional structure, is  known to be beyond context-free.", "acronym": "CT", "label": "constituent tree", "ID": "3628"}, {"sentence": "Essentially, a PTQL query is a hierarchical pattern that is matched against a set 87 of CTs together with additional require- ments on linkages between matches.", "acronym": "CT", "label": "constituent tree", "ID": "3629"}, {"sentence": "Conclusion and Future Work         We were able to compare and CT our results  directly with previous work of Bagga and Baldwin by  using the same corpus and evaluation technique.", "acronym": "CT", "label": "contrast", "ID": "3630"}, {"sentence": "In CT, OT  assigns a ranking to all of the candidale realisations of a  word, calling the scale a tlleasta'e o1' harmony.", "acronym": "CT", "label": "contrast", "ID": "3631"}, {"sentence": "The sentences  extracted form a summary that represents the entity (in  CT to our 55-word snippets).", "acronym": "CT", "label": "contrast", "ID": "3632"}, {"sentence": "2 Anaphora,  by CT, is a nonsymmetrical and nontransi-  tive relation: if NP1 is anaphoric to NP2 then,  usually, NP2 is not anaphoric to NP1, for ex-  ample.", "acronym": "CT", "label": "contrast", "ID": "3633"}, {"sentence": "In CT, the project described here has explored a non-cancellation analysis for Wambaya: even after a head combines with one of its argu- ments, that argument remains on the appropriate va- lence list of the mother, so that it is visible for further combination with modifiers.", "acronym": "CT", "label": "contrast", "ID": "3634"}, {"sentence": "In CT, German verbs with pre- fixes usually differ markedly in their preferences from the base verb.", "acronym": "CT", "label": "contrast", "ID": "3635"}, {"sentence": "Here we are concerned with the probability that  a proposed antecedent is correct given that it  has been repeated a CT number of times.", "acronym": "CT", "label": "certain", "ID": "3636"}, {"sentence": "This strategy is  CTly simple-minded but, as noted earlier, it  achieves an accuracy of 43%.", "acronym": "CT", "label": "certain", "ID": "3637"}, {"sentence": "A character was retained if any of its hypernyms was found to fall into CT types of WordNet concepts: person, animal, plant, artifact, spiritual being, physical entity.", "acronym": "CT", "label": "certain", "ID": "3638"}, {"sentence": "We  also transform our trees under CT condi-  tions to meet Hobbs' assumptions as much as  possible.", "acronym": "CT", "label": "certain", "ID": "3639"}, {"sentence": "The decomposition  makes use of Bayes' theorem and is based on  CT independence assumptions discussed be-  low.", "acronym": "CT", "label": "certain", "ID": "3640"}, {"sentence": "However, although there is indeed a CT corre- lation between morphological class and semantic class, we claim that morphology is not sufficient for a reliable classification because it is by no means a one-to-one relationship.", "acronym": "CT", "label": "certain", "ID": "3641"}, {"sentence": "For each DBLP record we searched on the web for matching CT using the first author?s last name and words in the title.", "acronym": "CT", "label": "citation texts", "ID": "3642"}, {"sentence": "We reduced the num- ber of schema labels by: (1) mapping the labels address, booktitle, journal and school to venue, (2) mapping month and year to date, and (3) dropping the fields url, ee, cdrom, note, isbn and chapter, since they never appeared in CT.", "acronym": "CT", "label": "citation texts", "ID": "3643"}, {"sentence": "As is common practice (Haghighi and Klein, 2006; Mann and McCallum, 2008), we simulate user-specified expectation criteria through statis- tics on manually labeled CT.", "acronym": "CT", "label": "citation texts", "ID": "3644"}, {"sentence": "More recently, Qazvinian and Radev (2008) argue that CT are useful in creating a summary of the important contributions of a research paper.", "acronym": "CT", "label": "citation texts", "ID": "3645"}, {"sentence": "and year to date, and (3) dropping the fields url, ee, cdrom, note, isbn and chapter, since they never appeared in CT.", "acronym": "CT", "label": "citation texts", "ID": "3646"}, {"sentence": "We eval- uate our method on a citation extraction task in which alignments between DBLP database records and CT are used to train an extractor.", "acronym": "CT", "label": "citation texts", "ID": "3647"}, {"sentence": "GS-CRF: CRF trained on human annotated CT.", "acronym": "CT", "label": "citation texts", "ID": "3648"}, {"sentence": "To support the use of such an in- trinsic definition, we examine the theoret- ical problems that the CT def- inition faces, show how the intrinsic defi- nition solves those problems, and explain how the intrinsic definition adheres to psy- chological reality, at least for our annota- tion purposes, better than the counterfac- tual definition.", "acronym": "CT", "label": "counterfactual", "ID": "3649"}, {"sentence": "Finally, No-emb requires the detection of a CT context in the Premise.", "acronym": "CT", "label": "counterfactual", "ID": "3650"}, {"sentence": "Suppose your adversary  accepts the invitation to discuss, and takes the  antecedent of the CT s a temporary  additi", "acronym": "CT", "label": "counterfactual", "ID": "3651"}, {"sentence": "We present the challenges faced using the definition of causation in terms of CT dependence and propose new guidelines for cause-effect annotation using an alternative definition which treats causation as an intrinsic relation between events.", "acronym": "CT", "label": "counterfactual", "ID": "3652"}, {"sentence": "Suppose your adversary  accepts the invitation to discuss, and takes the  antecedent of the CT s a temporary  additional concession.", "acronym": "CT", "label": "counterfactual", "ID": "3653"}, {"sentence": "We take seriously Tichy's suggestion to  look for the use of counteffactuals and formulate  a semantics for counteffactuals not in terms of  \"troth\" and \"falsehood\", but rather in terms of what  is done and not done in a dialogue in which a  CT ppears.", "acronym": "CT", "label": "counterfactual", "ID": "3654"}, {"sentence": "To support the use of such an in- trinsic definition, we examine the theoret- ical problems that the CT def- inition faces, show how the intrinsic defi- nition solves those problems, and explain how the intrinsi", "acronym": "CT", "label": "counterfactual", "ID": "3655"}, {"sentence": "Apart from the results  on CTs mentioned above, the prover  works well on a range of cases of default reasoning,  including \"double diamonds\", hierarchies of predi-  cates and relevant implication.", "acronym": "CT", "label": "counterfactual", "ID": "3656"}, {"sentence": "not done in a dialogue in which a  CT ppears.", "acronym": "CT", "label": "counterfactual", "ID": "3657"}, {"sentence": "The New CT.", "acronym": "CT", "label": "Collins Thesaurus", "ID": "3658"}, {"sentence": "\\[1\\] \"IBM SAA bnagePlus Object Distribution  Manager MVS/ESA High-Speed Capture Sub-  system Guide Version 2 Release 1.1,\" IBM Cor-  1, (1991)  \\[2\\] \"IBM Application System/400 New User's  Guide Version 2,\" IBM Corp. (1992)  \\[3\\] \"The New CT,\" Collins Publish-  ers, Glasgow (1.984)  1163   Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1370?1380, Baltimore, Maryland, USA, June 23-25 2014.", "acronym": "CT", "label": "Collins Thesaurus", "ID": "3659"}, {"sentence": "3.1 Acquisition of Conjunctive Rela-  tionships from Corpora  The New CT, which is used in SENA  as a source of synonym or is-a relationships, gives  the following synonyms of \"store\":  store:  accumulate, deposit, garner, hoard, keep, etc.", "acronym": "CT", "label": "Collins Thesaurus", "ID": "3660"}, {"sentence": "Our examples describe analyses of data from Webster's Seventh  Collegiate Dictionary, the Longman Dictionary of Contemporary English, the Collins bilingual  dictionaries, the CT, and the Zingarelli Italian dictionary.", "acronym": "CT", "label": "Collins Thesaurus", "ID": "3661"}, {"sentence": "Collins (1984) The New CT,  Collins Publishers, Glasgow.", "acronym": "CT", "label": "Collins Thesaurus", "ID": "3662"}, {"sentence": "1984 The New CT.", "acronym": "CT", "label": "Collins Thesaurus", "ID": "3663"}, {"sentence": "The AB algorithm (Freund and Schapire, 1997) was the first practical boost- ing algorithm, and remains one of the most widely used and studied, with applications in numerous fields.", "acronym": "AB", "label": "AdaBoost", "ID": "3664"}, {"sentence": "We use a modified version of AB (Freund and Schapire, 1995) that opti- mizes for recall.", "acronym": "AB", "label": "AdaBoost", "ID": "3665"}, {"sentence": "We compare our methods against baselines including a majority baseline, a baseline logistic regression classifier with L2 regularized features, and two common en- semble methods, AB (Freund and Schapire, 1996) and bagging (Breiman, 1996) with logistic regression base classifiers5.", "acronym": "AB", "label": "AdaBoost", "ID": "3666"}, {"sentence": "638 0% 10% 20% 30% 40% 50% 0 5 10 15 20 25 30 35 40 45 50 T e s t   s e t   r e c a l l - a t - 1 2 2 0 Number of languages (k) Combined with AB Individual Bilingual Statistics looking forward to mirando adelante a looking forward to deseando looking mirando looking buscando  a 3 1 5 3 Aligned Phrase Pair N(e, f) ?", "acronym": "AB", "label": "AdaBoost", "ID": "3667"}, {"sentence": "We can eval- uate the quality of the ranker by outputting the top N ranked candidates and measuring recall relative Algorithm 1 Recall-Oriented Ranking AB 1: for i = 1 : |X| do 2: w[i]?", "acronym": "AB", "label": "AdaBoost", "ID": "3668"}, {"sentence": "The three final lines in Table 3 evaluate our 642 0% 10% 20% 30% 40% 50% 0 5 10 15 20 25 30 35 40 45 50 T e s t   s e t   r e c a l l - a t - 1 2 2 0 Number of languages (k) Combined with AB Individual Bilingual Statistics Figure 2: The solid line shows recall-at-1220 when com- bining the k best-performing bilingual statistics and three monolingual statistics.", "acronym": "AB", "label": "AdaBoost", "ID": "3669"}, {"sentence": "2 Preprocessing The original data format provided by the shared task organizers consists of (a) a collection biomedical AB, and (b) standoff anno- tation that describes the proteins, events and sites mentioned in these AB.", "acronym": "AB", "label": "abstracts", "ID": "3670"}, {"sentence": "Note that these parses are based on a different tokenisation of the text in the AB.", "acronym": "AB", "label": "abstracts", "ID": "3671"}, {"sentence": "The data source used by the system is the set of MEDLINE AB, a large bibliographic database that is accessed on-line via PubMed.", "acronym": "AB", "label": "abstracts", "ID": "3672"}, {"sentence": "The organiz- ers also provided a set of dependency and con- stituent parses for the AB.", "acronym": "AB", "label": "abstracts", "ID": "3673"}, {"sentence": "In the course of the shared task the organizers provided a training/development set of AB for biomedical papers, annotated with the mentioned events.", "acronym": "AB", "label": "abstracts", "ID": "3674"}, {"sentence": "A central task of the system is the automatic identification of PICO elements in the MEDLINE AB and their matching with the input query frame.", "acronym": "AB", "label": "abstracts", "ID": "3675"}, {"sentence": "Demner-Fushman and Lin (2005) operationalize knowledge extraction for populat- ing a database with PICO (Population, Intervention, Comparison, and Outcome) ele- ments from medical AB obtained from MEDLINE.", "acronym": "AB", "label": "abstracts", "ID": "3676"}, {"sentence": "1504   Proceedings of the Eighth Meeting of the ACL SIGPHON at HLT-NAACL 2006, pages 60?68, New York City, USA, June 2006.", "acronym": "SIGPHON", "label": "Special Interest Group on Computational Phonology", "ID": "3677"}, {"sentence": "611  Proceedings of the Eighth Meeting of the ACL SIGPHON at HLT-NAACL 2006, page 31, New York City, USA, June 2006.", "acronym": "SIGPHON", "label": "Special Interest Group on Computational Phonology", "ID": "3678"}, {"sentence": "260   Proceedings of the Eighth Meeting of the ACL SIGPHON at HLT-NAACL 2006, pages 69?78, New York City, USA, June 2006.", "acronym": "SIGPHON", "label": "Special Interest Group on Computational Phonology", "ID": "3679"}, {"sentence": "Proceedings of the Eighth Meeting of the ACL SIGPHON at HLT-NAACL 2006, pages 32?40, New York City, USA, June 2006.", "acronym": "SIGPHON", "label": "Special Interest Group on Computational Phonology", "ID": "3680"}, {"sentence": "Proceedings of the Eighth Meeting of the ACL SIGPHON at HLT-NAACL 2006, pages 79?88, New York City, USA, June 2006.", "acronym": "SIGPHON", "label": "Special Interest Group on Computational Phonology", "ID": "3681"}, {"sentence": "1996  47    Proceedings of the Eighth Meeting of the ACL SIGPHON at HLT-NAACL 2006, pages 41?49, New York City, USA, June 2006.", "acronym": "SIGPHON", "label": "Special Interest Group on Computational Phonology", "ID": "3682"}, {"sentence": "In Proceedings of the Sixth Meeting of  the Association for Computational Linguistics  SIGPHON  in Philadelphia, July 2002, ed.", "acronym": "SIGPHON", "label": "Special Interest Group in Computational Phonology", "ID": "3683"}, {"sentence": "In Workshop of the ACL SIGPHON, pages 21?30, 2002.", "acronym": "SIGPHON", "label": "Special Interest Group in Computational Phonology", "ID": "3684"}, {"sentence": "In Proceedings of the Seventh Meeting of the ACL SIGPHON, pages 78?85, Barcelona, Spain, July.", "acronym": "SIGPHON", "label": "Special Interest Group in Computational Phonology", "ID": "3685"}, {"sentence": "Other work in this area is to be found in the Proceedings  of the First Meeting of the ACL SIGPHON, published  by the ACL in 1994, and in two edited collections (Bird 1991; Ellison and Scobbie  1993).", "acronym": "SIGPHON", "label": "Special Interest Group in Computational Phonology", "ID": "3686"}, {"sentence": "In Proceedings of the Seventh Meeting of the ACL SIGPHON, pages 43?51, Barcelona, Spain, July.", "acronym": "SIGPHON", "label": "Special Interest Group in Computational Phonology", "ID": "3687"}, {"sentence": "of the ACL SIGPHON, pages 11?18, Madrid, Spain.", "acronym": "SIGPHON", "label": "Special Interest Group in Computational Phonology", "ID": "3688"}, {"sentence": "David Vilar, Jan-T. Peter and Hermann Ney Lehrstuhl fu?r Informatik 6 RWTH Aachen University D-52056 Aachen, Germany {vilar,peter,ney}@cs.rwth-aachen.de Abstract Current statistical machine translation sys- tems handle the TP as the transformation of a string of symbols into another string of symbols.", "acronym": "TP", "label": "translation process", "ID": "3689"}, {"sentence": "1In this query TP, we compared an MT software with a bilingual lexicon.", "acronym": "TP", "label": "translation process", "ID": "3690"}, {"sentence": "Posterior Regularization Word alignment models in general and the HMM in particular are very gross over- simplifications of the TP and the optimal likelihood parameters learned often do not correspond to sensible alignments.", "acronym": "TP", "label": "translation process", "ID": "3691"}, {"sentence": "One solution to this problem is to add more complexity to the model to better reflect the TP.", "acronym": "TP", "label": "translation process", "ID": "3692"}, {"sentence": "Since a lot of the idioms in Japanese are originally from China, the conversion of kanji/hanzi will make the TP faster and more ac- curate.", "acronym": "TP", "label": "translation process", "ID": "3693"}, {"sentence": "For ex- ample, Vintar (2000) presented two methods for  extraction of terminological collocations in order  to assist the TP in Slovene.", "acronym": "TP", "label": "translation process", "ID": "3694"}, {"sentence": "For com- puting the counts of TP and negative words (Feature 15 and 16) we used the General Inquirer database (Stone et al.,", "acronym": "TP", "label": "positive", "ID": "3695"}, {"sentence": "has been shown to be TPly correlated with children?s linguis- tic and intellectual development (Natsiopoulou et al.,", "acronym": "TP", "label": "positive", "ID": "3696"}, {"sentence": "TP surprise? (", "acronym": "TP", "label": "positive", "ID": "3697"}, {"sentence": "In order to align the existing annotations to our three-class scheme the following mapping5 was adopted: (i) AN, DI, FE, SA were mapped to negative affect, (ii) NE was mapped to neutral affect, and (iii) HA was mapped to TP affect.", "acronym": "TP", "label": "positive", "ID": "3698"}, {"sentence": "pronouns 12 count of quote tokens 13 count of 1st person plural pronouns 14 count of 2nd person singular pronouns 15 count of quote TP words 16 count of quote negative words 17 count of nouns 18 count of verbs 19 count of adjectives 20 count of adverbs 21 up to 3-grams extracted from quote Table 1: Common feature set.", "acronym": "TP", "label": "positive", "ID": "3699"}, {"sentence": "TP?.", "acronym": "TP", "label": "positive", "ID": "3700"}, {"sentence": "These authors can be 129 Collaboration Type TP False Positives Accuracy New Blood, Catalyst or High Synergy Papers 43 23 65.15% Apprentice or Low Synergy Papers 32 22 59.25% Overall 75 45 62.50% Table 3: Evaluation based on annotation by one expert considered the hedgehogs, as they have highly sta- ble signatures that their new papers resemble.", "acronym": "TP", "label": "True Positives", "ID": "3701"}, {"sentence": "14  TP 191 246  False Positives 82 133  False Negatives 1587 1874  Correct Labels 189 237  Precision 0.700 0.649  Recall 0.107 0.116  F-Score 0.186 0.197  Label Accuracy 0.106 0.112    As can be seen, for entries with patterns (albeit  a low recall), a substantial number of frame ele- ments could be recognized with high precision  from a very small number of constituent match- ing function", "acronym": "TP", "label": "True Positives", "ID": "3702"}, {"sentence": "The number of TP, True Negatives, and  False Positives are listed.", "acronym": "TP", "label": "True Positives", "ID": "3703"}, {"sentence": "The Expected TP and Expected True Negatives for Cohen Kappa, as well as Chi-squared significance, are estimated as the product of Bias and Prevalence, and the product of Inverse Bias and Inverse Prevalence, resp.,", "acronym": "TP", "label": "True Positives", "ID": "3704"}, {"sentence": "Figure 4: Algorithm for calculating the F-measure confusion matrix of TP (T.P.), False Positives (F.P.), True Negatives (T.N.), and False Negatives (F.N.).", "acronym": "TP", "label": "True Positives", "ID": "3705"}, {"sentence": "There were 62  decisions involving Subphrase Relations (with 44  TP and 18 False Negatives), and 10  decisions involving WordNet (with 12 True  Positives).", "acronym": "TP", "label": "True Positives", "ID": "3706"}, {"sentence": "We say that s?k is a TP if s?k ?", "acronym": "TP", "label": "true positive", "ID": "3707"}, {"sentence": "For NEs and zone titles, IAA was calculated using P, R and F1, defining two mentions as equal if they had the same left and right 8P, R and F1 are calculated in standard fashion from the number of TPs, false positives and false negatives.", "acronym": "TP", "label": "true positive", "ID": "3708"}, {"sentence": "Secondly, a sentence is correct if all decoded segments are TPs.", "acronym": "TP", "label": "true positive", "ID": "3709"}, {"sentence": "s. The precision and recall, then, are measured as the total num- ber of TPs divided by the total num- ber of decoded and true segments respectively.", "acronym": "TP", "label": "true positive", "ID": "3710"}, {"sentence": "For each query, Table 3 shows total, the number of documents returned; good, the number of TPs; and top 5, the number of TPs in the top five returned documents.", "acronym": "TP", "label": "true positive", "ID": "3711"}, {"sentence": "The number of TPs (correctly detected alternate linkings) is 27,606, the number of false positives (incorrectly marked al- ternations) is 32,031, the number of true negatives (cases where the model correctly did not detect an alternate linking) is 132,556, and the number of false negatives (alternate linkings that the model should have detected but did not) is 32,516.4.", "acronym": "TP", "label": "true positive", "ID": "3712"}, {"sentence": "14  TPs 191 246  False Positives 82 133  False Negatives 1587 1874  Correct Labels 189 237  Precision 0.700 0.649  Recall 0.107 0.116  F-Score 0.186 0.197  Label Accuracy 0.106 0.112    As can be seen, for entries with patterns (albeit  a low recall), a substantial number of frame ele- ments could be recognized with high precision  from a very small number of constituent match- ing function", "acronym": "TP", "label": "True Positive", "ID": "3713"}, {"sentence": "Table 3 rep- resents the values for such commonly used met- rics as: Accuracy, TP Rate, False Posi- tive Rate, Precision, Recall and F-Measure respec- tively for each one of the tested methods.", "acronym": "TP", "label": "True Positive", "ID": "3714"}, {"sentence": "The (1,0) point represents perfect performance with 100% TP Rate and 0% False Negative Rate.", "acronym": "TP", "label": "True Positive", "ID": "3715"}, {"sentence": "We see that striving loses by a slight amount to stacking early in the curve but still  0.5  0.6  0.7  0.8  0.9  1  0  0.2  0.4  0.6  0.8  1 TP Rate False Positive R ate M BN B (sent ,ng ram) SV M (sent ,ng ram) Stacking STR IV E Figure 4: ROC curves (rotated).", "acronym": "TP", "label": "True Positive", "ID": "3716"}, {"sentence": "The number of TPs, True Negatives, and  False Positives are listed.", "acronym": "TP", "label": "True Positive", "ID": "3717"}, {"sentence": "The Expected TPs and Expected True Negatives for Cohen Kappa, as well as Chi-squared significance, are estimated as the product of Bias and Prevalence, and the product of Inverse Bias and Inverse Prevalence, resp.,", "acronym": "TP", "label": "True Positive", "ID": "3718"}, {"sentence": "Natalie M. Schrimpf Department of Linguistics Yale University natalie.schrimpf@yale.edu  Gaja Jarosz Department of Linguistics Yale University gaja.jarosz@yale.edu  19 While the experimental work emphasizes syl-lable-level TP, recent com-putational modeling work and corpus analyses have primarily focused on the utility of pho-neme-level statistics.", "acronym": "TP", "label": "transitional probability", "ID": "3719"}, {"sentence": "Mc- 197 Donald and Shillcock (2003) show that forward and backward transitional probabilities are pre- dictive of first fixation and first pass durations: the higher the TP, the shorter the fixation time.", "acronym": "TP", "label": "transitional probability", "ID": "3720"}, {"sentence": "The syllable-based TP model achieves a word token f-score of nearly 80%, the high-est reported performance for a phonotactic segmentation model with no lexicon.", "acronym": "TP", "label": "transitional probability", "ID": "3721"}, {"sentence": "One statistical cue that has received a great deal of support in experimental work on infant speech segmentation is TP calculated over syllables.", "acronym": "TP", "label": "transitional probability", "ID": "3722"}, {"sentence": "34 Computational Linguistics, Volume 14, Number 1, Winter 1988  Steven J. DeRose Grammatical Category Disambiguation by Statistical Optimization  2 THE LINEAR-TIME ALGORITHM (VOLSUNGA)  The algorithm described here depends on a similar  empirically-derived TP matrix to  that of CLAWS, and has a similar definition of \"optimal  path\".", "acronym": "TP", "label": "transitional probability", "ID": "3723"}, {"sentence": "sensitivity to TP in an artificial language learning set-ting, the utility of these statistical cues in a natu-ral language learning context is disputed.", "acronym": "TP", "label": "transitional probability", "ID": "3724"}, {"sentence": "The reason for the  significant drop in the precision in the second and  third intervals is mainly the same: apart from few  TN11, the majority of false term candi- dates contained common ?", "acronym": "TN", "label": "true negatives", "ID": "3725"}, {"sentence": "The proposed method uses tokens as the unit of evaluation (instead of phrase-level edits), which pro- vides a stable unit of comparison and facilitates the computation of TN.", "acronym": "TN", "label": "true negatives", "ID": "3726"}, {"sentence": "The overall accuracy of the  classifier (% of true positives + TN) is 82%.", "acronym": "TN", "label": "true negatives", "ID": "3727"}, {"sentence": "Alternatively, we can compute precision (P ), re- call (R) and F -score by comparing system edits to gold-standard edits and thus circumvent the problem of counting TN.", "acronym": "TN", "label": "true negatives", "ID": "3728"}, {"sentence": "The number of true positives (correctly detected alternate linkings) is 27,606, the number of false positives (incorrectly marked al- ternations) is 32,031, the number of TN (cases where the model correctly did not detect an alternate linking) is 132,556, and the number of false negatives (alternate linkings that the model should have detected but did not) is 32,516.4.", "acronym": "TN", "label": "true negatives", "ID": "3729"}, {"sentence": "In addition, Leacock et al (2014) discuss key is- sues concerning system evaluation, such as the es- timation of TN and good practices for re- porting results, which are currently not addressed by the M 2 scorer.", "acronym": "TN", "label": "true negatives", "ID": "3730"}, {"sentence": "van Eynde, Frank 1988: The Analysis of TN and  Aspect in Eurotra.", "acronym": "TN", "label": "Tense", "ID": "3731"}, {"sentence": "Sing POS=AUX Mood=Ind Number=Sing Person=3 TN=Pres VerbForm=Fin POS=VERB TN=Past VerbForm=Part Voice=Pass POS=ADP POS=NOUN Number=Sing POS=NOUN Number=Sing POS=PUNCT Our independence is guaranteed by law today .", "acronym": "TN", "label": "Tense", "ID": "3732"}, {"sentence": "POS=PRON Number=Plur Person=1 Poss=Yes PronType=Prs POS=NOUN Number=Sing POS=AUX Aspect=Imp Mood=Ind Number=Sing Person=3 TN=Pres VerbForm=Fin POS=ADV Degree=Pos POS=ADP AdpType=Prep POS=NOUN Num", "acronym": "TN", "label": "Tense", "ID": "3733"}, {"sentence": "TN  If none of the aforesaid criteria applies ome German  tenses determine the aspect choice:  Past perfect is translated to perfective aspect form,  in the case of the present tense (pracsens futuri ex-  cluded) the imperfective aspect is preferred.", "acronym": "TN", "label": "Tense", "ID": "3734"}, {"sentence": "To 1955 POS=PRON Number=Plur Person=1 Poss=Yes PronType=Prs POS=NOUN Number=Sing POS=AUX Mood=Ind Number=Sing Person=3 TN=Pres VerbForm=Fin POS=VERB TN=Past VerbForm=Part Voice=Pass POS=ADP POS=NOUN Number=Sing POS=NOUN Number=Sing POS=PUNCT Our independence is guaranteed by law today .", "acronym": "TN", "label": "Tense", "ID": "3735"}, {"sentence": "POS=PRON Number=Plur Person=1 Poss=Yes PronType=Prs POS=NOUN Number=Sing POS=AUX Aspect=Imp Mood=Ind Number=Sing Person=3 TN=Pres VerbForm=Fin POS=ADV Degree=Pos", "acronym": "TN", "label": "Tense", "ID": "3736"}, {"sentence": "POS=PRON Number=Plur Person=1 Poss=Yes PronType=Prs POS=NOUN Number=Sing POS=AUX Aspect=Imp Mood=Ind Number=Sing Person=3 TN=Pres VerbForm=Fin POS=ADV Degree=Pos POS=ADP AdpType=Prep POS=NOUN Number=Sing POS=VERB TN=Past VerbForm=Part SubCat=Tran POS=PUNCT Figure 1: A parallel sentence in English and Dutch annotated with universal morphological tags, showing high-confidence automatic word-alignments.", "acronym": "TN", "label": "Tense", "ID": "3737"}, {"sentence": "For each CS and for each ei E CS DO:  e2_.ez/.t N IF -prior > 1 + r THE  (F  ei is correct,  i.e. manually validated, THEN  ++TruePositives;  OTHERWISE  ++ FalsePositives;  OTHERWISE IF ~ < 1 - r THEN  IF e~ is correct pp~,Or~HEN  ++ FalseNegatives;  OTHERWISE  ++TN;  ++Ncases  precision =  TruePositives-~ TrueNe~atives  TruePositives+ TrueNegatives+ FalsePositives?", "acronym": "TN", "label": "True Negatives", "ID": "3738"}, {"sentence": "The number of True Positives, TN, and  False Positives are listed.", "acronym": "TN", "label": "True Negatives", "ID": "3739"}, {"sentence": "The Expected True Positives and Expected TN for Cohen Kappa, as well as Chi-squared significance, are estimated as the product of Bias and Prevalence, and the product of Inverse Bias and Inverse Prevalence, resp.,", "acronym": "TN", "label": "True Negatives", "ID": "3740"}, {"sentence": "since for us TN have same importance as True Positives.", "acronym": "TN", "label": "True Negatives", "ID": "3741"}, {"sentence": "Figure 4: Algorithm for calculating the F-measure confusion matrix of True Positives (T.P.), False Positives (F.P.), TN (T.N.), and False Negatives (F.N.).", "acronym": "TN", "label": "True Negatives", "ID": "3742"}, {"sentence": "For each sub-task, we have tables listing the 51 TN Abbreviation Bobicev BOB Chonger CHO CMU-Haifa HAI Cologne-Nijmegen CN CoRAL Lab @ UAB COR CUNI (Charles University) CUN cywu CYW dartmouth DAR eurac EUR HAUTCS HAU ItaliaNLP ITA Jarvis JAR kyle, crossley, dai, mcnamara KYL LIMSI LIM LTRC IIIT Hyderabad HYD Michigan MIC MITRE ?", "acronym": "TN", "label": "Team Name", "ID": "3743"}, {"sentence": "2 0.3558 0.3607 0.4087 0.3470 36 UNED-run22 p np 0.1043 0.3148 0.0374 0.3243 0.5086 0.4898 0.3097 37 LIPN-run2 0.0843 - - - - - 0.0101 38 Our difference against the average 9% 7% 10% 2% -2% 17% 7% - Table 3: Results obtained at the Task 10 of the Semeval competition for the Spanish language (NOTE: The * symbol denotes a system that used Wikipedia to build its model for the Wikipedia test dataset) TN System type Wikipedia News Weighted correlation Rank UMCC DLSI-run2 supervised 0.7802 0.8254 0.8072 1 Meerkat Mafia-run2 unsupervised 0.7431 0.8454 0.8042 2 UNAL-NLP-run1 weakly supervised 0.7804 0.8154 0.8013 3 BUAP-run2 unsupervised 0.6396 0.7637 0.7137 14 Overall average - 0.6193 0.7504 0.6976 14-15 BUAP-run1 supervised 0.5504 0.6785 0.6269 17 RTM-DCU-run2 supervised 0.3689 0.6253 0.5", "acronym": "TN", "label": "Team Name", "ID": "3744"}, {"sentence": "TN Affiliation CLMB (Rozovskaya et al.,", "acronym": "TN", "label": "Team Name", "ID": "3745"}, {"sentence": "The reason for the  significant drop in the precision in the second and  third intervals is mainly the same: apart from few  TNs11, the majority of false term candi- dates contained common ?", "acronym": "TN", "label": "true negative", "ID": "3746"}, {"sentence": "The overall accuracy of the  classifier (% of true positives + TNs) is 82%.", "acronym": "TN", "label": "true negative", "ID": "3747"}, {"sentence": "Alternatively, we can compute precision (P ), re- call (R) and F -score by comparing system edits to gold-standard edits and thus circumvent the problem of counting TNs.", "acronym": "TN", "label": "true negative", "ID": "3748"}, {"sentence": "The number of true positives (correctly detected alternate linkings) is 27,606, the number of false positives (incorrectly marked al- ternations) is 32,031, the number of TNs (cases where the model correctly did not detect an alternate linking) is 132,556, and the number of false negatives (alternate linkings that the model should have detected but did not) is 32,516.4.", "acronym": "TN", "label": "true negative", "ID": "3749"}, {"sentence": "(g) The lack of a TN count (i.e. the num- ber of non-errors) precludes the computation of accuracy, which is useful for discriminating be- tween systems with F = 0.", "acronym": "TN", "label": "true negative", "ID": "3750"}, {"sentence": "In addition, Leacock et al (2014) discuss key is- sues concerning system evaluation, such as the es- timation of TNs and good practices for re- porting results, which are currently not addressed by the M 2 scorer.", "acronym": "TN", "label": "true negative", "ID": "3751"}, {"sentence": "We use the BP as the prior to construct a Bayesian framework for summary sentence selec- tion.", "acronym": "BP", "label": "beta process", "ID": "3752"}, {"sentence": "Nonparametric factor analysis with BP pri- ors.", "acronym": "BP", "label": "beta process", "ID": "3753"}, {"sentence": "For the sentence selection step, we use the varia- tional inference described in Section 4, where the parameters in the BP (5) are set as ?", "acronym": "BP", "label": "beta process", "ID": "3754"}, {"sentence": "Integrating the linear reconstruction (4) and the BP3 (1), we get the complete process of summary sentence selection as follows.", "acronym": "BP", "label": "beta process", "ID": "3755"}, {"sentence": "Nonparametric factor analysis with BP priors.", "acronym": "BP", "label": "beta process", "ID": "3756"}, {"sentence": "Using a BP as a prior for the binary vector zi, we can automatically infer the number of active component associated with zi.", "acronym": "BP", "label": "beta process", "ID": "3757"}, {"sentence": "Integrating the linear reconstruction (4) and the BP3 (1), we ge", "acronym": "BP", "label": "beta process", "ID": "3758"}, {"sentence": "BPs the former generates are smaller than that transfor- mation patterns treat.", "acronym": "BP", "label": "Base phrase", "ID": "3759"}, {"sentence": "BP chunk We add a boolean feature to detect whether there is any base phrase chunk in the text span between the two mentions.", "acronym": "BP", "label": "Base phrase", "ID": "3760"}, {"sentence": "(5) BP chunking: The base phrase  chunking is proved to play an important role in  semantic relation extraction.", "acronym": "BP", "label": "Base phrase", "ID": "3761"}, {"sentence": "SP-NIST(i)p-n BP chunks ?", "acronym": "BP", "label": "Base phrase", "ID": "3762"}, {"sentence": "Input Type and bracket tagging  model   Obtain feature vectors   Predict the phrase boundary   Brackets matching Grammar rules                            Correct the types of base  phrases  Lexical  information Output BPs acquisition model Figure 1: system overview  4 Predicting the phrase boundaries  with MBL  Memory-Based Learning (MBL) is a classification  based, supervised learning approach: a  memory-based learning algorithm constructs a  classifier for a task by storing a set of examples.", "acronym": "BP", "label": "Base phrase", "ID": "3763"}, {"sentence": "SP-NISTc BP chunks.", "acronym": "BP", "label": "Base phrase", "ID": "3764"}, {"sentence": "Dependency Parsing by BP.", "acronym": "BP", "label": "Belief Propagation", "ID": "3765"}, {"sentence": "Understand- ing BP and its Generalizations.", "acronym": "BP", "label": "Belief Propagation", "ID": "3766"}, {"sentence": "Understanding BP and its Gen- eralizations, Exploring Artificial Intelligence in the New Millennium, chapter 8, pages 236?239.", "acronym": "BP", "label": "Belief Propagation", "ID": "3767"}, {"sentence": "Fast Inference in Phrase Extraction Models with BP.", "acronym": "BP", "label": "Belief Propagation", "ID": "3768"}, {"sentence": "c?2012 Association for Computational Linguistics Fast Inference in Phrase Extraction Models with BP David Burkett and Dan Klein Computer Science Division University of California, Berkeley {dburkett,klein}@cs.berkeley.edu Abstract Modeling overlapping phrases in an align- ment model can improve alignment quality but comes with a high inference cost.", "acronym": "BP", "label": "Belief Propagation", "ID": "3769"}, {"sentence": "We apply Loopy BP to propagate sentiments among entities.", "acronym": "BP", "label": "Belief Propagation", "ID": "3770"}, {"sentence": "In or- der to achieve this, we used machine learning Corpus Read Spontaneous r p-value r p-value Baseline (Unigram) r = \u00000:166 p = 0:0002 r = \u00000:02 p = 0:39 BP r = \u00000:236 p < 0:0001 r = \u00000:36 p < 0:0001 Pointwise Mutual Information r = \u00000:185 p < 0:0001 r = \u00000:177 p < 0:0001 Dice Coe?cient r = \u00000:079 p = 0:066 r = \u00000:094 p < 0:0001 Table 2: Correlation of Di\u000berent Collocation Measures with Accent Decision techniques to automatically build accent pre- diction models using bigram word predictabil- ity scores.", "acronym": "BP", "label": "Bigram Predictability", "ID": "3771"}, {"sentence": "Of these, PMI and Dice Coefficient are symmetric mea- sures while BP and PKL are non- symmetric (unidirectional) measures.", "acronym": "BP", "label": "Bigram Predictability", "ID": "3772"}, {"sentence": "Automatic Tagging of Arabic Text: From  Raw Text to BP Chunks, In Proc.", "acronym": "BP", "label": "Base Phrase", "ID": "3773"}, {"sentence": "2.1 BPs Following Ratnaparkhi (1999), we define a base phrase as any parse node with only preterminal chil- dren.", "acronym": "BP", "label": "Base Phrase", "ID": "3774"}, {"sentence": "Automatic Tagging of Arabic Text: From Raw Text to BP Chunks.", "acronym": "BP", "label": "Base Phrase", "ID": "3775"}, {"sentence": "Second Generation Tools (AMIRA 2.0): Fast and Robust Tokenization, POS tagging, and BP Chunking.", "acronym": "BP", "label": "Base Phrase", "ID": "3776"}, {"sentence": "Statistics  Based Hybrid Approach to Chinese BP  Identification?,", "acronym": "BP", "label": "Base Phrase", "ID": "3777"}, {"sentence": "Moreover, using loopy BP means that the in- ference method is not closely coupled to the task structure, and need not change when applying this method to other types of graphs.", "acronym": "BP", "label": "belief propagation", "ID": "3778"}, {"sentence": "Computing the gradient requires computation of the marginals which can be performed efficiently using BP (Yedidia et al, 2003).", "acronym": "BP", "label": "belief propagation", "ID": "3779"}, {"sentence": "More specifically, we imple- ment the model as a factor graph, a bipartite graph composed of factors and variables in which we can efficiently compute the marginal beliefs of any vari- able set with the sum-product algorithm for cyclic graphs, loopy BP,.", "acronym": "BP", "label": "belief propagation", "ID": "3780"}, {"sentence": "Infer- ence is done via loopy BP, making this framework trivially extensible to most graph structures.", "acronym": "BP", "label": "belief propagation", "ID": "3781"}, {"sentence": "First, we perform sum- product BP on the full factor graph.", "acronym": "BP", "label": "belief propagation", "ID": "3782"}, {"sentence": "Finally, we use sum-product variant of BP inference, but more specialized inference schemes may show additional benefits.", "acronym": "BP", "label": "belief propagation", "ID": "3783"}, {"sentence": "For instance, the top system outperforms the bottom system by 15 BP!", "acronym": "BP", "label": "BLEU points", "ID": "3784"}, {"sentence": "We show that it is possible to use our data selection methods to subse- lect less than 1% (or discard 99%) of a large general training corpus and still increase translation perfor- mance by nearly 2 BP.", "acronym": "BP", "label": "BLEU points", "ID": "3785"}, {"sentence": "Aug- menting same models with same paraphrases filtered for antonyms resulted in further gains of 1.6 and 1 BP for both subset and full models, respec- tively, relative to the respective baselines.", "acronym": "BP", "label": "BLEU points", "ID": "3786"}, {"sentence": "4http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html 5http://nlp.stanford.edu/software/segmenter.shtml 6http://nlp.cs.berkeley.edu/Software.shtml 7http://triplet.cc/software/corbit As it can be observed, our DPC method obtains around 0.7 BP of improvement when compared to the second best system in both cor- pora.", "acronym": "BP", "label": "BLEU points", "ID": "3787"}, {"sentence": "However, a closer examination of the length difference evident through the BLEU brevity penalty and the reference:system-output length ra- tio (columns 4-5 of Table 2), reveals that the dif- ferences are small and inconsistent; on average, the brevity penalty difference accounts for roughly 0.1 absolute BP and 0.2 absolute lemmatized BP of the respective differences.7 Last, Modern Standard Arabic is a morphologi- cally rich language: It has many inflected forms for most verbs, and several inflected forms for nouns, adjectives and other parts of speech ?", "acronym": "BP", "label": "BLEU points", "ID": "3788"}, {"sentence": "In these chains of utterances, the SP is not explicitly mentioned because the author relies on the shared understanding with the reader that adja- cent pieces of quoted speech are not independent (Zhang et al.,", "acronym": "SP", "label": "speaker", "ID": "3789"}, {"sentence": "First is the problem of iden- tifying anaphoric SPs, i.e., in the utterance ?", "acronym": "SP", "label": "speaker", "ID": "3790"}, {"sentence": "In this work, we address both sub- problems, namely, anaphoric SP and implicit SP identification.", "acronym": "SP", "label": "speaker", "ID": "3791"}, {"sentence": "2003), the quote-identification module detects whether a piece of quoted speech is a new quote (NEW), spoken by a SP dif- ferent from the previous SP, or a continuation quote (CONT) spoken by the same SP as that of the previous quote.", "acronym": "SP", "label": "speaker", "ID": "3792"}, {"sentence": "The second problem is resolving utterance chains with implicit SPs.", "acronym": "SP", "label": "speaker", "ID": "3793"}, {"sentence": "2 Related Work Elson and McKeown (2010) used rule-based and statistical learning approaches to identify candi- date characters and attribute each quote to the most likely SP.", "acronym": "SP", "label": "speaker", "ID": "3794"}, {"sentence": "Then, the language, noise, and sentence models (sans \u000f substitutions) are composed together, and the SP is computed.", "acronym": "SP", "label": "shortest path", "ID": "3795"}, {"sentence": "A SP dependency kernel for relation extrac- tion.", "acronym": "SP", "label": "shortest path", "ID": "3796"}, {"sentence": "inference process is performed in a similar fashion, the main difference being that we use the negative log Viterbi semiring for com- puting SPs instead of the V-expectation semiring.", "acronym": "SP", "label": "shortest path", "ID": "3797"}, {"sentence": "We plan to replace our shortest-path extraction  algorithm with one of the recently developed k-  SP algorithms (Eppstein, 1994).", "acronym": "SP", "label": "shortest path", "ID": "3798"}, {"sentence": "Methods based on manually built lexical knowledge bases, such as WordNet, compute the SP be- tween two concepts in the knowledge base and/or look at word overlap in the glosses (see Budan- itsky and Hirst (2006) for an overview).", "acronym": "SP", "label": "shortest path", "ID": "3799"}, {"sentence": "Finding the k SPs.", "acronym": "SP", "label": "shortest path", "ID": "3800"}, {"sentence": "The decoding or inference process is performed in a similar fashion, the main difference being that we use the negative log Viterbi semiring for com- puting SPs instead of the V-expectation semiring.", "acronym": "SP", "label": "shortest path", "ID": "3801"}, {"sentence": "8 Limitations  The small sample size of reports and few condi- tions found in three report genres (Operative Ga- strointestinal, Radiology, and SP)  is a limitation in this study.", "acronym": "SP", "label": "Surgical Pathology", "ID": "3802"}, {"sentence": "SP reports may be the most  temporally distinct report in our study, showing the  highest proportion of historical conditions.", "acronym": "SP", "label": "Surgical Pathology", "ID": "3803"}, {"sentence": "For instance, one may want to reward clauses in SP or present tenses, reflecting the fact that such clauses are more likely to be descriptive than those in perfect or progressive tenses.", "acronym": "SP", "label": "simple past", "ID": "3804"}, {"sentence": "For instance, a fine-grained feature vector has three different features with seven possi- ble values to carry tense-related information: tense, is progressive, and is perfect, whereas a coarse-grained vector carries only one binary feature, is SP or present.6 Finally, the system selects salient descriptive sentences.", "acronym": "SP", "label": "simple past", "ID": "3805"}, {"sentence": "for a male speaker (here suis is used to form the SP of the verb aller, ?", "acronym": "SP", "label": "simple past", "ID": "3806"}, {"sentence": "and Olsen 1997) n/a has modal A yes, no yes if the clause contains a modal verb no past perfect A yes, no yes if the clause is realized in past perfect tense no politeness with be A yes, no yes if the clause contains one of the following expressions: to be sorry, to be delighted, to be glad, to be sad; the feature is designed to help capture politeness expressions (e.g., I am glad to see you) no SP present A yes, no yes if the clause is realized in simple present or past tense no tmp exp long duration A no, long, short long if the clause contains a temporal expression denoting a long period of time, short if it contains an expression denoting a short period of time and no otherwise no is assertive clause O yes, no no if the clause is not an assertion yes is assertive sent O yes,", "acronym": "SP", "label": "simple past", "ID": "3807"}, {"sentence": "This information is expressed in the coarse-grained data set using one binary feature SP present and fine-tuning the score is trivial.", "acronym": "SP", "label": "simple past", "ID": "3808"}, {"sentence": "English SP is written using a single word, so join together French passe?", "acronym": "SP", "label": "simple past", "ID": "3809"}, {"sentence": "SPs onto the simplex.", "acronym": "SP", "label": "Sparse projection", "ID": "3810"}, {"sentence": "if the two words to be compared are antonyms, then the returned score is 0; 1 http://www.d.umn.edu/?tpederse/similarity.html 2 https://github.com/CNGLdlab/LORG-Release 3We used the default built-in converter provided with the SP (2012-11-12 revision).", "acronym": "SP", "label": "Stanford Parser", "ID": "3811"}, {"sentence": "We use Semgrex atop the typed dependen- cies from the SP (de Marneffe et al, 2006b), as aligned in the alignment phase, to iden- tify both semantic patterns in a single text and over two aligned pieces of text.", "acronym": "SP", "label": "Stanford Parser", "ID": "3812"}, {"sentence": "4.1 Learning Reordering Rules We tag both language sides of the bilingual corpus with POS information using the SP3 and extract POS based reordering patterns from word alignment information.", "acronym": "SP", "label": "Stanford Parser", "ID": "3813"}, {"sentence": "Documents are tokenized and parsed with the SP.", "acronym": "SP", "label": "Stanford Parser", "ID": "3814"}, {"sentence": "We use the freely available SP and NER system1 to generate the syntactic interpretation for these features.", "acronym": "SP", "label": "Stanford Parser", "ID": "3815"}, {"sentence": "In the future, we plan to add syntactic parse information for each span such as that generated using the SP (De Marneffe et al, 2006).", "acronym": "SP", "label": "Stanford Parser", "ID": "3816"}, {"sentence": "ted this  process,  Basque texts  with this  tagged  clause SP were not available.", "acronym": "SP", "label": "splits", "ID": "3817"}, {"sentence": "We used the same training/test set SP as in the previous experiments and used both ungeneralized and generalized fragments up to depth 4 together with the discounted RF estimator.", "acronym": "SP", "label": "splits", "ID": "3818"}, {"sentence": "gain ratio(X) = gain(X)/split info~')  where:  split info(X) = - ~ IT, \\[ x \" \\[ Ti I  I T \\[ rag2 I T I  *-=l  The  ratio decreases with an increase in the number  of SP.", "acronym": "SP", "label": "splits", "ID": "3819"}, {"sentence": "We used the same training/test set SP as in the previous experiments.", "acronym": "SP", "label": "splits", "ID": "3820"}, {"sentence": "Because of the small size of the corpora we averaged our results on 10 different training/test set SP.", "acronym": "SP", "label": "splits", "ID": "3821"}, {"sentence": "We used the same training/test set SP as in the previous experiments and used both ungeneralized and generalized fragments together with the discounted RF estimator.", "acronym": "SP", "label": "splits", "ID": "3822"}, {"sentence": "SP The spelling prior is estimated as pspelling(d) = 1?", "acronym": "SP", "label": "Spelling", "ID": "3823"}, {"sentence": "SP error correction can be formalized as a  classification problem.", "acronym": "SP", "label": "Spelling", "ID": "3824"}, {"sentence": "Type Frequency Informal phonological variation 804 (92.9%) SP error 27 (3.1%) Twitter-specific abbreviation 34 (3.9%) Total 865 (100%) Table 2 illustrates that phonological variations constitute a vast majority of ill-spelled words in Japanese microblog texts.", "acronym": "SP", "label": "Spelling", "ID": "3825"}, {"sentence": "SP variation is clearest in cases where an En-  glish word like swiIeh shows up transliterated vari-  ously (:~ ~\" :, ?-, :", "acronym": "SP", "label": "Spelling", "ID": "3826"}, {"sentence": "Prescher et al (2000): verb-object 49.40 68.20 Table 4: Performance comparison with the literature for candidate selection for MT 4 Context-sensitive SP Correction Context-sensitive spelling correction is the task of cor- recting spelling errors that result in valid words.", "acronym": "SP", "label": "Spelling", "ID": "3827"}, {"sentence": "1,2 V, N Sem Generation SP correction 1,2,3 Any Syn/Sem Generation Adjective ordering 1,2 Adj Sem Generation Compound bracketing 1,2 N Syn Analysis Compound interpret.", "acronym": "SP", "label": "Spelling", "ID": "3828"}, {"sentence": "3.2 SP Overlay By exploiting existing lexical resources for senti- ment analysis, an explicit affective dimension can be overlaid on this basic text model.", "acronym": "SP", "label": "Sentiment Polarity", "ID": "3829"}, {"sentence": "2.2 Classification of SP Let us consider how to infer the sentiment polarity p ? {", "acronym": "SP", "label": "Sentiment Polarity", "ID": "3830"}, {"sentence": "3.3 SP Classification Given the large collection of emotion-labelled ex- amples, it may seem straightforward to develop a trainable model for emotion classification.", "acronym": "SP", "label": "Sentiment Polarity", "ID": "3831"}, {"sentence": "As Table 3 shows, the SP is cor- rect in 57.0% of cases and partially correct (Cor- rect + Context-dep.)", "acronym": "SP", "label": "Sentiment Polarity", "ID": "3832"}, {"sentence": "then filter out this pair if w, ant is an antonymous pair, and ant is in cand, and there is no negator up to two words before w and ant, or there is such a negator before both then filter out this pair 3 Antonyms, Trends, SP Native speakers of a language are good at deter- mining whether two words are antonyms (hot?cold, ascend?descend, friend?foe) or not (penguin?clown, cold?chilly, boat?rudder) (Cruse, 1986; Lehrer and Lehrer, 1982; Deese, 1965).", "acronym": "SP", "label": "Sentiment Polarity", "ID": "3833"}, {"sentence": "c?2007 Association for Computational Linguistics SP Identification in Financial News: A Cohesion-based Approach Ann Devitt School of Computer Science & Statistics, Trinity College Dublin, Ireland Ann.Devitt@cs.tcd.ie Khurshid Ahmad School of Computer Science & Statistics, Trinity College Dublin, Ireland Khurshid.Ahmad@cs.tcd.ie Abstract Text is not unadulterated fact.", "acronym": "SP", "label": "Sentiment Polarity", "ID": "3834"}, {"sentence": "The tutorial will be useful for senior and junior researchers who are  interested in structured prediction and global decision problems in NLP, providing a  concise overview of recent SPs and research results.", "acronym": "SP", "label": "perspective", "ID": "3835"}, {"sentence": "2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 1 Research in QA has been developed from two different scientific SPs, artificial intelligence (AI) and information retrieval (IR).", "acronym": "SP", "label": "perspective", "ID": "3836"}, {"sentence": "From this SP, question answering focuses on finding text excerpts that contain the answer within large collections of documents.", "acronym": "SP", "label": "perspective", "ID": "3837"}, {"sentence": "onslraint violation and con\\[licl  from the SP ofdeclarative phonology.", "acronym": "SP", "label": "perspective", "ID": "3838"}, {"sentence": "The article contains a historical SP on question answering over restricted domains and an overview of the current methods and applications used in restricted domains.", "acronym": "SP", "label": "perspective", "ID": "3839"}, {"sentence": "Currently we are witnessing a surge of activity in the area from the SP of IR, initiated by the Question Answering track of TREC1 in 1999 (Voorhees 2001).", "acronym": "SP", "label": "perspective", "ID": "3840"}, {"sentence": "dprel Dependent label h Head lm Leftmost child rm Rightmost child rn Right nearest child form Word form lemma Word lemma pos Predicted PoS tag sp Y SP Y , which may be form, lemma or pos.", "acronym": "SP", "label": "Split", "ID": "3841"}, {"sentence": "SP the cluster with largest intra-cluster distance into two by finding the pair of vectors with largest distance in the cluster.", "acronym": "SP", "label": "Split", "ID": "3842"}, {"sentence": "SPting of Compound Terms in non-Prototypical Compounding Languages.", "acronym": "SP", "label": "Split", "ID": "3843"}, {"sentence": "SPting a natu- ral group into subgroups reduces the criterion  less than when well-separated clusters are dis- covered.", "acronym": "SP", "label": "Split", "ID": "3844"}, {"sentence": "SP- ting noun compounds via monolingual and bilingual paraphrasing: A study on Japanese Katakana words.", "acronym": "SP", "label": "Split", "ID": "3845"}, {"sentence": "excises verbs from existing chunks; SPRule(?<NN>?, ?", "acronym": "SP", "label": "Split", "ID": "3846"}, {"sentence": "In: Proceedings SP,  Second International Conference on Theoretical  and Methodological Issues in Machine Translation  of Natural Languages, Carnegie Mellon University,  Pittsburgh, 12-14 June 1988.", "acronym": "SP", "label": "Supplement", "ID": "3847"}, {"sentence": "CT-6, Special SP (May  1959), pp.", "acronym": "SP", "label": "Supplement", "ID": "3848"}, {"sentence": "Journal of Experimental Psychology Mo- nograph SP, 76(1, Pt.", "acronym": "SP", "label": "Supplement", "ID": "3849"}, {"sentence": "4.1 SPary Lexicon The tagger may use an external lexicon which sup- plies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.", "acronym": "SP", "label": "Supplement", "ID": "3850"}, {"sentence": "A SPal Material Our Wikipedia dump from which the training, de- velopment, and test sets are constructed is from Jan 2, 2014.", "acronym": "SP", "label": "Supplement", "ID": "3851"}, {"sentence": "Tables in SPary Material show the fea- tures for the classifier built over each language?s en- tire dataset.", "acronym": "SP", "label": "Supplement", "ID": "3852"}, {"sentence": "4.4 ES In Experiment IV, we use the manually annotated data collected by Dodds et al (2015).", "acronym": "ES", "label": "Emotion Score", "ID": "3853"}, {"sentence": "04[24.98] 61.32[87.66] 20.12[39.10]  sadness 37.78[69.86] 15.60[25.31] 22.08[37.22] 13.33[23.07] 12.12[22.57] 12.70[18.71]  surprise 17.72[20.34] 8.14[18.56] 11.16[20.35] 3.80[8.50] 7.50[12.50] 5.04[10.11]  Table 6: Precision, Recall and F-Scores (in %) of the system per emotion class on the translated Japanese  SemEval 2007 test corpus before and after including morphology on different ranges of ESs.", "acronym": "ES", "label": "Emotion Score", "ID": "3854"}, {"sentence": "In SIGCHI 2007 Workshop on ES and HCI Workshop.", "acronym": "ES", "label": "Exploratory Search", "ID": "3855"}, {"sentence": "ES Using Timelines.", "acronym": "ES", "label": "Exploratory Search", "ID": "3856"}, {"sentence": "1.2 ES for Relations To address these limitations, we propose a process of exploration for relations of interest in available data.", "acronym": "ES", "label": "Exploratory Search", "ID": "3857"}, {"sentence": "ES: from Finding to Understanding.", "acronym": "ES", "label": "Exploratory Search", "ID": "3858"}, {"sentence": "2 Background 2.1 ES Exploratory search addresses the need of users to quickly identify the important pieces of information in a target set of documents.", "acronym": "ES", "label": "Exploratory Search", "ID": "3859"}, {"sentence": "In Proceedings of the ACM SIGCHI Workshop on ES and HCI.", "acronym": "ES", "label": "Exploratory Search", "ID": "3860"}, {"sentence": "After splitting the TS into three partitions, given the first partition as the TS, the fitness is measured by the score of predicting the second partition.", "acronym": "TS", "label": "training set", "ID": "3861"}, {"sentence": "To fit the weight vector w using the TS {(xi,yi)}ni=1, we use a standard gradient-descent method to find the weight vector that maximizes the log likelihood?n i logP (yi|xi) (Sha and Pereira, 2003).", "acronym": "TS", "label": "training set", "ID": "3862"}, {"sentence": "The actual feature vector is created by instan- tiating all the combinations in the table using the TS.", "acronym": "TS", "label": "training set", "ID": "3863"}, {"sentence": "In order to evaluate the effects of feature templates, in Section 5, we re- move each feature template and find that several feature templates overfit the TS.", "acronym": "TS", "label": "training set", "ID": "3864"}, {"sentence": "The ith candidate is regarded as oc-  curring at \"Hobbs distance\" dH = i. Then the  probability P(dH = ila) is simply:  P(du -= ila)  164  I correct antecedent at Hobbs distance i i  \\[ correct antecedents 1  We use \\[ z \\[ to denote the number of times z is  observed in our TS.", "acronym": "TS", "label": "training set", "ID": "3865"}, {"sentence": "However, word  alignment-based TS has some  drawbacks.", "acronym": "TS", "label": "translation spotting", "ID": "3866"}, {"sentence": "The  performances of the TS  module and the ranking module are  evaluated in terms of precision-recall  measures and coverage rate respectively.", "acronym": "TS", "label": "translation spotting", "ID": "3867"}, {"sentence": "Identifying the translation equivalents,  TS, is the most challenging part of  a bilingual concordancer.", "acronym": "TS", "label": "translation spotting", "ID": "3868"}, {"sentence": "A multi-view approach for term TS.", "acronym": "TS", "label": "translation spotting", "ID": "3869"}, {"sentence": "However, it has other drawbacks in  TS task.", "acronym": "TS", "label": "translation spotting", "ID": "3870"}, {"sentence": "Re- visiting context-based projection methods for term- TS in comparable corpora.", "acronym": "TS", "label": "translation spotting", "ID": "3871"}, {"sentence": "When the outcome of a match is observed, TS uses the relative status of the two systems to update these estimates.", "acronym": "TS", "label": "TrueSkill", "ID": "3872"}, {"sentence": "S j , and a per- system measure of TS?s uncertainty of those estimates, ?", "acronym": "TS", "label": "TrueSkill", "ID": "3873"}, {"sentence": "The TS approach was best overall, so we used it to produce the official rankings for all lan- 6 It is a total ordering when r = 0, or when all the system scores are outside the decision radius.", "acronym": "TS", "label": "TrueSkill", "ID": "3874"}, {"sentence": "We sample from the set of pairwise rankings an equal sized set of pairwise rankings (allowing for multi- ple drawings of the same pairwise ranking), com- pute a TS model score for each system based on this sample, and then rank the systems from 1..|{S j }|.", "acronym": "TS", "label": "TrueSkill", "ID": "3875"}, {"sentence": "3.3.2), and another based on TS (Sakaguchi et al.,", "acronym": "TS", "label": "TrueSkill", "ID": "3876"}, {"sentence": "Each player S j is modeled by two parameters: TS?s current estimate of each system?s relative ability, ?", "acronym": "TS", "label": "TrueSkill", "ID": "3877"}, {"sentence": "On average, there is a small improvement in accuracy moving from Expected Wins to the H&M model, and then again to the TS model; however, there is no pat- tern to the best model for each class.", "acronym": "TS", "label": "TrueSkill", "ID": "3878"}, {"sentence": "It has been sug- 878 Change test years BOW sLDA FWD SemTreeFWD Consumer Staples 2008-2010 0.1015 0.0774 0.1079 0.1426 2011-2012 0.1663 0.1203 0.1664 0.1736 5 years 0.1274 0.0945 0.1313 0.1550 Information Technology 2008-2010 0.0580 0.0585 0.0701 0.0846 2011-2012 0.0894 0.0681 0.1076 0.1273 5 years 0.0705 0.0623 0.0851 0.1017 TS 2008-2010 0.1501 0.1615 0.1497 0.2409 2011-2012 0.2256 0.2084 0.2191 0.4009 5 years 0.1803 0.1803 0.1774 0.3049 Polarity Consumer Staples 2008-2010 0.0359 0.0383 0.0956 0.1054 2011-2012 0.0938 0.0270 0.1131 0.1285 5 years 0.0590 0.0338 0.1026 0.1147 p-value >>0.1000 0.0918 0.0489 Information Technology 2008-2010 0.0551 0.0332 0.0697 0.0763 2011-2012 0.0591 0.0516 0.0764", "acronym": "TS", "label": "Telecommunication Services", "ID": "3879"}, {"sentence": "12 0.2256 0.2084 0.2191 0.4009 5 years 0.1803 0.1803 0.1774 0.3049 Polarity Consumer Staples 2008-2010 0.0359 0.0383 0.0956 0.1054 2011-2012 0.0938 0.0270 0.1131 0.1285 5 years 0.0590 0.0338 0.1026 0.1147 p-value >>0.1000 0.0918 0.0489 Information Technology 2008-2010 0.0551 0.0332 0.0697 0.0763 2011-2012 0.0591 0.0516 0.0764 0.0857 5 years 0.0567 0.0405 0.0723 0.0801 p-value 0.0626 0.0948 0.0103 TS 2008-2010 0.0402 0.0464 0.0821 0.0745 2011-2012 0.0366 0.0781 0.0611 0.0809 5 years 0.0388 0.0591 0.0737 0.0770 p-value >>0.1000 0.0950 0.0222 Table 4: Average MCC for the change and polarity tasks by feature representation, for 2008-2010; for 2011-2012; for all 5 years and associated p-values of ANOVAs for comparison to BOW.", "acronym": "TS", "label": "Telecommunication Services", "ID": "3880"}, {"sentence": "Evaluating Spoken Dialogue Systems for TS.", "acronym": "TS", "label": "Telecommunication Services", "ID": "3881"}, {"sentence": "26 2 TSting The ID task broadly follows the task definition and event types of the BioNLP ST?09, extending it with new entity categories, correspondingly broadening the scope of events, and introducing a new class of events, high-level biological processes.", "acronym": "TS", "label": "Task Set", "ID": "3882"}, {"sentence": "2 TSup The SemEval 2016 Stance Detection for Twitter shared task (Mohammad et al, 2016) consists of two subtasks, Task A and Task B. In Task A the goal is to detect the stance of tweets towards tar- gets given labelled training data for all test targets (Climate Change is a Real Concern, Feminist Move- ment, Atheism, Legalization of Abortion and Hillary Clinton).", "acronym": "TS", "label": "Task Set", "ID": "3883"}, {"sentence": "2 TSting In the design of the REL task, we followed the gen- eral policy of the shared task in assuming named entity recognition (NER) as a given starting point: participants were provided with manually annotated gold standard annotations identifying gene/protein names in all of the training, development, and final test data.", "acronym": "TS", "label": "Task Set", "ID": "3884"}, {"sentence": "2 TSting The EPI task is an event extraction task in the sense popularized by a number of recent domain resources and challenges (e.g. (Pyysalo et al, 2007; Kim et al, 2008; Thompson et al, 2009; Kim et al, 2009; Ana- niadou et al, 2010)).", "acronym": "TS", "label": "Task Set", "ID": "3885"}, {"sentence": "3 TSting In the task, the training, development and test data sets are provided in three types of files: the text, the protein annotation, and the coreference annotation files.", "acronym": "TS", "label": "Task Set", "ID": "3886"}, {"sentence": "3 Shared TSup The following section describes the system setup using the Spanish?English and French?English Eu- roParl, and Czech?English CzEng training data.", "acronym": "TS", "label": "Task Set", "ID": "3887"}, {"sentence": "Figure 4 shows F1  scores on VS using LAF.", "acronym": "VS", "label": "validating set", "ID": "3888"}, {"sentence": "We divide the  training portion of the Tsinghua Chinese  Treebank provided by CLP2010 into three parts  as follows: 500 trees are randomly extracted as  development set, another 500 as VS  and the rest trees are taken as training set.", "acronym": "VS", "label": "validating set", "ID": "3889"}, {"sentence": "F1 scores of VS  varying with D in equation (4) are shown in  Figure 2.", "acronym": "VS", "label": "validating set", "ID": "3890"}, {"sentence": "We apply SSPTC to the test set of  Task 2 in CLP2010, and get 1.275 percentage  points improvement over baseline parser using  the parameters tuned on VS.", "acronym": "VS", "label": "validating set", "ID": "3891"}, {"sentence": "Figure 3 shows F1 scores of VS  using UAF to select higher quality parses.", "acronym": "VS", "label": "validating set", "ID": "3892"}, {"sentence": "The parsing  results on VS show SSPTC is  effective.", "acronym": "VS", "label": "validating set", "ID": "3893"}, {"sentence": "The F1 score of  VS parsed by baseline parser is  85.72%.", "acronym": "VS", "label": "validating set", "ID": "3894"}, {"sentence": "In all the experiments, we clustered the whole set of 2283 adjectives, as the set of objects alters the VS and thus the classification results.", "acronym": "VS", "label": "vector space", "ID": "3895"}, {"sentence": "Much of the work in this study is based on that by  Bagga and Baldwin (1998), where they presented a  successful cross-document coreference resolution  algorithm to resolve ambiguities between people having  the same name using the VS model.", "acronym": "VS", "label": "vector space", "ID": "3896"}, {"sentence": "Incremental VS          Our intent with the incremental VS model  is to approximate the work reported by Bagga and  Baldwin (1998).", "acronym": "VS", "label": "vector space", "ID": "3897"}, {"sentence": "We show that the previous  approach is effective but that a variation on it,  agglomerative VS, provides improved and  much more stable results.", "acronym": "VS", "label": "vector space", "ID": "3898"}, {"sentence": "In the  remainder of this section, we describe the three  methods: incremental VS, KL divergence, and  agglomerative VS.", "acronym": "VS", "label": "vector space", "ID": "3899"}, {"sentence": "EP Network member institutions were contacted who had access to language learners and who had previously participated in data collec- tion for the EP Programme2.", "acronym": "EP", "label": "English Profile", "ID": "3900"}, {"sentence": "EP Journal, 2:e1.", "acronym": "EP", "label": "English Profile", "ID": "3901"}, {"sentence": "EP Journal, vol2:e1.", "acronym": "EP", "label": "English Profile", "ID": "3902"}, {"sentence": "EP Journal, 1(1):1?23.", "acronym": "EP", "label": "English Profile", "ID": "3903"}, {"sentence": "EP Journal, 2.", "acronym": "EP", "label": "English Profile", "ID": "3904"}, {"sentence": "Table 3:  Sample EP  Name Mohamed Atta  Aliases Atta; Mohamed   Position apparent mastermind;   ring leader; engineer; leader   Age 33; 29; 33-year-old;  34-year-old   Where-from United Arab Emirates;  Spain; Hamburg; Egyptian;  ??", "acronym": "EP", "label": "Entity Profile", "ID": "3905"}, {"sentence": "Our results in the Reuters RCV1/RCV2 task, obtained using EP v7 as parallel data, show that our method has no trouble handling different levels of representations simutaneously (document, sen- tence and word).", "acronym": "EP", "label": "Europarl", "ID": "3906"}, {"sentence": "EP: A parallel corpus 2027 for statistical machine translation.", "acronym": "EP", "label": "Europarl", "ID": "3907"}, {"sentence": "used the full 1.8M parallel sentences in EP.", "acronym": "EP", "label": "Europarl", "ID": "3908"}, {"sentence": "In our experiments we learn taggers for a set of 11 European lan- guages that have both UD training data with mor- phological features, and parallel data in EP: Bulgarian, Czech, Danish, Dutch, Finnish, Ital- ian, Polish, Portuguese, Slovene, Spanish and Swedish.", "acronym": "EP", "label": "Europarl", "ID": "3909"}, {"sentence": "EP: A parallel corpus for statistical machine translation.", "acronym": "EP", "label": "Europarl", "ID": "3910"}, {"sentence": "Following prior work, our dataset D u consists of 500,000 parallel sen- tences from the EP v7 English-German cor- pus (Koehn, 2005); and our labeled dataset D l consists of English and German documents from the RCV1/RCV2 corpora (Lewis et al, 2004), each categorized with one out of L = 4 labels.", "acronym": "EP", "label": "Europarl", "ID": "3911"}, {"sentence": "We extend  the notion of adjacent indices to be any two non-  overlapping indices where one has a start position  that equals an EP of the other.", "acronym": "EP", "label": "end position", "ID": "3912"}, {"sentence": "Redo step 1 from those EPs, rather than  from the beginning, if there is any new EP  equal to or exceeding previous greatest one, a type I or  type I1 ambiguity, respectively, is found.", "acronym": "EP", "label": "end position", "ID": "3913"}, {"sentence": "Due to discontinuous nuclei, each edge spans not a  single pair of string positions, indicating its start  and EP, \\])tit a set of such string-position  pairs, and we call this set an index.", "acronym": "EP", "label": "end position", "ID": "3914"}, {"sentence": "1) The exact-word-match measure considers annotations to match if their start and EPs are exactly the same. (", "acronym": "EP", "label": "end position", "ID": "3915"}, {"sentence": "Find all possible words from the beginning of the  string and record their EPs;  2.", "acronym": "EP", "label": "end position", "ID": "3916"}, {"sentence": "es Fails Fails/Trees CopulaBe 60 1 1% ilV 2 0 0% n0V 10 0 0% n0ClV 9 0 0% n0ClVn1 45 2 4% n0ClVden1 36 3 8% n0ClVpn1 29 3 10% n0Vn1 84 3 3% n0Vn1Adj2 24 6 25% n0Van1 87 3 3% n0Vden1 38 3 7% n0Vpn1 30 3 10% ilVcs1 2 0 0% n0Vcs1 30 23 74% n0Vas1 15 10 66% n0Vn1Adj2 24 0 0% s0Vn1 72 9 12% n0Vs1int 15 12 80% n0Vn1n2 24 0 0% n0Vn1an2 681 54 7% Table 1: Checking for Gaps in the Grammar (impersonal with EP subject, ?", "acronym": "EP", "label": "expletive", "ID": "3917"}, {"sentence": "What I mean by semantic filtering my be illus-  trated by reference to the analysis of EP  NP's like there in Sag (1982).", "acronym": "EP", "label": "expletive", "ID": "3918"}, {"sentence": "If they are bound, they are associated to an antecedent?s index; else they might also be interpreted as EPs, i.e. they receive a label that prevents the following submodule to con-sider them for further computation.", "acronym": "EP", "label": "expletive", "ID": "3919"}, {"sentence": "For example, i f   someone uses an EP, is this a sign of intense  anger or is i t  her/his usual way of talking?", "acronym": "EP", "label": "expletive", "ID": "3920"}, {"sentence": "A similar example is the use of EPs (e.g., There is a unicorn in the garden.)", "acronym": "EP", "label": "expletive", "ID": "3921"}, {"sentence": "We extract HLDS- based quasi logical form graphs from the CCG- bank and semantically empty function words such as complementizers, infinitival-to, EP subjects, and case-marking prepositions are adjusted to reflect their purely syntactic status.", "acronym": "EP", "label": "expletive", "ID": "3922"}, {"sentence": "5 Learning Representations with CE In recent years, many NLP practitioners have be- gun using discriminative models, and especially maximum-entropy-based models like CRFs, be- cause they allow the modeler to incorporate ar- bitrary, interacting features of the observation se- quence while still providing tractable inference.", "acronym": "CE", "label": "Contrastive Estimation", "ID": "3923"}, {"sentence": "Noise- CE: A New Estimation Principle for Unnormalized Statistical Models.", "acronym": "CE", "label": "Contrastive Estimation", "ID": "3924"}, {"sentence": "Labeled Pseudo-Projective Dependency ... 2006 27 5 84 P05-1044 Smith & Eisner CE: Training Log-Linear ... 2005 30 13 262 P05-1073 Toutanova et, al.", "acronym": "CE", "label": "Contrastive Estimation", "ID": "3925"}, {"sentence": "Sparse priors have 819 45 tag set 17 tag set All train 973k train All train 973k train Observational initialization (this work) 92.1 92.8 93.9 94.8 CE (Smith and Eisner, 2005) ? ?", "acronym": "CE", "label": "Contrastive Estimation", "ID": "3926"}, {"sentence": "CE max- imizes the conditional probability of the observed sentences given a neighborhood of similar unseen sequences.", "acronym": "CE", "label": "Contrastive Estimation", "ID": "3927"}, {"sentence": "5 Training Although unsupervised training technique such as CE as in (Smith and Eisner, 2005), (Dyer et al, 2011) can be adapted to train 1In practice, the number of non-zero parameters in clas- sic HMM model would be much smaller, as many words do not co-occur in bilingual sentence pairs.", "acronym": "CE", "label": "Contrastive Estimation", "ID": "3928"}, {"sentence": "4.5 CE for Extracted Triples The automatically extracted triples inevitably con- tain errors and are often considered as with high recall but low precision.", "acronym": "CE", "label": "Confidence Estimation", "ID": "3929"}, {"sentence": "5.5 Results with CEs Now, we will investigate the results from another perspective with the help of confidence estima- tions.", "acronym": "CE", "label": "Confidence Estimation", "ID": "3930"}, {"sentence": "2 Related Work Reference-free MT quality assessment was ini- tially approached as a CE task, strongly biased towards exploiting data from a Sta- tistical MT (SMT) system and the translation pro- cess to model the confidence of the system in the produced translation.", "acronym": "CE", "label": "Confidence Estimation", "ID": "3931"}, {"sentence": "TREC 2002 QA at BBN: Answer Selection  and CE.", "acronym": "CE", "label": "Confidence Estimation", "ID": "3932"}, {"sentence": "3.3 CE by Random Walking with Restart We believe that considering confidence of patterns can potentially improve the extraction accuracy.", "acronym": "CE", "label": "Confidence Estimation", "ID": "3933"}, {"sentence": "5 CE In addition to its merits for computing the entropy gradient, subsequence constrained entropy has other uses, including confidence estimation.", "acronym": "CE", "label": "Confidence Estimation", "ID": "3934"}, {"sentence": "DR RR SS  Central Venous Pressure  Inotropic State 0  Stroke Volume  HR +  Cardiac Output +  Total Peripheral Resistance +  Mean Arterial Pressure +  T> Can you tell me what controls TPR?", "acronym": "HR", "label": "Heart Rate", "ID": "3935"}, {"sentence": "Subsequent work has addressed improvements and extensions to the search proce- dure itself, the extraction of the HRs needed for translation, and has also reported con- trastive experiments with other SMT architectures.", "acronym": "HR", "label": "hierarchical rule", "ID": "3936"}, {"sentence": "Marton and Resnik (2008) exploit shal- low correspondences of HRs with source syntactic constituents extracted from par- allel text, an approach also investigated by Chiang (2005).", "acronym": "HR", "label": "hierarchical rule", "ID": "3937"}, {"sentence": "We then describe techniques to analyze and reduce the set of HRs.", "acronym": "HR", "label": "hierarchical rule", "ID": "3938"}, {"sentence": "search through HRs which greatly speeds translation without any effect on quality.", "acronym": "HR", "label": "hierarchical rule", "ID": "3939"}, {"sentence": "The number of HRs extracted far exceeds the number of phrase translations typ- ically found in aligned text.", "acronym": "HR", "label": "hierarchical rule", "ID": "3940"}, {"sentence": "Hierarchical rule extraction Zhang et al (2008) describe a linear algorithm, a modified version of shift-reduce, to extract phrase pairs organized into a tree from which HRs can be directly extracted.", "acronym": "HR", "label": "hierarchical rule", "ID": "3941"}, {"sentence": "Zhang and Gildea (2006) propose bina- rization for synchronous grammars as a means to control search complexity arising from more com- plex, syntactic, HRs sets.", "acronym": "HR", "label": "hierarchical rule", "ID": "3942"}, {"sentence": "In addition, we manually collected English diminishers (e.g. less or approximately), in- tensifiers (e.g. very or indeed) and INV (e.g. not or barely).", "acronym": "INV", "label": "invertors", "ID": "3943"}, {"sentence": "The negation expressions (don?t, can?t...) are rep- resented by the list of INV from Steinberger?s lexicon (Steinberger et al.,", "acronym": "INV", "label": "invertors", "ID": "3944"}, {"sentence": "In ALL diminishers, intensifiers and INV are included as well.", "acronym": "INV", "label": "invertors", "ID": "3945"}, {"sentence": "They were followed by features derived from the lexicons of Steinberger, which includes INV, intensifiers and four polarity levels of words.", "acronym": "INV", "label": "invertors", "ID": "3946"}, {"sentence": "The authors of the best approach in this task re- port that their knowledge-based system has a con- siderable vocabulary including 15 thousand nega- tive expressions, 7 thousand positive expressions, around 120 so-called operators (intensifiers and INV) and around 200 neutral stop expressions including sentiment words as their components.", "acronym": "INV", "label": "invertors", "ID": "3947"}, {"sentence": "Optimization of  INV vector searches.", "acronym": "INV", "label": "inverted", "ID": "3948"}, {"sentence": "o mea- sured the number of INV alignments over all pairs of English word positions.", "acronym": "INV", "label": "inverted", "ID": "3949"}, {"sentence": "IDF: the INV document frequency.", "acronym": "INV", "label": "inverted", "ID": "3950"}, {"sentence": "We redesign our system  in two aspects: 1) on-demand network creation for  retrieval and learning - this eliminates full 'INV  file' creation saving space and reducing 'dead time'  between a collection is acquired and made searchable,  and provides for fast learning capability; 2)  'subeollecfions within a master' file design - this  enables us to handle very large collections in an  incremental nd robust fashion, yet retaining retrieval  ranking flexibility as if all items are in one single  large file.", "acronym": "INV", "label": "inverted", "ID": "3951"}, {"sentence": "Since the relaxed score takes into ac- count renaming relations even if the arguments are INV, it will necessarily be greater or equal than the strict score.", "acronym": "INV", "label": "inverted", "ID": "3952"}, {"sentence": "Dividing this sum by the obvious denominator (replacing (1 if i1 > i2) with (1) in the sum) yielded avalue of 1.6% INV alignments.", "acronym": "INV", "label": "inverted", "ID": "3953"}, {"sentence": "Since this method of counting retrograde alignments would assign a low count to mass movements of large contiguous chunks, we also mea- sured the number of INV alignments over all pairs of English word positions.", "acronym": "INV", "label": "inverted", "ID": "3954"}, {"sentence": "Jar  Completing Synset  Finding Semantic Relations  Creating GUI   Creating Linguistics Interface  Drafting Reports  Writing Final Report  Publishing The Kurdnet  Figure 5: Management Plan References Purya Aliabadi, Mohammad Sina Ahmadi, Shahin Salavati, and Kyumars Sheykh Esmaili.", "acronym": "GUI", "label": "Graphical User Interface", "ID": "3955"}, {"sentence": "c?2012 Association for Computational Linguistics A GUI for Feature-Based Opinion Mining  Pedro Balage Filho   University of Wolverhampton  pedrobalage@gmail.com  Caroline Brun  Xerox Research Centre Europe  Caroline.Brun@xrce.xerox.com  Gilbert Rondeau   Xerox Research Centre Europe  Gilbert.Rondeau@xrce.xerox.com      Abstract  In this paper, we present XOpin, a graphical  user interface that have been developed to  provide a", "acronym": "GUI", "label": "Graphical User Interface", "ID": "3956"}, {"sentence": "95 Workshop on Nicht-visuelle graphische Benutzungsoberfla?chen (Non-visual GUIs), Darmstadt, Germany, February.", "acronym": "GUI", "label": "Graphical User Interface", "ID": "3957"}, {"sentence": "In practice the interaction with the natural language system is via a GUI.", "acronym": "GUI", "label": "Graphical User Interface", "ID": "3958"}, {"sentence": "Use of perceptron learning to create a routing  query given a set of relevance judgments  6) GUI  ?", "acronym": "GUI", "label": "Graphical User Interface", "ID": "3959"}, {"sentence": "4 GUI We developed a graphical user interface to interac- tively experiment with the software for computing semantic relatedness.", "acronym": "GUI", "label": "Graphical User Interface", "ID": "3960"}, {"sentence": "This tool, which runs on a Sun workstation and uses the Xwindows GUI , provided an interface that allowed analysts to easily visualize the relationships among objects and thus avoid errors i n linking objects together.", "acronym": "GUI", "label": "graphical user interface", "ID": "3961"}, {"sentence": "schmid/tools/TreeTagger/ 6 https://code.google.com/p/mate-tools/ 7 https://github.com/termsuite/ 8 Maven group id is fr.univ-nantes.termsuite 17 Figure 2: TermSuite GUI and Technical Information.", "acronym": "GUI", "label": "graphical user interface", "ID": "3962"}, {"sentence": "Each participant is seated at a personal computer with a simple GUI with a button which plays or replays the audio (up to 5 times), a text box in which to write responses, and a second button to submit those responses.", "acronym": "GUI", "label": "graphical user interface", "ID": "3963"}, {"sentence": "It can be used in three ways: the Java API, the command line API, or the GUI as shown on Figure 2.", "acronym": "GUI", "label": "graphical user interface", "ID": "3964"}, {"sentence": "Finally, the language must have an easy-to-use graphics library to support the development of GUIs.", "acronym": "GUI", "label": "graphical user interface", "ID": "3965"}, {"sentence": "Each of the different ypes of user interacts  with a GUI, which allows the users to  add components from a component s ore to the develop-  ing design.", "acronym": "GUI", "label": "graphical user interface", "ID": "3966"}, {"sentence": "Concise ILP formu- lations for dependency parsing.", "acronym": "ILP", "label": "integer linear programming", "ID": "3967"}, {"sentence": "Multi-lingual dependency parsing with incremental ILP.", "acronym": "ILP", "label": "integer linear programming", "ID": "3968"}, {"sentence": "Grammatical error correction using ILP.", "acronym": "ILP", "label": "integer linear programming", "ID": "3969"}, {"sentence": "Thadani and McKeown (2011) substituted MANLI?s simulated annealing-based decoding with ILP, and achieved a consider- able speed-up.", "acronym": "ILP", "label": "integer linear programming", "ID": "3970"}, {"sentence": "Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al, 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al, 2009), in- tegration of ILP into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the num- ber of possible tags that each word can get is small (Grac?a et al, 2009).", "acronym": "ILP", "label": "integer linear programming", "ID": "3971"}, {"sentence": "Systems have also tried to take advantage of more global in- formation to ensure that the pair-wise classifications satisfy temporal logic transitivity constraints, using frameworks such as ILP and Markov logic networks (Bramsen et al, 2006; Cham- bers and Jurafsky, 2008; Yoshikawa et al, 2009; Uz- Zaman and Allen, 2010).", "acronym": "ILP", "label": "integer linear programming", "ID": "3972"}, {"sentence": "Concise ILPmming formu- lations for dependency parsing.", "acronym": "ILP", "label": "integer linear progra", "ID": "3973"}, {"sentence": "2010) also improved upon the work by Baldridge (2008) by using ILPm- ming to find a minimal model of supertag transi- tions, thereby generating a better starting point for EM than the grammatical constraints alone could provide.", "acronym": "ILP", "label": "integer linear progra", "ID": "3974"}, {"sentence": "Multi-lingual dependency parsing with incremental ILPmming.", "acronym": "ILP", "label": "integer linear progra", "ID": "3975"}, {"sentence": "Grammatical error correction using ILPmming.", "acronym": "ILP", "label": "integer linear progra", "ID": "3976"}, {"sentence": "Thadani and McKeown (2011) substituted MANLI?s simulated annealing-based decoding with ILPmming, and achieved a consider- able speed-up.", "acronym": "ILP", "label": "integer linear progra", "ID": "3977"}, {"sentence": "Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al, 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al, 2009), in- tegration of ILPmming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the num- ber of possible tags that each word can get is small (Grac?a et al, 2009).", "acronym": "ILP", "label": "integer linear progra", "ID": "3978"}, {"sentence": "Concise ILPming formu- lations for dependency parsing.", "acronym": "ILP", "label": "integer linear program", "ID": "3979"}, {"sentence": "2010) also improved upon the work by Baldridge (2008) by using ILP- ming to find a minimal model of supertag transi- tions, thereby generating a better starting point for EM than the grammatical constraints alone could provide.", "acronym": "ILP", "label": "integer linear program", "ID": "3980"}, {"sentence": "Multi-lingual dependency parsing with incremental ILPming.", "acronym": "ILP", "label": "integer linear program", "ID": "3981"}, {"sentence": "Grammatical error correction using ILPming.", "acronym": "ILP", "label": "integer linear program", "ID": "3982"}, {"sentence": "Thadani and McKeown (2011) substituted MANLI?s simulated annealing-based decoding with ILPming, and achieved a consider- able speed-up.", "acronym": "ILP", "label": "integer linear program", "ID": "3983"}, {"sentence": "Several recent works have tried to improve this model using Bayesian estimation (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008), sophisticated initialization (Goldberg et al, 2008), induction of an initial clustering used to train an HMM (Freitag, 2004; Biemann, 2006), infinite HMM models (Van Gael et al, 2009), in- tegration of ILPming into the parameter estimation process (Ravi and Knight, 2009), and biasing the model such that the num- ber of possible tags that each word can get is small (Grac?a et al, 2009).", "acronym": "ILP", "label": "integer linear program", "ID": "3984"}, {"sentence": "In Proceedings of the NAACL Workshop on ILP for Natural Langauge Processing.", "acronym": "ILP", "label": "Integer Linear Programming", "ID": "3985"}, {"sentence": "Constrained Conditional Models (CCM) formulation of NLP problems (also known as:  ILP for NLP) is a learning and inference framework that  augments the learning of conditional (probabilistic or discriminative) models with  declarative constraints (written, for example, using a first-order representation).", "acronym": "ILP", "label": "Integer Linear Programming", "ID": "3986"}, {"sentence": "Bergsma and Kondrak (2007b) present a method for identifying sets of cognates across groups of languages using the global inference framework of ILP.", "acronym": "ILP", "label": "Integer Linear Programming", "ID": "3987"}, {"sentence": "In Proceedings of the Workshop on ILP for Natural Lan- guage Processing, pages 1?9, Boulder, Colorado, June.", "acronym": "ILP", "label": "Integer Linear Programming", "ID": "3988"}, {"sentence": "We will also mention various possibilities for  performing the inference, from commercial ILP  packages to search techniques to Lagrangian relaxation approximation methods.", "acronym": "ILP", "label": "Integer Linear Programming", "ID": "3989"}, {"sentence": "InProceedings of the  Workshop on ILP for  Natural Langauge Processing, pp.", "acronym": "ILP", "label": "Integer Linear Programming", "ID": "3990"}, {"sentence": "9//6   Learning Constraint Grammar-style disambiguation rules using  ILP  Nikolaj  L indberg   Centre for Speech Technology  Royal Institute of Technology  SE-100 44 Stockholm, Sweden  nikolaj ~speech.", "acronym": "ILP", "label": "Inductive Logic Programming", "ID": "3991"}, {"sentence": "J. M. Zelle and R. J. Mooney, J. B. Konvisser,  Combining Top-down and Bottom-up Meth-  ods in ILP, Proc  of The 11th Tntcrnational Conference on Ma-  chine Learning (ML-94), pp.343-351, 1994.", "acronym": "ILP", "label": "Inductive Logic Programming", "ID": "3992"}, {"sentence": "of the 21st International Conference on ILP, pages 347?357.", "acronym": "ILP", "label": "Inductive Logic Programming", "ID": "3993"}, {"sentence": "Moreover, results were compared to a base line set of rules produced without learning and the difference reaches a maximum improve- ment using ILP of 22%.", "acronym": "ILP", "label": "Inductive Logic Programming", "ID": "3994"}, {"sentence": "Probabilistic ILP - Theory and Appli- cations, volume 4911 of Lecture Notes in Computer Science.", "acronym": "ILP", "label": "Inductive Logic Programming", "ID": "3995"}, {"sentence": "ILP: derivations, successes and shortcomings.", "acronym": "ILP", "label": "Inductive Logic Programming", "ID": "3996"}, {"sentence": "By treating inference problems as instances of ILP, we proposed three exact theorems which identify exam- ples for which the inference procedure need not be called at all and previous solutions can be re-used with the guarantee of optimality.", "acronym": "ILP", "label": "integer linear programs", "ID": "3997"}, {"sentence": "While in the past first-order logic has been translated to NP-hard ILP, we use polynomial-time-solvable linear programs, al- 877 lowing us to readily scale to large problems with extensive prior knowledge, as demonstrated by our experiments.", "acronym": "ILP", "label": "integer linear programs", "ID": "3998"}, {"sentence": "We describe two formulations of ILP that learn the edges: one maximizing a global score function, and another maximizing a global probability function.", "acronym": "ILP", "label": "integer linear programs", "ID": "3999"}, {"sentence": "On this data, the unaltered baseline system, processes 5127 ILP and achieves an F1 of 75.85%.", "acronym": "ILP", "label": "integer linear programs", "ID": "4000"}, {"sentence": "We present an approach which solves the problem in- crementally, thus we avoid creating in- tractable ILP.", "acronym": "ILP", "label": "integer linear programs", "ID": "4001"}, {"sentence": "We use ILOG CPLEX to solve all of the ILP in our experiments.", "acronym": "ILP", "label": "integer linear programs", "ID": "4002"}, {"sentence": "In Proceedings of the NAACL Workshop on ILPg for Natural Langauge Processing.", "acronym": "ILP", "label": "Integer Linear Programmin", "ID": "4003"}, {"sentence": "Constrained Conditional Models (CCM) formulation of NLP problems (also known as:  ILPg for NLP) is a learning and inference framework that  augments the learning of conditional (probabilistic or discriminative) models with  declarative constraints (written, for example, using a first-order representation).", "acronym": "ILP", "label": "Integer Linear Programmin", "ID": "4004"}, {"sentence": "Bergsma and Kondrak (2007b) present a method for identifying sets of cognates across groups of languages using the global inference framework of ILPg.", "acronym": "ILP", "label": "Integer Linear Programmin", "ID": "4005"}, {"sentence": "In Proceedings of the Workshop on ILPg for Natural Lan- guage Processing, pages 1?9, Boulder, Colorado, June.", "acronym": "ILP", "label": "Integer Linear Programmin", "ID": "4006"}, {"sentence": "We will also mention various possibilities for  performing the inference, from commercial ILPg  packages to search techniques to Lagrangian relaxation approximation methods.", "acronym": "ILP", "label": "Integer Linear Programmin", "ID": "4007"}, {"sentence": "InProceedings of the  Workshop on ILPg for  Natural Langauge Processing, pp.", "acronym": "ILP", "label": "Integer Linear Programmin", "ID": "4008"}, {"sentence": "This FAC is  especially useful when CLARE is being tai-  lored for use in a particular domain, since  it allows people not expert in linguistics or  the CLARE grammar to extend grammati-  cal coverage in simple and approximate, but  often practically important, ways.", "acronym": "FAC", "label": "facility", "ID": "4009"}, {"sentence": "land (1998) is one of the broad- est studies about hedging functions in scientific articles, and which makes use of categories that have strong relationship, at face value, to the like- lihood that the reader of hedged material will find the material sufficiently useful or sufficiently well expressed to prompt the reader to rate highly the message containing the material, whether with an explicit FAC to record kudos or otherwise.", "acronym": "FAC", "label": "facility", "ID": "4010"}, {"sentence": "Moreover, the visualization system can be used  for interfaces which include a FAC for pro-  gramming by demonstration (with macro defi-  nitions) and can offer textual support for inter-  action through other media.", "acronym": "FAC", "label": "facility", "ID": "4011"}, {"sentence": "As is usl~.lly  the case in automata-based approaches, the system  treats analysis and generation symmetrically, and  tile same description can be run with equal FAC  in either direction.", "acronym": "FAC", "label": "facility", "ID": "4012"}, {"sentence": "The version of the system used in our own  FAC has been augmented with list-processing routines  and other specialized programming which greatly increased  its efficiency and data-capacity.", "acronym": "FAC", "label": "facility", "ID": "4013"}, {"sentence": "labasc FAC.", "acronym": "FAC", "label": "facility", "ID": "4014"}, {"sentence": "However, based on preliminary investigation results, we decided to exclude the following semantic classes from tar- gets of the annotation: Timex (Temporal Expres- sion, 12 classes), Numex (Numerical Expression, 34 classes), Address (e.g., postal address and urls, 1 class), Title Other (e.g., Mr., Mrs., 1 class), FAC Part (e.g, 9th floor, sec- ond basement, 1 class).", "acronym": "FAC", "label": "Facility", "ID": "4015"}, {"sentence": "Study of a l,lultiiingual FAC fo_ir  videote?t information networks , Utrecht,  The Netherlands; BSO  434   Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 32?41, Atlanta, Georgia, June 13 2013.", "acronym": "FAC", "label": "Facility", "ID": "4016"}, {"sentence": "Hendrix, G.G. 1977 LIFER: a Natural Language Interface FAC.", "acronym": "FAC", "label": "Facility", "ID": "4017"}, {"sentence": "The project made use of the resources provided by the Edinburgh Compute and Data FAC (http://www.ecdf.ed.ac.uk/).", "acronym": "FAC", "label": "Facility", "ID": "4018"}, {"sentence": "IBM (1983, 1984): Query Management FAC, General Information  GC26-4071, International Business Machines Corporation, San Jose,  California.", "acronym": "FAC", "label": "Facility", "ID": "4019"}, {"sentence": "IBM (1982): Interactive System Productivity FAC, General Informa-  tion, GC34-2181, International Business Machines Corporation, Cary,  North Carohna.", "acronym": "FAC", "label": "Facility", "ID": "4020"}, {"sentence": "First, Stoyanov 1The ACE-2004/05 semantic types are person, organiza- tion, GPE, location, facility, vehicle, weapon.", "acronym": "GPE", "label": "geo-political entity", "ID": "4021"}, {"sentence": "ACE restricts CEs to entities that belong to one of seven semantic classes: per- son, organization, GPE, location, fa- cility, vehicle, and weapon.", "acronym": "GPE", "label": "geo-political entity", "ID": "4022"}, {"sentence": "Similarly, one can express a GPE, e.g. United States, as his country or another person entity, e.g. Hillary Clinton, as his wife, and their relation to the entity Bill Clinton as ?", "acronym": "GPE", "label": "geo-political entity", "ID": "4023"}, {"sentence": "The data is anno- tated with five types of entities: person, organization, GPE, location, facility; each mention can be either named, nominal or pronominal, and can be ei- ther generic (not referring to a clearly described entity) or specific.", "acronym": "GPE", "label": "geo-political entity", "ID": "4024"}, {"sentence": "GPE mentions (?", "acronym": "GPE", "label": "geo-political entity", "ID": "4025"}, {"sentence": "The  ACE 2005 task can detect seven types of named  entities: person, organization, GPE,  location, facility, vehicle, and weapon; each type  of named entity can occur in a document with any  of three distinct formats: name, nominal construc- tion, and pronoun.", "acronym": "GPE", "label": "geo-political entity", "ID": "4026"}, {"sentence": "They are of three  types: GPE entities are composite entities  comprised of a physical location, a population, a  government, and a nation (or province, state,  county, city, etc.).", "acronym": "GPE", "label": "Geo-Political", "ID": "4027"}, {"sentence": "Type Inventory: ACE and ERE share the Per- son, Organization, GPE Entity, and Location Types.", "acronym": "GPE", "label": "Geo-Political", "ID": "4028"}, {"sentence": "This feature is a single binary fea- ture to guarantee that the type of entity in docu- ment (i.e. Person, GPE Entity and Or- ganization) is consistent with the type of entity  in KB.", "acronym": "GPE", "label": "Geo-Political", "ID": "4029"}, {"sentence": "The test data has  3904 queries across three named entity types:  Person, GPE Entity and Organization.", "acronym": "GPE", "label": "Geo-Political", "ID": "4030"}, {"sentence": "Therefore, there were eight coarse-grained cate- gories in total: Facility, GPE, Location, Organisation, Person, Vehicle, Weapon, and Prod- uct.", "acronym": "GPE", "label": "Geo-Political", "ID": "4031"}, {"sentence": "Then we link the document to  the entity in KB if the document contains a  named entity whose name exactly matches with  the unambiguous mention and type (i.e. Person,  Organization and GPE Entity) exactly  matches with the type of entity in KB.", "acronym": "GPE", "label": "Geo-Political", "ID": "4032"}, {"sentence": "The ACE 2005 named entity recognition dataset includes 7 named entity class labels (person, organization, location, GPE, facility, vehicle, weapon) for 5 text genres (newswire, broad- cast news, broadcast conversations, conversational telephone speech, weblogs).", "acronym": "GPE", "label": "geopolitical entity", "ID": "4033"}, {"sentence": "it has as its antecedent a GPE?,", "acronym": "GPE", "label": "geopolitical entity", "ID": "4034"}, {"sentence": "Finin et al (2010) obtained named entity an- notations (person, organization, GPE) for several hundred Twitter messages.", "acronym": "GPE", "label": "geopolitical entity", "ID": "4035"}, {"sentence": "254 O NATURAL OBJECT natural feature or nonliving object in nature barrier reef nest neutron star planet sky fishpond metamorphic rock Mediterranean cave stepping stone boulder Orion ember universe A ARTIFACT man-made structures and objects bridge restaurant bedroom stage cabinet toaster antidote aspirin L LOCATION any name of a GPE, as well as other nouns functioning as locations or regions Cote d?Ivoire New York City downtown stage left India Newark interior airspace P PERSON humans or personified beings; names of social groups (ethnic, political, etc.)", "acronym": "GPE", "label": "geopolitical entity", "ID": "4036"}, {"sentence": "GPE) mention.", "acronym": "GPE", "label": "geopolitical entity", "ID": "4037"}, {"sentence": "Australia and country are mentions of type named and nominal, respectively, of a single GPE.", "acronym": "GPE", "label": "geopolitical entity", "ID": "4038"}, {"sentence": "The procedure is not based on the deno-  tative meaning of a word, but only on the connota-  tive EMs attached to the word; it is difficult to  choose the relevant dimensions, i.e. the dimensions  required for the sufficient semantic space.", "acronym": "EM", "label": "emotion", "ID": "4039"}, {"sentence": "2012), and express quotes demonstrating EMs such as sadness, fear, happiness, anger and surprise (Alm, 2008) with realistic expression (Murray and Arnott, 2008).", "acronym": "EM", "label": "emotion", "ID": "4040"}, {"sentence": "Overall, two anno- tators were employed, while each annotator pro- vided two annotations: one for EM and one for mood.", "acronym": "EM", "label": "emotion", "ID": "4041"}, {"sentence": "Each story sentence (regard- less if quotes were included or not) was anno- tated regarding primary EMs and mood us- ing the following labels: ?", "acronym": "EM", "label": "emotion", "ID": "4042"}, {"sentence": "This method can capture Mmost all types of se-  mantic relations (except EMal and situational  relation), such as paraphrasing by superordinate ( x.  cat/pet) ,  systematic relation (ex.", "acronym": "EM", "label": "emotion", "ID": "4043"}, {"sentence": "Emotions from text: Machine learning for text-based EM prediction.", "acronym": "EM", "label": "emotion", "ID": "4044"}, {"sentence": "Conditional EM-based: Aspects from a struc- tured ontology are generally quite meaningful, but they are not designed specifically for organizing the opinions in our data set.", "acronym": "EM", "label": "Entropy", "ID": "4045"}, {"sentence": "On the Robustness of EM-based Similarity Measures in Evaluation of Subcategorization Acquisiton Sys- tems.", "acronym": "EM", "label": "Entropy", "ID": "4046"}, {"sentence": "This Conditional EM measures the uncer- tainty about the cluster label of a sentence given the knowledge of its aspect.", "acronym": "EM", "label": "Entropy", "ID": "4047"}, {"sentence": "Intu- itively, the conditional entropy-based method es- sentially selects the most appropriate aspects from 736 Algorithm 1 Greedy Algorithm for Conditional EM Based Aspect Selection Input: A = {A1, ..., Am} Output: k-sized A? ?", "acronym": "EM", "label": "Entropy", "ID": "4048"}, {"sentence": "A Maximum EM Model for Prepositional Phrase Attachment.", "acronym": "EM", "label": "Entropy", "ID": "4049"}, {"sentence": "IDF as features, and then choose the sub- set of aspects that minimize Conditional EM of the cluster label given the aspect: A?", "acronym": "EM", "label": "Entropy", "ID": "4050"}, {"sentence": "All queries (other than the ones using the NEAR oper- ator) were performed as EMes (using quotation marks in Altavista).", "acronym": "EM", "label": "exact match", "ID": "4051"}, {"sentence": "EM?:", "acronym": "EM", "label": "exact match", "ID": "4052"}, {"sentence": "Besides an EM accuracy metric, we also used a more fine-grained score based on the well-known PARSEVAL metrics that evaluate phrase-structure trees (Black et al 1991).", "acronym": "EM", "label": "exact match", "ID": "4053"}, {"sentence": "For the EM scheme, the obtained performance is higher7 than the baseline (random guess) that equals to 0.250.", "acronym": "EM", "label": "exact match", "ID": "4054"}, {"sentence": "The search is for word forms, which may be phrases (n-grams), in which case EMes are sought, i.e. respecting the actual sequence of words.", "acronym": "EM", "label": "exact match", "ID": "4055"}, {"sentence": "Experimental results on the Homecentre The tables show that the simple RF estimator scores extremely bad if all fragments are used: the EM is only 1.1% on the Verbmobil corpus and 2.7% on the Homecentre corpus, whereas the discounted RF estimator scores respectively 35.9% and 38.4% on these corpora.", "acronym": "EM", "label": "exact match", "ID": "4056"}, {"sentence": "EMnuel Navarro, Franck Sajous, Bruno Gaume, Laurent Pre?vot, ShuKai Hsieh, Tzu Y. Kuo, Pierre Magistry, and Chu R. Huang.", "acronym": "EM", "label": "Emma", "ID": "4057"}, {"sentence": "Ahmet Aker, Monica Paramita, EM Barker, and Robert Gaizauskas.", "acronym": "EM", "label": "Emma", "ID": "4058"}, {"sentence": "License details: http://creativecommons.org/licenses/by/4.0/ Assigning Terms to Domains by Document Classification Robert Gaizauskas, EM Barker, Monica Lestari Paramita and Ahmet Aker Department of Computer Science, University of Sheffield, United Kingdom {r.gaizauskas,e.barker,m.paramita,ahmet.aker}@sheffield.ac.uk Abstract In this paper we investigate a number of questions relating to the identification of the domain of a term by domain classification of the document in which the term occurs.", "acronym": "EM", "label": "Emma", "ID": "4059"}, {"sentence": "Assigning Terms to Domains by Document Classification Robert Gaizauskas, EM Barker, Monica Lestari Paramita and Ahmet Aker Department of Computer Science, University of Sheffield, United Kingdom {r.gaizauskas,e.barker,m.paramita,ahmet.aker}@sheffield.ac.uk Abstract In this paper we investigate a number of questions relating to the identification of the domain of a term by domain classification of the document in which the term occurs.", "acronym": "EM", "label": "Emma", "ID": "4060"}, {"sentence": "EMnuel Roche and Yves Schabes.", "acronym": "EM", "label": "Emma", "ID": "4061"}, {"sentence": "In EMnuel Roche and Yves Sch-  abes, edito", "acronym": "EM", "label": "Emma", "ID": "4062"}, {"sentence": "May. EMnuel Morin and B?atrice Daille.", "acronym": "EM", "label": "Emma", "ID": "4063"}, {"sentence": "EMnuel Roche and Yves Schabes, editors.", "acronym": "EM", "label": "Emma", "ID": "4064"}, {"sentence": "Moon, T. K. (1996) The EM Algorithm, IEEE Signal Processing Magazine, No- vember, 1996, pp.", "acronym": "EM", "label": "Expectation-Maximization", "ID": "4065"}, {"sentence": "Viterbi EM is performed over each sentence in the corpus to infer the pa- rameters of the model.", "acronym": "EM", "label": "Expectation-Maximization", "ID": "4066"}, {"sentence": "(3)    and the efficient EM al- gorithm can be used to optimize ?", "acronym": "EM", "label": "Expectation-Maximization", "ID": "4067"}, {"sentence": "Our method selects the translation based on the context in an EM style training algo- rithm which explicitly handles polysemy through in- corporating multiple dictionary translations (word sense and translation are closely linked (Resnik and Yarowsky, 1999)).", "acronym": "EM", "label": "Expectation-Maximization", "ID": "4068"}, {"sentence": "This model is trained by using hard discriminative EM.", "acronym": "EM", "label": "Expectation-Maximization", "ID": "4069"}, {"sentence": "Section 4 intro- duces a basic EM model and two extensions for the alignment task.", "acronym": "EM", "label": "Expectation-Maximization", "ID": "4070"}, {"sentence": "3.2.5 Training and Decoding For training, we hold language model parameters constant and use EM (Demp- ster et al, 1977) to learn noise model parameters as follows.", "acronym": "EM", "label": "expectation maximization", "ID": "4071"}, {"sentence": "15) We use the EM algorithm (Dempster et al, 1977) for the maximum likelihood estimate of ?", "acronym": "EM", "label": "expectation maximization", "ID": "4072"}, {"sentence": "We use the CCG grammatical framework and train a non-parametric Bayesian model of parse structure with online variational Bayesian EM.", "acronym": "EM", "label": "expectation maximization", "ID": "4073"}, {"sentence": "The semantic fit between a verb and its argument is modeled using a class-based lexicon that is derived from unlabeled data using the EM algorithm (verb-argument model).", "acronym": "EM", "label": "expectation maximization", "ID": "4074"}, {"sentence": "Self-training and EM are perhaps the best known semi-supervised learn- ing algorithms (Abney, 2008).", "acronym": "EM", "label": "expectation maximization", "ID": "4075"}, {"sentence": "2010) train a probabilistic model of a variety of sentence simplification rules using EM with a parallel corpus of aligned sentences from Wikipedia and Simple Wikipedia.", "acronym": "EM", "label": "expectation maximization", "ID": "4076"}, {"sentence": "These probabilities can be estimated by us- ing the Baum-Welch EM al- gorithm (Baum et al, 1970).", "acronym": "EM", "label": "expectation maximisation", "ID": "4077"}, {"sentence": "2008), which use EM to simultaneously cluster verbs into verb classes and nominal arguments into noun classes; these ap- proaches are not compatible with the evaluation framework we have used here.", "acronym": "EM", "label": "expectation maximisation", "ID": "4078"}, {"sentence": "Marcu and Wong (2002) proposed a phrase-based alignment model which suffered from a massive parameter space and intractable inference using EM.", "acronym": "EM", "label": "expectation maximisation", "ID": "4079"}, {"sentence": "Kupiec et al, 95) and (Aone et al, 97)  employ the EM algorithm to derive  coefficients for their systems.", "acronym": "EM", "label": "Expectation Maximization", "ID": "4080"}, {"sentence": "2) We use the EM algo- rithm to solve this maximization problem.", "acronym": "EM", "label": "Expectation Maximization", "ID": "4081"}, {"sentence": "The algorithm 10 combines EM and Naive Bayes algo- rithms, and we used randomly extracted 50,000 unlabeled synsets in WordNet as the necessary un- labeled data.", "acronym": "EM", "label": "Expectation Maximization", "ID": "4082"}, {"sentence": "Work on type-supervision goes back to (Merialdo, 1994), who introduced the still standard procedure of using a bigram Hidden Markov Model (HMM) trained via EM.", "acronym": "EM", "label": "Expectation Maximization", "ID": "4083"}, {"sentence": "But these translations can be ambiguous, hence we can use EM approach similar to (Khapra et al, 2011) as follows: E-Step: P (SL1 |u, a) = ?", "acronym": "EM", "label": "Expectation Maximization", "ID": "4084"}, {"sentence": "We learn these parame- ters via EM (Dempster et al, 1977), iterating between computing expected counts and adjusting parameters to maximize the posterior probability of the parameters.", "acronym": "EM", "label": "Expectation Maximization", "ID": "4085"}, {"sentence": "Experimental results demonstrate an ER of 35% over a previous state-of-the-art method that uses heuristic alignments.", "acronym": "ER", "label": "error reduction", "ID": "4086"}, {"sentence": "Empirical results demonstrate a 20.6% ER in token labeling accuracy com- pared to a strong baseline method that employs a set of high-precision alignments.", "acronym": "ER", "label": "error reduction", "ID": "4087"}, {"sentence": "Furthermore, we provide a 63.8% ER compared to IBM Model 4 (Brown et al, 1993).", "acronym": "ER", "label": "error reduction", "ID": "4088"}, {"sentence": "2000), with about 3% relative improve- ment (ER from 18.6% to 18%, trained on small data) over the best original system.", "acronym": "ER", "label": "error reduction", "ID": "4089"}, {"sentence": "We obtain an ER of 35.1% over a previous state-of-the-art extraction method that us", "acronym": "ER", "label": "error reduction", "ID": "4090"}, {"sentence": "ExtrCRF also pro- duces an ER of 21.7% compared to M+R-CRF without the use of matching records.", "acronym": "ER", "label": "error reduction", "ID": "4091"}, {"sentence": "We obtain an ER of 35.1% over a previous state-of-the-art extraction method that uses heuristically generated alignments.", "acronym": "ER", "label": "error reduction", "ID": "4092"}, {"sentence": "In comparison to the previous state- of-the-artM-CRF, theExtrCRFmethod provides an ER of 35.1%.", "acronym": "ER", "label": "error reduction", "ID": "4093"}, {"sentence": "ERs for standard recognizers are 5-10 times higher than for dictation tasks.", "acronym": "ER", "label": "Error rate", "ID": "4094"}, {"sentence": "5 98.3% 98.7% 99.1% 99.5% 99.9% 0% 5% 10 % 15 % 20 % 25 % 30 % 35 % 40 % 45 % 50 % p err  (ping error rate) Ta sk  c o pm le tio n   ra te   (% ) 5 5.4 5.8 6.2 6.6 7 Av e ra ge   di a lo g  le n gt h  (tu rn s) Task completion rate Average dialog length  Figure 5: ER of the ping action vs. success- ful task completion rate and average dialog length.", "acronym": "ER", "label": "Error rate", "ID": "4095"}, {"sentence": "Approach ER TwitterSA(maxconf) 18.7 TwitterSA(weights) 19.4 TwitterSA(single) 20 TwitterSA(voting) 22.6 Unigrams 20.9 ReviewSA 21.7 Unigrams-TS 24.3 Table 5: Results for polarity detection.", "acronym": "ER", "label": "Error rate", "ID": "4096"}, {"sentence": "All chunks be- gin with a B symbol, regardless of whether the previous word is tagged O or I. NP Chunking Results Method F-Measure Numits Perc, avg, cc=0 93.53 13 Perc, noavg, cc=0 93.04 35 Perc, avg, cc=5 93.33 9 Perc, noavg, cc=5 91.88 39 ME, cc=0 92.34 900 ME, cc=5 92.65 200 POS Tagging Results Method ER/% Numits Perc, avg, cc=0 2.93 10 Perc, noavg, cc=0 3.68 20 Perc, avg, cc=5 3.03 6 Perc, noavg, cc=5 4.04 17 ME, cc=0 3.4 100 ME, cc=5 3.28 200 Figure 4: Results for various methods on the part-of- speech tagging and chunking tasks on development data.", "acronym": "ER", "label": "Error rate", "ID": "4097"}, {"sentence": "Approach ER TwitterSA(cleaning) 18.1 TwitterSA(no-cleaning) 19.9 Unigrams 27.6 ReviewSA 32 Table 4: Results for subjectivity detection.", "acronym": "ER", "label": "Error rate", "ID": "4098"}, {"sentence": "As perr increases, the policy decreasingly employs the ping diagnostic action in favor of the ask-working-ok communicative action until perr = 20%, at which point the ping action is 84 85 86 87 88 89 90 91 92 93 94 0% 5% 10 % 15 % 20 % 25 % 30 % 35 % 40 % 45 % 50 % p err  (ping error rate) Av e ra ge   re tu rn Figure 4: ER of the ping action vs. reward gained per dialog.", "acronym": "ER", "label": "Error rate", "ID": "4099"}, {"sentence": "NUM?;", "acronym": "NUM", "label": "numeral", "ID": "4100"}, {"sentence": "It may seem surprising, at first, to see 9 gender values in an Indo-European language (as opposed to, say, a Bantu language), but this position is well argued for by (Saloni, 1976), who distinguishes those genders on the basis of agreement with ad- jectives and NUMs;4 we will not attempt to fur- ther justify this position here.", "acronym": "NUM", "label": "numeral", "ID": "4101"}, {"sentence": "Lexical adjectives, including demonstratives ad- verbs, NUMs, and possessive adjectives, as well as ordinary intersective adjectives ?", "acronym": "NUM", "label": "numeral", "ID": "4102"}, {"sentence": "Some Polish NUMs have forms that agree in case with noun (marked congr), as well as forms that require a noun in genitive case (marked rec): (2) Przyszli came dwaj two-nom.congr ch?opcy.", "acronym": "NUM", "label": "numeral", "ID": "4103"}, {"sentence": "The category of accomodability is important for the description of Polish NUM-nominal phrase.", "acronym": "NUM", "label": "numeral", "ID": "4104"}, {"sentence": "This c lass  covers  most nouns, verbs ,  verb ia l  ad~ect ives   and NUMs .", "acronym": "NUM", "label": "numeral", "ID": "4105"}, {"sentence": "Suushi below is a  grammar term of a class of NUM.", "acronym": "NUM", "label": "numerals", "ID": "4106"}, {"sentence": "It may seem surprising, at first, to see 9 gender values in an Indo-European language (as opposed to, say, a Bantu language), but this position is well argued for by (Saloni, 1976), who distinguishes those genders on the basis of agreement with ad- jectives and NUM;4 we will not attempt to fur- ther justify this position here.", "acronym": "NUM", "label": "numerals", "ID": "4107"}, {"sentence": "It is important to realize, however, that these classes are defined mainly on the basis of the inflectional properties of their members; e.g., the class numeral is much nar- rower here than traditionally, as it does not include so-called ordinal NUM (which, morphosyntac- tically, are adjectives).", "acronym": "NUM", "label": "numerals", "ID": "4108"}, {"sentence": "l emma exp lanat ion   o  suushi*  wa  suru   J  f  mo  (  )  nado  nai  aru   kara  koto  dewa  nen  hi  no  comma  period  NUM  topic marker  'do'  right angular parenthesis  left angular parenthesis  topic marker  left parenthes", "acronym": "NUM", "label": "numerals", "ID": "4109"}, {"sentence": "Lexical adjectives, including demonstratives ad- verbs, NUM, and possessive adjectives, as well as ordinary intersective adjectives ?", "acronym": "NUM", "label": "numerals", "ID": "4110"}, {"sentence": "Some Polish NUM have forms that agree in case with noun (marked congr), as well as forms that require a noun in genitive case (marked rec): (2) Przyszli came dwaj two-nom.congr ch?opcy.", "acronym": "NUM", "label": "numerals", "ID": "4111"}, {"sentence": "This c lass  covers  most nouns, verbs ,  verb ia l  ad~ect ives   and NUM .", "acronym": "NUM", "label": "numerals", "ID": "4112"}, {"sentence": "582  Corpus Building for Mongolian Language  Purev Jaimai  Center for Research on Language Processing,  NUM, Mongolia  purev@num.edu.mn  Odbayar Chimeddorj  Center for Research on Language Processing,  NUM, Mongolia  odbayar@num.edu.mn      Abstract  This paper presents an ongoing research  aimed to build the first corpus, 5 million  words, for Mongolian language by focus- ing on annotating and tagging corpus texts  according to TEI XML (McQuee", "acronym": "NUM", "label": "National University of Mongolia", "ID": "4113"}, {"sentence": "Two years ago, a research project to build  a tagged corpus for Mongolian began at the  Center for Research on Language Processing,  NUM.", "acronym": "NUM", "label": "National University of Mongolia", "ID": "4114"}, {"sentence": "c?2009 ACL and AFNLP Part of Speech Tagging for Mongolian Corpus       Purev Jaimai and Odbayar Chimeddorj  Center for Research on Language Processing  NUM  {purev, odbayar}@num.edu.mn         Abstract    This paper introduces the current result of a  research work which aims to build a 5 million  tagged word corpus for Mongolian.", "acronym": "NUM", "label": "National University of Mongolia", "ID": "4115"}, {"sentence": "582  Corpus Building for Mongolian Language  Purev Jaimai  Center for Research on Language Processing,  NUM, Mongolia  purev@num.edu.mn  Odbayar Chimeddorj  Center for Research on Language Processing,  NUM, Mongolia  odbayar@num.edu.mn      Abstract  This paper presents an ongoing research  aimed to build the first corpus, 5 million  words, for Mongolian language by focus- ing on annotating and tagging corpus texts  according to TEI XML (McQueen, 2004)  format.", "acronym": "NUM", "label": "National University of Mongolia", "ID": "4116"}, {"sentence": "1  \" \" \" ' ' ' \" |   10 100  NUMr of references  O ?", "acronym": "NUM", "label": "Numbe", "ID": "4117"}, {"sentence": "Thus,  Genre  NUMr of  Documents   NUMr of person- x entities  Art      3346       1455  Business        315         182  Education      6177       2351  Government      3374         945  Healthcare        914         405  Movies        677       2292  Music        976         366  Politics      4298         949  Religion      2699       1030  Science      7211       2783  Sports      4417       2009    Table 2: Breakdown of docu", "acronym": "NUM", "label": "Numbe", "ID": "4118"}, {"sentence": "2007 Association for Computational Linguistics Computational Linguistics Volume 33, NUMr 1 Research in QA has been developed from two different scientific perspectives, artificial intelligence (AI) and information retrieval (IR).", "acronym": "NUM", "label": "Numbe", "ID": "4119"}, {"sentence": "Thus,  Genre  NUMr of  Documents   NUMr of person- x entities  Art      3346       1455  Business        315         182  Education      6177       2351  Government      3374         945  Healthcare        914         405  Movies        677       2292  Music        976         366  Politics      4298         949  Religion      2699       1030  Science      7211       2783  Sports      4417       2009    Tab", "acronym": "NUM", "label": "Numbe", "ID": "4120"}, {"sentence": "We first obtained from 10,000 to 50,000 unique  documents from the TREC 1, 2 and 3 volumes using the  Inquery search engine from UMass Amherst for each of  the following subjects: art, business, education,  government, healthcare, movies, music, politics,  NUMr of  occurrences  Percentage  of entities  1 46.66  2 18.78  3 9.03  4 4.55  5 1.86  6 1.16  7 0.83  8 0.46  9 or more 16.67    Table 1: Breakdown of distribution by number of  occurrences within the Person X corpus.", "acronym": "NUM", "label": "Numbe", "ID": "4121"}, {"sentence": "Still, 43 Computational Linguistics Volume 33, NUMr 1 many of these early systems (including LUNAR and BASEBALL) were no more than ?", "acronym": "NUM", "label": "Numbe", "ID": "4122"}, {"sentence": "We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/NUM/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.", "acronym": "NUM", "label": "number", "ID": "4123"}, {"sentence": "The training corpus is  marked with the NUM of times a referent has  been mentioned up to that point in the story.", "acronym": "NUM", "label": "number", "ID": "4124"}, {"sentence": "Here we are concerned with the probability that  a proposed antecedent is correct given that it  has been repeated a certain NUM of times.", "acronym": "NUM", "label": "number", "ID": "4125"}, {"sentence": "The second  experiment investigates a method for unsuper-  vised learning of gender/NUM/animaticity  information.", "acronym": "NUM", "label": "number", "ID": "4126"}, {"sentence": "Next, the actual words in a proposed noun-  phrase antecedent give us information regarding  the gender, NUM, and animaticity of the pro-  posed referent.", "acronym": "NUM", "label": "number", "ID": "4127"}, {"sentence": "The words in the antecedent sometimes  also let us test for NUM agreement.", "acronym": "NUM", "label": "number", "ID": "4128"}, {"sentence": "1  \" \" \" ' ' ' \" |   10 100  NUM of references  O ?", "acronym": "NUM", "label": "Number", "ID": "4129"}, {"sentence": "Thus,  Genre  NUM of  Documents   NUM of person- x entities  Art      3346       1455  Business        315         182  Education      6177       2351  Government      3374         945  Healthcare        914         405  Movies        677       2292  Music        976         366  Politics      4298         949  Religion      2699       1030  Science      7211       2783  Sports      4417       2009    Table 2: Breakdown of docu", "acronym": "NUM", "label": "Number", "ID": "4130"}, {"sentence": "2007 Association for Computational Linguistics Computational Linguistics Volume 33, NUM 1 Research in QA has been developed from two different scientific perspectives, artificial intelligence (AI) and information retrieval (IR).", "acronym": "NUM", "label": "Number", "ID": "4131"}, {"sentence": "Thus,  Genre  NUM of  Documents   NUM of person- x entities  Art      3346       1455  Business        315         182  Education      6177       2351  Government      3374         945  Healthcare        914         405  Movies        677       2292  Music        976         366  Politics      4298         949  Religion      2699       1030  Science      7211       2783  Sports      4417       2009    Tab", "acronym": "NUM", "label": "Number", "ID": "4132"}, {"sentence": "We first obtained from 10,000 to 50,000 unique  documents from the TREC 1, 2 and 3 volumes using the  Inquery search engine from UMass Amherst for each of  the following subjects: art, business, education,  government, healthcare, movies, music, politics,  NUM of  occurrences  Percentage  of entities  1 46.66  2 18.78  3 9.03  4 4.55  5 1.86  6 1.16  7 0.83  8 0.46  9 or more 16.67    Table 1: Breakdown of distribution by number of  occurrences within the Person X corpus.", "acronym": "NUM", "label": "Number", "ID": "4133"}, {"sentence": "Still, 43 Computational Linguistics Volume 33, NUM 1 many of these early systems (including LUNAR and BASEBALL) were no more than ?", "acronym": "NUM", "label": "Number", "ID": "4134"}, {"sentence": "Efficient sampling with RB While caching the sampling equation as described in the previous section improved the efficiency, the smooth- ing only bucket s is small, but computing the asso- ciated mass is costly because it requires us to con- sider all topics and paths.", "acronym": "RB", "label": "refined bucket", "ID": "4135"}, {"sentence": "This program differs  from earlier work in its almost complete lack of  hAD-crafting, relying instead on a very small  corpus of Penn Wall Street Journal Tree-bank  text (Marcus et al, 1993) that has been mark", "acronym": "AD", "label": "and", "ID": "4136"}, {"sentence": "We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun AD the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion AD noun phrase repetition.", "acronym": "AD", "label": "and", "ID": "4137"}, {"sentence": "We present some experiments il-  lustrating the accuracy of the method AD note  that with this information added, our pronoun  resolution method achieves 84.2% accuracy.", "acronym": "AD", "label": "and", "ID": "4138"}, {"sentence": "A Statistical Approach to Anaphora Resolution  Niyu  Ge, John  Hale AD Eugene Charn iak   Dept.", "acronym": "AD", "label": "and", "ID": "4139"}, {"sentence": "brown, edu  Abst ract   This paper presents an algorithm for identi-  fying pronominal anaphora AD two experi-  ments based upon this algorithm.", "acronym": "AD", "label": "and", "ID": "4140"}, {"sentence": "yu  Ge, John  Hale AD Eugene Charn iak   Dept.", "acronym": "AD", "label": "and", "ID": "4141"}, {"sentence": "We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun AD the proposed  a", "acronym": "AD", "label": "and", "ID": "4142"}, {"sentence": "s into  a statistical framework - -  specifically the dis-  tance between the pronoun AD the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion AD noun phrase repetition.", "acronym": "AD", "label": "and", "ID": "4143"}, {"sentence": "Our first experiment  shows the relative contribution of each source  Of information AD demonstrates a uccess rate  of 82.9% for all sources combined.", "acronym": "AD", "label": "and", "ID": "4144"}, {"sentence": "AD as nonar- chimedean analysis.", "acronym": "AD", "label": "Automatic differentiation", "ID": "4145"}, {"sentence": "AD may be used on any function (e.g., a neural net), but for our simple sum-of-products function Z, it happens that ?", "acronym": "AD", "label": "Automatic differentiation", "ID": "4146"}, {"sentence": "It started off with the following declaration (Karttunen 1971b): It is evident that logical relations between main sentences and their complements are of great significance in any system of ADP that depends on natural language.", "acronym": "ADP", "label": "automatic data processing", "ID": "4147"}, {"sentence": "cture that  uses full custom integrated circuits to per-  form hidden Markov-moddel-based speech  87   Computing relative polarity for textual inference Rowan Nairn, Cleo Condoravdi, Lauri Karttunen Palo Alto Research Center rnairn@gmail.com , condorav@parc.com , Lauri.Karttunen@parc.com Abstract Semantic relations between main and complement sentences are of great signifi- cance in any system of ADP that depends on natural lan- guage.", "acronym": "ADP", "label": "automatic data processing", "ID": "4148"}, {"sentence": "9] Lauri Karttunen, 29, wrote: It is evident that logical relations between main sentences and their comple- ments are of great significance in any system of ADP that depends on natural language.", "acronym": "ADP", "label": "automatic data processing", "ID": "4149"}, {"sentence": "To quote again the opening paragraph of my 1970 ACL presentation (Karttunen 1971b): It is evident that logical relations between main sentences and their complements are of great significance in any system of ADP that depends on natural language.", "acronym": "ADP", "label": "automatic data processing", "ID": "4150"}, {"sentence": "A heuristic to deal with this is to specify for each of the two  languages whether ADP are more common, where \"preposi-  tion\" here is meant not in the usual part-of-speech sense, but rather in a broad sense  of the tendency of function words to attach left or right.", "acronym": "ADP", "label": "prepositions or postpositions", "ID": "4151"}, {"sentence": "We generally learn that adpositions (ADP) take nouns as argu- ments.", "acronym": "ADP", "label": "prepositions or postpositions", "ID": "4152"}, {"sentence": "A heuristic to deal with this is to specify for each of the  two languages whether ADP  more common, where \"preposition\" here is meant not  in the usual part-of-speech sense, but rather in a broad  sense of the tendency of function words to attach left  or right.", "acronym": "ADP", "label": "prepositions or postpositions", "ID": "4153"}, {"sentence": "Case-marking adpositions must be specified as either ADP.", "acronym": "ADP", "label": "prepositions or postpositions", "ID": "4154"}, {"sentence": "Note that the possibility of different means for realizing conjunction, e.g., using morphemes, punctuation or multiple ADPs, falls out naturally from this setup.)", "acronym": "ADP", "label": "adposition", "ID": "4155"}, {"sentence": "= \"possessive\"]; matcher P: [category==\"ADP\" & subCategory==\"preposition\"]; term \"an\": A+ N ; term \"npn\": N P D?", "acronym": "ADP", "label": "adposition", "ID": "4156"}, {"sentence": "The tag set selected for the annotation of the  COME and GO data frames consists of morpho- syntactic tags that characterize verb usage as well as  semantic tags that aim to highlight the semantic com- ponent of, for instance, adverbial and ADPal  phrases that accompany the verb.", "acronym": "ADP", "label": "adposition", "ID": "4157"}, {"sentence": "between the ordered elements (marked gr i : gr j ), signalling periphrastic ADPs and/or modifi- cation.", "acronym": "ADP", "label": "adposition", "ID": "4158"}, {"sentence": "and gender (M1), the fields included in M1 along with additional information (lemmas) for conjunc- tions, particles and ADPs (M2), and finally the information included in M2 enriched with per- son for pronouns and person, tense and aspect for verbs (M3).", "acronym": "ADP", "label": "adposition", "ID": "4159"}, {"sentence": "Major word classes include  thirteen tags: noun, verb, adjective, pro- noun/determiner, article, adverb, ADP, con- junction, numeral, interjection, unassigned, resi- dual and punctuation.", "acronym": "ADP", "label": "adposition", "ID": "4160"}, {"sentence": "2003; Mnih and Hinton, 2007; CW, 2008) have demonstrated impressive performance at the task of language modeling.", "acronym": "CW", "label": "Collobert and Weston", "ID": "4161"}, {"sentence": "In previous studies, these vectors themselves can capture distributionally de- rived similarities, by directly comparing the word vectors themselves using simple measures such as 155 Euclidean distance (CW, 2008).", "acronym": "CW", "label": "Collobert and Weston", "ID": "4162"}, {"sentence": "As shown in (CW, 2008) and then (Huang et al 2012), simple distance measures us- ing the representations derived from this process are both useful for assessing word similarity and relat- edness.", "acronym": "CW", "label": "Collobert and Weston", "ID": "4163"}, {"sentence": "Inspired by these successes and recent work us- ing neural networks to learn phrase- or sentence- 1621 level embeddings (CW, 2008; Huang et al, 2013; Le and Mikolov, 2014; Sutskever et al, 2014; Kiros et al, 2015), we propose a novel deep architecture for text under- standing, which we call a deep reinforcement rele- vance network (DRRN).", "acronym": "CW", "label": "Collobert and Weston", "ID": "4164"}, {"sentence": "Inspired by recent advances of neural network mod- els for NLP applications (CW, 2008; Mikolov et al.,", "acronym": "CW", "label": "Collobert and Weston", "ID": "4165"}, {"sentence": "1 Introduction Distributed representations of text (embeddings) have been the target of much research in natural language processing (CW, 2008; Mikolov et al, 2013; Pennington et al, 2014; Levy et al, 2015).", "acronym": "CW", "label": "Collobert and Weston", "ID": "4166"}, {"sentence": "4 Multi-Class CW Learning As in the binary case, we maintain a distribution over weight vectors w ?", "acronym": "CW", "label": "Confidence Weighted", "ID": "4167"}, {"sentence": "6 CW Margins In the previous section, we showed that margin val- ues could be used to detect domain shifts.", "acronym": "CW", "label": "Confidence Weighted", "ID": "4168"}, {"sentence": "Additionally, we showed improved detection results using a probabilistic margin based on CW learning.", "acronym": "CW", "label": "Confidence Weighted", "ID": "4169"}, {"sentence": "Furthermore, we show that the pre- viously proposed CW learning al- gorithm (Dredze et al, 2008) can provide a more informative measure than a simple margin for this task.", "acronym": "CW", "label": "Confidence Weighted", "ID": "4170"}, {"sentence": "Average number of CW.", "acronym": "CW", "label": "characters per word", "ID": "4171"}, {"sentence": "cpw: desired (mean) number of CW ?", "acronym": "CW", "label": "characters per word", "ID": "4172"}, {"sentence": "It is generally considered that frequently occurring words are usually short, so the average number of CW was broadly used for mea- suring readability in a robust manner.", "acronym": "CW", "label": "characters per word", "ID": "4173"}, {"sentence": "F2 - average number of CW ?", "acronym": "CW", "label": "characters per word", "ID": "4174"}, {"sentence": "CW 5.22 words per sentence 21.17 words per text 8,476 simple words 75.52% sentences per text 400.34 passive voice 15.11% total sentences 13,091 simplified sentences 16,71% Table 5: Statistics from the balanced text sample Figure 2: Clauses per sentence in the sample 4.2 Simplification analysis We manually analysed and annotated all sentences  in  our  samples.", "acronym": "CW", "label": "characters per word", "ID": "4175"}, {"sentence": "We put a threshold on the number of CW to decide whether it will be considered for edit distance 1 or 2 errors.", "acronym": "CW", "label": "characters per word", "ID": "4176"}, {"sentence": "Given the undirected graph G, we can turn it into a MRF by defining unary poten- tials over nodes and binary potentials over edges.", "acronym": "MRF", "label": "Markov Random Field", "ID": "4177"}, {"sentence": "Latent Concept Expansion Using MRFs.", "acronym": "MRF", "label": "Markov Random Field", "ID": "4178"}, {"sentence": "To do this, we define a MRF over the latent topic layer.", "acronym": "MRF", "label": "Markov Random Field", "ID": "4179"}, {"sentence": "Pat-  tern recognition techniques such as Hidden Markov Models,  MRFs, Multi Layer Perceptrons, Boltz-  mann Machines are used in speech, language and vision.", "acronym": "MRF", "label": "Markov Random Field", "ID": "4180"}, {"sentence": "Exam- ples of such techniques are MRFs (Rat- naparkhi et al, 1994; Abney, 1997; Della Pietra et al, 1997; Johnson et al, 1999), and boosting or perceptron approaches to reranking (Freund et al, 1998; Collins, 2000; Collins and Duffy, 2002).", "acronym": "MRF", "label": "Markov Random Field", "ID": "4181"}, {"sentence": "Discriminative methods such as Conditional Random Fields (CRFs) (Lafferty et al, 2001), Semi-MRFs (Sarawagi and Cohen, 2004), and perceptrons (Collins, 2002a) have been popular approaches for sequence label- ing because of their excellent performance, which is mainly due to their ability to incorporate many kinds of overlapping and non-independent features.", "acronym": "MRF", "label": "Markov Random Field", "ID": "4182"}, {"sentence": "Hinge-loss MRFs: Convex inference for structured prediction.", "acronym": "MRF", "label": "Markov random field", "ID": "4183"}, {"sentence": "Haghighi and Klein (2006) assign each label in the model certain prototypical features and train a MRF for sequence tagging from these labeled features.", "acronym": "MRF", "label": "Markov random field", "ID": "4184"}, {"sentence": "Their potential func-     Figure 2: The structure of the MRF for  representing the term dependency among the query    and the expansion terms        .", "acronym": "MRF", "label": "Markov random field", "ID": "4185"}, {"sentence": "6), this exponentiated form becomes a product of local multiplicative factors, and hence our model forms an undirected graphical model, or MRF.", "acronym": "MRF", "label": "Markov random field", "ID": "4186"}, {"sentence": "Other approaches include transformation based learning (Brill, 1995), contrastive estimation for conditional random fields (Smith and Eisner, 2005), MRFs (Haghighi and Klein, 2006), a multilingual approach (Snyder et al.,", "acronym": "MRF", "label": "Markov random field", "ID": "4187"}, {"sentence": "2 Latent Pseudo-Syntactic Structure The models presented in this paper are phrased in terms of variables in an undirected graphical model, MRF.", "acronym": "MRF", "label": "Markov random field", "ID": "4188"}, {"sentence": "This work was supported in  part by the Center for Intelligent IRl,  in part by SPAWARSYSCEN-SD grant number  N66001-02-1-8903 and in part by Advanced Research  and Development Activity under contract number  MDA904-01-C-0984.", "acronym": "IR", "label": "Information Retrieva", "ID": "4189"}, {"sentence": "In Proceedings of the Span- ish Conference in IRl.", "acronym": "IR", "label": "Information Retrieva", "ID": "4190"}, {"sentence": "For each entity mention e in the evaluation set, we  first locate the truth chain TC that contains that mention  (it can be in only one truth chain) and the system?s  hypothesized chain HC that contains it (again, there can  Chung Heong Gooi and James Allan  Center for Intelligent IRl  Department of Computer Science  University of Massachusetts  Amherst, MA 01003  {cgooi,allan}@cs.umass.edu  be only one hypothesis chain).", "acronym": "IR", "label": "Information Retrieva", "ID": "4191"}, {"sentence": "Readings in IRl.", "acronym": "IR", "label": "Information Retrieva", "ID": "4192"}, {"sentence": "83  Learning the Space of Word Meanings  for IRl Systems  Ko ich i  HORI ,  Se inosuke  TODA and  H isash i  YASUNAGA  Nat iona l  Ins t i tu te  of  Japanese  L i te ra ture   1 -16-10  Yutakacho  Sh inagawaku Tokyo  142 Japan   Abstract :  Several methods  to represent  mean ings   of words have been proposed.", "acronym": "IR", "label": "Information Retrieva", "ID": "4193"}, {"sentence": "Center for Intelligent  IRl, Department of Computer  Science, University of Massachusetts, 2002.", "acronym": "IR", "label": "Information Retrieva", "ID": "4194"}, {"sentence": "The SoNaR-500 corpus 3 requires IRl tools for efficient searching, as trivial solutions are simply too slow or cumbersome to use.", "acronym": "IR", "label": "information retrieva", "ID": "4195"}, {"sentence": "These resources are typically developed for the domain users to help them categorize the domain knowledge and agree on notational standards, and to help them retrieve information using conventional IRl applications.", "acronym": "IR", "label": "information retrieva", "ID": "4196"}, {"sentence": "In  order to perform a careful excursion into the limited  work on cross document coreferencing, we deployed  different IRl techniques for entity  disambiguation and clustering.", "acronym": "IR", "label": "information retrieva", "ID": "4197"}, {"sentence": "In the present paper, 1 By modeling lexical cohesion as a graph structure, we follow earlier approaches in IRl, notably by Salton and colleagues (Salton et al, 1994).", "acronym": "IR", "label": "information retrieva", "ID": "4198"}, {"sentence": "While not as obviously important to natural lan- guage IRl, this property of al- ternative phrases can be used to improve future queries (see Section 4.1).", "acronym": "IR", "label": "information retrieva", "ID": "4199"}, {"sentence": "Recogniz- ing MWEs has been shown to be useful for a num- ber of applications such as IRl (Lewis and Croft, 1990; Rila Mandala and Tanaka, 2000; Wacholder and Song, 2003) and POS tag- ging (Piao et al, 2003).", "acronym": "IR", "label": "information retrieva", "ID": "4200"}, {"sentence": "The SoNaR-500 corpus 3 requires IR tools for efficient searching, as trivial solutions are simply too slow or cumbersome to use.", "acronym": "IR", "label": "information retrieval", "ID": "4201"}, {"sentence": "These resources are typically developed for the domain users to help them categorize the domain knowledge and agree on notational standards, and to help them retrieve information using conventional IR applications.", "acronym": "IR", "label": "information retrieval", "ID": "4202"}, {"sentence": "In  order to perform a careful excursion into the limited  work on cross document coreferencing, we deployed  different IR techniques for entity  disambiguation and clustering.", "acronym": "IR", "label": "information retrieval", "ID": "4203"}, {"sentence": "In the present paper, 1 By modeling lexical cohesion as a graph structure, we follow earlier approaches in IR, notably by Salton and colleagues (Salton et al, 1994).", "acronym": "IR", "label": "information retrieval", "ID": "4204"}, {"sentence": "While not as obviously important to natural lan- guage IR, this property of al- ternative phrases can be used to improve future queries (see Section 4.1).", "acronym": "IR", "label": "information retrieval", "ID": "4205"}, {"sentence": "Recogniz- ing MWEs has been shown to be useful for a num- ber of applications such as IR (Lewis and Croft, 1990; Rila Mandala and Tanaka, 2000; Wacholder and Song, 2003) and POS tag- ging (Piao et al, 2003).", "acronym": "IR", "label": "information retrieval", "ID": "4206"}, {"sentence": "5 Application of Domain Model IR from spoken dialog data is an important requirement for call centers.", "acronym": "IR", "label": "Information retrieval", "ID": "4207"}, {"sentence": "202  Noun-Phrase Analysis in Unrestricted Text for Information Retrieval  David A. Evans, Chengxiang Zhai  Laboratory for Computational Linguistics  Carnegie Mellon Univeristy  Pittsburgh, PA 15213  dae@cmu.edu, cz25@andrew.cmu.edu  Abstract  IR is an important ap-  plication area of natural-language pro-  cessing where one encounters the gen-  uine challenge of processing large quanti-  ties of unrestricted natural-language t xt.", "acronym": "IR", "label": "Information retrieval", "ID": "4208"}, {"sentence": "IR and machine learning techniques were integrated to determine sentence importance (Kupiec et al, 1995; Wong et al, 2008).", "acronym": "IR", "label": "Information retrieval", "ID": "4209"}, {"sentence": "IR: when MWEs like pop star are indexed as a unit, the accuracy of the system im- proves on multiword queries (Acosta et al, 2011).", "acronym": "IR", "label": "Information retrieval", "ID": "4210"}, {"sentence": "IR with conceptual graph matching.", "acronym": "IR", "label": "Information retrieval", "ID": "4211"}, {"sentence": "The first three measures are known to be used in context of IR to capture top- ical informations.", "acronym": "IR", "label": "Information retrieval", "ID": "4212"}, {"sentence": "Our adapted strategy for selecting a dense subgraph of GI is based on the IRmoval of low-coherence vertices, i.e., fragment interpreta- tions.", "acronym": "IR", "label": "iterative re", "ID": "4213"}, {"sentence": "After IR-ranking,  the performance of alignment models over the 300  sentence pairs is calculated.", "acronym": "IR", "label": "iterative re", "ID": "4214"}, {"sentence": "Then the IRinforcement algorithm is used until the node weight values converge (the difference be- tween scores at two iterations is below 0.0001 for all nodes) or 5,000 iterations are reached.", "acronym": "IR", "label": "iterative re", "ID": "4215"}, {"sentence": "A natural way to incorporate ordering information is IR-estimation of the model parameters, since the content model itself provides such information through its transition structure.", "acronym": "IR", "label": "iterative re", "ID": "4216"}, {"sentence": "It would be interesting to use our proposed estimates as initialization for EM-style IR-estimation.", "acronym": "IR", "label": "iterative re", "ID": "4217"}, {"sentence": "3 Model Construction We employ an IR-estimation procedure that al- ternates between (1) creating clusters of text spans with similar word distributions to serve as representatives of within-document topics, and (2) computing models of word distributions and topic changes from the clusters so derived.3 Formalism preliminaries We treat texts as sequences of pre-defined text spans, each presumed to convey infor- mation abou", "acronym": "IR", "label": "iterative re", "ID": "4218"}, {"sentence": "This work was supported in  part by the Center for Intelligent IR,  in part by SPAWARSYSCEN-SD grant number  N66001-02-1-8903 and in part by Advanced Research  and Development Activity under contract number  MDA904-01-C-0984.", "acronym": "IR", "label": "Information Retrieval", "ID": "4219"}, {"sentence": "In Proceedings of the Span- ish Conference in IR.", "acronym": "IR", "label": "Information Retrieval", "ID": "4220"}, {"sentence": "For each entity mention e in the evaluation set, we  first locate the truth chain TC that contains that mention  (it can be in only one truth chain) and the system?s  hypothesized chain HC that contains it (again, there can  Chung Heong Gooi and James Allan  Center for Intelligent IR  Department of Computer Science  University of Massachusetts  Amherst, MA 01003  {cgooi,allan}@cs.umass.edu  be only one hypothesis chain).", "acronym": "IR", "label": "Information Retrieval", "ID": "4221"}, {"sentence": "Readings in IR.", "acronym": "IR", "label": "Information Retrieval", "ID": "4222"}, {"sentence": "83  Learning the Space of Word Meanings  for IR Systems  Ko ich i  HORI ,  Se inosuke  TODA and  H isash i  YASUNAGA  Nat iona l  Ins t i tu te  of  Japanese  L i te ra ture   1 -16-10  Yutakacho  Sh inagawaku Tokyo  142 Japan   Abstract :  Several methods  to represent  mean ings   of words have been proposed.", "acronym": "IR", "label": "Information Retrieval", "ID": "4223"}, {"sentence": "Center for Intelligent  IR, Department of Computer  Science, University of Massachusetts, 2002.", "acronym": "IR", "label": "Information Retrieval", "ID": "4224"}, {"sentence": "keypads and, last but not least, the fact that people mostly communicate between friends and relatives in an IR.", "acronym": "IR", "label": "informal register", "ID": "4225"}, {"sentence": "If IR, ?", "acronym": "IR", "label": "informal register", "ID": "4226"}, {"sentence": "This is an interesting problem because a good solution to it could be applied to many other tasks as well: to enhancing access to digital libraries (containing diachronic and dialectal variants), for example, or to improving treatment of IRs such as SMS messages and blogs, etc.", "acronym": "IR", "label": "informal register", "ID": "4227"}, {"sentence": "A reasonable conjecture is that in this IR, the acronyms AI and CS largely replaced the expansions.", "acronym": "IR", "label": "informal register", "ID": "4228"}, {"sentence": "Given that the author has chosen an IR, the phrase goin to is likely to be replaced by gonna.", "acronym": "IR", "label": "informal register", "ID": "4229"}, {"sentence": "are undoubtedly representative of casual speech.3 However, what may not be so obvious is that an IR does not license the use of subject ellipsis at all times.", "acronym": "IR", "label": "informal register", "ID": "4230"}, {"sentence": "2  Feedback-3  Non-Word Errors  Remain Corrected  3,049 3,457  2,816 3,690  2,791 3,715  2,784 3,722  RW Errors  Remain Corrected  1,692 0  1,692 0  1,692 : 0  1,692 !", "acronym": "RW", "label": "Real-Word", "ID": "4231"}, {"sentence": "Non-Word Errors  Number 6,506  % 79.4  RW Errors Total Errors  i,692 8,198  20.6 100  Table 1: OCR Errors Originating from Literal Words  We conducted three experiments:  1.", "acronym": "RW", "label": "Real-Word", "ID": "4232"}, {"sentence": "RW Errors Introduced Errors  Pass Unknown Wds Lex Wds  First 182 0  Feedback-1 182 0  Feedback-2  Feedback-3  Remain Corrected  2,684 3,822  1,972 4,354  1943 4,563  1948 4,558  Remain Corrected  1,692 0  1,692 0  1,692 0  1,692 0  182 0  182 0  Total Error  Errors Reduction (%)  4,558 44.4  3,846 53.1  3,817 53.4  3,822 53.4  Table 3: Results from Context-Dependent Non-Word Error Correctio", "acronym": "RW", "label": "Real-Word", "ID": "4233"}, {"sentence": "RW Errors Introduced Errors  Pass Unknown Wds Lex Wds  First 182 0  Feedback-1 182 0  Feedback-", "acronym": "RW", "label": "Real-Word", "ID": "4234"}, {"sentence": "c?2009 ACL and AFNLP RW Spelling Correction using Google Web 1T 3-grams Aminul Islam Department of Computer Science University of Ottawa Ottawa, ON, K1N 6N5, Canada mdislam@site.uottawa.ca Diana Inkpen Department of Computer Science University of Ottawa Ottawa, ON, K1N 6N5, Canada diana@site.uottawa.ca Abstract We present a method for detecting and correcting multiple real-word spelling er- rors using the Googl", "acronym": "RW", "label": "Real-Word", "ID": "4235"}, {"sentence": "Context-Dependent Non- and RW Error Correction: The system treated all input strings  as possible errors and tried to correct hem by taking into account he contexts in which the  strings appeared.", "acronym": "RW", "label": "Real-Word", "ID": "4236"}, {"sentence": "4 3,822  1,972 4,354  1943 4,563  1948 4,558  Remain Corrected  1,692 0  1,692 0  1,692 0  1,692 0  182 0  182 0  Total Error  Errors Reduction (%)  4,558 44.4  3,846 53.1  3,817 53.4  3,822 53.4  Table 3: Results from Context-Dependent Non-Word Error Correction  Pass  First  Feedback-1  Feedback-2  Feedback-3  Non-Word Errors  Remain Corrected  2,529 3,977  1,978 4,528  1,935 4,571  1,926 4,580  RW Errors  Remain Corrected  1,225 467  1,031 661  1,008 684  1,015 677  Introduced Errors Total Error  Unknown Wds Lex Wds Errors Reduction (%)  182 54 3,990 51.3  182 119 3,310 59.6  182 141 3,266 60.2  182 , 147 3,270 60.1  Table 4: Results from Context-Dependent Real- and Non-Word Error Correction  4 Analysis  Based on the results, we can see that the predominant, positive ffect in corr", "acronym": "RW", "label": "Real-Word", "ID": "4237"}, {"sentence": "Pass  F~st  Feedback-1  Feedback-2  Feedback-3  Non-Word Errors  Remain Corrected  3,049 3,457  2,816 3,690  2,791 3,715  2,784 3,722  RW Errors  Remain Corrected  1,692 0  1,692 0  1,692 : 0  1,692 !", "acronym": "RW", "label": "Real-Word", "ID": "4238"}, {"sentence": "To attack the problem, Wan et al proposed two models, i.e., the Cluster-based conditional Markov RW model and the Cluster-based HITS model, both make use of the theme clusters in the document set (Wan and Yang, 2008).", "acronym": "RW", "label": "Random Walk", "ID": "4239"}, {"sentence": "c?2007 Association for Computational Linguistics WIT: Web People Search Disambiguation using RWs Jose?", "acronym": "RW", "label": "Random Walk", "ID": "4240"}, {"sentence": "c?2011 Association for Computational Linguistics Bilingual RW Models for Automated Grammar Correction of ESL Author-Produced Text Randy West and Y. Albert Park Department of Computer Science & Engineering University of California, San Diego La Jolla, CA 92093-0533 {rdwest,yapark}@cs.ucsd.edu Roger Levy Department of Linguistics University of California, San Diego La Jolla, CA 92093-0533 rlevy@ucsd.edu Abstract We present a novel noisy channel mod", "acronym": "RW", "label": "Random Walk", "ID": "4241"}, {"sentence": "2.3 RWs Model We aim to determine the similarity between any two nodes of type Webpage in the graph.", "acronym": "RW", "label": "Random Walk", "ID": "4242"}, {"sentence": "Improving Diversity in Ranking using Absorbing RWs.", "acronym": "RW", "label": "Random Walk", "ID": "4243"}, {"sentence": "Answering Opinion Questions with  RWs on Graphs.", "acronym": "RW", "label": "Random Walk", "ID": "4244"}, {"sentence": "- MW: The annotators identify a correspondence of the same paraphrase class between two word strings which differ only in one word.", "acronym": "MW", "label": "Missing Word", "ID": "4245"}, {"sentence": "For 4 of the 20 sections in WSJ2-21, we apply the noise introduction procedure to its own output to 222 Error Type WSJ00 MW likely to bring new attention to the problem ?", "acronym": "MW", "label": "Missing Word", "ID": "4246"}, {"sentence": "Clusters Error Type Genitives Noun Preposition All Error (%) compounds WSD System 57 36 26 119 53.85% NERD module 11 5 9 25 11.31% Brill?s tagger 7 4 8 19 8.6% MWNet sense 10 1 7 18 8.14% Part and Whole identification 5 10 0 15 6.79% Classification rules 7 2 5 14 6.33% Unseen examples Noun compound annotation 0 9 2 11 4.98% Total 97 67 57 221 100% sentences prefers the PURPOSE interpretation (bag for cotton clothes) of the noun com- pound cotton bag over the PART?WHOLE meaning (bag made of cotton) (cf. (", "acronym": "MW", "label": "Missing Word", "ID": "4247"}, {"sentence": "\\[38\\] Ward, W.H.; Hauptmann, A.G.; Stern, R.M.; and  Chanak, T. 1988 Parsing Spoken Phrases Despite  MWs.", "acronym": "MW", "label": "Missing Word", "ID": "4248"}, {"sentence": "5 Conclusion We have shown that it is possible to tune a WSJ- trained statistical parser to ungrammatical text with- Error Type P R F P R F E0 E2-prob MW 88.5 83.7 86.0 88.9 84.3 86.5 Extra Word 87.2 89.4 88.3 89.2 89.7 89.4 Real Word Spell 84.3 83.0 83.7 89.5 88.2 88.9 Agreement 90.4 88.8 89.6 90.3 88.6 89.4 Verb Form 88.6 87.0 87.8 89.1 87.9 88.5 Table 5: Noisy23: Breakdown by Error Type out affecting its performance on grammatical text.", "acronym": "MW", "label": "Missing Word", "ID": "4249"}, {"sentence": "2Recent results indicate that test set adaptation by test set sampling of the training corpus achieves a cased Bleu of 53.26 on MT03 whereas a general system trained on all data achieves only 51.02 Verb Placement 3 MW 5 Extra Word 5 Word Choice 26 Word Order 3 Other error 1 Total 43 Table 4: Errors on last 25 sentences of MT-03.", "acronym": "MW", "label": "Missing Word", "ID": "4250"}, {"sentence": "We re- move the recognized metaphors in riddle sentences, 850 Feature Description Correct Radical number of radicals matched MW Radical number of radicals not matched Disappearing Radical number of radicals that disappear in all characters of riddle descriptions Single Matching number of clues derived from character itself Alignment Matching number of clues derived from alignments Rule Matching number of clues derived from rules Length Rate ratio of the length of clues Frequency prior probability of this character", "acronym": "MW", "label": "Missing", "ID": "4251"}, {"sentence": "MW/ExtraToken errors: this is a general- ized form of conjunction errors: either ?[", "acronym": "MW", "label": "Missing", "ID": "4252"}, {"sentence": "Here we use the GPSG notations:  (I) modification schema  S-> \\[l\\[sem~0 , Conj *l \\], lt\\[\\]semc~ 1\\]  where~Y~\\[ (0,1),  (O,O) \\]  (2) eoordinat:i .on schema  S -> \\[i\\[semrm , Conj .,'- \\], II\\[sem~l \\]  where~,(:{ (I,0), (1,1) \\]  5.2 MW construction  Korean and Japanese allow one of\" the constituents  of a sentence not to be explicitly stated when it is  understandable fr~ll the context.", "acronym": "MW", "label": "Missing", "ID": "4253"}, {"sentence": "Level Task Percentages of Noises Extra line break deletion 49.53  Paragraph  Paragraph boundary detection   Extra space deletion 15.58  Extra punctuation mark deletion 0.71  MW space insertion 1.55  MW punctuation mark insertion 3.85  Misused punctuation mark correction 0.64  Sentence  Sentence boundary detection   Case restoration 15.04  Unnecessary token deletion 9.69 Word  Misspelled word correction 3.41  Table 1.", "acronym": "MW", "label": "Missing", "ID": "4254"}, {"sentence": "MW spaces and  missing punctuation marks were added and marked.", "acronym": "MW", "label": "Missing", "ID": "4255"}, {"sentence": "German English  Exact Match (w/o TT) 46,3% 55,4%  hlcorrect parses 50,3% 39,3%  Not parsed 3,4% 5,3%  Exact Match (after 77) 53,8% 61,2%  Incorrect parses (after TT) 42,8% 33,5%  Labeled Precision (w/o 7T) 90,2% 90,6%  German English  Labeled Precision (after TT) 90,8% 91,4%  LR (all 83,5% 78,5%  utterances, w/o TT)  LR (all 84,0% 79,2%  utterances, after TT)  LR (parsed 91,0% 90,9%  utterances, w/o TT)  LR (parsed 91,6% 91,7%  utterances, after TT)  6 Conclusion  In this article we have extended probabilistic shift-  reduce parsing to be more context-sensitive than  previous works and have demonstrated that a bigger  context improves the performance of a probabilistic  shift-reduce parser.", "acronym": "LR", "label": "Labeled Recall", "ID": "4256"}, {"sentence": "German English  Exact Match (w/o TT) 46,3% 55,4%  hlcorrect parses 50,3% 39,3%  Not parsed 3,4% 5,3%  Exact Match (after 77) 53,8% 61,2%  Incorrect parses (after TT) 42,8% 33,5%  Labeled Precision (w/o 7T) 90,2% 90,6%  German English  Labeled Precision (after TT) 90,8% 91,4%  LR (all 83,5% 78,5%  utterances, w/o TT)  LR (all 84,0% 79,2%  utterances, after TT)  LR (parsed 91,0% 90,9%  utterances, w/o TT)  LR (parsed 91,6% 91,7%  utterances, after TT)  6 Conclusion  In this article we have extended probabilistic shift-  reduce parsing to be more context-sensitive than  previous works and have demonstrated that a bigger  con", "acronym": "LR", "label": "Labeled Recall", "ID": "4257"}, {"sentence": "5,4%  hlcorrect parses 50,3% 39,3%  Not parsed 3,4% 5,3%  Exact Match (after 77) 53,8% 61,2%  Incorrect parses (after TT) 42,8% 33,5%  Labeled Precision (w/o 7T) 90,2% 90,6%  German English  Labeled Precision (after TT) 90,8% 91,4%  LR (all 83,5% 78,5%  utterances, w/o TT)  LR (all 84,0% 79,2%  utterances, after TT)  LR (parsed 91,0% 90,9%  utterances, w/o TT)  LR (parsed 91,6% 91,7%  utterances, after TT)  6 Conclusion  In this article we have extended probabilistic shift-  reduce parsing to be more context-sensitive than  previous works and have demonstrated that a bigger  context improves the performance of a probabilistic  shift-reduce parser.", "acronym": "LR", "label": "Labeled Recall", "ID": "4258"}, {"sentence": "The results of this  ewtluation are given in the following table:  7)'aining set/trees\\]  Test set \\[utterances\\]  GelTilall  19.750  1.000  English  17.793  1.000  Eract Match 46,3% 55,4%  Incorrect parses 50,3% 39,3%  Not pmwed 3,4% 5,3%  contextj'ree rules 988 2.205  Labeled Precision 90,2% 90,6%  LR (all 83,5% 78,5%  utterances)  LR  (parsed utterances) 91,0% 90,9%  Japan.", "acronym": "LR", "label": "Labeled Recall", "ID": "4259"}, {"sentence": "German English  Exact Match (w/o TT) 46,3% 55,4%  hlcorrect parses 50,3% 39,3%  Not parsed 3,4% 5,3%  Exact Match (after 77) 53,8% 61,2%  Incorrect parses (after TT) 42,8% 33,5%  Labeled Precision (w/o 7T) 90,2% 90,6%  German English  Labeled Precision (after TT) 90,8% 91,4%  LR (all 83,5% 78,5%  utterances, w/o TT)  LR (all 84,0% 79,2%  utterances, after TT)  LR (parsed 91,0% 90,9%  utterances, w/o TT)  LR (parsed 91,6% 91,7%  utterances, after TT)  6 Conclusion  In this article we have extended probabilistic shift-  reduce parsing to be more context-sensitive than  previous works and have demonstrated that a bigger  context improves the performance of a probabilistic  shif", "acronym": "LR", "label": "Labeled Recall", "ID": "4260"}, {"sentence": "F1<40 is the F-Measure combining labeled precision and LR for sentences of less than 40 words.", "acronym": "LR", "label": "labeled recall", "ID": "4261"}, {"sentence": "But  2The figure indicates unLR and preci-  sion.", "acronym": "LR", "label": "labeled recall", "ID": "4262"}, {"sentence": "Most of the decrease in F1 is due to the drop in unLR.", "acronym": "LR", "label": "labeled recall", "ID": "4263"}, {"sentence": "For example, for the correct proposition: v1f1: ACT|EFF, ADDR the system that generates the following output for the same argument tokens: v1f1: ACT, ADDR|PAT receives a labeled precision score of 3/4 because the PAT is incorrect and LR 3/4 be- cause the EFF is missing (should the ACT|EFF and ADDR|PAT be taken as atomic values, the scores would then be zero).", "acronym": "LR", "label": "labeled recall", "ID": "4264"}, {"sentence": "Differences to LR/precision are small,  since the number of different non-terminal categories  is very restricted.", "acronym": "LR", "label": "labeled recall", "ID": "4265"}, {"sentence": "8 Czech English Unlabeled precision 99.09 96.03 UnLR 94.81 93.07 Unlabeled F-1 96.90 94.53 Labeled precision 78.38 81.58 Labeled recall 74.99 79.06 Labeled F-1 76.65 80.30 Frame selection accuracy 79.10 84.95 Ambiguous verbs baseline 66.68 68.44 classifier 72.41 80.03 Table 1: Experimental results of errors in the Czech evaluation data were caused just by idioms or light verb constructions not be- ing recognized by our system.", "acronym": "LR", "label": "labeled recall", "ID": "4266"}, {"sentence": "2.2 LR Dual decomposition (Komodakis et al, 2007) and LR in general are often used for solving joint inference problems which are decomposable into individual subproblems linked by equality constraints (Koo et al, 2010; Rush et al, 2010; Rush and Collins, 2011; DeNero and Macherey, 2011; Martins et al, 2011; Das et al, 2012; Almeida and Martins, 2013).", "acronym": "LR", "label": "Lagrangian relaxation", "ID": "4267"}, {"sentence": "We explore instead the use of LR to decou- ple the two subproblems and solve them separately.", "acronym": "LR", "label": "Lagrangian relaxation", "ID": "4268"}, {"sentence": "The LR of this optimization problem is L(a,b, c(a), c(b),u) = f(a, c(a))+ g(b, c(b))+ ?", "acronym": "LR", "label": "Lagrangian relaxation", "ID": "4269"}, {"sentence": "We will also mention various possibilities for  performing the inference, from commercial Integer Linear Programming  packages to search techniques to LR approximation methods.", "acronym": "LR", "label": "Lagrangian relaxation", "ID": "4270"}, {"sentence": "Multiple approaches to generate good ap- proximate solutions for joint multi-structure compression, based on LR to enforce equality between the sequential and syntactic inference subproblems.", "acronym": "LR", "label": "Lagrangian relaxation", "ID": "4271"}, {"sentence": "Thus more efficient graph decoding algorithms, e.g., based on LR or approximate algorithms, may be explored in future work.", "acronym": "LR", "label": "Lagrangian relaxation", "ID": "4272"}, {"sentence": "Also, as the present analysis is based on a small sample of manually annotated adjectives, we intend to obtain a LR Gold Standard, in order to establish statistically more reliable results.", "acronym": "LR", "label": "larger", "ID": "4273"}, {"sentence": "The next section looks at how transducers of single con-  straints or small hierarchies can be combined into single  transducers for LR hierarchies.", "acronym": "LR", "label": "larger", "ID": "4274"}, {"sentence": "Thus, they tend to form LR constituents that are mostly placed in predicative position.", "acronym": "LR", "label": "larger", "ID": "4275"}, {"sentence": "AT&T Labs, 33 Thomas Street, New York, NY 10007, USA iosife@telecom.tuc.gr, taniya@research.att.com Abstract We propose a multi-step system for the analysis of children?s stories that is in- tended to be part of a LR text-to-speech- based storytelling system.", "acronym": "LR", "label": "larger", "ID": "4276"}, {"sentence": "Frame Semantics suggests that the meanings of  lexical items (lexical units (LU)) are best defined  with respect to LR conceptual chunks, called  Frames.", "acronym": "LR", "label": "larger", "ID": "4277"}, {"sentence": "This story analysis system was developed to be part of a LR TTS-based storyteller system aimed at children.", "acronym": "LR", "label": "larger", "ID": "4278"}, {"sentence": "We also implement  the MEAD, LR baselines and our method                                                    5 www-01.ibm", "acronym": "LR", "label": "LexRank", "ID": "4279"}, {"sentence": "We compare our system with five baseline sys- tems: MEAD-WT, LR-WT, ARWG-WT,  MEAD and LR.", "acronym": "LR", "label": "LexRank", "ID": "4280"}, {"sentence": "We also implement  the MEAD, LR baselines and our method                                                    5 www-01.ibm.com/software/integration/optimization/cplex- optimizer/  6 http://www.summarization.com/mead/  7 In our experiments, LR performs much better than  the more complex variant - C-LR (Qazvinian and  Radev, 2008), and thus we choose LR, rather than C- LR, to represent graph-based summari", "acronym": "LR", "label": "LexRank", "ID": "4281"}, {"sentence": "2011) compared four different approaches  for multi-document scientific articles summariza- tion: MEAD, MEAD with corpus specific vo- cabulary, LR and W3SS.", "acronym": "LR", "label": "LexRank", "ID": "4282"}, {"sentence": "We also implement  the MEAD, LR baselines and our method                                                    5 www-01.ibm.com/software/integration/optim", "acronym": "LR", "label": "LexRank", "ID": "4283"}, {"sentence": "We also implement  the MEAD, LR baselines and our method                                                    5 www-01.ibm.com/software/integration/optimization/cplex- optimizer/  6 http://www.summarization.com/mead/  7 In our experiments, LR p", "acronym": "LR", "label": "LexRank", "ID": "4284"}, {"sentence": "We also implement  the MEAD, LR baselines and our method                                                    5 www-01.ibm.com/software/integration/optimization/cplex- optimizer/  6 http://www.summarization.com/mead/  7 In our experiments, LR performs much better than  the more complex variant - C-LR (Qazvinian and  Radev, 2008), and thus we choose LR, rather than C- LR, to represent graph-based summarization methods  for comparison in this paper.", "acronym": "LR", "label": "LexRank", "ID": "4285"}, {"sentence": "LR 7  (Eran and  Radev, 2004) is a multi-document summarization  system which is based on a random walk on the  similarity graph of sentences.", "acronym": "LR", "label": "LexRank", "ID": "4286"}, {"sentence": "Lopez (2008) explores whether LR or the phrase discontiguity in- herent in hierarchical rules explains improvements over phrase-based systems.", "acronym": "LR", "label": "lexical reordering", "ID": "4287"}, {"sentence": "The base system includes a distance distortion model; the lexical system adds LR; rule is the rule preordering system of Genzel (2010) plus LR; 1-step and 2-step are our classifier-based systems plus LR.", "acronym": "LR", "label": "lexical reordering", "ID": "4288"}, {"sentence": "From the merged alignments we also extracted a bidirectional LR model conditioned on the source and the target phrases (Koehn et al, 2007).", "acronym": "LR", "label": "lexical reordering", "ID": "4289"}, {"sentence": "From the aligned data, we also extracted a hierarchical reordering model that is similar to popular LR models (Koehn et al, 2007) but that models swaps containing more than just one phrase (Galley and returned during an earlier iteration of MERT.", "acronym": "LR", "label": "lexical reordering", "ID": "4290"}, {"sentence": "The 2-step classifier preordering approach pro- vides statistically significant improvements over the LR baseline on three out of the eight language pairs: English-Spanish (en-es: 1.4 BLEU), German-English (de-en: 1.2 BLEU), and English- French (en-fr: 1.0 BLEU).", "acronym": "LR", "label": "lexical reordering", "ID": "4291"}, {"sentence": "Our approach com- bines the strengths of LR and syntactic preordering models by performing long-distance reorderings using the structure of the parse tree, while utilizing a discrimina- tive model with a rich set of features, includ- ing lexical features.", "acronym": "LR", "label": "lexical reordering", "ID": "4292"}, {"sentence": "3.3 Logistic regression The use of discrete cells over the Earth?s sur- face allows any classification strategy to be em- ployed, including discriminative classifiers such as LR.", "acronym": "LR", "label": "logistic regression", "ID": "4293"}, {"sentence": "We also show that LR performs fea- ture selection effectively, assigning high weights to geocentric terms.", "acronym": "LR", "label": "logistic regression", "ID": "4294"}, {"sentence": "The first-level grid is constructed the same as for Naive Bayes or flat LR and is controlled by its own parameter.", "acronym": "LR", "label": "logistic regression", "ID": "4295"}, {"sentence": "In addition, because LR does not assume feature independence, complex and over- lapping features of various sorts can be employed.", "acronym": "LR", "label": "logistic regression", "ID": "4296"}, {"sentence": "4 allows us to take advan- tage of LR without incurring such a high training cost.", "acronym": "LR", "label": "logistic regression", "ID": "4297"}, {"sentence": "Unlike for LR, the top IGR features are mostly obscure words, only some of 344 Salt Lake San Francisco New Orleans Phoenix Denver Houston Montreal Seattle Tulsa Los Angeles utah sacramento orleans tucson denver houston montreal seattle tulsa knotts slc hella jtfo az colorado antonio mtl portland okc sd salt sac prelaw phoenix broncos texans quebec tacoma oklahoma pasadena byu niners saints a", "acronym": "LR", "label": "logistic regression", "ID": "4298"}, {"sentence": "N Table 3: Syllable Organization for the       LR Model 4.1 LR Model for Combining Syllables The model to combine syllables is built upon Binary LR whose answers are either combine or not combine.", "acronym": "LR", "label": "Logistic Regression", "ID": "4299"}, {"sentence": "6 LR Model To improve the accuracy of the Q?A algorithm and to learn about the importance of the single features for predicting whether an answer from A is correct, we want to learn optimal scores ?", "acronym": "LR", "label": "Logistic Regression", "ID": "4300"}, {"sentence": "1095   Combining Prediction by Partial Matching and LR                for Thai Word Segmentation Ohm Sornil Department of Computer Science National Institute of Development Administration, Bangkok, Thailand osornil@as.nida.ac.th Paweena Chaiwanarom National Statistical Office Bangkok, Thailand paweena@nso.go.th Abstract Word segmentation is an important part of many applications, including information retrieval, information filteri", "acronym": "LR", "label": "Logistic Regression", "ID": "4301"}, {"sentence": "We ran LR learners and eval- uated their performance using QWK scores.", "acronym": "LR", "label": "Logistic Regression", "ID": "4302"}, {"sentence": "V (D) is the VR space, which is the set of weightswi that classify the training data correctly, and |V (D)| is the size of the VR space.", "acronym": "VR", "label": "version", "ID": "4303"}, {"sentence": "Combining these  two notions into tin extended VR of the automaton pro-  duct operation allows us to build tip transducers capturing a  hierarchy of constraints fiom single constraint transducers.", "acronym": "VR", "label": "version", "ID": "4304"}, {"sentence": "We have im-  plemented a slightly modified VR of Hobbs  algorithm for the Tree-bank parse trees.", "acronym": "VR", "label": "version", "ID": "4305"}, {"sentence": "Also thanks to Nadjet Bouayad, Katrin Erk and 5 anonymous reviewers for revision and criticism of pre- vious VRs of the paper.", "acronym": "VR", "label": "version", "ID": "4306"}, {"sentence": "In practice, to explore the VR space of weights consistent with the training data, BPM trains a few different perceptrons (Collins, 2002) by shuffling the samples.", "acronym": "VR", "label": "version", "ID": "4307"}, {"sentence": "We also evaluated thispenalized VR, varying the trade-off parameter C. Bayes Point Machines (BPM) for structured prediction (Corston-Oliver et al, 2006) is an en- semble learning algorithm that attempts to set the weight w to be the Bayes Point which approxi- mates to Bayesian inference for linear classifiers.", "acronym": "VR", "label": "version", "ID": "4308"}, {"sentence": "mean word and sentence length) and the type-token ratio (which indicates VR) are also represented (tok).", "acronym": "VR", "label": "vocabulary richness", "ID": "4309"}, {"sentence": "14 The remaining features (extended) attempt to model the following five dimensions of the lyrics: VOCABULARY: These features estimate the VR (type-token ratio for n-grams up to n = 3) and the use of non-standard words, i.e., uncommon and slang words.", "acronym": "VR", "label": "vocabulary richness", "ID": "4310"}, {"sentence": "We can mea- sure the VR or lexical diversity of a narrative sample using a number of different metrics (see Table 7).", "acronym": "VR", "label": "vocabulary richness", "ID": "4311"}, {"sentence": "The measures of VR do not distin- guish between the SD and control groups, suggest- ing it is the words themselves, and not the number of different words being used, that is important.", "acronym": "VR", "label": "vocabulary richness", "ID": "4312"}, {"sentence": "The combination  808  of the best VR functions in a  lnultivariate model can then be used tbr  capturing the characteristics of a stylistic  category (llohnes, 1992).", "acronym": "VR", "label": "vocabulary richness", "ID": "4313"}, {"sentence": "Specifically, various  functions that attempt to represent the  VR have been proposed  (Honore 1979; Sichel, 1975).", "acronym": "VR", "label": "vocabulary richness", "ID": "4314"}, {"sentence": "We examine several different types of fea- tures, including part-of-speech, complex- ity, context-free grammar, fluency, psy- cholinguistic, VR, and acoustic, and discuss the circumstances under which they can be extracted.", "acronym": "VR", "label": "vocabulary richness", "ID": "4315"}, {"sentence": "Technical Correspondence The Extraction of a Minimum Set of Semantic Primitives from a Monolingual Dictionary is NP-Complete  Computational Linguistics, Volume 12, Number 4, October-December 1986 307   A Spoken Language Interface to a VR System (Video)  Stephanie S. Everett, Kenneth Wauchope  Navy Ctr.", "acronym": "VR", "label": "Virtual Reality", "ID": "4316"}, {"sentence": "through a VR town.", "acronym": "VR", "label": "Virtual Reality", "ID": "4317"}, {"sentence": "When we extend this to a real-time immersive  VR environment, a Virtual Kitchen in  this case, the ECAs will actually perform the task  of cooking a recipe together in the virtual kitchen  while conversing about the steps involved in doing  so, as laid out by the AI plan.", "acronym": "VR", "label": "Virtual Reality", "ID": "4318"}, {"sentence": "ACM SIGSOFT  Software Engineering Notes, Volume 21 Issue  2, March 1996  12.Nikos Kladias, Tassos Pantazidis, Manolis  Avagianos, A VR Learning  Environment Providing Access to Digital  Museums, 1998 MultiMedia Modeling October,  1998, p193  13.", "acronym": "VR", "label": "Virtual Reality", "ID": "4319"}, {"sentence": "DSTO and VR.", "acronym": "VR", "label": "Virtual Reality", "ID": "4320"}, {"sentence": "Shun-tzu Tsai, Chun-ko Hsieh, Diversity and  Aesthetic Appeal for a VR World of  Chinese Art, proceeding of the Seventh  International Conference on Virtual System and  Multimedia,  2001  5. ?", "acronym": "VR", "label": "Virtual Reality", "ID": "4321"}, {"sentence": "Table 4: Example output for original sentences (O) as generated by the RT baseline (R) and our tree labeling system (T), as well as the headline-generated Google compressions (C).", "acronym": "RT", "label": "Reluctant Trimmer", "ID": "4322"}, {"sentence": "our conclusion is that the tradeoff between coreference  quality versus rRT in our agglomerative approach is  definitely worthwhile if the number of same-named  entities to be disambiguated is relatively small.", "acronym": "RT", "label": "untime", "ID": "4323"}, {"sentence": "7.3 RRT comparison.", "acronym": "RT", "label": "untime", "ID": "4324"}, {"sentence": "Is the improvement in our results worth the  difference in rRT?", "acronym": "RT", "label": "untime", "ID": "4325"}, {"sentence": "The rRTs of incremental  approaches are linear whereas the rRT of our  agglomerative vector space approach is O(n?).", "acronym": "RT", "label": "untime", "ID": "4326"}, {"sentence": "If  the mean size of entities to be disambiguated is  relatively small, then there will not be a significant  degrade in rRT on the agglomerative approach.", "acronym": "RT", "label": "untime", "ID": "4327"}, {"sentence": "20  30  40  50  60  70  80  90  100  20  30  40  50  60  70  80  90  100 Pr ec isi o n RT Incremental VS KL Divergence Agglomerative VS Breck and Bagga Figure 2: RT and precision tradeoff of three  algorithms on the John Smith Corpus.", "acronym": "RT", "label": "Recall", "ID": "4328"}, {"sentence": "3.2 Gold Standard RT that we could not use any previously well- established classification.", "acronym": "RT", "label": "Recall", "ID": "4329"}, {"sentence": "RT also from the Introduction that we wanted to restrict ourselves to shallow dis- tributional features.", "acronym": "RT", "label": "Recall", "ID": "4330"}, {"sentence": "RT that GEN is the function which produces  the initial set of candidate forms which is reduced by the  constraints.", "acronym": "RT", "label": "Recall", "ID": "4331"}, {"sentence": "The systems achieved  an average of 55% RT while the precision was  68.8%.", "acronym": "RT", "label": "Recall", "ID": "4332"}, {"sentence": "The Reference Broadcast act consists mostly of usernames and urls.9 Also prominent is the word rt, which has special significance on Twitter, indicating that the user is RT another user?s post.", "acronym": "RT", "label": "re-posting", "ID": "4333"}, {"sentence": "Twitter provides two methods to respond to messages: replies and retweets (RT of a message to one?s follow- ers).", "acronym": "RT", "label": "re-posting", "ID": "4334"}, {"sentence": "Time Adverbials in English and RT.", "acronym": "RT", "label": "Reference Time", "ID": "4335"}, {"sentence": "\\[18\\] Nerbonne J.. RT and Time in Narration;  Linguistics and PhUosophy 9 N 1 (1986) 63-82.", "acronym": "RT", "label": "Reference Time", "ID": "4336"}, {"sentence": "Assum-  ing the Reichenbachian threefold istinction between  Event Time (E), RT (R), and Speaking  Time (S) (the Basic Tense Structure, BTS, (Reichen-  bach 1947)), we observe that the constraints imposed  by a marker on verb tense concern the underlying re-  lation between E and S of both clauses: Selecting  either a/s or wenn to express imultaneous events in  the main clause (era) and in the subordinate clause  (es) depends on whether the event times precede", "acronym": "RT", "label": "Reference Time", "ID": "4337"}, {"sentence": "Figure 1: Algorithm for Computing   RT (tval)    CTYPE: clause is a regular clause,  complement clause, or relative clause   CINDEX: subclause index   PARA: paragraph number   SENT: sentence number   SCONJ: subordinating conjunction  (e.g., while, since, before)   TPREP: preposition in a TIMEX2  PP  TIMEX2: string in the TIMEX2 tag   TMOD:  temporal modifier not at- tached to a TIMEX2, (e.g., after [an  altercation])   Q", "acronym": "RT", "label": "Reference Time", "ID": "4338"}, {"sentence": "I: RT, Tense and Adverbs, p. 71-110.", "acronym": "RT", "label": "Reference Time", "ID": "4339"}, {"sentence": "3 RT Dynamic-choosing  Mechanism  3.1 Referential feature in Implicit Time  In this paper, we define the Implicit Time con- sists of the modifier and the temporal noun  which is modified by modifiers.", "acronym": "RT", "label": "Reference Time", "ID": "4340"}, {"sentence": "I: RT, Tense and Adverbs.", "acronym": "RT", "label": "Reference Time", "ID": "4341"}, {"sentence": "slot.modifiers do applyModifier(RT,modifier) end for In question text, replace slot with RT end for return question text yield.", "acronym": "RT", "label": "role text", "ID": "4342"}, {"sentence": "t.plaintext slots do RT?", "acronym": "RT", "label": "role text", "ID": "4343"}, {"sentence": "Even in the absence of modifiers, all RT re- ceives some additional processing before being in- serted into its corresponding slot.", "acronym": "RT", "label": "role text", "ID": "4344"}, {"sentence": "Modifiers apply transformations to the RT in- serted into a slot, and filters enforce finer-grained matching criteria.", "acronym": "RT", "label": "role text", "ID": "4345"}, {"sentence": "A slot inside the plaintext acts as a variable to be replaced by the correspond- ing semantic RT from a matching sentence, while any slots appearing outside the plaintext serve only to provide additional pattern match- ing criteria.", "acronym": "RT", "label": "role text", "ID": "4346"}, {"sentence": "If a predicate frame and template are matched, they are passed to Algorithm 2, which fills tem- plate slots with RT to produce a question.", "acronym": "RT", "label": "role text", "ID": "4347"}, {"sentence": "Tweet, tweet, RT: Conversational aspects of RTing on twitter.", "acronym": "RT", "label": "retweet", "ID": "4348"}, {"sentence": "They are: RT; hashtag; reply; link, if the tweet con- tains a link; punctuation (exclamation and ques- tions marks); emoticons (textual expression rep- resenting facial expressions); and upper cases (the number of words that starts with upper case in the tweet).", "acronym": "RT", "label": "retweet", "ID": "4349"}, {"sentence": "659 There are also other measures to consider, e.g., the follower number and the times of being RTed.", "acronym": "RT", "label": "retweet", "ID": "4350"}, {"sentence": "The features we employed were: k-top words, k-top digrams and trigrams, k-top hashtags, k-top mentions, tweet/RT/hashtag/link/mention frequencies, and out/in-neighborhood size.", "acronym": "RT", "label": "retweet", "ID": "4351"}, {"sentence": "However, few of these studies have considered measures beyond simple hashtag fre- quencies, relative mention counts among politicians, and RT counts.", "acronym": "RT", "label": "retweet", "ID": "4352"}, {"sentence": "An Approach with MaxiMEMMs.", "acronym": "MEMMs", "label": "mum Entropy Markov Models", "ID": "4353"}, {"sentence": "68  CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 243?247 Manchester, August 2008 The Integration of Dependency Relation Classification and Semantic Role Labeling Using Bilayer MaxiMEMMs Weiwei Sun and Hongzhan Li and Zhifang Sui Institute of Computational Linguistics Peking University {weiwsun, lihongzhan.pku}@gmail.com, szf@pku.edu.cn Abstract This paper describes a system to solve the joint learning of syntactic and seman- tic dependencies.", "acronym": "MEMMs", "label": "mum Entropy Markov Models", "ID": "4354"}, {"sentence": "An aproach with MaxiMEMMs Ben?at Zapirain, Eneko Agirre IXA NLP Group University of the Basque Country Donostia, Basque Country {benat.zapirain,e.agirre}@ehu.es Llu??s Ma`rquez TALP Research Center Technical University of Catalonia Barcelona, Catalonia lluism@lsi.upc.edu Abstract We present a sequential Semantic Role La- beling system that describes the tagging problem as a Maximum Entropy Markov", "acronym": "MEMMs", "label": "mum Entropy Markov Models", "ID": "4355"}, {"sentence": "track only) 2.3 MaxiMEMMs MaxiMEMMs are a discrimi- native model for sequential tagging that models the local probability P (sn | sn?1, o), where o is the context of the observation.", "acronym": "MEMMs", "label": "mum Entropy Markov Models", "ID": "4356"}, {"sentence": "Rudnick et al(2013) present a combina- tion of MaxiMEMMs and HMM to perform lexical selection in the sense of cross-lingual word sense disam- biguation (i.e. by choice from the set of trans- lation alternatives).", "acronym": "MEMMs", "label": "mum Entropy Markov Models", "ID": "4357"}, {"sentence": "Note that we expect the REL structure L to be cycle free.", "acronym": "REL", "label": "relational", "ID": "4358"}, {"sentence": "described by three design decisions: (1) instead of building a pipeline using local classifier technology, we design and learn a joint probabilistic model over events in a sentence; (2) instead of developing spe- cific inference and learning algorithms for our joint model, we apply Markov Logic, a general purpose Statistical Relation Learn- ing language, for this task; (3) we represent events as REL structures over the to- kens of a sentence, as opposed to structures that explicitly mention abstract event en- tities.", "acronym": "REL", "label": "relational", "ID": "4359"}, {"sentence": "Then, the term arguments of each REL triple are assigned to the latter, originating a wordnet.", "acronym": "REL", "label": "relational", "ID": "4360"}, {"sentence": "By mapping to REL structure over grounded text, we also show a direct connection to recent formulations of Semantic Role Labelling which may be helpful in the future.", "acronym": "REL", "label": "relational", "ID": "4361"}, {"sentence": "Let us introduce Markov Logic by considering the event extraction task (as REL structure over tokens as generated by algorithm 1).", "acronym": "REL", "label": "relational", "ID": "4362"}, {"sentence": "which is called the REL  model), a word naziva can be initially tagged ei- ther as a noun naziv (Engl.", "acronym": "REL", "label": "relational", "ID": "4363"}, {"sentence": "It is use- ful for low-level tasks such as parsing (e.g. for PP-attachment ambiguity within NPs), but also for tasks oriented to semantics, such as the extraction of RELhips between individuals or concepts.", "acronym": "REL", "label": "relations", "ID": "4364"}, {"sentence": "The former captures more distant REL compared to the latter.", "acronym": "REL", "label": "relations", "ID": "4365"}, {"sentence": "Thus, unary adjectives denote properties and binary adjec- tives denote REL.", "acronym": "REL", "label": "relations", "ID": "4366"}, {"sentence": "The passive char- acters were identified via the following REL extracted by dependency parsing: nsubjpass (passive nominal subject) and pobj (object of a preposition).", "acronym": "REL", "label": "relations", "ID": "4367"}, {"sentence": "However, although there is indeed a certain corre- lation between morphological class and semantic class, we claim that morphology is not sufficient for a reliable classification because it is by no means a one-to-one RELhip.", "acronym": "REL", "label": "relations", "ID": "4368"}, {"sentence": "these kinds of RELhips could be automatically extracted if information on the class were available.", "acronym": "REL", "label": "relations", "ID": "4369"}, {"sentence": "Rule 7 attempts REL disam- biguation when it finds the three tokens ?", "acronym": "REL", "label": "relative pronoun", "ID": "4370"}, {"sentence": "The CLOSESTLINK (Soon et al 2001) method picks the closest mention to mj that is positively classified, while the BESTLINK (Ng and Cardie, 2002) method links mj to the preced- Types Features String- Similarity mention string match, head string match, head substring match, head word pair, men- tion substring match, acronym Syntax number match, gender match, apposition, REL, mention type, modifier match, head word POS tags Semantic synonym, antonym, hypernym, modifier re- lations, both mentions are surrounded by a verb meaning ?", "acronym": "REL", "label": "relative pronoun", "ID": "4371"}, {"sentence": "Examples 1 and 2 show the representation that would be obtained for two imaginary English sen- 2Clause delimiters are punctuation marks other than com- mata, RELs and subordinating conjunctions.", "acronym": "REL", "label": "relative pronoun", "ID": "4372"}, {"sentence": "Another systematic error is faulty classification of RELs/determiners as wh-question pronouns/determiners, e.g., ?", "acronym": "REL", "label": "relative pronoun", "ID": "4373"}, {"sentence": "In our account, the REL which does not  specify an interclausal coherence link, and therefore  sentences (26-28) are parallel constructions.", "acronym": "REL", "label": "relative pronoun", "ID": "4374"}, {"sentence": "ker Ha, Response conditional la + future marker Ha, Jussive li, Preposition li, Preposition min, Future marker sa, Preposition ta, Particle wa, Preposition wa, Vocative wA, vocative yA Proclitic Article proclitic: No proclitic, Not applicable, Demonstrative particle Aa, Determiner, Determiner Al + negative particle mA, Negative particle lA, Negative particle mA, Negative particle mA, Particle mA, REL mA Enclitics Pronominals: No enclitic, Not applicable, 1st person plural/singular, 2nd person dual/plural, 2nd person fem- inine plural/singular, 2nd person masculine plural/singular, 3rd person dual/plural, 3rd person feminine plu- ral/singular, 3rd person masculine plural/singular, Vocative particle, Negative particle lA, Interrogative pronoun mA, Interrogative pronoun mA, Inter", "acronym": "REL", "label": "relative pronoun", "ID": "4375"}, {"sentence": "lgorithm helps  us find the REL probability of higher level  nodes as a lexical category for the frame element  through a bottom up computation of the inter-node  messages.", "acronym": "REL", "label": "relevance", "ID": "4376"}, {"sentence": "Therefore, we defined the REL metric for  the WordNet nodes to achieve a larger coverage.", "acronym": "REL", "label": "relevance", "ID": "4377"}, {"sentence": "1 0\\)1( 1101000,1 )()|()()( N Nk k NmNNPNPNm   We should note that based on the WordNet?s  hypernym relation, the conditional REL  probability of each parent node (given any child  node) is equal to 1.", "acronym": "REL", "label": "relevance", "ID": "4378"}, {"sentence": "For each of the WordNet nodes we  defined the REL of the node based on the  proportion of the occurrence of the node in IE related  Text (Orel) to the", "acronym": "REL", "label": "relevance", "ID": "4379"}, {"sentence": "For each of the  frame elements, we took the terms in FrameNet FE  annotations as ground truth which means that the  REL probability of the WordNet nodes for those  terms is equal to 1.", "acronym": "REL", "label": "relevance", "ID": "4380"}, {"sentence": "For each of the WordNet nodes we  defined the REL of the node based on the  proportion of the occurrence of the node in IE related  Text (Orel) to the occurrence of the node in the  general text (Ogen).", "acronym": "REL", "label": "relevance", "ID": "4381"}, {"sentence": "The Sum Product algorithm helps  us find the REL probability of higher level  nodes as a lexical category for the frame element  through a bottom up computation of the inter-node  messages.", "acronym": "REL", "label": "relevance", "ID": "4382"}, {"sentence": "1 0\\)1( 1101000,1 )()|()()( N Nk k NmNNPNPNm   We should note that based on the WordNet?s  hypernym relation, the conditional REL  probability of ea", "acronym": "REL", "label": "relevance", "ID": "4383"}, {"sentence": "gen rel O O Nl =)(Re   Using this REL metric, we evaluated all of  the WordNet nodes for the training data (found in the  previous step) and re-ranked and picked the top ?", "acronym": "REL", "label": "relevance", "ID": "4384"}, {"sentence": "An intermediate approach would be to  mark up a \"core\" set of RE", "acronym": "RE", "label": "referring expression", "ID": "4385"}, {"sentence": "An extreme alternative is to have a first pass  where only REs which look like  anaphors are marked up, such as pronouns, def-  inite NPs and reduced forms of proper names.", "acronym": "RE", "label": "referring expression", "ID": "4386"}, {"sentence": "An intermediate approach would be to  mark up a \"core\" set of REs on  the first pass, allowing for further eferring ex-  pressions to be identified on subsequent passes  if this is necessary to resolve coreference.", "acronym": "RE", "label": "referring expression", "ID": "4387"}, {"sentence": "e number of \"basic\" REs.", "acronym": "RE", "label": "referring expression", "ID": "4388"}, {"sentence": "In the case of (3) we still have no modal and we use two different but co-REs to refer to the same in- dividual.", "acronym": "RE", "label": "referring expression", "ID": "4389"}, {"sentence": "This relation can  be annotated by adding new tags for composite  REs, but it is obviously unde-  sirable to encode these tags in advance for every  possible combination ofreferents in a text, since  the number would increase xponentially with  the number of \"basic\" REs.", "acronym": "RE", "label": "referring expression", "ID": "4390"}, {"sentence": "The problem with this interpretation is that if it is indeed the case that different co-REs simply pick out different guises of the same individual, then a sentence like (6) should have a non-contradictory reading, while this seems not to be the case.", "acronym": "RE", "label": "referring expression", "ID": "4391"}, {"sentence": "An extreme alternative is to have a first pass  where only REs which look like  anaphors are marked up, such as pronouns, def-  inite NPs a", "acronym": "RE", "label": "referring expression", "ID": "4392"}, {"sentence": "An intermediate approach would be to  mark up a \"core\" set of REs on  the first pass, allowing for further eferring ex-  pressions to be identified", "acronym": "RE", "label": "referring expression", "ID": "4393"}, {"sentence": "The Use of MMR, Diversity-Based Reranking for RE Docu- ments and Producing Summaries.", "acronym": "RE", "label": "Reordering", "ID": "4394"}, {"sentence": "RE methods are effective, but need reliable parsers to ex- tract the syntactic structure of the source sentences.", "acronym": "RE", "label": "Reordering", "ID": "4395"}, {"sentence": "Tables KOR-CHN  baseline  KOR-CHN  modified  Word 12.39 MB 12.52 MB  Phrase 19.41 MB 19.04 MB  RE 10.01 MB 9.83 MB    Table 3.", "acronym": "RE", "label": "Reordering", "ID": "4396"}, {"sentence": "RE strategies that use syntactic infor- mation have proved successful, but they are likely to magnify parsing errors if their reordering rules heavily rely on abundant parse information.", "acronym": "RE", "label": "Reordering", "ID": "4397"}, {"sentence": "c?2013 Association for Computational Linguistics Anchor Graph: Global RE Contexts for Statistical Machine Translation Hendra Setiawan ?", "acronym": "RE", "label": "Reordering", "ID": "4398"}, {"sentence": "Thomson Reuters 3 Times Square NY 10036, USA Abstract RE poses one of the greatest chal- lenges in Statistical Machine Translation re- search as the key contextual information may well be beyond the c", "acronym": "RE", "label": "Reordering", "ID": "4399"}, {"sentence": "RE in- variable grammatical particles in our framework can be summarized as: i) Find dependents of a verbal head (Vb-H) whose POS tags are in the Oth-DEP entry of Table 1.", "acronym": "RE", "label": "Reordering", "ID": "4400"}, {"sentence": "It is well known that dependency informa- tion plays a key role in many NLP problems,  such as syntactic parsing, semantic role label- ing as well as semantic RE.", "acronym": "RE", "label": "relation extraction", "ID": "4401"}, {"sentence": "Indeed, the kernel-based methods have  been successfully applied to mine structured  information in various NLP applications like  syntactic parsing (Collins and Duffy, 2001;  Moschitti, 2004), semantic RE  (Zelenko et al, 2003; Zhao and Grishman,  2005; Zhou et al 2007; Qian et al, 2008), se- mantic role labeling (Moschitti, 2004); corefer- ence resolution (Yang et al, 2006; Zhou et al,  2008).", "acronym": "RE", "label": "relation extraction", "ID": "4402"}, {"sentence": "Tree kernel-based RE with con- text-sensitive structured parse tree information.", "acronym": "RE", "label": "relation extraction", "ID": "4403"}, {"sentence": "Kernel methods for RE.", "acronym": "RE", "label": "relation extraction", "ID": "4404"}, {"sentence": "Qian et al (2008)  dynamically determined the parse tree structure  for semantic RE by exploiting  constituent dependencies to keep the necessary  information in the parse tree as well as remove  the noisy information.", "acronym": "RE", "label": "relation extraction", "ID": "4405"}, {"sentence": "Dependency tree kernels for RE.", "acronym": "RE", "label": "relation extraction", "ID": "4406"}, {"sentence": "3 RE Performing a syntax-based NLP task in most real- world scenarios requires that the incoming data first be parsed using a pre-trained parsing model.", "acronym": "RE", "label": "Relation Extraction", "ID": "4407"}, {"sentence": ".0 42.6 21.3 28.4 Baseline-Ent 87.2 65.4 74.8 85.8 64.4 73.6 55.2 31.1 39.8 51.2 29.4 37.4 Oracle D-Parse 89.3 67.4 76.8 89.3 66.2 75.4 60.0 32.6 42.2 58.1 31.3 40.7 Hidden D-Parse 87.8 69.8 77.7 85.3 67.8 75.6 48.0 32.0 38.4 47.2 30.0 36.7 Oracle C-Parse 89.1 68.7 77.6 87.5 67.5 76.2 66.8 37.8 48.3 63.8 37.0 46.8 Hidden C-Parse 90.5 69.9 78.9 88.8 68.6 77.4 56.3 32.3 41.0 53.4 31.6 39.7 Table 1: RE Results.", "acronym": "RE", "label": "Relation Extraction", "ID": "4408"}, {"sentence": "4.2 Discourse RE System Our work follows the approach and features de- scribed in the state-of-the-art Ruby-based discourse system of (Lin et al, 2010), to build an in- house Java-based discourse relation extraction sys- tem.", "acronym": "RE", "label": "Relation Extraction", "ID": "4409"}, {"sentence": "RE for Drug-Drug Interactions using Ensemble Learn- ing Proceedings of the 1st Challenge task on Drug- Drug Interaction Extraction Md. Faisal Mahbub Chowdhury, Asma Ben Abacha, Alberto Lavelli, and Pierre Zweigenbau.", "acronym": "RE", "label": "Relation Extraction", "ID": "4410"}, {"sentence": "Dan Roth has written the first paper on formulating global NLP decisions as ILP  problems with his student Scott Yih, presented in CoNLL'04, and since then has worked  on further developing Constrained Conditional Models, on learning and inference issues  within this framework and on applying it to several NLP problems, including Semantic  Role Labeling, Information and RE and Transliteration.", "acronym": "RE", "label": "Relation Extraction", "ID": "4411"}, {"sentence": "We will show that this kind of Web- based RE requires different tech- niques than the state-of-the-art seed-based ap- proaches as it has to acquire information from the long-tail of the World Wide Web.", "acronym": "RE", "label": "Relation Extraction", "ID": "4412"}, {"sentence": "loglike infection infection 1 infection .38, aids .27, tract .18, infected .18, positive .17 Table 2: Example results; R = RE target English word for source French word print out the top 5 ranked words.", "acronym": "RE", "label": "rank of expected", "ID": "4413"}, {"sentence": "RE using convolution tree kernel expanded with entity features.", "acronym": "RE", "label": "Relation extraction", "ID": "4414"}, {"sentence": "RE is the task of identifying se- mantic relations between sets of entities in text (as 812 illustrated in Fig.", "acronym": "RE", "label": "Relation extraction", "ID": "4415"}, {"sentence": "RE using dependency parse trees.", "acronym": "RE", "label": "Relation extraction", "ID": "4416"}, {"sentence": "2.2 Supervised Relation Extraction RE can be naturally cast as a su- pervised classification problem.", "acronym": "RE", "label": "Relation extraction", "ID": "4417"}, {"sentence": "RE with matrix factorization and universal schemas.", "acronym": "RE", "label": "Relation extraction", "ID": "4418"}, {"sentence": "1.1 Learning RE The problem of inducing regular languages from positive and negative examples has been studied in the past, even outside the context of information extraction (Alquezar and Sanfeliu, 1994; Dupont, 1996; Firoiu et al, 1998; Garofalakis et al, 2000; Denis, 2001; Denis et al, 2004; Fernau, 2005; Galassi and Giordana, 2005; Bex et al, 2006).", "acronym": "RE", "label": "Regular Expressions", "ID": "4419"}, {"sentence": "Java RE.", "acronym": "RE", "label": "Regular Expressions", "ID": "4420"}, {"sentence": "We used RE to implement the  linguistic motivated patterns that check for the in- formation just mentioned in a part of speech tagged  corpus.", "acronym": "RE", "label": "Regular Expressions", "ID": "4421"}, {"sentence": "doc))[a-zA-Z]{2,3}))(/[?\\s]+){0,20}\\b Table 4: Sample RE Learned by ReLIE(R0: input regex; Rfinal: final regex learned; the parts of R0 modified by ReLIE and the corresponding parts in Rfinal are highlighted.)", "acronym": "RE", "label": "Regular Expressions", "ID": "4422"}, {"sentence": "4.1 RE     We use regular expression pattern to detect.errors  in words by using word weight (Wazn) and affixes.", "acronym": "RE", "label": "Regular Expressions", "ID": "4423"}, {"sentence": "Clustered Lan- guage Models Based on RE for  SMT, Proc.", "acronym": "RE", "label": "Regular Expressions", "ID": "4424"}, {"sentence": "Morpho-syntactic filters describe general  term formation patterns, and are imple- mented as generic REs.", "acronym": "RE", "label": "regular expression", "ID": "4425"}, {"sentence": "The product of the RE or automaton  produced by GEN with all of the constraints in order pro-  ducts a transducer encoding the harmony evaluations of  all candidates.", "acronym": "RE", "label": "regular expression", "ID": "4426"}, {"sentence": "As an example, (5) shows a RE giving a  subset of the candidate syllabifications of alqalamu accor-  cling to the syllabification rules of P&S(p25).", "acronym": "RE", "label": "regular expression", "ID": "4427"}, {"sentence": "In another  approach, she used RE patterns to  extract term collocations from a morpho- syntactically tagged corpus.", "acronym": "RE", "label": "regular expression", "ID": "4428"}, {"sentence": "For brevity, then, the algorithms will  be phrased in terms of the states and arcs of an automa-  ton, while, for clarity, REs will be used to  present the inputs and outputs of examples.", "acronym": "RE", "label": "regular expression", "ID": "4429"}, {"sentence": "Adj | Noun)* Noun     Although these patterns are REs,  the filters are implemented as unification-like  LR(1) rules (Mima et al, 1995) in order to facili- tate processing of grammatical agreements (if  any) within term candidates.", "acronym": "RE", "label": "regular expression", "ID": "4430"}, {"sentence": "There are four possible operations (Right, Left,  Shift and RE) for the configuration at hand.", "acronym": "RE", "label": "Reduce", "ID": "4431"}, {"sentence": "RE B to the cores of C1 and C2.", "acronym": "RE", "label": "Reduce", "ID": "4432"}, {"sentence": "RE: If there is no word 'n  ( In ?' )", "acronym": "RE", "label": "Reduce", "ID": "4433"}, {"sentence": "The sentences were extracted from Arabic News (LDC2004T17), eTIRR (LDC2004E72), English translation of Arabic Treebank (LDC2005E46), and Ummah (LDC2004T18).4 For Arabic pre- processing, we follow previously reported best to- kenization scheme (TB)5 and orthographic word normalization condition (REd) when translat- ing from English to Arabic (El Kholy and Habash, 2010b).", "acronym": "RE", "label": "Reduce", "ID": "4434"}, {"sentence": "RE: Pop TOP from the stack.", "acronym": "RE", "label": "Reduce", "ID": "4435"}, {"sentence": "which may  depend on t, and t has a parent on its left side, the  parser removes t from the stack S.  Shift: If there is no dependency between n and t,  and the triple does not satisfy the conditions for  RE, then push n onto the stack S.  In this work, we adopt SVMs for estimating the  word dependency attachments.", "acronym": "RE", "label": "Reduce", "ID": "4436"}, {"sentence": "RE S to the separating synsets for {C1,C2}.", "acronym": "RE", "label": "Reduce", "ID": "4437"}, {"sentence": "In J. Carroll, N. Oostdijk, and R. Sutcliffe, editors, Proceedings of the Workshop on Grammar EG and Evalua- tion, COLING 19, pages 8?14, Taipei, Taiwan.", "acronym": "EG", "label": "Engineering", "ID": "4438"}, {"sentence": "Natural Lan- guage EG, 7(3):207?223.", "acronym": "EG", "label": "Engineering", "ID": "4439"}, {"sentence": "Knowledge and Data EG, 19(3):370?383.", "acronym": "EG", "label": "Engineering", "ID": "4440"}, {"sentence": "In J. Carroll, N. Oostdijk, and R. Sutcliffe, editors, Pro- ceedings of the Workshop on Grammar EG and Evaluation at COLING 19, pages 1?7.", "acronym": "EG", "label": "Engineering", "ID": "4441"}, {"sentence": "In Proceedings of the Workshop on Grammar EG and Evalua- tion, COLING 19, Taipei, Taiwan.", "acronym": "EG", "label": "Engineering", "ID": "4442"}, {"sentence": "ACL Workshop Incremental Parsing: Bringing EG and Cognition Together, pages 42?49, Barcelona, Spain.", "acronym": "EG", "label": "Engineering", "ID": "4443"}, {"sentence": "For EG:  Mar ie  Giraud carries historical sig-  nificance as one of the last women to  be ezecuted in France.", "acronym": "EG", "label": "example", "ID": "4444"}, {"sentence": "Here we would compare the degree to which  each possible candidate antecedent (A Japanese  company, television picture tubes, Japan, TV   sets, and Malaysia in this EG) could serve  as the direct object of \"export\".", "acronym": "EG", "label": "example", "ID": "4445"}, {"sentence": "A canonical EG of selectional  restriction is that of the verb \"eat\", which se-  lects food as its direct object.", "acronym": "EG", "label": "example", "ID": "4446"}, {"sentence": "However a singular noun phrase can be the ref-  erent of a plural pronoun, as illustrated by the  following EG:  \"I think if I tell Viacom I need more  time, they will take 'Cosby' across the  street,\" says the general manager ol a  network a~liate.", "acronym": "EG", "label": "example", "ID": "4447"}, {"sentence": "For EG:  A Japanese company might make tele-  vision picture tubes in Japan, assem-  ble the TV  sets in Malaysia and extort  them to I", "acronym": "EG", "label": "example", "ID": "4448"}, {"sentence": "For EG:  A Japanese company might make tele-  vision picture tubes in Japan, assem-  ble the TV  sets in Malaysia and extort  them to Indonesia.", "acronym": "EG", "label": "example", "ID": "4449"}, {"sentence": "3.1.1 The  ment ion  count  stat ist ics   The referents range from being mentioned only  once to begin mentioned 120 times in the train-  hag EGs.", "acronym": "EG", "label": "example", "ID": "4450"}, {"sentence": "The set of potential edges was pruned using the marginals produced by a first-order parser trained using EG descent (Collins et al, 2008) as in Koo and Collins (2010).", "acronym": "EG", "label": "exponentiated gradient", "ID": "4451"}, {"sentence": "t ewpos?fpos(x,t) We use the same feature set fpos defined in Sec- tion 2.1, and adopt the EG algo- rithm to learn the weight vector wpos (Collins et al, 2008).", "acronym": "EG", "label": "exponentiated gradient", "ID": "4452"}, {"sentence": "Matrix EG updates for on- line learning and bregman projection.", "acronym": "EG", "label": "exponentiated gradient", "ID": "4453"}, {"sentence": "EG has a more erratic be- haviour, and requires a careful tuning of ?", "acronym": "EG", "label": "exponentiated gradient", "ID": "4454"}, {"sentence": "Applying an EG rerank- ing algorithm (Bartlett et al, 2004) to the n-best out- put of our morphologically-informed Spanish pars- ing model gives us similar improvements.", "acronym": "EG", "label": "exponentiated gradient", "ID": "4455"}, {"sentence": "Some of the proposed algorithms, such as EG corresponds to block- coordinate descent in the dual, and uses the exact gradient with respect to the block being updated.", "acronym": "EG", "label": "exponentiated gradient", "ID": "4456"}, {"sentence": "In Proceedings of MEDAR International Conference on Arabic Language Resources and Tools, Cairo, EG.", "acronym": "EG", "label": "Egypt", "ID": "4457"}, {"sentence": "Texts included in  ArabiCorpus almost exclusively belong to the  written genre, save for a small sub-corpus of  spoken EGian Arabic.", "acronym": "EG", "label": "Egypt", "ID": "4458"}, {"sentence": "In Pro- ceedings of the 2nd International Conference  on Arabic Language Resources and Tools  (MEDAR), pages 102?109, Cairo, EG.", "acronym": "EG", "label": "Egypt", "ID": "4459"}, {"sentence": "In Proceedings of the 2nd International Con- ference on Arabic Language Resources and Tools (MEDAR), Cairo, EG.", "acronym": "EG", "label": "Egypt", "ID": "4460"}, {"sentence": "News articles included in  this sub-section of ArabiCorpus cover issues  from 1996 to 2010 and are extracted from peri- odicals published in different parts of the Arab  world (North Africa, EG, Arabian Gulf, the  Levant, etc.).", "acronym": "EG", "label": "Egypt", "ID": "4461"}, {"sentence": "In NEMLAR Conference on Arabic Language Resources and Tools, Cairo, EG.", "acronym": "EG", "label": "Egypt", "ID": "4462"}, {"sentence": "3 Relaxed EG In this section, we relax the troublesome assump- tion of independence between entities, thus mov- ing the probability distribution over documents away from blank sentences.", "acronym": "EG", "label": "Entity Grid", "ID": "4463"}, {"sentence": "2 The EG Model Barzilay and Lapata (2005; 2008) introduced the entity grid, a method for local coherence modeling that captures the distribution of discourse entities across sentences in a text.", "acronym": "EG", "label": "Entity Grid", "ID": "4464"}, {"sentence": "Naive EG .17 81 Relaxed EG .02 87 Topic-based (naive) .39 85 Topic-based (relaxed) .54 96 Table 2: Results for 10-fold cross-validation on AIR- PLANE training data.", "acronym": "EG", "label": "Entity Grid", "ID": "4465"}, {"sentence": "2 Naive EGs Entity grids, first described in (Lapata and Barzilay, 2005), are designed to capture some ideas of Cen- tering Theory (Grosz et al, 1995), namely that ad- jacent utterances in a locally coherent discourses are likely to contain the same nouns, and that important nouns often appear in syntactically important roles such as subject or object.", "acronym": "EG", "label": "Entity Grid", "ID": "4466"}, {"sentence": "In their EG model, a text is represented by a matrix with rows corresponding to each sen- tence in a text, and columns to each entity men- tioned anywhere in the text.", "acronym": "EG", "label": "Entity Grid", "ID": "4467"}, {"sentence": "As ~ CHAs, the points in M~ move continuously.", "acronym": "CHA", "label": "ehange", "ID": "4468"}, {"sentence": "stylistic wu'iants or the typical versions which can be used  interCHA:tbly in any c(mtexl \\[l~rguvanh 1979\\].", "acronym": "CHA", "label": "ehange", "ID": "4469"}, {"sentence": "eaning a t   t h e  k i n d 8  of u t tQrancer  t h a t  occur i n  rpaatanooua dialog, When  tnr detinltianr of the  grammar R t a V i d ~  tnformrtfon t h a t  h t l ~ s  a  prtrcr ehaora  these rule8 ao8z likely t o  l a a d  td correct   int@tpretatlonr of uttrrtanCs&, t h e  grrar@ar 1 8  r a i d  t o  be 9tunrd*,  Uhan t h e  tuning 1.8 crarily chrncraa wnrn t h r  dornrin o t  dircaurrr,   CHAs, thc grasmrr 1 6  s a i d  t o  br  *tunrrblr#.", "acronym": "CHA", "label": "ehange", "ID": "4470"}, {"sentence": "In January  1994, the TEl isstled its Guidelines for the Fmcoding and  hiterCHA of Machine-Readable Texts, which provide  standardized encoding conventions for a large range of  text types and features relevant for a broad range of  applications.", "acronym": "CHA", "label": "ehange", "ID": "4471"}, {"sentence": "Term Variant Type  ECHA d'ion (ion exchange)  Culture de eellules (cell culture)  Propridtd chimique  (chemical property)  Gestion d ' eau (water management)  Eau de surface  (surface water)  Huile de palme (palm oil)  Initiation de bourgeon  (bud initiation)  dchange ionique (ionic exchange) N to A  cultures primaires de cellules (primary cell cultures) Modif.", "acronym": "CHA", "label": "ehange", "ID": "4472"}, {"sentence": "E-dictionaries contain  exhaustive description of morpho-syntactic  CHA and are used for lexical  recognition and initial lemmatisation of words  that occur in a text.", "acronym": "CHA", "label": "characteristics", "ID": "4473"}, {"sentence": "We compare the results with a set of ad- jectives classified by human judges according to se- mantic CHA.", "acronym": "CHA", "label": "characteristics", "ID": "4474"}, {"sentence": "Among approaches to multilingual grammar engineering, the Grammar Matrix?s distinguishing CHA include the deployment of a shared core grammar for crosslinguistically consistent con- straints and a series of libraries modeling vary- ing linguistic properties.", "acronym": "CHA", "label": "characteristics", "ID": "4475"}, {"sentence": "They received instructions which referred only to semantic CHA, not to the expected syn- tactic behaviour.", "acronym": "CHA", "label": "characteristics", "ID": "4476"}, {"sentence": "Or they  can pursue a multi-dimensional inquiry, for  instance, the relations between a theory in physics  and the broader context of knowledge and action,  including identifiers uch as aesthetic value,  dominant cognitive CHA, the state of  technology, etc.", "acronym": "CHA", "label": "characteristics", "ID": "4477"}, {"sentence": "Sec- tion 3 presents desirable CHA of restricted domains for the development of NLP research in general, and question answering in particular.", "acronym": "CHA", "label": "characteristics", "ID": "4478"}, {"sentence": "We are also working  4Our approach allows morn than a single COP to appear at a  given level, however, as the ditransitive VP rule above shows.", "acronym": "COP", "label": "complement constituent", "ID": "4479"}, {"sentence": "Furthermore, the CLaC system will choose to trigger the subject passive focus heuristic in the case where the verb COP is empty, and the passive noun subject is present.", "acronym": "COP", "label": "complement constituent", "ID": "4480"}, {"sentence": "NegFocus will only make this assumption when the verb COP is empty, otherwise the baseline focus heuristic will be triggered, as depicted in Example (16).", "acronym": "COP", "label": "complement constituent", "ID": "4481"}, {"sentence": "Since we have only one  entry for a verb, then any semantic differences that are associated with variant subcat-  egorizations will have to be built from the COPs in a completely  compositional way.", "acronym": "COP", "label": "complement constituent", "ID": "4482"}, {"sentence": "The authors claim an advantage over chunker-based approaches with respect to the an- notation of markable adjectives due to the fact that the dependency relation between COP verb and predicative adjective is available.", "acronym": "COP", "label": "copula", "ID": "4483"}, {"sentence": "The COP itself constitutes the predicate.", "acronym": "COP", "label": "copula", "ID": "4484"}, {"sentence": "2.2 Unary vs. binary Unary adjectives have only one argument, usu- ally corresponding to the modified noun (a red ball \u0001\u0003\u0002\u0005\u0004\u0007\u0006 ) or the subject in a COPr sentence (this ball \u0001\u0003\u0002\b\u0004\u0007\u0006 is red).", "acronym": "COP", "label": "copula", "ID": "4485"}, {"sentence": "For example, if the preceding verb is a COP, the adjective is flagged as a markable.", "acronym": "COP", "label": "copula", "ID": "4486"}, {"sentence": "In English, the COPr verb is considered the syntactic head of the clause, with the pronoun being the subject and the predicate adjective be- ing an XCOMP.", "acronym": "COP", "label": "copula", "ID": "4487"}, {"sentence": "Verbs which express a connection between subjects and  objects are referred to as COPs, e.g. \"is\" in \"John is a boy\".", "acronym": "COP", "label": "copula", "ID": "4488"}, {"sentence": "The other main function of the adjective is that of predicate in a COPr sentence (6% of the tokens).", "acronym": "COP", "label": "copula", "ID": "4489"}, {"sentence": "A constraint that de- scribes this individual word pair would be trivial to write, but it is not feasible to model the general phenomenon in this way; thousands of constraints would be needed just to reflect the more impor- tant COs in a language, and the exact set of collocating words is impossible to predict ac- curately.", "acronym": "CO", "label": "collocation", "ID": "4490"}, {"sentence": "In another  approach, she used regular expression patterns to  extract term COs from a morpho- syntactically tagged corpus.", "acronym": "CO", "label": "collocation", "ID": "4491"}, {"sentence": "The  statistical approach was based on the mutual ex- pectation and LocalMax measures, and involved  CO extraction from raw text.", "acronym": "CO", "label": "collocation", "ID": "4492"}, {"sentence": "The ex- tracted COs were filtered with a stop- word list, and only COs containing sin- gle-word terms (devised previously by bilingual  alignment) were accepted as relevant.", "acronym": "CO", "label": "collocation", "ID": "4493"}, {"sentence": "atistical approach was based on the mutual ex- pectation and LocalMax measures, and involved  CO extraction from raw text.", "acronym": "CO", "label": "collocation", "ID": "4494"}, {"sentence": "For ex- ample, Vintar (2000) presented two methods for  extraction of terminological COs in order  to assist the translation process in Slovene.", "acronym": "CO", "label": "collocation", "ID": "4495"}, {"sentence": "This web- site represents an interactive platform that allows  people to share personal experiences, thoughts,  opinions, feelings, passions, and CO  through the network of personal stories.", "acronym": "CO", "label": "confessions", "ID": "4496"}, {"sentence": "n Q1 R2 comaker, imported, deceitful, huston, send, bright, remainderman?s Topic 2 in Q1 R1 descendent, younger, administrator?s, documentary, agreeable, emancipated Topic 2 in Q1 R2 younger, administrator?s, grandmother?s, plaintiffs, emancipated, learnedly Topic 3 in Q2 R1 heir-at-law, reconsidered, manumissions, birthplace, mon, mother-in-law Topic 3 in Q2 R2 heir-at-law, reconsideration, mon, CO, birthplace, father-in-law?s Topic 4 in Q2 R1 indentured, apprenticed, deputy collector, stepfather?s, traded, seizes Topic 4 in Q2 R2 deputy collector, seizes, traded, hiring, stepfather?s, indentured, teaching Topic 5 in Q4 R1 constitutionality, constitutional, unconstitutionally, Federal Army, violated Topic 5 in Q4 R2 petition, convictions, criminal court, murdered, constitutionali", "acronym": "CO", "label": "confessions", "ID": "4497"}, {"sentence": "(In)accuracy at detecting true and false CO and denials: An initial test of a projected motive model of veracity judgments.", "acronym": "CO", "label": "confessions", "ID": "4498"}, {"sentence": "In  order to evaluate the performance of our algo- rithm, we created the data set of sentences ex- tracted from personal stories about life expe- riences that were anonymously published on the  Experience Project website  (www.experienceproject.com), where  people share personal experiences, thoughts,  opinions, feelings, passions, and CO  through the network of personal stories.", "acronym": "CO", "label": "confessions", "ID": "4499"}, {"sentence": "The technique was adapted for author identifi-  cation by Morton (1978) - see also Farringdon (1996)  - and achieved some notoriety for its use in court cases  (e.g. to identify faked or coerced CO) as well as  in literary studies.", "acronym": "CO", "label": "confessions", "ID": "4500"}, {"sentence": "We also evaluate the model?s ability to predict sentiment distributions on a new dataset based on CO from the experience project.", "acronym": "CO", "label": "confessions", "ID": "4501"}, {"sentence": "What we get is  5.1 ; (A  ~. > I;((;))  ()ur counterlhctual becomes,intuitively, \"You,the  opponent,will fail in showing that C fails, aftcr A  has been added to the COs\".", "acronym": "CO", "label": "concession", "ID": "4502"}, {"sentence": "and in (5), as a CO for failing to do the  action (washing the dishes) denied by (a).", "acronym": "CO", "label": "concession", "ID": "4503"}, {"sentence": "f negation  which is even weaker than that of minimal  calculus,and the introduction of an opportuuity for  the opponent  to make us(\" of his own conccssiot~s  as exception rules+ The second of these is easily  etlk'ctuated: by inlroducing flK+ fail- operator twice  we cause a first change of rolcs which gives the op-  ponent, tmw as a ternporary proponent, the op+  portunity to britlg additional COs into play,  The second fail operator then restores the initial  order of roles.", "acronym": "CO", "label": "concession", "ID": "4504"}, {"sentence": "Suppose your adversary  accepts the invitation to discuss, and takes the  antecedent of the counterfactual s a temporary  additional CO.", "acronym": "CO", "label": "concession", "ID": "4505"}, {"sentence": "r twice  we cause a first change of rolcs which gives the op-  ponent, tmw as a ternporary proponent, the op+  portunity to britlg additional COs into play,  The second fail operator then restores the initial  order of roles.", "acronym": "CO", "label": "concession", "ID": "4506"}, {"sentence": "- -B -> -~A = yes  min  Suppose we add to tile COs A and B a  CO to the effect that stepping on tile brake  is the only reason for Jones' being alive (--, B ->  -, A) .", "acronym": "CO", "label": "concession", "ID": "4507"}, {"sentence": "Steedman et al(2003b) and Hwa et al (2003) also used several versions of cor- rected CO which are not comparable to ours and other suggested methods because their evalua- tion requires different measures (e.g. reviewed and corrected constituents are separately counted).", "acronym": "CO", "label": "co-training", "ID": "4508"}, {"sentence": "Moreover, CO model (Bian et al.,", "acronym": "CO", "label": "co-training", "ID": "4509"}, {"sentence": "Table 2 compares our results with self-training and CO results reported by (Steedman et al 20003a; 2003b).", "acronym": "CO", "label": "co-training", "ID": "4510"}, {"sentence": "pars- ing model using a small seed dataset (500 sentences for both methods and 1,000 sentences for CO only).", "acronym": "CO", "label": "co-training", "ID": "4511"}, {"sentence": "2003a), which applied CO (Blum and Mitchell, 1998) and self-training to Collins?", "acronym": "CO", "label": "co-training", "ID": "4512"}, {"sentence": "Steedman et al (2003b) followed a similar CO protocol except that the selection function (three functions were explored) considered the differences between the confidence scores of the two parsers.", "acronym": "CO", "label": "co-training", "ID": "4513"}, {"sentence": "~he f i r s t  conta ins  an unreso lved  reverence  to  some CO!", "acronym": "CO", "label": "country", "ID": "4514"}, {"sentence": "E.g, if one here selects the option of having the results presented by CO of origin of the hit texts, one is not presented directly with the KWIC list of results, but rather with a bar representation of the number of hits per CO.", "acronym": "CO", "label": "country", "ID": "4515"}, {"sentence": "Another example:  \"President and wife came to capital\"  (Articles and pronouns ere dropped to reflect Russian),  This phrase is processed as  \"President roof-CO with xhis wife came to capital Eof-  CO\".", "acronym": "CO", "label": "country", "ID": "4516"}, {"sentence": "For example, these queries im- ply that Afghanistan is a CO, Netscape is a web browser, Bushwackers are shoes, and BidFind is an auction search engine.", "acronym": "CO", "label": "country", "ID": "4517"}, {"sentence": "Nouns that are readily converted are marked as ei- ther strongly countable (for countable nouns that can be converted to uncountable, such as cake) or weakly countable (for uncountable nouns that are readily CO to countable, such as beer).", "acronym": "CO", "label": "convertible", "ID": "4518"}, {"sentence": "The format should be easily CO to the specific formats required by statistical packages such as R or SPSS.", "acronym": "CO", "label": "convertible", "ID": "4519"}, {"sentence": "Additionally, META?s forward index (used for classifica- tion), is easily CO to LIBSVM format.", "acronym": "CO", "label": "convertible", "ID": "4520"}, {"sentence": "Human: IMG_6892 Lookn up in the sky its a  bird its a plane its ah..... you  ILP: This is a sporty little red CO made for  a great day in Key West FL.", "acronym": "CO", "label": "convertible", "ID": "4521"}, {"sentence": ",Oct,... 3,A1,C1,C3,... Trading,Interest,Demand,Production,... 30 Treasury,mortgage-backed 50,1.50,0.50,0.05,... York,York-based age,identity,integrity,identification,... bond,floating-rate books,words,budgets,clothes,... consumer CO,bonus,Brady,subordinated,... increase interest loss months no-fly,year-ago,corresponding,buffer,... people quarter results rose there Figure 1: The bipartite graphs show the top 40 non-stopword Brown cluster pair features for all four classification tasks.", "acronym": "CO", "label": "convertible", "ID": "4522"}, {"sentence": "b~ / \\ pit head(p~ol>body) ~ \"\\\\  / ,~  corner /  goal i~cl>thing) \\[.41 obj mod \\[  ~left :  Figm'e I. 1: A UNL graph deCO as \"Ronaldo  has headed the ball into the left corner of the net\"  In a UNL graph, UWs appear with attributes  describing what is said from tile speaker's point  of view.", "acronym": "CO", "label": "convertible", "ID": "4523"}, {"sentence": "2.4 Agreement in ACs The present honorification system in the KPSG can offer us a streamlined way of explaining the agreement in auxiliary verb constructions we noted in section 1.1.", "acronym": "AC", "label": "Auxiliary Construction", "ID": "4524"}, {"sentence": "NN plan (.03, .9), offer (.2, .74), issues (0, 0), increase (.34, .66), end (.18, .81) 17 115 DT, NNP As (0, 0), One (0, .01), First (0, .82), Big (0, .91), On (0, .01) 18 11 NN, JJ market (.99, 0), U.S. (0, 0), bank (1, 0), cash (.98, 0), high (.06, .9) 20 22 VBN, JJ estimated (.58, .15), lost (.43, .03), failed (.35, .04), related (.74, .23), re- duced (.57, .12) Table 3: Selection of Predicted AC: Common ambiguity classes from the predicted part-of-speech assignments from the WSJ data set, and the five most common word types associated with each ambiguity class.", "acronym": "AC", "label": "Ambiguity Classes", "ID": "4525"}, {"sentence": "Elles  permettent de localiser la plupart des contraiutes syn-  taxiques (par exemple, sujet-VT, VT-objet) tout  en ddcrivant la syntaxe sous forme d'arbres.", "acronym": "VT", "label": "verbe", "ID": "4526"}, {"sentence": "In M. Loflin  and J. SilVTrg, eds.,", "acronym": "VT", "label": "verbe", "ID": "4527"}, {"sentence": "oVTaring pride evi- denced by a superior manner toward the weak?.", "acronym": "VT", "label": "verbe", "ID": "4528"}, {"sentence": "Grammaire transformation-  nelle du frangais: le VT, Paris, Larousse  II. - -", "acronym": "VT", "label": "verbe", "ID": "4529"}, {"sentence": "Morpholoo  gie et ggndration des VTs fran~ais.", "acronym": "VT", "label": "verbe", "ID": "4530"}, {"sentence": "1998:  VT: A Model of Global Discourse Cohe- sion and Coherence, in Proceedings of the 17th in- ternational conference on Computational linguis- tics.", "acronym": "VT", "label": "Veins Theory", "ID": "4531"}, {"sentence": "Formal proofs in Incremental  Discourse Processing and VT, Research  Report TR98-2 Dept.", "acronym": "VT", "label": "Veins Theory", "ID": "4532"}, {"sentence": "Following VT, the predecessor of approve(fda, elixirplus) is ban(fda, elixir); its linear predecessor contain(elixir, gestodene) (an embedded satellite) is inaccessible.", "acronym": "VT", "label": "Veins Theory", "ID": "4533"}, {"sentence": "It is worth noting that Tetreault (2005) has employed Grosz and Sid- ner?s (1986) discourse theory and VT (Ide and Cristea, 2000) to identify and remove candidate antecedents that are not referentially ac- cessible to an anaphoric pronoun in his heuristic pronoun resolvers.", "acronym": "VT", "label": "Veins Theory", "ID": "4534"}, {"sentence": "At each level, the parser goes on with a forest  of developing trees in parallel, ranking them by a  global score (Figure 6) based on heuristics that  are suggested by both VT (Cristea et  al.,", "acronym": "VT", "label": "Veins Theory", "ID": "4535"}, {"sentence": "184  VT:  A Model of Global Discourse Cohesion and Coherence  Dan CRISTEA  Dept.", "acronym": "VT", "label": "Veins Theory", "ID": "4536"}, {"sentence": "AZ: Information Extraction from Scientific Text.", "acronym": "AZ", "label": "Argumentative Zoning", "ID": "4537"}, {"sentence": "AZ for Im- proved Citation Indexing.", "acronym": "AZ", "label": "Argumentative Zoning", "ID": "4538"}, {"sentence": "AZ: Infor- mation Extraction from Scientific Text.", "acronym": "AZ", "label": "Argumentative Zoning", "ID": "4539"}, {"sentence": "We examine the features used to achieve this result and experiment with AZ as a sequence tag- ging task, decoded with Viterbi using up to four previous classification decisions.", "acronym": "AZ", "label": "Argumentative Zoning", "ID": "4540"}, {"sentence": "c?2009 ACL and AFNLP Accurate AZ with Maximum Entropy models Stephen Merity and Tara Murphy and James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia {smerity,tm,james}@it.usyd.edu.au Abstract We present a maximum entropy classifier that significantly improves the accuracy of AZ in scientific litera- ture.", "acronym": "AZ", "label": "Argumentative Zoning", "ID": "4541"}, {"sentence": "Ac- curate AZ with maximum entropy models.", "acronym": "AZ", "label": "argumentative zoning", "ID": "4542"}, {"sentence": "To- wards discipline-independent AZ: Evidence from chemistry and computational linguis- tics.", "acronym": "AZ", "label": "argumentative zoning", "ID": "4543"}, {"sentence": "In our ex- periments that treat AZ as a se- quence labelling task, the context xi incorporates history information ?", "acronym": "AZ", "label": "argumentative zoning", "ID": "4544"}, {"sentence": "To- wards domain-independent AZ: Ev- idence from chemistry and computational linguistics.", "acronym": "AZ", "label": "argumentative zoning", "ID": "4545"}, {"sentence": "Accurate AZ with maximum entropy models.", "acronym": "AZ", "label": "argumentative zoning", "ID": "4546"}, {"sentence": "Proceedings of the TREC-7, pp.", "acronym": "TREC-7", "label": "Seventh Text REtrieval Conference", "ID": "4547"}, {"sentence": "In The TREC-7.", "acronym": "TREC-7", "label": "Seventh Text Retrieval Conference", "ID": "4548"}, {"sentence": "c?2016 Association for Computational Linguistics Insertion Position Selection Model for Flexible Non-Terminals in Dependency Tree-to-Tree Machine Translation Toshiaki Nakazawa JST Agency 5-3, Yonbancho, Chiyoda-ku, Tokyo, 102-8666, Japan nakazawa@pa.jst.jp John Richardson and Sadao Kurohashi Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto, 606-8501, Japan john@nlp.ist.i.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp Abstract Dependency tree-to-tree translation models are powerful because they can naturally han- dle long range reorderings which is importa", "acronym": "JST", "label": "Japan Science and Technology", "ID": "4549"}, {"sentence": "Acknowledgments The research presented in this paper was partly funded by PREST, JST Corporation.", "acronym": "JST", "label": "Japan Science and Technology", "ID": "4550"}, {"sentence": "PSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics University of Manchester Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, University of Manchester SORST, JST Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremely lexi- calized probabilistic model for fast and accurate HPSG parsing.", "acronym": "JST", "label": "Japan Science and Technology", "ID": "4551"}, {"sentence": "The JST Agency Dictionary,? (", "acronym": "JST", "label": "Japan Science and Technology", "ID": "4552"}, {"sentence": "c?2005 Association for Computational Linguistics A Machine Learning Approach to Acronym Generation Yoshimasa Tsuruoka \u0000\u0002\u0001 \u0000 CREST JST Agency Japan Sophia Ananiadou School of Computing Salford University United Kingdom tsuruoka@is.s.u-tokyo.ac.jp S.Ananiadou@salford.ac.uk tsujii@is.s.u-tokyo.ac.jp Jun?ichi Tsujii \u0001\u0003\u0000 \u0001 Department of Computer Science The University of Tokyo Japan Abstract This paper presents a machine learning approach to acronym generation.", "acronym": "JST", "label": "Japan Science and Technology", "ID": "4553"}, {"sentence": "CREST, JST Corporation #Department of Complexity Science and Engineering, University of Tokyo?", "acronym": "JST", "label": "Japan Science and Technology", "ID": "4554"}, {"sentence": "JST: Let me make one thing perfectly clear.", "acronym": "JST", "label": "JUSTIFICATION", "ID": "4555"}, {"sentence": "On the basis of the data derived from the corpus ,anal-  ysis, the algorithm hypothesizes the following set of re-  lations between the textual units:  rhet_rel(JST, 1 2) V  rhet..rel(CONDITION, 1,2)  rhet_rel(ELABORATION, 3, \\[1,2\\]) V  rhet_reI(ELABORATION, \\[3, 6\\], \\[ 1,2\\])  rhet_rel(El_ABOgATlON, \\[4, 6\\], 3) V  rhet_ret(ELABOr~YlON, \\[4  6\\], \\[1, 3\\])  rhet_rel(CONTRAST, 4, 5)  (4) rhet_rel(EVIDENCE, 6, 5)  rhet_reI(ELABORATION, \\[7, 10\\], \\[1,6\\])  rhet_rel(CONCESSION, 7, 8)  rhet_rel(EXAMPLE, 9 \\[7, 8\\]) V  rhet_rel(EXAMPLE, \\[9, 10\\],", "acronym": "JST", "label": "JUSTIFICATION", "ID": "4556"}, {"sentence": "This  hypothesis is consistent with the information given in Table 6, which shows that, in  the corpus, the marker With consistently signaled BACKGROUND and JST  relations between a satellite, the unit that contained the marker, and a nucleus, the  unit that followed it.", "acronym": "JST", "label": "JUSTIFICATION", "ID": "4557"}, {"sentence": "es 17 0 0 0 0 0 17 0 * incident-location 91 22 9 6 7 0 69 13 54 0 phys-effects 29 3 1 1 1 0 26 5 50 0 human-effects 28 1 0 0 0 1 28 0 0 10 0 MATCHED ONLY 382 284 110 20 55 99 197 31 42 3 5 MATCHED/MISSING 1117 284 110 20 55 99 932 11 42 3 5 ALL TEMPLATES 1117 665 110 20 55 480 932 11 18 7 2 SET FILLS ONLY 463 94 47 5 26 16 385 11 53 17 Table 3 .0 : \t TST1-MUC3 rescored after Phase II developmen t JST AND ANALYSIS OF SCORE S Although the above stated scores seem rather discouraging or low, there are several valid justifications for suc h occurrences.", "acronym": "JST", "label": "JUSTIFICATION", "ID": "4558"}, {"sentence": "The original PDTB sense tags for meanwhile and as were respectively <COMPARISONCONTRASTJUXTAPOSITION> and <CONTINGENCYPRAGMATICCAUSE- JST>, where JUXTAPOSITION and JST were dropped because they stem from the third level of the PDTB sense hierarchy: 1.", "acronym": "JST", "label": "JUSTIFICATION", "ID": "4559"}, {"sentence": "Rule  when so-series demonst ra t ive  adverb   cataphor ica l ly  Refers  to  the  Verb  Phrase   in the  SS   Candidate numerating rule 10  When an anaphor is \"sou/soushite/sonoyouni\" and is  in the subordinate clause which has a conjunctive par-  ticle such as \"9a\", \"daga \", and \"keredo \"or an adjective  conjunction such as \"youni\",  {(the main clause, 45)}  4 Heur i s t i c  Ru le  fo r  Persona l  P ronouns   Candidate numerating rule 1  When an anaphor is a first personal pronoun,  {(t", "acronym": "SS", "label": "Same Sentence", "ID": "4560"}, {"sentence": "LiveJournal Blogspot JDPA ACE Tokens Per Sentence 19.2 18.6 16.5 19.7 Relations Per Sentence 1.08 1.71 2.56 0.56 Relations Not In SS 33% 30% 27% 0% Training Mention Pairs in One Sentence 58,452 54,480 95,630 77,572 Mentions Per Sentence 4.26 4.32 4.03 3.16 Mentions Per Entity 1.73 1.63 1.33 2.36 Mentions With Only One Token 77.3% 73.2% 61.2% 56.2% Table 2: Selected document statistics for three JDPA Corpus document sources.", "acronym": "SS", "label": "Same Sentence", "ID": "4561"}, {"sentence": "0 conserve_VVIB~.T.~ \\[nbarl \\[nla space_NNIMEASURE nla\\] nbarlJ v40\\]  and_CCAND Iv40 reduce_WIALTER \\[nbarl \\[nla weight_NNIMEASURE nla\\]  nbarl\\] v40\\] v41\\] \\[pl in_IIIN \\[nbar12 \\[jl new_JJTIME jl\\] \\[nla  cars_NN2DEVICE nla\\] nbar12\\] pl\\] v2\\] vibarl\\] tl\\] ilb\\] nbarq4\\] pl\\] v4\\]  vbar2\\] sdl\\] sprime2\\] ._. sprpdl\\] start\\]  Figure 2: IBM/Lancaster Treebank and ATl~/Lancaster Parses For SS  rougbJy 3,000 lexica\\] tags and about 1,100 d~erent non-terminal node labels, s?", "acronym": "SS", "label": "Same Sentence", "ID": "4562"}, {"sentence": "SSs: EN-ES-P ?", "acronym": "SS", "label": "Same Sentence", "ID": "4563"}, {"sentence": "SS/Paragraph 5 True if the two opinions are in the same sentence/paragraph.", "acronym": "SS", "label": "Same Sentence", "ID": "4564"}, {"sentence": "SS Indicator  This feature is either 0 or 1 indicating whether an  anaphor and a candidate antecedent are in the  same sentence.", "acronym": "SS", "label": "Same Sentence", "ID": "4565"}, {"sentence": "In the conditioning events, h is the  head constituent above p, l~ r is the list of candi-  date antecedents o be considered, t is the type  of phrase of the proposed antecedent (always  a noun-phrase in this study), I is the type of  the head constituent, sp describes the syntactic  SS in which p appears, dspecifies the dis-  tance of each antecedent from p and M\" is the  number of times the referent is mentioned.", "acronym": "SS", "label": "structure", "ID": "4566"}, {"sentence": "In clustering, objects are grouped together according to their feature value distribution, not to a predefined classification (as is the case when using supervised techniques), so that we achieve a better guarantee that we are learning a SS already present in the data.", "acronym": "SS", "label": "structure", "ID": "4567"}, {"sentence": "A broad semantic classification like the one we pro- pose is a first step for characterising their meaning and argument SS.", "acronym": "SS", "label": "structure", "ID": "4568"}, {"sentence": "We have not, however, been able to  duplicate exactly the syntactic SSs as-  sumed by Hobbs.", "acronym": "SS", "label": "structure", "ID": "4569"}, {"sentence": "Toward an adequate taxonomy of personality attributes: Replicated factor SS in peer nomination personality rating.", "acronym": "SS", "label": "structure", "ID": "4570"}, {"sentence": "The syntnctic SS st, and the distance  from the pronoun da are independent of the  number of times the referent is mentioned.", "acronym": "SS", "label": "structure", "ID": "4571"}, {"sentence": "PESA: Phrase Pair Extraction as  SS.", "acronym": "SS", "label": "Sentence Splitting", "ID": "4572"}, {"sentence": "7 FirstLevel Preprocessing Second Level Preprocessing Editors and Interfaces Models and Other Applications Higher Level Multilingual NLP Applications Text Language-Encoding Identification Encoding Converters Text Normalization SS Tokenization Morphological Analyzer Encoding Converter Generator Model of Scripts Spell Checker Model of Morphology Part Of Speech Tagger Other Specialized Interfaces Text Editor Annotation Interfaces Local Word Grouper or Chunker Figure 1: One view of the basic computational in- frastructure required for Natural Language Process- ing or Computational Linguistics.", "acronym": "SS", "label": "Sentence Splitting", "ID": "4573"}, {"sentence": "SS (40 Operations).", "acronym": "SS", "label": "Sentence Splitting", "ID": "4574"}, {"sentence": "79    Input SS and Translating  Takao Doi,  Eiichiro Sumita  ATR Spoken Language Translation Research Laboratories   2-2-2 Hikaridai, Kansai Science City, Kyoto, 619-0288 Japan  {takao.doi, eiichiro.sumita}@atr.co.jp         Abstract  We propose a method to split and translate  input sentences for speech translation in order  to overcome the long sentence problem.", "acronym": "SS", "label": "Sentence Splitting", "ID": "4575"}, {"sentence": "1 http://lhncbc.nlm.nih.gov/project/consumer-health- question-answering 30 SS  Request  Question  Sentence  Ignore  Sentence  Background  Sentence  Candidate Generation  UMLS  SVM Candidate Ranking  Boundary Fixing  Focus  Focus Recognition  Sentence Classification  Background Classification  SVM Comorbidity Classification  SVM Diagnosis Classification  SVM Family History Classification  SVM ISF Classification  SVM Lifestyle Classification  SVM Symptom Cl", "acronym": "SS", "label": "Sentence Splitting", "ID": "4576"}, {"sentence": "SS, Text Stemming and  Chunking: This module splits the context into sen- tences, then stems out the words and chunks those.", "acronym": "SS", "label": "Sentence Splitting", "ID": "4577"}, {"sentence": "Our aim is to estab- lish SSes for adjectives in Catalan by means of clustering, using only shallow syntactic evidence.", "acronym": "SS", "label": "semantic class", "ID": "4578"}, {"sentence": "2 Classification and Hypothesis As mentioned above, the SSification of adjectives is not settled in theoretical linguistics.", "acronym": "SS", "label": "semantic class", "ID": "4579"}, {"sentence": "A broad SSification like the one we pro- pose is a first step for characterising their meaning and argument structure.", "acronym": "SS", "label": "semantic class", "ID": "4580"}, {"sentence": "2.4 Morphology vs. syntax It could seem that the SSes established for the second parameter amount to morphological classes: not derived (basic adjectives), denominal (object adjectives), and deverbal (event adjectives).", "acronym": "SS", "label": "semantic class", "ID": "4581"}, {"sentence": "Our hypothesis, which will be tested on Sec- tion 4.3, is that syntax is more reliable than mor- phology as a basis for SSification.", "acronym": "SS", "label": "semantic class", "ID": "4582"}, {"sentence": "In recent research in the field, the main effort has been to infer SSes for verbs, in English (Stevenson et al, 1999) and German (Schulte im Walde and Brew, 2002).", "acronym": "SS", "label": "semantic class", "ID": "4583"}, {"sentence": "Because it often takes the same nominative SS such as ?", "acronym": "SS", "label": "subjects", "ID": "4584"}, {"sentence": "5=no, 1=quickly) 2.35 3.35 +43% Table 5: Di\u000berence in response with ten SS when viewing 1-agent and 2-agent versions of Mike Announcer interrupts expert Sorry, E-MIKE.", "acronym": "SS", "label": "subjects", "ID": "4585"}, {"sentence": "All these SS were familiar with the RoboCup domain and the Soccer Server en- vironment.", "acronym": "SS", "label": "subjects", "ID": "4586"}, {"sentence": "The system also con-  tains domain knowledge including the domain  concepts, specific list of SS and verbs, and  topic headings.", "acronym": "SS", "label": "subjects", "ID": "4587"}, {"sentence": "We first obtained from 10,000 to 50,000 unique  documents from the TREC 1, 2 and 3 volumes using the  Inquery search engine from UMass Amherst for each of  the following SS: art, business, education,  government, healthcare, movies, music, politics,  Number of  occurrences  Percentage  of entities  1 46.66  2 18.78  3 9.03  4 4.55  5 1.86  6 1.16  7 0.83  8 0.46  9 or more 16.67    Table 1: Breakdown of distribution by number of  occurrences within the Person X corpus.", "acronym": "SS", "label": "subjects", "ID": "4588"}, {"sentence": "5=no, 1=quickly) 3.97 Table 4: Average responses of 20 SS to \frst questionnaire evaluation of (two-agent) Mike Question Scale 1-agent 2-agent Di\u000b Is the game better with or without...? (", "acronym": "SS", "label": "subjects", "ID": "4589"}, {"sentence": "SS and In- terpretation.", "acronym": "SS", "label": "Surface Structure", "ID": "4590"}, {"sentence": "SS and Inter- pretation.", "acronym": "SS", "label": "Surface Structure", "ID": "4591"}, {"sentence": "A relatively recent approach to this problem is to use  a set of rules like (61) which Kuroda (1976) calls  Canonical SS Filters and Miyagawa  (1980) calls Case Redundancy Rules.", "acronym": "SS", "label": "Surface Structure", "ID": "4592"}, {"sentence": "\\[Lesmo, Lombardo 91\\] Lesmo L., Lombardo V., A  Dependency Syntax for the SS of  Sentences, Pro,:.", "acronym": "SS", "label": "Surface Structure", "ID": "4593"}, {"sentence": "REFERENCES  Chomsky, N. (1971), Deep Structure,  SS and Semantic Inter-  wretation, in: Semantics (ed.", "acronym": "SS", "label": "Surface Structure", "ID": "4594"}, {"sentence": "We propose more efficient al- gorithms approximating Tree Kernel: Tree Overlapping and SS.", "acronym": "SS", "label": "Subpath Set", "ID": "4595"}, {"sentence": "4 SS 4.1 Definition of similarity SS similarity between two trees is de- fined as the number of subpaths shared by the trees.", "acronym": "SS", "label": "Subpath Set", "ID": "4596"}, {"sentence": "The results of the experiments comparing these three algo- rithms showed that structural retrieval with Tree Overlapping and SS were faster than that with Tree Kernel by 100 times and 1,000 times respectively.", "acronym": "SS", "label": "Subpath Set", "ID": "4597"}, {"sentence": "The experiments comparing these three algorithms showed that Tree Overlapping is 100 times faster and SS is 1,000 times faster than Tree Kernel when being used for struc- tural retrieval.", "acronym": "SS", "label": "Subpath Set", "ID": "4598"}, {"sentence": "In this paper, we propose two efficient algo- 399 rithms to calculate similarity of syntactic struc- tures: Tree Overlapping and SS.", "acronym": "SS", "label": "Subpath Set", "ID": "4599"}, {"sentence": "For syntactic parsing, Goodman (1996) proposed a vari- ational method for summing out spurious ambiguity that was equivalent to MBR decoding (Goel and Byrne, 2000; Kumar and Byrne, 2004) with a constituent-recall loss function.", "acronym": "MBR", "label": "minimum Bayes risk", "ID": "4600"}, {"sentence": "The book concludes with a discussion of MBR decod- ing, and a few other variants.", "acronym": "MBR", "label": "minimum Bayes risk", "ID": "4601"}, {"sentence": "7 A MBR decoding procedure to pick an output clustering.", "acronym": "MBR", "label": "minimum Bayes risk", "ID": "4602"}, {"sentence": "Minimum Bayes Retrieval Risk {4}: Calculates the translation probability for the translation having the MBR among the re- trieved training instances.", "acronym": "MBR", "label": "minimum Bayes risk", "ID": "4603"}, {"sentence": "Such rescoring is implemented using a MBR technique (Kumar and Byrne 2004; Tromble et al2008).", "acronym": "MBR", "label": "minimum Bayes risk", "ID": "4604"}, {"sentence": "Generalized MBR System  Combination.", "acronym": "MBR", "label": "Minimum Bayes Risk", "ID": "4605"}, {"sentence": "Modifying the multitask objective to incorpo- rate application-specific loss/decoding, such as MBR (Kumar and Byrne, 2004) ?", "acronym": "MBR", "label": "Minimum Bayes Risk", "ID": "4606"}, {"sentence": "An improved consensus-like method for MBR decoding and lattice combi- nation.", "acronym": "MBR", "label": "Minimum Bayes Risk", "ID": "4607"}, {"sentence": "de Gispert et al, 2009) adopted the MBR decoding strategy to combine output from identical SMT system, which is trained on alternative morphological decompositions of the source language.", "acronym": "MBR", "label": "Minimum Bayes Risk", "ID": "4608"}, {"sentence": "MBR  Decoding for Statistical Machine Translation.", "acronym": "MBR", "label": "Minimum Bayes Risk", "ID": "4609"}, {"sentence": "MBR Com- bination of Translation Hypotheses from Alterna- tive Morphological Decompositions.", "acronym": "MBR", "label": "Minimum Bayes Risk", "ID": "4610"}, {"sentence": "50  60  70  80  90  100  110  120  130  140  0  1  2  3  4 A v e r a g e   D F Etymology depth Average document frequency for words in the training data ENIT Figure 5: DF changes with the addition of etymological features The shape of the document frequency curves mirror the LSA results ?", "acronym": "DF", "label": "Document frequency", "ID": "4611"}, {"sentence": "We therefore only used the Balanced Winnow algorithm for our classification experiments, which were run with the following LCS configuration, based on tuning experiments on the same data by Koster et al(2011): \u0001 Global term selection (GTS): DF minimum is 2, term frequency minimum is 3.", "acronym": "DF", "label": "Document frequency", "ID": "4612"}, {"sentence": "DF (df) is borrowed for the information  * Institute of Information Sciences and Electronics, 1-1-1 Tennodai, Tsukuba 305-8573, Japan  t 180 Park Avenue, Florham Park, NJ 07932  Computational Linguistics Volume 27, Number 1  retrieval iterature (Sparck Jones 1972); it counts the number of documents that con-  tain a type at least once.", "acronym": "DF", "label": "Document frequency", "ID": "4613"}, {"sentence": "(5) DF of bigrams in the sen- tence:  1 1( ) i i D i iw w S df w w + +?? .", "acronym": "DF", "label": "Document frequency", "ID": "4614"}, {"sentence": "DF (df ) counts are derived from the definitions contained in all our resources.", "acronym": "DF", "label": "Document frequency", "ID": "4615"}, {"sentence": "The first hall' is used to count document  frequency rl/: (DF will be used  instead of standard (term) frequency.)", "acronym": "DF", "label": "Document frequency", "ID": "4616"}, {"sentence": "c?2013 Association for Computational Linguistics Automatic Prediction of Friendship via Multi-model DF  Zhou Yu, David Gerritsen, Amy Ogan, Alan W Black, Justine Cassell  School of Computer Science, Carnegie Mellon University  {zhouyu, dgerrits, aeo, awb, justine }@cs.cmu.edu    Abstract  In this paper we focus on modeling  friendships between humans as a way of  working towards technology that can initiate  and sustain a lifelong relationship with users.", "acronym": "DF", "label": "Dyadic Features", "ID": "4617"}, {"sentence": "4.4 Re-estimating Bilingual Term Correspondences with Monolin- gual Web Documents For the 100 target English terms evaluated in the previous section, this section describes the result of applying the technique presented in Section 3.3, i.e., re-estimating bilingual term 8When the co-occurrence DF of t E and t J in the whole news articles is less than x, all the co-occurring dates are included.", "acronym": "DF", "label": "document frequency", "ID": "4618"}, {"sentence": "Each element of the vector )(ts  is an index term in the sentence, weighted by its text frequency (tf) and inverse DF (idf) where tf is defined as the frequency of the word in that particular sentence, and idf  is the inverse frequency of the word in the larger document collection N dflog?", "acronym": "DF", "label": "document frequency", "ID": "4619"}, {"sentence": "Figure 6: Inter-DF statistics 7 Conclusions and Future Work In this paper we have presented a statistical approach for the extraction of a lexicon which contains the verbs and nouns that can be considered as candidates for use as predicates for the induction of predicate/argument structures that we call messages.", "acronym": "DF", "label": "document frequency", "ID": "4620"}, {"sentence": "A first remark that we can make in respect to those graphs is that con- cerning the collection frequency, DF and tf.idf measures, for small threshold numbers we have more or less high precision values while the recall and fallout values are low.", "acronym": "DF", "label": "document frequency", "ID": "4621"}, {"sentence": "Since we represent word to- kens rather than word types in the cohesion graph, we do not need to model the term frequency tf separately, instead we set salience to the log value of the inverse DF idf : salience(t i ) = log |D| |{d : t i ?", "acronym": "DF", "label": "document frequency", "ID": "4622"}, {"sentence": "2 statistic estimation is re- stricted to certain portion of the whole news articles so that the following condition be satis- fied: i) co-occurrence DF of a target English term and its reference Japanese translation is fixed to be x,8 ii) the number of days be greater than or equal to y. For each news articles data set, Table 3 shows document frequencies df(tE) of a target English term tE , co-occurrence document frequencies df(tE, tJ ) of tE and its reference Japanese translation tJ , and the numbers of days for English as wel", "acronym": "DF", "label": "document frequency", "ID": "4623"}, {"sentence": "The following is the category structure that we need:  (27) a. F = {PRONOUN, CASE, PERSON, GENDER, NUMBER,  ANIMACY, PROXIMITY}  b. A = {question, personal, DEM, subjec-  tive, objective, reflexive, possessive, posses-  sive-determiner, first, second, third, feminine  masculine, neuter, singular, plural}  C. T O  d. p = {<PRONOUN, {question, personal,  DEM}),  <CASE, {subjective, objective, reflexive, posses-  sive, possessive-determiner}),  <PERSON, {first, second third}),  <GENDER, {feminine, masculine, neuter}),  <NUMBER, {singu", "acronym": "DEM", "label": "demonstrative", "ID": "4624"}, {"sentence": "The following is the category structure that we need:  (27) a. F = {PRONOUN, CASE, PERSON, GENDER, NUMBER,  ANIMACY, PROXIMITY}  b. A = {question, personal, DEM, subjec-  tive, objective, reflexive, possessive, posses-  sive-determiner, first, second, third, feminine  masculine, neuter, singular, plural}  C. T O  d. p = {<PRONOUN, {question, personal,  DEM}),  <CASE, {subjective, objective, reflexive, posses-  sive, possessive-determiner}),  <PERSON, {first, second third}),  <GENDER, {feminine, masculine, neuter}),  <NUMBER, {singular, plural}),  <ANIMACY, 2>,  <PROXIMITY, 2>}  The constraints that must be imposed are the following:  (28) a. PRONOUN  b. (PRONOUN:question) ~ (CASE /~ -'-I PERSON /'k ---I  NUMBER /~ ANIMACY /~ 7 PROXIMIT", "acronym": "DEM", "label": "demonstrative", "ID": "4625"}, {"sentence": "For a clue c, an associated Clues  feature d takes one of the four values, depending  on the way c appears in A and B. c' = 0 if c ap-  pears in neither A or B; d = 1 if c appears in both  A and B; d = 2 if c appears in A and not in B;  and d = 3 if c appears not in A but in B. We con-  sider clue expressions from the following grammat-  ical classes: nominals, adjectives, DEMs,  adverbs, sentence connectives, verbs, sentence-final  particles, topic-marking particles, and punctuation  marks.", "acronym": "DEM", "label": "demonstrative", "ID": "4626"}, {"sentence": "The former is morphosyntactically very heterogeneous: \u0000 some pronouns inflect for gender (e.g., the DEM pronoun ten, the possessive pronoun m?j, but not the interrogative pro- noun kto or the negative pronoun nikt); \u0000 some pronouns, but not all, inflect for per- son; \u0000 some pronouns, but not all, inflect for num- ber; \u0000 the short reflexive pronoun si\be does not overtly inflect at all, although it may be con- strued as a weak form of the anaphoric pro- noun siebie.", "acronym": "DEM", "label": "demonstrative", "ID": "4627"}, {"sentence": "Lexical adjectives, including DEMs ad- verbs, numerals, and possessive adjectives, as well as ordinary intersective adjectives ?", "acronym": "DEM", "label": "demonstrative", "ID": "4628"}, {"sentence": "<PERSON, {first, second third}),  <GENDER, {feminine, masculine, neuter}),  <NUMBER, {singular, plural}),  <ANIMACY, 2>,  <PROXIMITY, 2>}  The constraints that must be imposed are the following:  (28) a. PRONOUN  b. (PRONOUN:question) ~ (CASE /~ -'-I PERSON /'k ---I  NUMBER /~ ANIMACY /~ 7 PROXIMITY)  C. (PRONOUN:personaD <--> (CASE /~ PERSON /~ NUMBER  /k -q ANIMACY /~ \"7 PROXIMITY)  d. (PRONOUN:DEM) <--) (7 CASE /~ 7 PERSON  /~ NUMBER /~ -3 ANIMACY /~ PROXIMITY)  e. GENDER ~ (PRONOUN A (PERSON:thirD A (NUMBER:  singular))  Note that this description of the pronominal system of  English is artificially complicated by its isolation from  the rest of the grammar.", "acronym": "DEM", "label": "demonstrative", "ID": "4629"}, {"sentence": "Pronouns and DEM are ignored.", "acronym": "DEM", "label": "demonstratives", "ID": "4630"}, {"sentence": "We compute a number of shallow features that provide a cheap way of capturing the above intu- itions: the number of DEM, pronouns, and definite descriptions as well as the number of sentence-initial discourse connectives.", "acronym": "DEM", "label": "demonstratives", "ID": "4631"}, {"sentence": "For a clue c, an associated Clues  feature d takes one of the four values, depending  on the way c appears in A and B. c' = 0 if c ap-  pears in neither A or B; d = 1 if c appears in both  A and B; d = 2 if c appears in A and not in B;  and d = 3 if c appears not in A but in B. We con-  sider clue expressions from the following grammat-  ical classes: nominals, adjectives, DEM,  adverbs, sentence connectives, verbs, sentence-final  particles, topic-marking particles, and punctuation  marks.", "acronym": "DEM", "label": "demonstratives", "ID": "4632"}, {"sentence": "Lexical adjectives, including DEM ad- verbs, numerals, and possessive adjectives, as well as ordinary intersective adjectives ?", "acronym": "DEM", "label": "demonstratives", "ID": "4633"}, {"sentence": "For each of the cohesive devices discussed in Section 3.4?DEM, pronouns, definite descriptions, and sentence-initial discourse con- nectives?we compare the previous sentence in the summary with the previous sentence in the in- put article.", "acronym": "DEM", "label": "demonstratives", "ID": "4634"}, {"sentence": "To distinguish between DEM and definite de- terminers, a gradation of givenness markers as sug- gested by Gundel et al (Gundel et al, 1989) is nec- essary: ?", "acronym": "DEM", "label": "demonstratives", "ID": "4635"}, {"sentence": "le latin) seuls les d~-  finis anaphoriaue et DEM  existent, ce qui con-  firme le caract~re redondant du \"d~fini g~n~rique\".", "acronym": "DEM", "label": "d~monstrat i f", "ID": "4636"}, {"sentence": "RONNE-  MENT INTER-  PHRASTIQUE  le  ce  ENVIRONNE-  MENT  REFLEXIF  (le)  ENVIRONNE-  MENT  RELATIF  lequel  il - le~uel  gui  le se gue  lui se ~ qu i   auquel  quoi  son son dont  lui soi lequel  lui gui  guoi  Nous avons mis ainsi sous un m~me paradiqme  des morphemes grammaticaux qui appart iennent  aux Dara-  digmes tradit ionnels  suivants:  I- art ic le d~fini  II- adject i fs  et pronoms DEMs   II I- pronom personnel  (3e personne)  IV- adject i f  et pronom possess i f  (3e personne)  V- pronom r~f lexi f  (3e personne)  35 -  Remaruuons aue les noms personnels  {mo__~i, to__ii,  nous et vous) n 'admettent pas la pronominal isat ion cf.", "acronym": "DEM", "label": "d~monstrat i f", "ID": "4637"}, {"sentence": "XV CBNTEhTLbIL ANALIS I S   4.1 Basilq r(pproac'lr t o  C e n t o x t u a l  RHSlv.", "acronym": "RHS", "label": "rhs", "ID": "4638"}, {"sentence": "TTT also supports constructive functions, with bound variables as arguments, in the RHS tem- plates, such as join-with-dash!,", "acronym": "RHS", "label": "rhs", "ID": "4639"}, {"sentence": "When a match to the transduction lhs pattern oc- curs, the resulting bindings and transduction RHS are used to create a new tree, which then replaces the tree (or subtree) that matched the lhs.", "acronym": "RHS", "label": "rhs", "ID": "4640"}, {"sentence": "However, variables that appear in both a rule?s lhs and RHS must occur at a depth less than two on the left, and Tiburon cannot easily simulate our verti- cal path or sequence operators.", "acronym": "RHS", "label": "rhs", "ID": "4641"}, {"sentence": "Formally, the reordering rule is a triple {p, lhs,  RHS}, where p is the reordering probability, lhs is  the left hand side of the rule, i.e., the constituent  label sequence of a parse tree node, and RHS is the  reordering permutation derived either from hand- crafted rules as in (Collins et.", "acronym": "RHS", "label": "rhs", "ID": "4642"}, {"sentence": "The RHS of such a transduction is allowed to reference the bindings of variables that appear in the enclos- ing pattern.", "acronym": "RHS", "label": "rhs", "ID": "4643"}, {"sentence": "In anal--  ogy to a eontext-flee phrase structure rule, a DATR  sentence has a left hand side that consists of exactly  one non-terminal symbol (i.e. a node-path pair) and  a RHS that consists of an arbitrary num-  ber of non-terminal and terminal symbols (i.e. DATR  atoms).", "acronym": "RHS", "label": "right hand side", "ID": "4644"}, {"sentence": "RHSs of the sentences o\\[ the DATR theory.", "acronym": "RHS", "label": "right hand side", "ID": "4645"}, {"sentence": "In the  case of an active edge, a rewriting rule in the grammar  and a position on the RHS of that rule are  provided, thus indicating what is still in order to com-  plete the recognition of the constituent.", "acronym": "RHS", "label": "right hand side", "ID": "4646"}, {"sentence": "Epenlhetic  material arises when syllabic slots which are not occultied  by segments are realised, l lerc the nlarks are given on the  RHS of the cohm in each pair.", "acronym": "RHS", "label": "right hand side", "ID": "4647"}, {"sentence": "C, there are specifications R and i, with 0  < i < n, where i is the position on the RHS of  rule R. A word in the string is itself represented as an  inactive edge connecting two adjoining vertices.", "acronym": "RHS", "label": "right hand side", "ID": "4648"}, {"sentence": "For example, (9b)  says that for any slot name o-, a constituent labelled {(o-,  mNc)} may have the immediate constituent analysis  seen on the RHS of the equation.", "acronym": "RHS", "label": "right hand side", "ID": "4649"}, {"sentence": "Thus the left  hand side of a formula (before the equality sign) consists  of an atomic label, and the RHS is a string of  tagmemes, which are ordered triples (a, b, c) where a is  an indication of optional (-+) or obligatory (+) status, b  is a slot or function name, and c is a filler or category  label.", "acronym": "RHS", "label": "right hand side", "ID": "4650"}, {"sentence": "d) AER with Soft-Union.", "acronym": "AER", "label": "Alignment Error Rate", "ID": "4651"}, {"sentence": "(f) AER with with the Best Strategy.", "acronym": "AER", "label": "Alignment Error Rate", "ID": "4652"}, {"sentence": "Firstly, the (bidirec- tional) reference alignments used in the computation of the AER were split into two sets of unidirectional alignments.", "acronym": "AER", "label": "Alignment Error Rate", "ID": "4653"}, {"sentence": "Figure 2: Precision/Recall Curve and AER with Different Models and Strategies.", "acronym": "AER", "label": "Alignment Error Rate", "ID": "4654"}, {"sentence": "Use of this technique dramati- cally reduces the AER  of the extracted corpora over heuristic  methods based on position of the sen- tences in the text.", "acronym": "AER", "label": "Alignment Error Rate", "ID": "4655"}, {"sentence": "10 Corpus English French English French English French English French 10K 47% 50% 61% 66% 74% 77% 84% 87% 25K 43% 44% 57% 59% 69% 72% 80% 83% 50K 42% 44% 55% 57% 67% 69% 78% 81% Table 2: AER by Model and Corpus Size Corpus Baseline Null SG Tags Tags+SG Tags+Null Tags+Null+SG 5K 17.53 16.86 16.72 16.20 15.31 15.36 15.14 15K 15.03 14.29 13.52 13.90 12.63 13.22 12.52 25K 13.85 13.05 12.79 13.10 11.91 12.30 11.79 35K 13.19 11.98 12.03 12.60 11.45 11.56 11.07 50K 12.63 11.76 11.78 12.10 11.19 11.11 10.69 ing size increases the Och model catches up with the Tags model a", "acronym": "AER", "label": "Alignment Error Rate", "ID": "4656"}, {"sentence": "In  section 5, we present word alignment results that  show significant AER reductions  compared to the baseline HMM and IBM model 4.", "acronym": "AER", "label": "alignment error rate", "ID": "4657"}, {"sentence": "interp to reduce AER (Koehn, 2005) over a hand-aligned development set.", "acronym": "AER", "label": "alignment error rate", "ID": "4658"}, {"sentence": "0,3 we trained several mod- els on a 10,000-sentence-pair subset of the French- English Hansards, and chose values that minimized the AER, as evaluated on a 447 sen- tence set of manually created alignments (Mihalcea and Pedersen, 2003).", "acronym": "AER", "label": "alignment error rate", "ID": "4659"}, {"sentence": "Our most successful systems were based on classifier combination, and we found different combination methods performed best under the target evalua- tion metrics of F-measure and AER.", "acronym": "AER", "label": "alignment error rate", "ID": "4660"}, {"sentence": "We can see that switching the search method from weighted maximum matching to a cohesion- constrained ITG (D-ITG) has produced a 34% rel- ative reduction in AER.", "acronym": "AER", "label": "alignment error rate", "ID": "4661"}, {"sentence": "This constraint produces a sig- nificant reduction in AER.", "acronym": "AER", "label": "alignment error rate", "ID": "4662"}, {"sentence": "it will be fine) JSL3 FINE / DREAM Alignment pairs that consist of many more or fewer sign words than Japanese words are dis- carded as AERs.", "acronym": "AER", "label": "alignment error", "ID": "4663"}, {"sentence": "These spurious words cause signifi- cant word AERs (as shown with dash lines), which in turn directly affect the q", "acronym": "AER", "label": "alignment error", "ID": "4664"}, {"sentence": "As some of the links are correctly aligned in the HMM and BM alignments (shown with solid lines), the combined alignment corrects some AERs while still contains com- mon incorrect alignment links.", "acronym": "AER", "label": "alignment error", "ID": "4665"}, {"sentence": "In this paper, we regard the alignment pair as the AER when n sign > (N JP + ?)", "acronym": "AER", "label": "alignment error", "ID": "4666"}, {"sentence": "These spurious words cause signifi- cant word AERs (as shown with dash lines), which in turn directly affect the quality of phrase translation tables or translation rules that are learned based on word alignment.", "acronym": "AER", "label": "alignment error", "ID": "4667"}, {"sentence": "and (Ittycheriah and Roukos, 2005)), which also introduce many word AERs.", "acronym": "AER", "label": "alignment error", "ID": "4668"}, {"sentence": "In this paper we introduce a confidence mea- sure for word alignment, which is robust to extra or missing words in the bilingual sentence pairs, as well as word AERs.", "acronym": "AER", "label": "alignment error", "ID": "4669"}, {"sentence": "This relatively small improve- ment is mainly due to the selection of the whole sentence alignment: for many sentences the best alignment still contains AERs, some of which could be fixed by other aligners.", "acronym": "AER", "label": "alignment error", "ID": "4670"}, {"sentence": "In addition, it may include  MNR  of motion and the inclusion of a COMITATIVE  phrase (i.e. accompaniment by an ob- ject/individual in the GO or COME event).", "acronym": "MNR", "label": "MANNER", "ID": "4671"}, {"sentence": "TENSE PRESENT  ASPECT SIMPLE MORPH_ASP/  MOOD  IMPERFEC- TIVE  SUBJ_NUM SINGULAR SUBJ_PER 3RD  SUBJ_GEN FEM SUBJ_CAT GROUP  INTEROG NO NEGATION NO  SVC NO PP YES  LOC_ADV NO ADVERBIAL YES  GOAL NO SOURCE NO  MNR YES SETTING YES  PATH NO PURPOSIVE NO  COMITATIVE NO TEMPORAL NO  DEGREE NO      4 Statistical analyses  A wide range of statistical tests can be ap- plied in order to explore the data frames de- scribed above for various purposes.", "acronym": "MNR", "label": "MANNER", "ID": "4672"}, {"sentence": "PERFECTIVE,  SUBJUNCTIVE, JUSSIVE, IMPER- ATIVE  SUBJECT PERSON 1ST, 2ND, 3RD   SUBJECT NUMBER SINGULAR, DUAL, PLURAL  SUBJECT GENDER FEMININE, MASCULINE, NIL (for  1st person inflections)                                                    1 The data frame was, in fact, coded for more variables than  the set laid out in Table 4, such as the different  morphosyntactic realizations of GOAL, SOURCE, MNR,  etc.,", "acronym": "MNR", "label": "MANNER", "ID": "4673"}, {"sentence": "B  PHRASE  YES, NO  ADVERBIAL PHRASE YES, NO  SERIAL VERB CON- STRUCTION  YES, NO    Semantic variables    Levels   SUBJECT CATEGORY ACTIVITY, ANIMAL, ATTRIB- UTE, BODY, COGNITION, COM- MUNICATION, CONTENT (of a  document/speech), DEMON- STRATIVE, DUMMY SUBJECT,  EVENT, GROUP, HUMAN, LOCA- TION, NOTION, OB- JECT/ARTIFACT, SENSE, STATE,  SUBSTANCE, TIME  GOAL PHRASE YES, NO  SOURCE PHRASE YES, NO  MNR PHRASE YES, NO  SETTING PHRASE YES, NO  PATH PHRASE YES, NO  PURPOSIVE PHRASE YES, NO  COMITATIVE PHRASE YES, NO  TEMPORAL PHRASE YES, NO  DEGREE PHRASE YES, NO    Table 1.", "acronym": "MNR", "label": "MANNER", "ID": "4674"}, {"sentence": "a = 0.247  qadima = 0.416  contextual features used (in  the model):  TESNE.PAST + ASPECT.SIMPLE +  SUBJ_PER.3RD +  SUBJ_CAT.HUMAN + PP.YES +  LOC.ADV.YES + MNR.YES +  COMITATIVE.YES     ????? ?? ???? ???? ????? ??? ??? ??? ???", "acronym": "MNR", "label": "MANNER", "ID": "4675"}, {"sentence": "Michael decided to go to the beach), we can add an  adjunct of type MNR to it (Michael quickly  decided to go to the beach) but we cannot add an adjunct  of type PROPERTY (*Michael important(ly) decided  to go to the beach) 6.", "acronym": "MNR", "label": "MANNER", "ID": "4676"}, {"sentence": "Section 3 presents the Posterior Regularization (PR) framework and describes how to encode such constraints in an efficient MNR, requiring only repeated inference in the original model to enforce the constraints.", "acronym": "MNR", "label": "manner", "ID": "4677"}, {"sentence": "Unfortunately, the selection of paradigm words rarely receives sufficient attention and is typically done in an ad hoc MNR.", "acronym": "MNR", "label": "manner", "ID": "4678"}, {"sentence": "Since in queries, quantifiers rarely interact with alternative phrases in the MNR discussed in Hoeksema and von Fintel?s work, their analyses have not been carried over into the present work.", "acronym": "MNR", "label": "manner", "ID": "4679"}, {"sentence": "However, a variety of  pronouns indicate the same class: Plural pro-  pronoun gender class  he,himself, him,his HE  she,herself, her,hers SHE  it,itself, its IT  nouns like \"they\" and \"us\" reveal no gender in-  formation about their referent and consequently  aren't useful, although this might be a way to  learn pluralization in an unsupervised MNR.", "acronym": "MNR", "label": "manner", "ID": "4680"}, {"sentence": "7.2 Domain-specific sub-corpora         The person-x corpus may appear to be biased due to  the MNR of its construction.", "acronym": "MNR", "label": "manner", "ID": "4681"}, {"sentence": "Lexical adverbs, including MNR, time, and loca- tion, and adverbs of negation, which vary by clause type (declarative, imperative, or interrogative) ?", "acronym": "MNR", "label": "manner", "ID": "4682"}, {"sentence": "In attempting to  b.ootstraPLexical information about referents'  gender, we consider two strategies, both com-  pletely blind to any kind of semantics.", "acronym": "PL", "label": "p l", "ID": "4683"}, {"sentence": "I f  the master   i s  p resent ,  the master -s lave  re la t ion  need not to  be express -   ed syntact i ca l l y :   \"Peter  ~ave the so.._nn an apPLe . \"", "acronym": "PL", "label": "p l", "ID": "4684"}, {"sentence": "When NL text  has been t ransformed in to  a se t  of   p roper ty  l i s t s ,  fe tched  from the  vocabu lar ies  and augmented by  morpholog ica l  preeoan,  there  are many ways to o rder  the apPL i c -   a t ion  of  the re levant  GSRs, fo r ,  wh i le  each GSR i s  descr ibed   procedura l ly ,  the descr ip t ion  as a whole i s  dec la ra t ive .", "acronym": "PL", "label": "p l", "ID": "4685"}, {"sentence": "S ign i f i cant  words are devided in to  c lasses  depending on  the  ro le  they  PLay  in  n~ing  cor responding not ions .", "acronym": "PL", "label": "p l", "ID": "4686"}, {"sentence": "a l l  i n fo rmat ion  from  supernot ion  i s  re levant  to ac tua l  not ion ,  i f  i t  i sn ' t  exPL i c i t -   l y  euperoeded.", "acronym": "PL", "label": "p l", "ID": "4687"}, {"sentence": "- 52  -  Indeed,  un l i ke  a t t r ibutes  of  c lass  A, Subetant iona l   A t t r ibutes  may be used wi thout  exPL i c i t  re fe rence  to i t s   master :  e ,g ,  \" the  t ra in  goes to the cap i te?\" .", "acronym": "PL", "label": "p l", "ID": "4688"}, {"sentence": "Before the flood season comes , it is necessary to seize the time to formulate plans for forecasting floods and to carry out work with clear \u0001\u0002\u0003\u0004\u0005\u0006\u0005 \u0007\u0004\b\t\u0003\u0006\u0002 Figure 1: Chinese/English Parallel Corpus Aligned at the Sentence, Word, and PL: horizontal lines denote the segmentations of a sentence alignment and arrows denote a word-level mapping.", "acronym": "PL", "label": "Phrase Levels", "ID": "4689"}, {"sentence": "In fact, it also pat-  terns closely with SPATIAL-SEQUENCE; such simplifications of semantic diversity are found in several areas,  as where the semantic relations PW, PLAN-STEP, ABSTRACT-INSTANCE all pattern with rhetori-  cal ELABORATION.", "acronym": "PW", "label": "PART-WHOLE", "ID": "4690"}, {"sentence": "Most existing approaches target either a single relation, e.g., PW (Girju et al.,", "acronym": "PW", "label": "PART-WHOLE", "ID": "4691"}, {"sentence": "Also assueiated with each entry is an  image-seheme (Lakoff 1987) such as the COrCI'AIN~R Schema, or the  PW Schema.", "acronym": "PW", "label": "PART-WHOLE", "ID": "4692"}, {"sentence": "Not all the relations of RST are rhetorical  - -  for example JUSTIFY and MOTIVATE are clearly intentional, and CAUSE and PW are clearly  semantic.", "acronym": "PW", "label": "PART-WHOLE", "ID": "4693"}, {"sentence": "There are propositions that differentiate  these two relations; thus, combinations in (a),  with PW relation are possible with  a preposition U, while combinations in (b),  with a SET-ELEMENT relation, are not:  a. nozka 'leg' U stula 'chair'  pugovica 'button'  U paljto ~coat'  b. *chaschka 'cup' U serviza 'service'  *korova 'cow' U stada 'herd'  ACTES DE COL", "acronym": "PW", "label": "PART-WHOLE", "ID": "4694"}, {"sentence": "These include relations such as CAUSE, PW, IS-A, TEMPORAL-SEQUENCEp  SPATIAL-SEQUENCE.", "acronym": "PW", "label": "PART-WHOLE", "ID": "4695"}, {"sentence": "As was stated earlier, domains  II.1, II.2 define the following relations:  1) PW;  2) SET-ELEMENT.", "acronym": "PW", "label": "PART-WHOLE", "ID": "4696"}, {"sentence": "3.1 Similarity measure  Punctuation and uninformative words are removed  from each sentence using a simple regular expression  pattern mateher and a stoPWord list.", "acronym": "PW", "label": "pw", "ID": "4697"}, {"sentence": "We sus-  pect this is due to the use of a different stoPWord list  and stemming algorithm.", "acronym": "PW", "label": "pw", "ID": "4698"}, {"sentence": "a) a new constituent is started by projecting a  complete rule uPWards; (b) the constituent then takes left and right modifiers (or none if it is unary). (", "acronym": "PW", "label": "pw", "ID": "4699"}, {"sentence": "At the start of the conversation, the belief (probability) that the connection is working p(allOk) is 56% and the belief that the power to the DSL modem is on p(PWrOn) is 98.0% (these are 2 of the 19 compo- nents in the product state x).", "acronym": "PW", "label": "pw", "ID": "4700"}, {"sentence": "Practi- cally, the effect may be that some local rules will have variants propagating uPWards a certain va- lency: \u0018?? %", "acronym": "PW", "label": "pw", "ID": "4701"}, {"sentence": "ID p(allOk) p(PWrOn) Transcript *S1 56% 98.0% I?m going to try pinging your DSL modem from here.", "acronym": "PW", "label": "pw", "ID": "4702"}, {"sentence": "In  (Conolly et al, 1994) a decision tree is trained on  a small number of 15 features concerning anaphor  type, GF, recency, morphosyntac-  tic agreement and subsuming concepts.", "acronym": "GF", "label": "grammatical function", "ID": "4703"}, {"sentence": "References    Baker, M. C. (1988) Incorporation, A Theory of  GF changing.", "acronym": "GF", "label": "grammatical function", "ID": "4704"}, {"sentence": "Our evaluation results indicate that the basic performance of the parser trained with the treebank almost equals bunsetsus-based parsers and has the potential to supply detailed syntactic information by GF labels for se- mantic analysis, such as predicate-argument struc- ture analysis.", "acronym": "GF", "label": "grammatical function", "ID": "4705"}, {"sentence": "Our goal is to construct a practical constituent parser that can deal with appropriate grammatical units and output GFs as semi- semantic information, e.g., grammatical or seman- tic roles of arguments and gapping types of relative clauses.", "acronym": "GF", "label": "grammatical function", "ID": "4706"}, {"sentence": "Most of the Korean corpus was built using grammat- ical chunks eojeols, which resemble Japanese bun- setsus and consist of content words and morphemes that represent GFs.", "acronym": "GF", "label": "grammatical function", "ID": "4707"}, {"sentence": "Base contains only ba- sic tags, not GF tags.", "acronym": "GF", "label": "grammatical function", "ID": "4708"}, {"sentence": "Expressivity and Complexity of the GF.", "acronym": "GF", "label": "Grammatical Framework", "ID": "4709"}, {"sentence": "c?2007 Association for Computational Linguistics Converting GF to Regulus Peter Ljungl?f Department of Linguistics G?teborg University Gothenburg, Sweden peb@ling.gu.se Abstract We present an algorithm for converting GF grammars (Ranta, 2004) into the Regulus unification-based framework (Rayner et al, 2006).", "acronym": "GF", "label": "Grammatical Framework", "ID": "4710"}, {"sentence": "Speech recognition grammar com- pilation in GF.", "acronym": "GF", "label": "Grammatical Framework", "ID": "4711"}, {"sentence": "Compared to GF, the Regu- lus formalism is quite restricted.", "acronym": "GF", "label": "Grammatical Framework", "ID": "4712"}, {"sentence": "Since GF is more ex- pressive than Regulus,", "acronym": "GF", "label": "Grammatical Framework", "ID": "4713"}, {"sentence": "The Expressivity and Complexity of GF.", "acronym": "GF", "label": "Grammatical Framework", "ID": "4714"}, {"sentence": "1.1 GF GF (Ranta, 2004) is a gram- mar formalism based on type theory.", "acronym": "GF", "label": "Grammatical Framework", "ID": "4715"}, {"sentence": "In each FrameNet sentence, a single target  predicate is identified and all of its relevant Frame  Elements are tagged with their element-type (e.g.,  Agent, Judge), their syntactic Phrase Type (e.g., NP,  PP), and their GF (e.g., External  Argument, Object Argument).", "acronym": "GF", "label": "Grammatical Function", "ID": "4716"}, {"sentence": "We classify an adjective as object-biased if the mean of the judgments for the object interpretation of this particular adjective is larger than the mean for the subject interpretation; subject-biased adjec- tives are classified accordingly, whereas adjectives for which no effect of GF is found are classified as equi-biased.", "acronym": "GF", "label": "Grammatical Function", "ID": "4717"}, {"sentence": "GFs and  Verb Subcategorization in Madarin Chinese.", "acronym": "GF", "label": "Grammatical Function", "ID": "4718"}, {"sentence": "3.4.1 GF Label Evaluation Ku?bler et al (2006) present the results shown in Ta- ble 3 for the parsing performance of the unlexical- ized model of the Stanford Parser (Klein and Man- ning, 2002).", "acronym": "GF", "label": "Grammatical Function", "ID": "4719"}, {"sentence": "The bi- ases and the significance of the GF effect (p) are shown in Table 6.", "acronym": "GF", "label": "Grammatical Function", "ID": "4720"}, {"sentence": "3 Parsing with GFs 3.1 Model As explained above, this paper focuses on unlexi- calized grammars.", "acronym": "GF", "label": "Grammatical Function", "ID": "4721"}, {"sentence": "TREC-9?,", "acronym": "TREC-9", "label": "The Ninth Text Retrieval Conference", "ID": "4722"}, {"sentence": "Proceedings of the TREC-9, pp.", "acronym": "TREC-9", "label": "Eighth Text REtrieval Conference", "ID": "4723"}, {"sentence": "In Proceedings of the TREC-9 (TREC-8), pages 83?106, Gaithersburg, MD.", "acronym": "TREC-9", "label": "Eighth Text REtrieval Conference", "ID": "4724"}, {"sentence": "In Proceedings of The TREC-9 (TREC-8), http://trec.", "acronym": "TREC-9", "label": "Eighth Text REtrieval Conference", "ID": "4725"}, {"sentence": "In Proceedings of the TREC-9 (TREC-8).", "acronym": "TREC-9", "label": "Eighth Text REtrieval Conference", "ID": "4726"}, {"sentence": "Proceedings of the TREC-9, November, 1999.", "acronym": "TREC-9", "label": "Eighth Text REtrieval Conference", "ID": "4727"}, {"sentence": "In the TREC-9 (TREC-8).", "acronym": "TREC-9", "label": "Eighth Text REtrieval Conference", "ID": "4728"}, {"sentence": "In the area of lexicon acquisition, many  researchers have employed public KBs  such as WordNet in IE systems.", "acronym": "KB", "label": "knowledge base", "ID": "4729"}, {"sentence": "We will look at  ways in which Constrained Conditional Models can be used to augment  probabilistic models with declarative KBd constraints and how these  support expressive global decisions.", "acronym": "KB", "label": "knowledge base", "ID": "4730"}, {"sentence": "Methods based on manually built lexical KBs, such as WordNet, compute the shortest path be- tween two concepts in the KB and/or look at word overlap in the glosses (see Budan- itsky and Hirst (2006) for an overview).", "acronym": "KB", "label": "knowledge base", "ID": "4731"}, {"sentence": "Our approach demonstrates the  use of two human built KBs  (WordNet and FrameNet) for the task of  semantic extraction.", "acronym": "KB", "label": "knowledge base", "ID": "4732"}, {"sentence": "2.2 A Thesaurus -based  Approach   Morris and Hirst \\[1991\\] used Roget's thesaurus as  KB for determining whether or not two  words are semantically related.", "acronym": "KB", "label": "knowledge base", "ID": "4733"}, {"sentence": "The ongoing development of public  KBs such as WordNet, FrameNet, CYC,  etc.", "acronym": "KB", "label": "knowledge base", "ID": "4734"}, {"sentence": "In the area of lexicon acquisition, many  researchers have employed public KB  such as WordNet in IE systems.", "acronym": "KB", "label": "knowledge bases", "ID": "4735"}, {"sentence": "Computing power has increased since the first such systems were developed, and the general methodology has changed from the use of hand-encoded KB about simple domains to the use of text collections as the main knowledge source over more complex domains.", "acronym": "KB", "label": "knowledge bases", "ID": "4736"}, {"sentence": "MOSES (Basili et al 2004) is an ontology-based QA system in which users pose questions in natural language to KB of facts extracted from a federation of Web sites and organized in topic map repositories.", "acronym": "KB", "label": "knowledge bases", "ID": "4737"}, {"sentence": "Methods based on manually built lexical KB, such as WordNet, compute the shortest path be- tween two concepts in the knowledge base and/or look at word overlap in the glosses (see Budan- itsky and Hirst (2006) for an overview).", "acronym": "KB", "label": "knowledge bases", "ID": "4738"}, {"sentence": "Our approach demonstrates the  use of two human built KB  (WordNet and FrameNet) for the task of  semantic extraction.", "acronym": "KB", "label": "knowledge bases", "ID": "4739"}, {"sentence": "The ongoing development of public  KB such as WordNet, FrameNet, CYC,  etc.", "acronym": "KB", "label": "knowledge bases", "ID": "4740"}, {"sentence": "Introduction to the Articles in this Special Section Demner-Fushman and Lin?s article (Answering clinical questions with KB and statistical techniques) extends previous work by the authors (Demner-Fushman and Lin 2005) on a QA system in the medical domain.", "acronym": "KB", "label": "knowledge-based", "ID": "4741"}, {"sentence": "These structures (with sorne enhancements) are to  provide the basis for KB parsing.", "acronym": "KB", "label": "knowledge-based", "ID": "4742"}, {"sentence": "In fact, as a not- too-long-term vision, we are convinced that research in restricted domains will drive the convergence between structured KB and free text-based question answering.", "acronym": "KB", "label": "knowledge-based", "ID": "4743"}, {"sentence": "Whereas structured KB QA systems are well adapted to applications managing complex queries in a very structured information environment, the kind of research developed in TREC, CLEF, and NTCIR is probably better suited to broad-purpose generic applications dealing with simple factual ques- tions such as World Wide Web?based ques", "acronym": "KB", "label": "knowledge-based", "ID": "4744"}, {"sentence": "Whereas structured KB QA systems are well adapted to applications managing complex queries in a very structured information environment, the kind of research developed in TREC, CLEF, and NTCIR is probably better suited to broad-purpose generic applications dealing with simple factual ques- tions such as World Wide Web?based question answering.", "acronym": "KB", "label": "knowledge-based", "ID": "4745"}, {"sentence": "Both trends have developed in parallel and represent the opposite ends of a spec- trum connecting what we might label as structured KB and free text- based question answering.", "acronym": "KB", "label": "knowledge-based", "ID": "4746"}, {"sentence": "Whereas KB ystems like (Carbonell  and Brown, 1988) and (Rich and LuperFoy, 1988)  combining multiple resolution strategies are expen-  sive in the cost of human effort at development time  and limited ability to scale to new domains, more re-  cent knowledge-poor approaches like (Kennedy and  Boguraev, 1996) and (Mitkov, 1998) address the  problem without sophisticated linguistic knowledge.", "acronym": "KB", "label": "knowledge-based", "ID": "4747"}, {"sentence": "616  Bui ld ing  KBs  for the Generat ion  of   Sof tware Documentat ion  *  C4cile Paristand Keith Vander  L inden  :I  ITRI, University of Brighton  Lewes Road  Brighton BN2 4AT, UK  {clp,knvl}~itri.brighton.ac.uk  Abst rac t   Automated text generation requires a  underlying knowledge base fl'om which  to generate, which is often difficult to  produce.", "acronym": "KB", "label": "Knowledge Base", "ID": "4748"}, {"sentence": "ANGUAGE  SYSTEM  I lubcrt l.chnmnn  IBM l)cutschhmd Gmbl l ,  Scientific ('enter  institute for KBd Systems  Wilckensstr.", "acronym": "KB", "label": "Knowledge Base", "ID": "4749"}, {"sentence": "es  Abstract  This paper explores the automatic onstruction  of a multilingual Lexical KB from  pre-existing lexical resources.", "acronym": "KB", "label": "Knowledge Base", "ID": "4750"}, {"sentence": "c?2015 Association for Computational Linguistics Semantic Parsing via Staged Query Graph Generation: Question Answering with KB Wen-tau Yih Ming-Wei Chang Xiaodong He Jianfeng Gao Microsoft Research Redmond, WA 98052, USA {scottyih,minchang,xiaohe,jfgao}@microsoft.com Abstract We propose a novel semantic parsing framework for question answering using a knowledge base.", "acronym": "KB", "label": "Knowledge Base", "ID": "4751"}, {"sentence": "LREC 2002 Workshop on Ontologies and  Lexical KBs (2002)  22.", "acronym": "KB", "label": "Knowledge Base", "ID": "4752"}, {"sentence": "Representational Interoperability of Linguistic and Collaborative KBs.", "acronym": "KB", "label": "Knowledge Base", "ID": "4753"}, {"sentence": "The unLPs and recalls of the pre- vious model and models 1, 2, and 3 were signifi- cantly different as measured using stratified shuf- fling tests (Cohen, 1995) with p-values < 0.05.", "acronym": "LP", "label": "labeled precision", "ID": "4754"}, {"sentence": "Parsing performance is measured by f-score, f = 2?P?RP+R , where P, R are LP and recall.", "acronym": "LP", "label": "labeled precision", "ID": "4755"}, {"sentence": "The columns LP, recall, f-score and accuracy represent aggregates over sentences 1 . . .", "acronym": "LP", "label": "labeled precision", "ID": "4756"}, {"sentence": "F1<40 is the F-Measure combining LP and labeled recall for sentences of less than 40 words.", "acronym": "LP", "label": "labeled precision", "ID": "4757"}, {"sentence": "6.4 Evaluation metrics We evaluate parsing performance using labeled F- Measure (combining LP and labeled 80 DEV SET TERMINAL SYMBOLS F1<40 F1 UAS Tagging Acc.", "acronym": "LP", "label": "labeled precision", "ID": "4758"}, {"sentence": "The LPs and recalls were signifi- cantly different among models 1, 2, and 3 and between the previous model and model 3, but were not significantly different between the previ- ous model and mod", "acronym": "LP", "label": "labeled precision", "ID": "4759"}, {"sentence": "The LPs and recalls were signifi- cantly different among models 1, 2, and 3 and between the previous model and model 3, but were not significantly different between the previ- ous model and model 1 and between the previous model and model 2.", "acronym": "LP", "label": "labeled precision", "ID": "4760"}, {"sentence": "On the other hand, inference in PSL reduces to a LP problem, which is theoretically and practically much more efficient.", "acronym": "LP", "label": "linear programming", "ID": "4761"}, {"sentence": "Multi-lingual dependency parsing with incremental integer LP.", "acronym": "LP", "label": "linear programming", "ID": "4762"}, {"sentence": "A LP formulation for global inference in natural language tasks.", "acronym": "LP", "label": "linear programming", "ID": "4763"}, {"sentence": "Grammatical error correction using integer LP.", "acronym": "LP", "label": "linear programming", "ID": "4764"}, {"sentence": "Thadani and McKeown (2011) substituted MANLI?s simulated annealing-based decoding with integer LP, and achieved a consider- able speed-up.", "acronym": "LP", "label": "linear programming", "ID": "4765"}, {"sentence": "i=1 score(wi, h, l i)ewi,li subject to C(s) Constraints In the {0, 1} LP for- mulation described above, we can encode linguis- tic constraints that reflect the interactions among the linguistic phenomena.", "acronym": "LP", "label": "linear programming", "ID": "4766"}, {"sentence": "NOTES  This work was supported by SITRA Foundation (Finnish National  Fund for Research and Development), Helsinki, Finland  Current address:  SITRA Foundation  P.O. Box 329  SF-00121 Helsinki, Finland  272 Computational Linguistics, Volume 12, Number 4, October-December 1986   Topological Dependency Trees: A Constraint-Based Account of LP Denys Duchier Programming Systems Lab Universita?t des Saarlandes, Geb.", "acronym": "LP", "label": "Linear Precedence", "ID": "4767"}, {"sentence": "Duchier D. & R. Debusmann (2001) \"Topological  Dependency Trees: A Constraint-Based Account of  LP'', in proceedings of  ACL.", "acronym": "LP", "label": "Linear Precedence", "ID": "4768"}, {"sentence": "4.4 Underspecification of LP  In our proposed tree description language, we provide for underspecified dominance  but not for underspecified linear precedence.", "acronym": "LP", "label": "Linear Precedence", "ID": "4769"}, {"sentence": "2.4 LP Constraints The elaboration above has assumed the absence of any linear precedence constraints.", "acronym": "LP", "label": "Linear Precedence", "ID": "4770"}, {"sentence": "LP in D&eontinuous  Constituents.\"", "acronym": "LP", "label": "Linear Precedence", "ID": "4771"}, {"sentence": "de- pendency graph, where each dimension (e.g. Im- mediate Dominance and LP) is as- sociated with its own set of well-formedness con- ditions (called principles).", "acronym": "LP", "label": "Linear Precedence", "ID": "4772"}, {"sentence": "For bootstrapping purposes, however, it is appropriate for the resulting score vector f c to depend on the seed vector y. 3.2 Laplacian LP To make f seed dependent, Komachi et al(2008) noted that we should use a power series of a ma- trix rather than a simple power of a matrix.", "acronym": "LP", "label": "label propagation", "ID": "4773"}, {"sentence": "Komachi et al(2008) shows that simple bootstrapping algorithms can be inter- preted as LP on graphs (Komachi et al 2008).", "acronym": "LP", "label": "label propagation", "ID": "4774"}, {"sentence": "irst reduced bootstrapping al- gorithms to LP using Komachi et al 44 (2008)?s theorization.", "acronym": "LP", "label": "label propagation", "ID": "4775"}, {"sentence": "This accords with the fact that many papers such as (Talukdar and Pereira, 2010; Kozareva et al 2011) suggest that graph-based semi-supervised learning, or LP, is another effective met", "acronym": "LP", "label": "label propagation", "ID": "4776"}, {"sentence": "To this end, we first reduced bootstrapping al- gorithms to LP using Komachi et al 44 (2008)?s theorization.", "acronym": "LP", "label": "label propagation", "ID": "4777"}, {"sentence": "Zhou et al (2008) integrate the advantages of  SVM bootstrapping in learning critical instances  and LP in capturing the manifold  structure in both the labeled and unlabeled data,  by first bootstrapping a moderate number of  weighted support vectors through a co-training  procedure from all the available data, and then  applying LP algorithm via the  bootstrapped support vectors.", "acronym": "LP", "label": "label propagation", "ID": "4778"}, {"sentence": "This accords with the fact that many papers such as (Talukdar and Pereira, 2010; Kozareva et al 2011) suggest that graph-based semi-supervised learning, or LP, is another effective method for this harvesting task.", "acronym": "LP", "label": "label propagation", "ID": "4779"}, {"sentence": "13 Other solutions are possible that do not require extended erivations or LP constraints.", "acronym": "LP", "label": "linear precedence", "ID": "4780"}, {"sentence": "SMTNs  can impose generalized LP on labeled  substrings.", "acronym": "LP", "label": "linear precedence", "ID": "4781"}, {"sentence": "It is obtained in  compiling LP, requirement and  exclusion properties described in the previous  sections together with, indirectly, that of  constituency.", "acronym": "LP", "label": "linear precedence", "ID": "4782"}, {"sentence": "Topo- logical dependency trees: A constraint-based ac- count of LP.", "acronym": "LP", "label": "linear precedence", "ID": "4783"}, {"sentence": "b. imposing restrictions on the spaces of search for  pieces to be linked together, possibly taking into  account general criteria of LP.", "acronym": "LP", "label": "linear precedence", "ID": "4784"}, {"sentence": "FERGUS consists of three models: tree chooser, unraveler, and LP chooser.", "acronym": "LP", "label": "linear precedence", "ID": "4785"}, {"sentence": "POLY2 is implemented in Java and it uses lp-solve software (Berkelaar, 1999) in order to perform LPming.", "acronym": "LP", "label": "Linear Program", "ID": "4786"}, {"sentence": "Bergsma and Kondrak (2007b) present a method for identifying sets of cognates across groups of languages using the global inference framework of Integer LPming.", "acronym": "LP", "label": "Linear Program", "ID": "4787"}, {"sentence": "LPming.", "acronym": "LP", "label": "Linear Program", "ID": "4788"}, {"sentence": "In Proceedings of the Workshop on In- teger LPming for Natural Langauge Pro- cessing, pages 1?9.", "acronym": "LP", "label": "Linear Program", "ID": "4789"}, {"sentence": "We will also mention various possibilities for  performing the inference, from commercial Integer LPming  packages to search techniques to Lagrangian relaxation approximation methods.", "acronym": "LP", "label": "Linear Program", "ID": "4790"}, {"sentence": "InProceedings of the  Workshop on Integer LPming for  Natural Langauge Processing, pp.", "acronym": "LP", "label": "Linear Program", "ID": "4791"}, {"sentence": "13 Other solutions are possible that do not require extended erivations or LPe constraints.", "acronym": "LP", "label": "linear precedenc", "ID": "4792"}, {"sentence": "SMTNs  can impose generalized LPe on labeled  substrings.", "acronym": "LP", "label": "linear precedenc", "ID": "4793"}, {"sentence": "It is obtained in  compiling LPe, requirement and  exclusion properties described in the previous  sections together with, indirectly, that of  constituency.", "acronym": "LP", "label": "linear precedenc", "ID": "4794"}, {"sentence": "Topo- logical dependency trees: A constraint-based ac- count of LPe.", "acronym": "LP", "label": "linear precedenc", "ID": "4795"}, {"sentence": "b. imposing restrictions on the spaces of search for  pieces to be linked together, possibly taking into  account general criteria of LPe.", "acronym": "LP", "label": "linear precedenc", "ID": "4796"}, {"sentence": "FERGUS consists of three models: tree chooser, unraveler, and LPe chooser.", "acronym": "LP", "label": "linear precedenc", "ID": "4797"}, {"sentence": "LP reflects the number of cor-  rectly labeled constituents identified by the rhetorical parser with respect o the total  number of labeled constituents identified by the parser.", "acronym": "LP", "label": "Labeled precision", "ID": "4798"}, {"sentence": "\"Metric  'Precision  Recall  LP  Labeled recall  Tagging accuracy  Number of crossing brackets J  Operations  Operation sequence  Syntax Semantics  -0.63 -0.63  -0.64 -0.66  -0.75 -0.78  -0.65 -0.65  -0.66 -0.56  0.58 0.54  -0.45 -0.41  -0.39 -0.36  Table 5: Correla", "acronym": "LP", "label": "Labeled precision", "ID": "4799"}, {"sentence": "LP has  the strongest correlation with both the syntactic and  semantic translation evaluation grades.", "acronym": "LP", "label": "Labeled precision", "ID": "4800"}, {"sentence": "Segmentation/ Regular Simulating  Tagging seg/tag as perfect  ( \"seg/tag\" ) implemented seg/tag  LP  Labeled recall  Tagging accuracy  Crossings/sentence  0 crossings  < 2 crossings  Structure&Label  86.9%  85 .O%  94.2%  1.63  42.9%  74.2%  16.0%  93.4%  92.9%  100.0%  1.13  48.5%  85.3%  28.8%  Table 2: Impact of segmentation/tagging errors  in the process of becoming available for Korean, will  greatly improve parsing accuracy.", "acronym": "LP", "label": "Labeled precision", "ID": "4801"}, {"sentence": "We use the PARSEVAL measures (Black et al 1991) to compare performance: LP = number of correct constituents in proposed parse number of constituents in proposed parse Labeled recall = number of correct constituents in proposed parse number of constituents in treebank parse Crossing brackets = number of constituents that violate constituent boundaries with a constituent in the treebank parse For a constituent to be ?", "acronym": "LP", "label": "Labeled precision", "ID": "4802"}, {"sentence": "8 Czech English Unlabeled precision 99.09 96.03 Unlabeled recall 94.81 93.07 Unlabeled F-1 96.90 94.53 LP 78.38 81.58 Labeled recall 74.99 79.06 Labeled F-1 76.65 80.30 Frame selection accuracy 79.10 84.95 Ambiguous verbs baseline 66.68 68.44 classifier 72.41 80.03 Table 1: Experimental results of errors in the Czech evaluation data were caused just by idioms or light verb constructions not be- ing recognized by our system.", "acronym": "LP", "label": "Labeled precision", "ID": "4803"}, {"sentence": "However, we decided not to do so,  because many training sentences were also used for  feature set and background knowledge development  121  Training sentences 32 64 128 256 512 1024  Precision  Recall  LP  Labeled recall  Tagging accuracy  Crossings/sentence  0 crossings  < 1 crossing  < 2 crossings  < 3 crossings  < 4 crossings  Correct operations  Operation Sequence  Structure&Label  88.6%  87.3%  84.1%  81.2%  94.3%  1.97  27.6%  56.4%  70.6%  81.0%  88.3%  63.0%  2.5%  5.5%  88.1%  87.4%  83.9%  81.9%  92.9%  2.00  35.0%  58.9%  72.4%  81.6%  84.0%  68.3%  6.1%  12.9%  90.0%", "acronym": "LP", "label": "Labeled precision", "ID": "4804"}, {"sentence": "Negra Tu?Ba-D/Z Unlabeled Precision 78.69 89.92 Unlabeled Recall 82.29 86.48 LP 64.08 75.36 Labeled Recall 67.01 72.47 Coverage 97.00 99.90 Table 2: PARSEVAL Evaluation The parser trained on Tu?Ba-D/Z performs much better than the one trained on Negra on all labeled and unlabeled bracketing scores.", "acronym": "LP", "label": "Labeled Precision", "ID": "4805"}, {"sentence": "German English  Exact Match (w/o TT) 46,3% 55,4%  hlcorrect parses 50,3% 39,3%  Not parsed 3,4% 5,3%  Exact Match (after 77) 53,8% 61,2%  Incorrect parses (after TT) 42,8% 33,5%  LP (w/o 7T) 90,2% 90,6%  German English  LP (after TT) 90,8% 91,4%  Labeled Recall (all 83,5% 78,5%  utterances, w/o TT)  Labeled Recall (all 84,0% 79,2%  utterances, after TT)  Labeled Recall (parsed 91,0% 90,9%  utterances, w/o TT)  Labeled Recall (parsed 91,6% 91,7%  utterances, after TT)  6 Conclusion  In this article we have extended probabilistic shift-  reduce", "acronym": "LP", "label": "Labeled Precision", "ID": "4806"}, {"sentence": "German English  Exact Match (w/o TT) 46,3% 55,4%  hlcorrect parses 50,3% 39,3%  Not parsed 3,4% 5,3%  Exact Match (after 77) 53,8% 61,2%  Incorrect parses (after TT) 42,8% 33,5%  LP (w/o 7T) 90,2% 90,6%  German English  LP (after TT) 90,8% 91,4%  Labeled Recall (all 83,5% 78,5%  utterances, w/o TT)  Labeled Recall (all 84,0% 79,2%  utterances, after TT)  Labeled Recall (parsed 91,0% 90,9%  utterances, w/o TT)  Labeled Recall (parsed 91,6% 91,7%  utterances, after TT)  6 Conclusion  In this article we have extended probabilistic shift-  reduce parsing to be more context-sensitive than  previous work", "acronym": "LP", "label": "Labeled Precision", "ID": "4807"}, {"sentence": "4http://bllip.cs.brown.edu/biomedical/ 103 0 25000 50000 75000 100000 125000 150000 175000 200000 225000 250000 275000 Number of sentences added 80.0 80.2 80.4 80.6 80.8 81.0 81.2 81.4 81.6 81.8 82.0 82.2 82.4 82.6 82.8 83.0 83.2 83.4 83.6 83.8 84.0 84.2 84.4 R e r a n k i n g   p a r s e r   f - s c o r e WSJ+Medline WSJ+BioBooks WSJ+NANC WSJ (baseline) Figure 2: LP-Recall results on development data for four versions of the parser as a function of number of self-training sentences References Michiel Bacchiani, Michael Riley, Brian Roark, and Richard Sproat.", "acronym": "LP", "label": "Labeled Precision", "ID": "4808"}, {"sentence": "The results of this  ewtluation are given in the following table:  7)'aining set/trees\\]  Test set \\[utterances\\]  GelTilall  19.750  1.000  English  17.793  1.000  Eract Match 46,3% 55,4%  Incorrect parses 50,3% 39,3%  Not pmwed 3,4% 5,3%  contextj'ree rules 988 2.205  LP 90,2% 90,6%  Labeled Recall (all 83,5% 78,5%  utterances)  Labeled Recall  (parsed utterances) 91,0% 90,9%  Japan.", "acronym": "LP", "label": "Labeled Precision", "ID": "4809"}, {"sentence": "i, A, j?|A spans from i to j} represent the test/gold parses, respectively, and we calculate:8 LP = #(G ?", "acronym": "LP", "label": "Labeled Precision", "ID": "4810"}, {"sentence": "Reevaluation was conducted on the LP test set released by the shared task organizers after our system?s output had been initially evaluated.", "acronym": "LP", "label": "labeled", "ID": "4811"}, {"sentence": "We make use of partial syntac- tic information together with features ob- tained from the unLP corpus, and con- vert the task into one of sequential BIO- tagging.", "acronym": "LP", "label": "labeled", "ID": "4812"}, {"sentence": "For each stem, we acquire a histogram of sur- rounding words, with a window size of 3, from the unLP corpus.", "acronym": "LP", "label": "labeled", "ID": "4813"}, {"sentence": "Similarly to previous work in hedge cue detec- tion (Morante and Daelemans, 2009), we first con- vert the task into a sequential labeling task based on the BIO scheme, where each word in a hedge cue is LP as B-CUE, I-CUE, or O, indicating respectively the LP word is at the beginning of a cue, inside of a cue, or outside of a hedge cue; this is similar to the tagging scheme from the CoNLL-2001 shared task.", "acronym": "LP", "label": "labeled", "ID": "4814"}, {"sentence": "For each stem, we acquired the distribu- tion of surrounding words from the unLP cor- pus, and calculated the similarity between these distributions and the distribution of hedge cues in the training corpus.", "acronym": "LP", "label": "labeled", "ID": "4815"}, {"sentence": "The problem is especially acute for LP languages, but even in the case of English, we are aware of many category types entirely missing from the Penn Treebank (Clark et al, 2004).", "acronym": "LP", "label": "less privileged", "ID": "4816"}, {"sentence": "Kin- yarwanda is a LP language characterised by lack of electronic resources and insignificant presence on the Internet.", "acronym": "LP", "label": "less privileged", "ID": "4817"}, {"sentence": "We know that any HLT project related to  a LP language should follow those  guidelines, but from our experience we know that  in most cases they do not.", "acronym": "LP", "label": "less privileged", "ID": "4818"}, {"sentence": "As  Gujarati is currently a LP lan- guage in the sense of being resource poor,  manually tagged data is only around 600  sentences.", "acronym": "LP", "label": "less privileged", "ID": "4819"}, {"sentence": "i.alegria@ehu.es        Abstract  We present some Language Technology  applications that have proven to be effec- tive tools to promote the use of Basque, a  European LP language.", "acronym": "LP", "label": "less privileged", "ID": "4820"}, {"sentence": "Abstract Sinhala, spoken in Sri Lanka as an official language, is one of the LP lan- guages; still there are no established text in- put methods.", "acronym": "LP", "label": "less privileged", "ID": "4821"}, {"sentence": "Each feature is weighted according to a set of hand-crafted or machine-learned weights over 165 the DevSet.", "acronym": "DevSet", "label": "development dataset", "ID": "4822"}, {"sentence": "3http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi Feature Set R P F1 Baseline (current word) 44.82 2.86 05.38 + POS & char 3-gram 77.41 27.96 41.09 + previous POS tag 79.77 29.32 42.88 + lexicon (final tagger) 80.44 29.65 43.33 Table 1: Recall (R), precision (P), and F1-measure for the trigger words tagger (in %s) on the DevSet for different feature sets using MIRA training with false negatives as a loss function.", "acronym": "DevSet", "label": "development dataset", "ID": "4823"}, {"sentence": "4 Experimental Setup 4.1 Datasets For our DevSet, we used a subset of the reddit irony corpus (Wallace et al, 2014) com- prising annotated comments from the progressive and conservative subreddits.", "acronym": "DevSet", "label": "development dataset", "ID": "4824"}, {"sentence": "The DevSet includes 1,825 anno- tated comments (876 and 949 from the progressive and conservative subreddits, respectively).", "acronym": "DevSet", "label": "development dataset", "ID": "4825"}, {"sentence": "` 1 ` 2 ) +0.035; +0.034 (+0.000, +0.062) +0.001; +0.000 (-0.011, +0.011) Table 2: Summary results over 500 random train/test splits of the DevSet.", "acronym": "DevSet", "label": "development dataset", "ID": "4826"}, {"sentence": "= 5 is tuned by grid search using the DevSet.", "acronym": "DevSet", "label": "development dataset", "ID": "4827"}, {"sentence": ".01/72.48 73.56/73.79 73.44/73.61 (RR-PCFG) 65.86/66.86 71.84/72.76 74.06/74.28 75.13/75.29 BaseDef (PCFG) 67.68/68.86 71.17/72.47 74.13/74.39 72.54/72.79 (RR-PCFG) 66.65/67.86 73.09/74.13 74.59/74.59 76.05/76.34 BaseDefAcc (PCFG) 68.11/69.30 71.50/72.75 74.16/ 74.41 72.77/73.01 (RR-PCFG) 67.13/68.01 73.63/74.69 74.65/74.79 76.15/ 76.43 Table 1: Parsing Results for Sentences of Length < 40 in the DevSet: Averaged F-Measure With/Without Punctuation.", "acronym": "DevSet", "label": "Development Set", "ID": "4828"}, {"sentence": "339 DevSet (KBP) KBP MPQA Positive Negative Positive Negative Positive Negative P R F1 P R F1 P R F1 P R F1 P R F1 P R F1 KM Gold 90.9 2.5 4.8 93.8 8.6 15.8 93.9 4.3 8.3 93.5 6.6 12.4 61.5 1.3 2.5 90.0 5.2 9.8 Random 16.6 13.1 14.7 4.9 4.0 4.4 13.3 12.7 13.0 10.1 6.9 8.2 10.9 15.4 12.8 8.9 6.7 7.7 Sentence 60.0 16.3 25.7 21.7 43.1 28.8 40.9 20.6 27.4 21.0 31.4 25.2 18.9 3.7 6.2 16.7 18.2 17.4", "acronym": "DevSet", "label": "Development Set", "ID": "4829"}, {"sentence": "no Table 4: Illustrative examples from the RTE3 test suite RTE3 DevSet (800 problems) System % yes precision recall accuracy Core +coref 50.25 68.66 66.99 67.25 Core -coref 49.88 66.42 64.32 64.88 NatLog 18.00 76.39 26.70 58.00 Hybrid, bal.", "acronym": "DevSet", "label": "Development Set", "ID": "4830"}, {"sentence": "of Evidence Instances Training Set 677 DevSet 178 Test Set 183 Table 1: Evidence per data set The evidence instances were obtained from the corpus developed by Molla-Aliod and Santiago- Martinez (2011).", "acronym": "DevSet", "label": "Development Set", "ID": "4831"}, {"sentence": "In the DevSetup, we use ABST as the training corpus and FULL as the de- velopment corpus.", "acronym": "DevSet", "label": "Development Set", "ID": "4832"}, {"sentence": "Training Set Set Positive Negative Neutral Total Train 3,640 (37%) 1,458 (15%) 4,586 (48%) 9,684 DevSet Set Positive Negative Neutral Total Dev 575 (35%) 340(20%) 739 (45%) 1,654 Testing Sets Set Positive Negative Neutral Total LiveJournal 427 (37%) 304 (27%) 411 (36%) 1,142 SMS2013 492 (23%) 394(19%) 1,207 (58%) 2,093 Twitter2013 1,572 (41%) 601 (16%) 1,640 (43%) 3,813 Twitter2014 982 (53%) 202 (11%) 669 (36%) 1,853 Twitter2014Sar 33 (38%) 40 (47%) 13 (15%) 86 Table 4: Class distrib", "acronym": "DevSet", "label": "Development Set", "ID": "4833"}, {"sentence": "c?2009 Association for Computational Linguistics Automatic Agenda Graph Construction from Human-Human Dialogs  using Clustering Method    Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, Gary Geunbae Lee  Department of Computer Science and Engineering  POSTECH  Pohang, South Korea  {lcj80,hugman,getta,gblee}@postech.ac.kr         Abstract  Various knowledge sources are used for spo- ken dialog systems such as task model, do- main model, and agenda.", "acronym": "POSTECH", "label": "Pohang University of Science and Technology", "ID": "4834"}, {"sentence": "of Electrical and  Computer Engineering24, POSTECH  Advanced Information Technology Research Center(Altrc) {meixunj1,colorful2,jhlee4}@  postech.ac.kr  Language Engineering Institute3 Div.", "acronym": "POSTECH", "label": "Pohang University of Science and Technology", "ID": "4835"}, {"sentence": "c?2008 Association for Computational Linguistics Robust Dialog Management with N-best Hypotheses Using Dialog Examples and Agenda Cheongjae Lee, Sangkeun Jung and Gary Geunbae Lee POSTECH Department of Computer Science and Engineering Pohang, Republic of Korea {lcj80,hugman,gblee}@postech.ac.kr Abstract This work presents an agenda-based approach to improve the robustness of the dialog man- ager by using dialog examples and n-best recognition hypotheses.", "acronym": "POSTECH", "label": "Pohang University of Science and Technology", "ID": "4836"}, {"sentence": "939  Heuristic Methods for Reducing Errors of Geographic Named Entities Learned by Bootstrapping Seungwoo Lee and Gary Geunbae Lee Department of Computer Science and Engineering, POSTECH, San 31, Hyoja-dong, Nam-gu, Pohang, 790-784, Republic of Korea Abstract.", "acronym": "POSTECH", "label": "Pohang University of Science and Technology", "ID": "4837"}, {"sentence": "c?2013 Association for Computational Linguistics Enriching Entity Translation Discovery using Selective Temporality Gae-won You, Young-rok Cha, Jinhan Kim, and Seung-won Hwang POSTECH, Republic of Korea {gwyou, line0930, wlsgks08, swhwang}@postech.edu Abstract This paper studies named entity trans- lation and proposes ?", "acronym": "POSTECH", "label": "Pohang University of Science and Technology", "ID": "4838"}, {"sentence": "c?2009 Association for Computational Linguistics A Local Tree Alignment-based Soft Pattern Matching Approach for Information Extraction Seokhwan Kim, Minwoo Jeong, and Gary Geunbae Lee Department of Computer Science and Engineering POSTECH San 31, Hyoja-dong, Nam-gu, Pohang, 790-784, Korea {megaup, stardust, gblee}@postech.ac.kr Abstract This paper presents a new soft pattern match- ing method which aims to improve the recall with minimized precision loss in information extraction tasks.", "acronym": "POSTECH", "label": "Pohang University of Science and Technology", "ID": "4839"}, {"sentence": "POSTECH, Pohang, Korea ?", "acronym": "POSTECH", "label": "Pohang University of Science & Technology", "ID": "4840"}, {"sentence": "In short, I am making a plea for making the  speci f icat ion language used for theory  development in natural language  understanding be a communicat ion language  intended and engineered for human  139   Dec is ion -Tree  based Error  Cor rec t ion  for Stat is t ica l  Phrase  Break   Pred ic t ion  in Korean  *  Byeongchang K im and Geunbae Lee  Del)artment of Computer  Science & Engineering  POSTECH  Pohang, 790-784, South Korea  {bckim, gblee}((~postech.ac.kr  Abst ract   tn this paper, we present a new 1)hrase break  prediction architecture that integrates proba-  bilistic apt)roach with decision-tree based error  correction.", "acronym": "POSTECH", "label": "Pohang University of Science & Technology", "ID": "4841"}, {"sentence": "60  St ructura l  d i sambiguat ion  of  morpho-syntact i c  categor ia l  pars ing  for Korean  *  Jeongwon Cha and Geunbae Lee  Department of Computer Science & Engineering  POSTECH  Pohang, Korea  {himen, gblee}@postech.ac.kr  Abstract  The Korean Combinatory Categorial Grammar  (KCCG) tbrmalism can unitbrmly handle word  order variation among arguments and adjuncts  within a clause as well as in complex clauses  and across clause boundaries, i.e., long distance  scrambling.", "acronym": "POSTECH", "label": "Pohang University of Science & Technology", "ID": "4842"}, {"sentence": "of Computer Science and Engineering  POSTECH  San 31, Hyoja-Dong, Pohang, 790-784, Korea  {leeck,gblee }@postech.ac.kr  Seo JungYun  Natural Language Processing Lab  Dept.", "acronym": "POSTECH", "label": "Pohang University of Science & Technology", "ID": "4843"}, {"sentence": "1361  Unlimited Vocabulary Grapheme to Phoneme Conversion for  Korean TTS  Byeongchang Kim and WonI1 Lee and Geunbae Lee and Jong-Hyeok  Lee  Department of Computer Science & Engineering  POSTECH  Pohang, Korea  {bckim, bdragon, gblee, jhlee)@postech.ac.kr  Abst ract   This paper describes a grapheme-to-phoneme  conversion method using phoneme connectivity  and CCV conversion rules.", "acronym": "POSTECH", "label": "Pohang University of Science & Technology", "ID": "4844"}, {"sentence": "Similarly, Asher (1993) argues that a simple ontology of eventualities is too coarse-grained, and that DR need to distinguish different kinds of abstract objects, including actions, propositions, and facts as well as eventualities.", "acronym": "DR", "label": "discourse representations", "ID": "4845"}, {"sentence": "Some features contain first-order formulas like  the conditions in DR.", "acronym": "DR", "label": "discourse representations", "ID": "4846"}, {"sentence": "As an additional contri- bution, we have presented a novel family of met- rics which operate at the semantic level by analyz- ing DR.", "acronym": "DR", "label": "discourse representations", "ID": "4847"}, {"sentence": "Since segmentation is the first stage of discourse parsing, quality discourse segments are critical to build- ing quality DR (Soricut and Marcu, 2003).", "acronym": "DR", "label": "discourse representations", "ID": "4848"}, {"sentence": "1 Introduction Our proposal is based on a rich set of individual metrics operating at different linguistic levels: lex- ical (i.e., on word forms), shallow-syntactic (e.g., on word lemmas, part-of-speech tags, and base phrase chunks), syntactic (e.g., on dependency and con- stituency trees), shallow-semantic (e.g., on named entities and semantic roles), and semantic (e.g., on DR).", "acronym": "DR", "label": "discourse representations", "ID": "4849"}, {"sentence": "The important point  for our present purposes is that the representa-  tion of the meaning of the S is built up from the  DR of the subject and the  predicate.", "acronym": "DR", "label": "discourse representations", "ID": "4850"}, {"sentence": "In Workshop on DR, pages 114?120.", "acronym": "DR", "label": "Discourse Relations", "ID": "4851"}, {"sentence": "Posters and Demonstrations, pages 87?90 Manchester, August 2008 Easily Identifiable DR Emily Pitler, Mridhula Raghupathy, Hena Mehta, Ani Nenkova, Alan Lee, Aravind Joshi University of Pennsylvania 3330 Walnut Street Philadelphia, PA 19104 Abstract We present a corpus study of local dis- course relations based on the Penn Dis- course Tree Bank, a large manually anno- tated corpus of explicitly or implicitly re- alized relations.", "acronym": "DR", "label": "Discourse Relations", "ID": "4852"}, {"sentence": "In COLING/ACL Workshop on DR and Discourse Markers, pages 86?92, Montreal, Quebec, Canada.", "acronym": "DR", "label": "Discourse Relations", "ID": "4853"}, {"sentence": "Inferring DR in Context.", "acronym": "DR", "label": "Discourse Relations", "ID": "4854"}, {"sentence": "In  Intentionality And Structure In DR:  Proceedings Of A Workshop Sponsored By The Special  Interest Group On Generation Of The Association For  Computational Linguistics, Columbus, OH, June.", "acronym": "DR", "label": "Discourse Relations", "ID": "4855"}, {"sentence": "4.1 DR PDTB contains annotations for four coarse-grained discourse relation types, as shown in the left column of Table 1.", "acronym": "DR", "label": "Discourse Relations", "ID": "4856"}, {"sentence": "We incorporate DR in our general ranking framework, which creates a dynamicM during each iteration, rather than a static one.", "acronym": "DR", "label": "DivRank", "ID": "4857"}, {"sentence": "By incorporating DR, we obtain rank r?i and the global biased ranking score Gi for sentence si from date t to summarize Ct.", "acronym": "DR", "label": "DivRank", "ID": "4858"}, {"sentence": "2007) and DR (Mei et al.,", "acronym": "DR", "label": "DivRank", "ID": "4859"}, {"sentence": "Most re- cently diversity rank DR is another solution to diversity penalization in (Mei et al, 2010).", "acronym": "DR", "label": "DivRank", "ID": "4860"}, {"sentence": "4) DR (Mei et al.,", "acronym": "DR", "label": "DivRank", "ID": "4861"}, {"sentence": "R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 R-1 R-2 RSU4 Random 0.4091 0.1046 0.1576 0.4496 0.1100 0.1925 0.4442 0.1192 0.1858 0.4154 0.1130 0.1693 0.4309 0.1115 0.1771 Centroid 0.4029 0.0993 0.1484 0.4228 0.1100 0.1764 0.4235 0.1192 0.1722 0.3763 0.0787 0.1386 0.4133 0.1077 0.1640 LexRank 0.4396 0.1451 0.1891 0.4406 0.1296 0.1955 0.4304 0.1397 0.1859 0.4032 0.0992 0.1661 0.4350 0.1331 0.1894 DR 0.4534 0.1504 0.1888 0.4473 0.1161 0.1925 0.4391 0.1167 0.1804 0.4275 0.1180 0.1733 0.4487 0.1317 0.1888 GMDS 0.3918 0.0890 0.1415 0.4339 0.1066 0.1784 0.4064 0.0845 0.1576 0.3846 0.0809 0.1413 0.4045 0.0916 0.1553 ILP-BL 0.4635 0.1650 0.2000 0.4948 0.1731 0.2333 0.4691 0.1613 0.2073 0.4545 0.1445 0.1981 0.4755 0.1654 0.2136 Our Method 0.4723 0.1655 0.2035 0.5078 0.1787 0.2397 0.4716 0.171", "acronym": "DR", "label": "DivRank", "ID": "4862"}, {"sentence": "Examples 1 and 2 show the representation that would be obtained for two imaginary EN sen- 2Clause delimiters are punctuation marks other than com- mata, relative pronouns and subordinating conjunctions.", "acronym": "EN", "label": "English", "ID": "4863"}, {"sentence": "5 Unsuperv ised  Learn ing  o f  Gender   In fo rmat ion   The importance of gender information as re-  vealed in the previous experiments caused us to  consider automatic methods for estimating the  probability that nouns occurring in a large cor-  pus of EN text deonote inanimate, mascu-  line or feminine things.", "acronym": "EN", "label": "English", "ID": "4864"}, {"sentence": "The second half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of EN words, information that is  itself used in the pronoun resolution program.", "acronym": "EN", "label": "English", "ID": "4865"}, {"sentence": "We believe that the approach could be straightforwardly extended to other Indoeuropean languages, such as Spanish, German or EN.", "acronym": "EN", "label": "English", "ID": "4866"}, {"sentence": "The Grammar Ma- trix was developed initially on the basis of broad- coverage grammars for EN (Flickinger, 2000) and Japanese (Siegel and Bender, 2002), and has since been extended and refined as it has been used in the development of broad-coverage grammars for Norwegian (Hellan and Haugereid, 2003), Modern Greek (Kordoni and Neu, 2005), and Spanish (Ma- rimon et al, 2007), as well as being applied to 42 other languages from a variety of language families in a classroom context (B", "acronym": "EN", "label": "English", "ID": "4867"}, {"sentence": "In recent research in the field, the main effort has been to infer semantic classes for verbs, in EN (Stevenson et al, 1999) and German (Schulte im Walde and Brew, 2002).", "acronym": "EN", "label": "English", "ID": "4868"}, {"sentence": "Examples 1 and 2 show the representation that would be obtained for two imaginary ENh sen- 2Clause delimiters are punctuation marks other than com- mata, relative pronouns and subordinating conjunctions.", "acronym": "EN", "label": "Englis", "ID": "4869"}, {"sentence": "5 Unsuperv ised  Learn ing  o f  Gender   In fo rmat ion   The importance of gender information as re-  vealed in the previous experiments caused us to  consider automatic methods for estimating the  probability that nouns occurring in a large cor-  pus of ENh text deonote inanimate, mascu-  line or feminine things.", "acronym": "EN", "label": "Englis", "ID": "4870"}, {"sentence": "The second half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of ENh words, information that is  itself used in the pronoun resolution program.", "acronym": "EN", "label": "Englis", "ID": "4871"}, {"sentence": "We believe that the approach could be straightforwardly extended to other Indoeuropean languages, such as Spanish, German or ENh.", "acronym": "EN", "label": "Englis", "ID": "4872"}, {"sentence": "The Grammar Ma- trix was developed initially on the basis of broad- coverage grammars for ENh (Flickinger, 2000) and Japanese (Siegel and Bender, 2002), and has since been extended and refined as it has been used in the development of broad-coverage grammars for Norwegian (Hellan and Haugereid, 2003), Modern Greek (Kordoni and Neu, 2005), and Spanish (Ma- rimon et al, 2007), as well as being applied to 42 other languages from a variety of language families in a classroom context (B", "acronym": "EN", "label": "Englis", "ID": "4873"}, {"sentence": "In recent research in the field, the main effort has been to infer semantic classes for verbs, in ENh (Stevenson et al, 1999) and German (Schulte im Walde and Brew, 2002).", "acronym": "EN", "label": "Englis", "ID": "4874"}, {"sentence": "l as t - s ide   CONTROL a * NATIVE-OR-HOT  VALUENon-nat ive   NORTH/YOUNG NORTH/OLD ? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "acronym": "EN", "label": "E - n", "ID": "4875"}, {"sentence": "HATIVE-OR-NOT  VALUENo,-ne~.lve  HORTH/YOUNG NORTH/OLD . . . . . . . . . . . . . . . . . . . . . .", "acronym": "EN", "label": "E - n", "ID": "4876"}, {"sentence": "Sri and ubc: Simple similarity features for STS.", "acronym": "STS", "label": "semantic textual similarity", "ID": "4877"}, {"sentence": "Semeval-2012 task 6: A pilot on STS.", "acronym": "STS", "label": "semantic textual similarity", "ID": "4878"}, {"sentence": "Ukp: Computing STS by combining multiple content similarity measures.", "acronym": "STS", "label": "semantic textual similarity", "ID": "4879"}, {"sentence": "SemEval-2012 Task 6: A pilot on STS.", "acronym": "STS", "label": "semantic textual similarity", "ID": "4880"}, {"sentence": "SemEval-2012 Task 6: A Pilot on STSy.", "acronym": "STS", "label": "Semantic Textual Similarit", "ID": "4881"}, {"sentence": "For details on annotation guidelines and                                                    1 In *SEM?2013, the shared task is changed with focus on  \"STSy\".", "acronym": "STS", "label": "Semantic Textual Similarit", "ID": "4882"}, {"sentence": "UMBC EBIQUITY-CORE: STSy Systems.", "acronym": "STS", "label": "Semantic Textual Similarit", "ID": "4883"}, {"sentence": "SEM 2013 shared task: STSy.", "acronym": "STS", "label": "Semantic Textual Similarit", "ID": "4884"}, {"sentence": "F-93430, Villetaneuse, France {buscaldi,joseph.le-roux,jgflores} @lipn.univ-paris13.fr Adrian Popescu CEA, LIST, Vision & Content Engineering Laboratory F-91190 Gif-sur-Yvette, France adrian.popescu@cea.fr Abstract This paper describes the system used by the LIPN team in the STSy task at *SEM 2013.", "acronym": "STS", "label": "Semantic Textual Similarit", "ID": "4885"}, {"sentence": "c?2014 Association for Computational Linguistics Probabilistic Soft Logic for STSy Islam Beltagy ?", "acronym": "STS", "label": "Semantic Textual Similarit", "ID": "4886"}, {"sentence": "3.2 Consistency Analysis of Different STS In Section 3.1 we have refuted two hypotheses.", "acronym": "STS", "label": "Segmenters", "ID": "4887"}, {"sentence": "3.1 Character-based, Lexicon-based and Feature-based STS The training data for the segmenter is two orders of magnitude smaller than for the MT system, it is not terribly well matched to it in terms of genre and variety, and the information an MT system learns about alignment of Chinese to English might be the basis for a task appropriate segmentation style for Chinese-English MT.", "acronym": "STS", "label": "Segmenters", "ID": "4888"}, {"sentence": "5.2 Compar ing  STS   Word segmentation is a big issue for Chinese since  linguistics-strong applications uch as POS tagging,  sentence parsing, machine translation, text to voice,  etc.", "acronym": "STS", "label": "Segmenters", "ID": "4889"}, {"sentence": "6 Evaluation of Automatic STS Having looked at how S, WD, and B perform at a small scale in ?", "acronym": "STS", "label": "Segmenters", "ID": "4890"}, {"sentence": "3.4 Evaluating Automatic STS Coders often disagree in segmentation tasks (Hearst, 1997, p. 56), making it improbable that a single, correct, reference segmentation could be identified from human codings.", "acronym": "STS", "label": "Segmenters", "ID": "4891"}, {"sentence": "C5  <- 28 Long Queries @~- 28 Short Queries --->  Queens UMASS Queens UMASS  RR 2059 2070 1972 1991  AvPre .467 .460 .417 .414  P@10 .625 .589 .554 .561  R.Pre .471 .453 .413 .412  RR  AvPre  P@IO  R.Pre  TREC6  ~.- 26 Long Queries @~-26 Short Queries--~  Queens UMASS Queens UMASS  2791 2761 2547 2488  .603 .587 .476 .491  .869 .850 .712 .750  .567 .557 .463 .476  Table4: Comparing Queens & UMASS STS  134  to segment he string into short-words, thereby also  discovering unknown words.", "acronym": "STS", "label": "Segmenters", "ID": "4892"}, {"sentence": "SemEval-2012 Task 6: A Pilot on STS.", "acronym": "STS", "label": "Semantic Textual Similarity", "ID": "4893"}, {"sentence": "For details on annotation guidelines and                                                    1 In *SEM?2013, the shared task is changed with focus on  \"STS\".", "acronym": "STS", "label": "Semantic Textual Similarity", "ID": "4894"}, {"sentence": "UMBC EBIQUITY-CORE: STS Systems.", "acronym": "STS", "label": "Semantic Textual Similarity", "ID": "4895"}, {"sentence": "SEM 2013 shared task: STS.", "acronym": "STS", "label": "Semantic Textual Similarity", "ID": "4896"}, {"sentence": "F-93430, Villetaneuse, France {buscaldi,joseph.le-roux,jgflores} @lipn.univ-paris13.fr Adrian Popescu CEA, LIST, Vision & Content Engineering Laboratory F-91190 Gif-sur-Yvette, France adrian.popescu@cea.fr Abstract This paper describes the system used by the LIPN team in the STS task at *SEM 2013.", "acronym": "STS", "label": "Semantic Textual Similarity", "ID": "4897"}, {"sentence": "c?2014 Association for Computational Linguistics Probabilistic Soft Logic for STS Islam Beltagy ?", "acronym": "STS", "label": "Semantic Textual Similarity", "ID": "4898"}, {"sentence": "The performance of the experiments on two  datasets indicates feasibility and potentiality of  the QC.", "acronym": "QC", "label": "quantum classifier", "ID": "4899"}, {"sentence": "1008  A Language Independent Method for QC Thamar Solorio1, Manuel Pe?rez-Coutin?o1, Manuel Montes-y-Go?mez1,2 Luis Villasen?or-Pineda1 and Aurelio Lo?pez-Lo?pez1 1Language Technologies Group, Computer Science Department National Institute of Astrophysics, Optics and Electronics 72840 Tonantzintla, Puebla, Mexico 2Departamento de Sistemas Informa?ticos y Computacio?n Universidad Polite?cnica de Valencia Espan?a {tha", "acronym": "QC", "label": "Question Classification", "ID": "4900"}, {"sentence": "QC Data: We built a new taxonomy for Question Classifica- tion based on the NE categories discussed above.", "acronym": "QC", "label": "Question Classification", "ID": "4901"}, {"sentence": "3.2.2 QC We classify the question to the anticipated type of the answer.", "acronym": "QC", "label": "Question Classification", "ID": "4902"}, {"sentence": "6.1 QC Question classification is defined as a task similar to text categorization; it maps a given question into a question type.", "acronym": "QC", "label": "Question Classification", "ID": "4903"}, {"sentence": "5.2 QC Module We evaluated the classifier based on our proposed taxonomy using 230 Arabic questions.", "acronym": "QC", "label": "Question Classification", "ID": "4904"}, {"sentence": "43 3 QC  First, we bought five language test books for  second grade students and one of them, pub- lished by KUMON, was used as a training text  to develop our system.", "acronym": "QC", "label": "Question Classification", "ID": "4905"}, {"sentence": "1008  A Language Independent Method for QCn Thamar Solorio1, Manuel Pe?rez-Coutin?o1, Manuel Montes-y-Go?mez1,2 Luis Villasen?or-Pineda1 and Aurelio Lo?pez-Lo?pez1 1Language Technologies Group, Computer Science Department National Institute of Astrophysics, Optics and Electronics 72840 Tonantzintla, Puebla, Mexico 2Departamento de Sistemas Informa?ticos y Computacio?n Universidad Polite?cnica de Valencia Espan?a {tha", "acronym": "QC", "label": "Question Classificatio", "ID": "4906"}, {"sentence": "QCn Data: We built a new taxonomy for Question Classifica- tion based on the NE categories discussed above.", "acronym": "QC", "label": "Question Classificatio", "ID": "4907"}, {"sentence": "3.2.2 QCn We classify the question to the anticipated type of the answer.", "acronym": "QC", "label": "Question Classificatio", "ID": "4908"}, {"sentence": "6.1 QCn Question classification is defined as a task similar to text categorization; it maps a given question into a question type.", "acronym": "QC", "label": "Question Classificatio", "ID": "4909"}, {"sentence": "5.2 QCn Module We evaluated the classifier based on our proposed taxonomy using 230 Arabic questions.", "acronym": "QC", "label": "Question Classificatio", "ID": "4910"}, {"sentence": "43 3 QCn  First, we bought five language test books for  second grade students and one of them, pub- lished by KUMON, was used as a training text  to develop our system.", "acronym": "QC", "label": "Question Classificatio", "ID": "4911"}, {"sentence": "2006), QC (Nguyen et al, 2008) and POS tagging (S?gaard, 2010).", "acronym": "QC", "label": "question classification", "ID": "4912"}, {"sentence": "Given that the QA system does not do any kind of QC and it does not use any NE recogniser, the results are sat- isfactory.", "acronym": "QC", "label": "question classification", "ID": "4913"}, {"sentence": "In Section 6, we compare the performance of conven- tional methods with that of the proposed method by using real NLP tasks: QC and sentence modality identification.", "acronym": "QC", "label": "question classification", "ID": "4914"}, {"sentence": "Medical domain \u0001 EpoCare (Niu and Hirst 2004) \u0001 system by University of Maryland (Demner-Fushman and Lin 2005) \u0001 QC by Columbia University and Cooper Union (Yu, Sable, and Zhu 2005) \u0001 IMIX 12.", "acronym": "QC", "label": "question classification", "ID": "4915"}, {"sentence": "Analysis of statistical QC for fact-based ques- tions.", "acronym": "QC", "label": "question classification", "ID": "4916"}, {"sentence": "Task Classification for Multitask Learning: We explore two simple QC schemes.", "acronym": "QC", "label": "question classification", "ID": "4917"}, {"sentence": "The  subscript i ranges over all the QC.", "acronym": "QC", "label": "query classes", "ID": "4918"}, {"sentence": "To distinguish between these QC, we introduce novel met- rics based on the entropy of the click distri- butions of individual searchers.", "acronym": "QC", "label": "query classes", "ID": "4919"}, {"sentence": "To maximize the little information we have  about the QC, we treat the words in  query class names as additional example queries.", "acronym": "QC", "label": "query classes", "ID": "4920"}, {"sentence": "Definitions of QC: we now more for- mally define the QC we consider: ?", "acronym": "QC", "label": "query classes", "ID": "4921"}, {"sentence": "To maximize the little information we have  about the QC, we treat the words in  query class names as", "acronym": "QC", "label": "query classes", "ID": "4922"}, {"sentence": "4.2 Baseline classifier  Since the QC are not mutually  exclusive, we treat the query classification task  as 67 binary classification problems.", "acronym": "QC", "label": "query classes", "ID": "4923"}, {"sentence": "Table 2 reports the distribution of QC in our dataset.", "acronym": "QC", "label": "query classes", "ID": "4924"}, {"sentence": "The first clause (monotone, reverse) in- dicates whether the TO follows the source order; the second (adjacent, gap) indicates whether the anchor and its neighboring constituent are adja- cent or separated by an intervening when projected.", "acronym": "TO", "label": "target order", "ID": "4925"}, {"sentence": "2.2 Recurs ive  Head Transduct ion   We can apply a set of head transducers recursively  to derive a pair of source-TOed depen-  dency trees.", "acronym": "TO", "label": "target order", "ID": "4926"}, {"sentence": "In this paper, we take 843 Figure 1: An example with a source sentence F re- ordered into TO F ?,", "acronym": "TO", "label": "target order", "ID": "4927"}, {"sentence": "Unlike with the Bleu score, rank order centroid weights (rather than the geometric mean) are used to combine scores of different orders, which avoids problems with scoring partial realizations which have no n- gram matches of the TO.", "acronym": "TO", "label": "target order", "ID": "4928"}, {"sentence": "4.1 Calculating Oracle Orderings In order to calculate reordering quality, we first define a ranking function r(fj |F,A), which indi- cates the relative position of source word fj in the proper TO (Figure 2 (a)).", "acronym": "TO", "label": "target order", "ID": "4929"}, {"sentence": "Source reordering is based on a set of learned rewrite rules that non- deterministically reorder the input words so as to match the TO thereby generating a lattice of possible reorderings.", "acronym": "TO", "label": "target order", "ID": "4930"}, {"sentence": "We incorpo-  rate multiple anaphora resolution facTOrs inTO  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.", "acronym": "TO", "label": "to", "ID": "4931"}, {"sentence": "In particular, the scheme infers the gender of a  referent from the gender of the pronouns that  161  refer TO it and selects referents using the pro-  noun anaphora program.", "acronym": "TO", "label": "to", "ID": "4932"}, {"sentence": "A Statistical Approach TO Anaphora Resolution  Niyu  Ge, John  Hale and Eugene Charn iak   Dept.", "acronym": "TO", "label": "to", "ID": "4933"}, {"sentence": "The second half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program TO learn auTOmatically the typi-  cal gender of English words, information that is  itself used in the pronoun resolution program.", "acronym": "TO", "label": "to", "ID": "4934"}, {"sentence": "We combine  them inTO a single probability that enables us  TO identify the referent.", "acronym": "TO", "label": "to", "ID": "4935"}, {"sentence": "We incorpo-  rate multiple anaphora resolution facTOrs inTO  a statistical framework - -  specifically the dis-", "acronym": "TO", "label": "to", "ID": "4936"}, {"sentence": "504  Proceedings of the Workshop on BioNLP: ST, pages 95?98, Boulder, Colorado, June 2009.", "acronym": "ST", "label": "Shared Task", "ID": "4937"}, {"sentence": "National Centre for Text Mining, UK {sebastian,chun,takagi}@dbcls.rois.ac.jp tsujii@is.s.u-tokyo.ac.jp Abstract In this paper we describe our entry to the BioNLP 2009 ST regarding bio- molecular event extraction.", "acronym": "ST", "label": "Shared Task", "ID": "4938"}, {"sentence": "7 Conclusion Our approach the BioNLP ST 2009 can be characterized by three decisions: (a) jointly CORE VALID FULL eventType 52.8 63.2 64.3 role 44.0 53.5 55.7 site 42.0 46.0 51.5 Total 50.7 60.1 61.9 Table 4: Ground atom F-scores for global formulae.", "acronym": "ST", "label": "Shared Task", "ID": "4939"}, {"sentence": "The CoNLL- 2010 ST: Learning to Detect Hedges and their Scope in Natural Language Text.", "acronym": "ST", "label": "Shared Task", "ID": "4940"}, {"sentence": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning: ST, pages 138?143, Uppsala, Sweden, 15-16 July 2010.", "acronym": "ST", "label": "Shared Task", "ID": "4941"}, {"sentence": "Proceedings of the Workshop on BioNLP: ST, pages 41?49, Boulder, Colorado, June 2009.", "acronym": "ST", "label": "Shared Task", "ID": "4942"}, {"sentence": "The algorithm ST until all features are pruned.", "acronym": "ST", "label": "stops", "ID": "4943"}, {"sentence": "The chosen fragment is composed with the current subanalysis to produce a new one; the process ST when an analysis results with no non-terminal leaves.", "acronym": "ST", "label": "stops", "ID": "4944"}, {"sentence": "We run each training procedure until the area under the precision/recall curve measured on a development corpus ST increasing (see Figure 4 for an example of such a curve).", "acronym": "ST", "label": "stops", "ID": "4945"}, {"sentence": "The moni tor  mon i tors  the whole reconst ruct ion  pro-  cess and ST the process by raising the th resho ld  to  judge the incons istency when it judges that the  reconst rnet ion  takes too much t ime.", "acronym": "ST", "label": "stops", "ID": "4946"}, {"sentence": "Announcer speaks when expert ST Thanks.", "acronym": "ST", "label": "stops", "ID": "4947"}, {"sentence": "In order to rule out the possibility that the sampling process never ST, we use a maximum sample size of 10,000 derivations.", "acronym": "ST", "label": "stops", "ID": "4948"}, {"sentence": "5.1 ST Statistics Out of 57,809 sentences, 6,047 (10.5%) are anno- tated (see Table 2); and 4,934 (8.5%) have multi- token bracketings.", "acronym": "ST", "label": "Surface Text", "ID": "4949"}, {"sentence": "Automatic Event Classification Using ST Features.", "acronym": "ST", "label": "Surface Text", "ID": "4950"}, {"sentence": "Learning ST Patterns for a Question An- swering System.", "acronym": "ST", "label": "Surface Text", "ID": "4951"}, {"sentence": "Learn- ing ST Patterns for a Question Answering  System.", "acronym": "ST", "label": "Surface Text", "ID": "4952"}, {"sentence": "4.1 Extraction with ST Patterns To extract information about roles, we used the set of surface patterns originally developed for the QA system we used at TREC 2003 (Jijkoun et al, 2004).", "acronym": "ST", "label": "Surface Text", "ID": "4953"}, {"sentence": "4.2 Generating ST  SUMO-based story plans provide a form of inter- lingua where story details are represented in logi- cal form.", "acronym": "ST", "label": "Surface Text", "ID": "4954"}, {"sentence": "One way to speed up Gibbs sampling is by ST (Marinari and Parisi, 1992), which performs simulation in a gener- alized ensemble, and can rapidly achieve an equilibrium state.", "acronym": "ST", "label": "Simulated Tempering", "ID": "4955"}, {"sentence": "ST: A newMonte Carlo scheme.", "acronym": "ST", "label": "Simulated Tempering", "ID": "4956"}, {"sentence": "We use three Markov chain Monte Carlo (MCMC) algorithms, including Gibbs sampling, ST, as well as MC-SAT, andMaximum a posteriori/Most Probable Expla- nation (MAP/MPE) algorithm for probabilistic inference in MLNs.", "acronym": "ST", "label": "Simulated Tempering", "ID": "4957"}, {"sentence": "There are 476 En- glish Wikipedia articles in the Controversial corpus 187 Classification Frequency Totally Different 2 Same General Topic 3 Overlapping Topics 11 STs 33 Parallel 1 and 384 associated ?", "acronym": "ST", "label": "Same Topic", "ID": "4958"}, {"sentence": "10 Conclusion We presented the first context-dependent semantic parsing system to detect and reST expres- sions.", "acronym": "ST", "label": "solve time", "ID": "4959"}, {"sentence": "5 Experiments In the following sub-sections we present experi- mental results on learning to reST expres- sions in Twitter using minimal supervision.", "acronym": "ST", "label": "solve time", "ID": "4960"}, {"sentence": "First we turn our attention to the STs obtained using ILP (for the sentences for which the solution was found within 30 min- utes).", "acronym": "ST", "label": "solve time", "ID": "4961"}, {"sentence": "Recently, Angeli et al introduced the idea of learning semantic parsers to reST expres- sions (Angeli et al, 2012) and showed that the ap- proach can generalize to multiple languages (An- geli and Uszkoreit, 2013).", "acronym": "ST", "label": "solve time", "ID": "4962"}, {"sentence": "The table shows that the average ST is under one minute per sentence.", "acronym": "ST", "label": "solve time", "ID": "4963"}, {"sentence": "The sparse sequence alignment technique mentioned  above is especially intriguing, because the complexity introduced in dealing with sparsity resembles in certain  respects that of the time warp problem; further, the fact that the sparse alignment problem can be solved efficiently  gives rise to hope that the same techniques can be used to ST warping.", "acronym": "ST", "label": "solve time", "ID": "4964"}, {"sentence": "As we increase the sentence length we see the ST increases, however, we never see an order of magnitude in- crease between brackets as witnessed by Germann et al (2004) thus optimal decoding is more practi- ca", "acronym": "ST", "label": "solve time", "ID": "4965"}, {"sentence": "As we increase the sentence length we see the ST increases, however, we never see an order of magnitude in- crease between brackets as witnessed by Germann et al (2004) thus optimal decoding is more practi- cal than previously suggested.", "acronym": "ST", "label": "solve time", "ID": "4966"}, {"sentence": "2.2  Decomposition operations and Fragments The fragments for LFG-DOP consist of connected STs whose nodes are in ?-", "acronym": "ST", "label": "subtree", "ID": "4967"}, {"sentence": "Af- ter ranking the first level, the beam zooms in on the top-ranked cells and constructs a finer k-d tree under each one (one such ST is shown in the top-right map callout).", "acronym": "ST", "label": "subtree", "ID": "4968"}, {"sentence": "disjunct from the ST above).", "acronym": "ST", "label": "subtree", "ID": "4969"}, {"sentence": "nsist of connected STs whose nodes are in ?-", "acronym": "ST", "label": "subtree", "ID": "4970"}, {"sentence": "To give a precise definition of LFG-DOP fragments, it is convenient to recall the decomposition operations employed by the orginal DOP model which is also known as the \"Tree-DOP\" model (Bod 1993, 1998): (1)  Root: the Root operation selects any node of a tree to be the root of the new ST and erases all nodes except the selected node and the nodes it dominates.", "acronym": "ST", "label": "subtree", "ID": "4971"}, {"sentence": "To give a precise definition of LFG-DOP fragments, it is convenient to recall the decomposition operations employed by the orginal DOP model which is also known as the \"Tree-DOP\" model (Bod 1993, 1998): (1)  Root: the Root operation selects any node of a tree to be the root of the new ST and erases", "acronym": "ST", "label": "subtree", "ID": "4972"}, {"sentence": "When a node is selected by the Root operation, all nodes outside of that node's ST are erased, just as in Tree-DOP.", "acronym": "ST", "label": "subtree", "ID": "4973"}, {"sentence": "loyed by the orginal DOP model which is also known as the \"Tree-DOP\" model (Bod 1993, 1998): (1)  Root: the Root operation selects any node of a tree to be the root of the new ST and erases all nodes except the selected node and the nodes it dominates.", "acronym": "ST", "label": "subtree", "ID": "4974"}, {"sentence": "ient to recall the decomposition operations employed by the orginal DOP model which is also known as the \"Tree-DOP\" model (Bod 1993, 1998): (1)  Root: the Root operation selects any node of a tree to be the root of the new ST and erases all nodes except the selected node and the nodes it dominates.", "acronym": "ST", "label": "subtree", "ID": "4975"}, {"sentence": "(2)  Frontier : the Frontier operation then chooses a set (possibly empty) of nodes in the new ST different from its root and erases all STs dominated by the chosen nodes.", "acronym": "ST", "label": "subtree", "ID": "4976"}, {"sentence": "F the forest made of the STs of all the  considered lexical entries plus the output (the type  we want to derive).", "acronym": "ST", "label": "syntactic tree", "ID": "4977"}, {"sentence": "For others it is unclear exactly how much of the syntactic annotations can be ef- fectively leveraged with current models, and what structures in the STs are most relevant to the current task.", "acronym": "ST", "label": "syntactic tree", "ID": "4978"}, {"sentence": "For En- glish, not only is the hidden marginalization method a suitable replacement for the STs pro- vided by pre-trained, state-of-the-art models, but in both configurations we find that inducing an optimal hidden structure is preferable to the parser-produced annotations.", "acronym": "ST", "label": "syntactic tree", "ID": "4979"}, {"sentence": "Similarly, dependency-based  models (e.g., Collins, 1996; Chelba et al, 1997) use  a dependency structure D of W instead of a parse  tree T, where D is extracted from STs.", "acronym": "ST", "label": "syntactic tree", "ID": "4980"}, {"sentence": "3.0.1 The  Hobbs a lgor i thm  The Hobbs algorithm makes a few assumptions  about the STs upon which it operates  that are not satisfied by the tree-bank trees that  form the substrate for our algorithm.", "acronym": "ST", "label": "syntactic tree", "ID": "4981"}, {"sentence": "1 ) *l-R-?llq -',4?I 'ffd-<: 11,II?1 -~.:~.  This utterance has two distinct STs:  ( 1 ) S/PP j i / \\  / \\  PP\\[de\\]  '\\] ~J ~b lJ, 6 #,/PP VP  / \" . . . .", "acronym": "ST", "label": "syntactic tree", "ID": "4982"}, {"sentence": "The more the feature terms  illustrates the ST feature, the higher the  weight of the topic feature.", "acronym": "ST", "label": "same topic", "ID": "4983"}, {"sentence": "ftj\u0019fi  shows feature term set illustrating the ST  feature fi.", "acronym": "ST", "label": "same topic", "ID": "4984"}, {"sentence": "ftj\u0019fi  shows feature term set illustrating the ST", "acronym": "ST", "label": "same topic", "ID": "4985"}, {"sentence": "To test the hypothesis, we compare the performance of our proposed model trained on documents from all available out-of-topic data with two models, one trained on single out-of-topic data and another trained on the ST (intra-topic) This work is licenced under a Creative Commons Attribution 4.0 International License.", "acronym": "ST", "label": "same topic", "ID": "4986"}, {"sentence": "Since the repetition of lexi-  cal items occurs more frequently within regions of a text  which are about the ST or group of topics, the  visually apparent squares along the main diagonal of  the plot correspond to regions of the text.", "acronym": "ST", "label": "same topic", "ID": "4987"}, {"sentence": "rates the ST feature, the higher the  weight of the topic feature.", "acronym": "ST", "label": "same topic", "ID": "4988"}, {"sentence": "1 In t roduct ion   Even moderately ong documents typically address  sew~ral topics or different aspects of the ST.", "acronym": "ST", "label": "same topic", "ID": "4989"}, {"sentence": "A FU Q can be used to ask (i) a similar question as the previous one but with different constraints or different participants (topic extension); (ii) a ques- tion concerning a different aspect of the ST (topic exploration); (iii) a question concerning a related activity or a related entity (topic shift).", "acronym": "ST", "label": "same topic", "ID": "4990"}, {"sentence": "Kiros et al (2015) also use GRUs instead of LSTM.", "acronym": "GRU", "label": "Gated Recurrent Unit", "ID": "4991"}, {"sentence": "Kiros et al (2015) train an encoder-decoder model to encode a sentence into a fixed-length vector and subsequently decode both the following and preceding sentence, using GRUs (Chung et al, 2014).", "acronym": "GRU", "label": "Gated Recurrent Unit", "ID": "4992"}, {"sentence": "Two independent GRUs are used to model the word embeddings and utterance embeddings separately on word sequence view and utterance sequence view, the former of which captures dependencies in word level and the latter captures utterance-level semantic and discourse information.", "acronym": "GRU", "label": "Gated Recurrent Unit", "ID": "4993"}, {"sentence": "The encoder is a bidirectional neural network with GRUs (Cho et al, 2014) that reads an input sequence x = (x 1 , ..., x m ) and calculates a forward sequence of hidden states ( ??", "acronym": "GRU", "label": "gated recurrent unit", "ID": "4994"}, {"sentence": "The BASIC NMT system is built using the Blocks framework (van Merri?enboer et al, 2015) based on the Theano library (Bastien et al, 2012) with standard hyper-parameters (Bahdanau et al, 2015): the encoder and decoder networks consist of 1000 GRUs (Cho et al, 2014).", "acronym": "GRU", "label": "gated recurrent unit", "ID": "4995"}, {"sentence": "Recurrent neural networks Recent years have shown a resurgence of interest in RNN, particularly variants with long short-term memory (Hochreiter and Schmidhuber, 1997) or GRUs (Cho et al, 2014).", "acronym": "GRU", "label": "gated recurrent unit", "ID": "4996"}, {"sentence": "They used a GRU for f (see, e.g., (Cho et al, 2014b)).", "acronym": "GRU", "label": "gated recurrent unit", "ID": "4997"}, {"sentence": "Kumar et al (2015) proposed a similar model where a memory was designed to change the gate of the GRU for each iteration.", "acronym": "GRU", "label": "gated recurrent unit", "ID": "4998"}, {"sentence": "s (W r x t +U r h t?1 ) (4) Our GRUs use steep sigmoids for gate activations: ?", "acronym": "GRU", "label": "gated recurrent unit", "ID": "4999"}, {"sentence": "As an example, the left of Figure 4 shows (solid line) the change of the importance score with time when an INT takes place (the dotted line represents the importance score without interrup- tion).", "acronym": "INT", "label": "interruption", "ID": "5000"}, {"sentence": "Further, it illustrates that discourse issues such as controlling INT, abbreviation, and main- taining consistency can also be decom- posed: rather than considering them at the single level of one linear explana- tion they can also be tackled separately within each individual agent.", "acronym": "INT", "label": "interruption", "ID": "5001"}, {"sentence": "Further, discourse issues such as control- ling INT, abbreviation, and maintaining consistency can also be decomposed: rather than considering them at the single level of one linear explanation they can be tackled separately within each individual agent and then also at the level of inter-agent cooperation.", "acronym": "INT", "label": "interruption", "ID": "5002"}, {"sentence": "Similarly, it may happen that when the two most important propositions in shared memory Importance Score/Time Ends the first utterrance without INT An Important event occurs with INT without INT time < > ?", "acronym": "INT", "label": "interruption", "ID": "5003"}, {"sentence": "The discourse control techniques of INT, rep- etition, abbreviation, and silence are used to con- trol both the dialogue strategies of each individual agent and also the interaction between them.", "acronym": "INT", "label": "interruption", "ID": "5004"}, {"sentence": "He takes her out arg1 arg2 (4) This example demonstrates that we cannot sim- ply treat MWEs as contiguous word strings and include those in the lexicon, since the MWE takes out is INT by the object her in (3).", "acronym": "INT", "label": "interrupted", "ID": "5005"}, {"sentence": "We also show a graph on how many of the utterances would have been INT in Figure 5.", "acronym": "INT", "label": "interrupted", "ID": "5006"}, {"sentence": "However, at this point, about 20% of the callers would be INT.", "acronym": "INT", "label": "interrupted", "ID": "5007"}, {"sentence": "Figure 5: Percentage of utterances INT by maximum speech time-out.", "acronym": "INT", "label": "interrupted", "ID": "5008"}, {"sentence": "Speci\fcally, we add a bonus to the scores of propo- sitions uttered directly before a period where both commentators are silent (the longer that a com- mentary continues unINT, the higher the silence bonus).", "acronym": "INT", "label": "interrupted", "ID": "5009"}, {"sentence": "As it anecdotally happened to callers that they were INT by the dialog system, on the one hand, some voice user interface designers tend to chose rather large values for this time-out setting, e.g., 15 or 20 seconds.", "acronym": "INT", "label": "interrupted", "ID": "5010"}, {"sentence": "An  INT cognate identification algorithm would  take as input unordered wordlists from two or more  related languages, and produce alist of aligned cog-  nate pairs as output.", "acronym": "INT", "label": "integrated", "ID": "5011"}, {"sentence": "However, it could well be good enough to be INT into a full parser and provide a benefit to it.", "acronym": "INT", "label": "integrated", "ID": "5012"}, {"sentence": "This  finite bound on path length can be INT into our  algorithm by modifying the add-item procedure such  that only items with a path shorter than the permitted  maximum path length are added to the chart.", "acronym": "INT", "label": "integrated", "ID": "5013"}, {"sentence": "This is still rather different from our setup, where PP attachment is fully INT into the parsing problem.", "acronym": "INT", "label": "integrated", "ID": "5014"}, {"sentence": "In our approach, the recognition of foreign  words has been INT into the ATR process  for Serbian.", "acronym": "INT", "label": "integrated", "ID": "5015"}, {"sentence": "They can be  INT in Web interactions, but not in  cooperative forms of learning.", "acronym": "INT", "label": "integrated", "ID": "5016"}, {"sentence": "IWPT,  Pittsburgh, PA.", "acronym": "IWPT", "label": "International Workshop on Parsing Technologies", "ID": "5017"}, {"sentence": "In Proceedings of the IWPT, pp.", "acronym": "IWPT", "label": "International Workshop on Parsing Technologies", "ID": "5018"}, {"sentence": "In Proceedings of  the Third IWPT, Tilburg, The Netherlands.", "acronym": "IWPT", "label": "International Workshop on Parsing Technologies", "ID": "5019"}, {"sentence": "In Proceedings of the Fourth IWPT.", "acronym": "IWPT", "label": "International Workshop on Parsing Technologies", "ID": "5020"}, {"sentence": "In: Proceedings of the  IWPT,  Carnegie Mellon University, Pittsburgh, August  1989, pp.", "acronym": "IWPT", "label": "International Workshop on Parsing Technologies", "ID": "5021"}, {"sentence": "Third IWPT, pages 277?292.", "acronym": "IWPT", "label": "International Workshop on Parsing Technologies", "ID": "5022"}, {"sentence": "10th IWPT, ACL: 1-10.", "acronym": "IWPT", "label": "International Conference on Parsing Technologies", "ID": "5023"}, {"sentence": "In Proceedings of the 11th IWPT.", "acronym": "IWPT", "label": "International Conference on Parsing Technologies", "ID": "5024"}, {"sentence": "In Proceedings of the 11th IWPT, pages 192- 201.", "acronym": "IWPT", "label": "International Conference on Parsing Technologies", "ID": "5025"}, {"sentence": "In Proceedings of the 11th IWPT, pages 138?141.", "acronym": "IWPT", "label": "International Conference on Parsing Technologies", "ID": "5026"}, {"sentence": "s positive correlation with frequency; i.e., since frequency and poly- semy are HPly correlated, one would expect them to have similar effects on seman- tic change, but we found that the effect of poly- semy completely reversed after controlling for fre- quency.", "acronym": "HP", "label": "highly positive", "ID": "5027"}, {"sentence": "More- over, the data exhibits a HP-skewed distribution.", "acronym": "HP", "label": "highly positive", "ID": "5028"}, {"sentence": "which would be HP.", "acronym": "HP", "label": "highly positive", "ID": "5029"}, {"sentence": "If the test is HPly corre- lated with external tagging quality measures (e.g., those based on gold standard tagging), Q(B) will produce better results than B with high probability.", "acronym": "HP", "label": "highly positive", "ID": "5030"}, {"sentence": "The  feedback was globally HP.", "acronym": "HP", "label": "highly positive", "ID": "5031"}, {"sentence": "We therefore collected a total of 1,000 movie review videos that were either HP or negative.", "acronym": "HP", "label": "highly positive", "ID": "5032"}, {"sentence": "14 Association for Computational Linguistics Learning Emotion Indicators from Tweets: Hashtags, Hashtag Patterns, and Phrases Ashequl Qadir School of Computing University of Utah Salt Lake City, UT 84112, USA asheq@cs.utah.edu Ellen Riloff School of Computing University of Utah Salt Lake City, UT 84112, USA riloff@cs.utah.edu Abstract We present a weakly supervised approach for learning hashtags, HPs, and phrases associated with five emotions: AFFEC- TION, ANGER/RAGE, FEAR/ANXIETY, JOY, and SADNESS/DISAPPOINTMENT.", "acronym": "HP", "label": "hashtag pattern", "ID": "5033"}, {"sentence": "We then harvest emotion phrases from the hash- tags and HPs for contextual emotion clas- sification.", "acronym": "HP", "label": "hashtag pattern", "ID": "5034"}, {"sentence": "3.2 Learning Hashtag Patterns We learn HPs in a similar but separate boot- strapping process.", "acronym": "HP", "label": "hashtag pattern", "ID": "5035"}, {"sentence": "We use probabil- ity estimates to determine which HPs are reliable indicators for an emotion.", "acronym": "HP", "label": "hashtag pattern", "ID": "5036"}, {"sentence": "Conse- quently, we generalize beyond specific hashtags to cre- ate HPs that will match all hashtags with the same prefix, such as", "acronym": "HP", "label": "hashtag pattern", "ID": "5037"}, {"sentence": "Our research learns three types of emotion in- dicators for tweets: hashtags, HPs, and phrases for one of five emotions: AFFEC- TION, ANGER/RAGE, FEAR/ANXIETY, JOY, or SAD- NESS/DISAPPOINTMENT.", "acronym": "HP", "label": "hashtag pattern", "ID": "5038"}, {"sentence": "Conse- quently, we generalize beyond specific hashtags to cre- ate HPs that will match all hashtags with the same prefix, such as the pattern #angryat* which will match both #angryattheworld and #angryatlife.", "acronym": "HP", "label": "hashtag pattern", "ID": "5039"}, {"sentence": "r research learns three types of emotion in- dicators for tweets: hashtags, HPs, and phrases for one of five emotions: AFFEC- TION, ANGER/RAGE, FEAR/ANXIETY, JOY, or SAD- NESS/DISAPPOINTMENT.", "acronym": "HP", "label": "hashtag pattern", "ID": "5040"}, {"sentence": "At a higher level, (Mann and Yarowsky, 2003) disambiguated personal names by clustering people?s HPs using a TFIDF similarity, and several other researchers have ap- plied clustering at the same level in the context of the entity identification problem (Bilenko et al, 2003; Mc- Callum and Wellner, 2003; Li et al, 2004).", "acronym": "HP", "label": "home page", "ID": "5041"}, {"sentence": "These features however only covered small amount of disambiguation evidence and certain types of web pages (such as personal HPs).", "acronym": "HP", "label": "home page", "ID": "5042"}, {"sentence": "Conll-99 HP.", "acronym": "HP", "label": "home page", "ID": "5043"}, {"sentence": "s HP is a Zamboni game in celebration of Frank Zam- boni ?", "acronym": "HP", "label": "home page", "ID": "5044"}, {"sentence": "Corpus Selection The corpus selection page (tool HP) provides information about all available corpora, and allows for corpora upload and deletion.", "acronym": "HP", "label": "home page", "ID": "5045"}, {"sentence": "The retrieved English  HPs are presented in Chinese and/or  English.", "acronym": "HP", "label": "home page", "ID": "5046"}, {"sentence": "2 Lexical and SF  We view VSD as a supervised learning problem,  solving which requires three groups of features:  lexical, syntactic, and semantic.", "acronym": "SF", "label": "Syntactic Features", "ID": "5047"}, {"sentence": "Combin- ing Lexical and SF for Supervised Word Sense Disambiguation. (", "acronym": "SF", "label": "Syntactic Features", "ID": "5048"}, {"sentence": "4.4 SyntaLex-4: Combination of Lexical and Simple SF SyntaLex-4 also relies on a combination of PoS and bigram features but uses unified decision trees that can have either kind of feature at a particular node.", "acronym": "SF", "label": "Syntactic Features", "ID": "5049"}, {"sentence": "4.3 SyntaLex-3: Ensemble of Lexical and Simple SF Prior research has shown that both lexical and syn- tactic features can individually achieve a reasonable quality of disambiguation.", "acronym": "SF", "label": "Syntactic Features", "ID": "5050"}, {"sentence": "400  Complementarity of Lexical and Simple SF: The SyntaLex Approach to SENSEVAL-3 Saif Mohammad University of Toronto Toronto, ON M5S1A1 Canada smm@cs.toronto.edu http://www.cs.toronto.edu/?smm Ted Pedersen University of Minnesota Duluth, MN 55812 USA tpederse@d.umn.edu http://www.d.umn.edu/?tpederse Abstract This paper describes the SyntaLex entries in the English Lexical Sample Task of SENSEVAL-3.", "acronym": "SF", "label": "Syntactic Features", "ID": "5051"}, {"sentence": "Exploring SF for Pronoun Reso- lution Using Context-Sensitive Convolution Tree  Kernel.", "acronym": "SF", "label": "Syntactic Features", "ID": "5052"}, {"sentence": "When performing joint word segmen- tation on the Buckeye corpus, our Bigram model reaches around above 55% F-score for recovering deleted /t/s with a word SF-score of around 72% which is 2% better than running a Bi- gram model that does not model /t/-deletion.", "acronym": "SF", "label": "segmentation F", "ID": "5053"}, {"sentence": "al Random N?best Candidates Field Local Features Global Features Average Perceptron Input Sentence Average Perceptron Output Conditional RandomField (10?Fold Split) Figure 1: Outline of the segmentation process 2.1 Learning Algorithm Given an unsegmented sentence x, the word seg- mentation problem can be defined as finding the 143 Sixth SIGHAN Workshop on Chinese Language Processing most probable SF (x) from a set of pos- sible segmentations of x. F (x) = argmax y?GEN(x) ?(", "acronym": "SF", "label": "segmentation F", "ID": "5054"}, {"sentence": "Experiments show that our system outperforms existing systems on newswire, broadcast news and Egyptian dialect, im- proving SF 1 score on a recently released Egyptian Arabic corpus to 95.1%, compared to 90.8% for another segmenter designed specifically for Egyptian Arabic.", "acronym": "SF", "label": "segmentation F", "ID": "5055"}, {"sentence": "At the same  Chinese word SF-measure,  the number of bigrams in the model can  be reduced by up to 90%.", "acronym": "SF", "label": "segmentation F", "ID": "5056"}, {"sentence": "When combined with the joint segmentation and POS-tagging system, the SF-score, joint segmentation and POS-tagging F-score were 95.00% and 90.17%, respectively, and the joint segmentation and parsing F-score for non-root words (excluding punctuations) was 75.09%, where the corresponding accuracy with gold-standard segmented and POS- tagged input was 86.21%, as shown in Table 18.", "acronym": "SF", "label": "segmentation F", "ID": "5057"}, {"sentence": "54.61 70.12 NO-VAR 54.12 73.99 Table 6: Word SF-scores for the /t/- recovery F-scores in Table 5 averaged over two runs (standard errors less than 2% unless given).", "acronym": "SF", "label": "segmentation F", "ID": "5058"}, {"sentence": "SF can be framed as a sequential label- ing problem in which the most probable semantic slot labels are estimated for each word of the given word sequence.", "acronym": "SF", "label": "Slot filling", "ID": "5059"}, {"sentence": "SF is a sequential labeling task to map a sequence of T words xT1 to a sequence of T slot labels yT1 .", "acronym": "SF", "label": "Slot filling", "ID": "5060"}, {"sentence": "SF is a traditional task and tremendous efforts have been done, especially since the 1980s when the Defense Advanced Research Program Agency (DARPA) Airline Travel Informa- tion System (ATIS) projects started (Price, 1990).", "acronym": "SF", "label": "Slot filling", "ID": "5061"}, {"sentence": "Figure 2: Input and output to topic grouping for TST-MUC3-0099 157 SF SF consists of five parts : (1) pure set fills, (2) string fills, (3) cross-referenced slots, (4) date extraction, and (5) location extraction.", "acronym": "SF", "label": "Slot filling", "ID": "5062"}, {"sentence": "4 Related Work SF is used in dialogue systems such as the Ravenclaw-Olympus system5, but the slots are filled by using output from a chart parser (Ward, 2008).", "acronym": "SF", "label": "Slot filling", "ID": "5063"}, {"sentence": "at the end of a name (for in- stance, a noun phrase) hints on a family-member or 4The original algorithm decides whether a given SF can be explained by a given long form.", "acronym": "SF", "label": "short form", "ID": "5064"}, {"sentence": "a r e  s ty led   \"template-like\" here  only to ind ica te  t h a t  they have not  been matched  with input t e x t ,  And hence the infer red  proposit ions they represent   have not necessar i ly  been s ta ted  e x p l i c i t l y  i n  the input tex t ,  Let  us f i r s t  see t h e  e f f e c t  of doing t h i s ,  and then the mechanism tha t   does i t ,  In  what follows, we extend the \"SF\" of templates  (obtained by wr i t ing  square brackets round English words, c lus tered  a t  th ree  nodes to show the d i s t r ibu t ion  of formulas i n  the f u l l   template) by wr i t ing  extract ions  a s  English words ins ide  double  square brackets,   L e t  us consider  (37) John f i r e d  a t  a l i n e  of s tags  with a shotgun  The r e s u l t  of matching t h i s", "acronym": "SF", "label": "short form", "ID": "5065"}, {"sentence": "can be trans- ferred to all synonyms from other entries in the dic- tionary that have the same long or SFs (but possibly do not mention the respective other in any synonym.)", "acronym": "SF", "label": "short form", "ID": "5066"}, {"sentence": "Sentences start their life in SF, s, are ranked by a source language model, p(s), and then probabilistically ex- panded to form the long sentence, p(l|s).", "acronym": "SF", "label": "short form", "ID": "5067"}, {"sentence": "Finally, the subjects were asked to fill in  a SF and describe their reactions to using the system.", "acronym": "SF", "label": "short form", "ID": "5068"}, {"sentence": "More esoterically, perhaps, we also exploit SFs of attribute names so as to minimize the size of elementize d messages.", "acronym": "SF", "label": "short form", "ID": "5069"}, {"sentence": "As Collins notes, removing both the distance metric and SF results in a gigantic drop in performance, since without both of these features, the model has no way to encode the fact that flatter structures should be avoided in several crucial cases, such as for PPs, which tend to prefer one argument to the right of their head-children.", "acronym": "SF", "label": "subcat features", "ID": "5070"}, {"sentence": "For a  maximum number of four, this will take the form:  \\[(A, (A ...... )), (B, (_,B .... )), (C, ( .... C,_)), (D, ( ...... D))\\]  The declaration will also allow us to work out the mnemonic values  (npl , f l ,  etc) needed for the scat and SF (types of  323  Computational Linguistics Volume 22, Number 3  booZ_comb_value f ature).", "acronym": "SF", "label": "subcat features", "ID": "5071"}, {"sentence": "Edges with  SF are combined with other elements  in the chart in accordance with general combinatory  schemata.", "acronym": "SF", "label": "subcat features", "ID": "5072"}, {"sentence": "has 2 correct sub- cat frames but receives 8 predictions which differ only in the SF.", "acronym": "SF", "label": "subcat features", "ID": "5073"}, {"sentence": "However, the cor- rect SF for it are intransitive and sbar.", "acronym": "SF", "label": "subcat features", "ID": "5074"}, {"sentence": "We extracted the SF from their type definitions in the Alpino lexicon to cre- ate a gold standard of subcat frames.", "acronym": "SF", "label": "subcat features", "ID": "5075"}, {"sentence": "10) Intuitively, the above link confidence definition compares the lexical translation probability of the aligned word pair with the translation probabilities of all the TW given the source word.", "acronym": "TW", "label": "target words", "ID": "5076"}, {"sentence": "J} corresponds to a sequence of source word positions, where J is the source sentence length, and with null representing un- aligned TW.", "acronym": "TW", "label": "target words", "ID": "5077"}, {"sentence": "Baseline: For the baseline system, we applied  two different algorithms for sentences which  have opinion-bearing verbs as TW and  for those that have opinion-bearing adjectives as  TW.", "acronym": "TW", "label": "target words", "ID": "5078"}, {"sentence": "IBM Models 3, 4, and 5 attempt to capture fertility (the tendency of each source word to generate several TW), resulting in probabilis- tically deficient, intractable models that require local heuristic search and are difficult to implement and extend.", "acronym": "TW", "label": "target words", "ID": "5079"}, {"sentence": "\u0002 Non-bijective: Multiple TW can be linked to a single source word.", "acronym": "TW", "label": "target words", "ID": "5080"}, {"sentence": "The reason this happens is that the generative model has to distribute translation probability for each source word among different candidate TW.", "acronym": "TW", "label": "target words", "ID": "5081"}, {"sentence": "Subsequently, we trained several sequence taggers in order to identify the TWs in text.", "acronym": "TW", "label": "trigger word", "ID": "5082"}, {"sentence": "In stage (2), we generate relations between a TW and one or more proteins, while in stage (3), we look for complex in- teractions between simple events, TWs and proteins.", "acronym": "TW", "label": "trigger word", "ID": "5083"}, {"sentence": "Our approach is based on a three-stage clas- sification: (1) TW tagging, (2) sim- ple event extraction, and (3) complex event extraction.", "acronym": "TW", "label": "trigger word", "ID": "5084"}, {"sentence": "In our experiments, we found that simultaneously identifying TWs and the event types they trigger yielded low recall; thus, we settled on", "acronym": "TW", "label": "trigger word", "ID": "5085"}, {"sentence": "In our experiments, we found that simultaneously identifying TWs and the event types they trigger yielded low recall; thus, we settled on identifying TWs in text as one kind of entity, regardless of event types.", "acronym": "TW", "label": "trigger word", "ID": "5086"}, {"sentence": "Our system is based on a three-stage classification process: (1) TW tagging using a linear se- quence model, (2) simple event extraction, and (3) complex event extraction.", "acronym": "TW", "label": "trigger word", "ID": "5087"}, {"sentence": "media, such as chat texts, SMS and TW.", "acronym": "TW", "label": "tweets", "ID": "5088"}, {"sentence": "2013) likewise cluster TW using K-means but predict location only at the country level.", "acronym": "TW", "label": "tweets", "ID": "5089"}, {"sentence": "Analysis of text to estimate affect or sentiment is a relatively recent research topic that has at- tracted great interest, as reflected by a series of shared evaluation tasks, e.g., analysis of news headlines (Strapparava and Mihalcea, 2007) and TW (Nakov et al.,", "acronym": "TW", "label": "tweets", "ID": "5090"}, {"sentence": "Dis- advantages of these methods are the dependency on time-specific population data, making them un- suitable for some corpora (e.g. 19th-century doc- uments); the difficulty in adjusting grid resolution in a principled fashion; and the fact that not all documents are near a city (Han14 find that 8% of TW are ?", "acronym": "TW", "label": "tweets", "ID": "5091"}, {"sentence": "Like- wise, TW in Twitter are often geotagged; in this case, it is possible to view either an individual tweet or the collection of TW for a given user as a document, respectively identifying the loca- tion as the place from which the tweet was sent or the home location of the user.", "acronym": "TW", "label": "tweets", "ID": "5092"}, {"sentence": "Notice that we normalize the LD by the length of the lexicon element, as we em- pirically found it performing better compared with normalizing by the length of the segment.", "acronym": "LD", "label": "Levenshtein distance", "ID": "5093"}, {"sentence": "if one of the words to be compared is not in WordNet, their similarity is calculated using the LD.", "acronym": "LD", "label": "Levenshtein distance", "ID": "5094"}, {"sentence": "Lev(xuj :vj , l)/|l| (4) where Lev represents the LD.", "acronym": "LD", "label": "Levenshtein distance", "ID": "5095"}, {"sentence": "This is a dynamic programming algorithm sim- ilar to computing LD between two strings, except the cost function used to compute the match between tokens is as shown below.", "acronym": "LD", "label": "Levenshtein distance", "ID": "5096"}, {"sentence": "The most common definition of distance be- tween two strings is the LD, also known as edit distance (ED).", "acronym": "LD", "label": "Levenshtein distance", "ID": "5097"}, {"sentence": "The typical sentences computed as the centroids of the clusters created using Algorithm 2 are them- selves color coded using the same identification sys- 1We have also experimented with a symmetric version of LD, but we prefer the n-gram overlap score due to its linear run time complexity.", "acronym": "LD", "label": "Levenshtein distance", "ID": "5098"}, {"sentence": "Dis- criminative Induction of Sub-Tree Alignment using  Limited LD.", "acronym": "LD", "label": "Labeled Data", "ID": "5099"}, {"sentence": "Text Mining Techniques for Leveraging Positively LD.", "acronym": "LD", "label": "Labeled Data", "ID": "5100"}, {"sentence": "LD For experiments within the paral- lel text, we manually labeled 1320 of the 94K co- ordinate NP examples.", "acronym": "LD", "label": "Labeled Data", "ID": "5101"}, {"sentence": "Distant Supervision for Relation Ex- traction Without LD.", "acronym": "LD", "label": "Labeled Data", "ID": "5102"}, {"sentence": "Variable Possible Values Domain (D) {Movie Review classifica- tion (MR), Webpage classi- fication (WebKB)} Instance Granu- larity (G) {document (doc), sentence (sent)} Feature Space (F ) {unigram only (u), uni- gram+dependency (u+d)} LD (#AUs) (L) {64, 128, 256, 512, 1024} Irrelevant Text (I) {0, 200, 400, 600,? }", "acronym": "LD", "label": "Labeled Data", "ID": "5103"}, {"sentence": "Limitations of Co-Training for Natural Language Learning from LDs.", "acronym": "LD", "label": "Large Dataset", "ID": "5104"}, {"sentence": "5.2 Result on LD Table 2 shows the translation results.", "acronym": "LD", "label": "Large Dataset", "ID": "5105"}, {"sentence": "Eliminating  Class Noise in LDs.", "acronym": "LD", "label": "Large Dataset", "ID": "5106"}, {"sentence": "The other is the Stanford LD 2 (Maas et al 2011), a collection of 50,000 comments from IMDB, evenly divided into training and test sets.", "acronym": "LD", "label": "Large Dataset", "ID": "5107"}, {"sentence": "4 Using LDs Effectively In the previous section, we found our crowdsourced data was good at predicting AAC-like test sets.", "acronym": "LD", "label": "Large Dataset", "ID": "5108"}, {"sentence": "6.2 LD Experiment The large experiment considers 968 English words (468,028 pairs) over a range of sampling rates.", "acronym": "LD", "label": "Large Dataset", "ID": "5109"}, {"sentence": "While using UD (Nivre et al, 2016) could potentially simplify porting the rules, we chose not to investigate this option due to the on- going nature of the project and focused on the estab- lished representations for now.", "acronym": "UD", "label": "Universal Dependencies", "ID": "5110"}, {"sentence": "4.1 Experimental Setup We first investigated our model on 19 lan- guages from the UD Tree- banks v1.2.", "acronym": "UD", "label": "Universal Dependencies", "ID": "5111"}, {"sentence": "3 74.5 80.4 78.1 76.2 Pipeline P tag 73.7 83.6 72.0 73.0 79.3 79.5 63.0 78.0 66.9 78.5 87.8 73.5 84.2 75.4 70.3 83.6 73.4 79.5 79.4 76.6 RBGParser 75.8 83.6 73.9 73.5 79.9 79.6 68.0 78.5 65.4 78.9 87.7 74.2 84.7 77.6 72.4 83.9 75.4 81.3 80.7 77.6 Stackprop 77.0 84.3 73.8 74.2 80.7 80.7 70.1 78.5 74.5 80.0 88.9 74.1 85.8 77.5 73.6 84.7 79.2 80.4 81.8 78.9 Table 2: Labeled Attachment Score (LAS) on UD Treebank.", "acronym": "UD", "label": "Universal Dependencies", "ID": "5112"}, {"sentence": "We evaluate our approach on 19 languages from the UD treebank in Section 4.", "acronym": "UD", "label": "Universal Dependencies", "ID": "5113"}, {"sentence": "UD v1: A Multilingual Treebank Collection.", "acronym": "UD", "label": "Universal Dependencies", "ID": "5114"}, {"sentence": "On 19 lan- guages from the UD, our method is 1.3% (absolute) more accu- rate than a state-of-the-art graph-based ap- proach and 2.7% more accurate than the most comparable greedy model.", "acronym": "UD", "label": "Universal Dependencies", "ID": "5115"}, {"sentence": "then the description S used to instantiate the UD is the conjunction of the properties in the description SD with the value of the differentiation", "acronym": "UD", "label": "underspecified domain", "ID": "5116"}, {"sentence": "V } 3: for all U(N, t) sorted by Givenness do 4: if U(N, t) matches D then 5: restructure(D, N , RS) 6: return U(N, t) 7: end if 8: end for 9: return failure Figure 2: Reference algorithms, relying on the same UDs 5 Example In this section we present the interpretation side of some expressions we generated in the GIVE set- ting (table 2).", "acronym": "UD", "label": "underspecified domain", "ID": "5117"}, {"sentence": "then the description S used to instantiate the UD is the conjunction of the properties in the description SD with the value of the differentiation criterion used to create the partition namely, properties of val(c) true of the referent (line 2).", "acronym": "UD", "label": "underspecified domain", "ID": "5118"}, {"sentence": "hich refer- ence domain, referring will be processed and thereby, which description will be used for instantiating the UDs.", "acronym": "UD", "label": "underspecified domain", "ID": "5119"}, {"sentence": "Then this domain is used to match a so called UD (Salmon-Alt and Romary, 2001).", "acronym": "UD", "label": "underspecified domain", "ID": "5120"}, {"sentence": "In D0, the first matching UD is the definite ?", "acronym": "UD", "label": "underspecified domain", "ID": "5121"}, {"sentence": "The first step (line 1?2) determines in which refer- ence domain, referring will be processed and thereby, which description will be used for instantiating the UDs.", "acronym": "UD", "label": "underspecified domain", "ID": "5122"}, {"sentence": "Self-taught Learn- ing : Transfer Learning from UD.", "acronym": "UD", "label": "Unlabeled Data", "ID": "5123"}, {"sentence": "7 Previous Work: Combining Labeled and UD The two-step procedure used in our Co-Training method for statistical parsing was incipient in the SuperTag- ger (Srinivas, 1997) which is a statistical model for tag- ging sentences with elementary lexicalized structures.", "acronym": "UD", "label": "Unlabeled Data", "ID": "5124"}, {"sentence": "Detection of Agreement vs. Disagreement in  Meetings: Training with UD.", "acronym": "UD", "label": "Unlabeled Data", "ID": "5125"}, {"sentence": "Enhancing Chinese Word Segmentation Using UD.", "acronym": "UD", "label": "Unlabeled Data", "ID": "5126"}, {"sentence": "c?2006 Association for Computational Linguistics Agreement/Disagreement Classi\u0002cation: Exploiting UD using Contrast Classi\u0002ers Sangyun Hahn Richard Ladner Dept.", "acronym": "UD", "label": "Unlabeled Data", "ID": "5127"}, {"sentence": "03, Workshop on the Continuum from Labeled to UD in Machine Learning and Data Mining.", "acronym": "UD", "label": "Unlabeled Data", "ID": "5128"}, {"sentence": "The speech corpora consist of one MS sponta- neous corpus, containing twenty segments and totaling \ffty minutes, and one read corpus of \fve segments, read by a single speaker and to- taling eleven minutes of speech.", "acronym": "MS", "label": "multi-speaker", "ID": "5129"}, {"sentence": "We tried to restrict ourselves to features whose inclusion is motivated by previous work (pauses, speech rate) and added features that are specific to MS speech (overlap, changes in speaker activity).", "acronym": "MS", "label": "multi-speaker", "ID": "5130"}, {"sentence": "Guest actors and MS turns are not considered in the gen- der analysis.", "acronym": "MS", "label": "multi-speaker", "ID": "5131"}, {"sentence": "In the future, we hope  to further enlarge our MS database.", "acronym": "MS", "label": "multi-speaker", "ID": "5132"}, {"sentence": "Performance measures used to date within the DARPA spo-  ken language research community (and included in this  paper) do not conform to the recommended approach, since  the scoring software, in general, generates a single measure-  merit for the ensemble of test data (e.g., one datum indicat-  ing word or utterance error rate for the entire MS,  multi-utterance, t st subset, rather than the mean error rate  for the ensemble of speakers).", "acronym": "MS", "label": "multi-speaker", "ID": "5133"}, {"sentence": "Towards MS unsupervised speech pattern discovery.", "acronym": "MS", "label": "multi-speaker", "ID": "5134"}, {"sentence": "8 In an experiment, we used the MS (2013)?s parser as an initializer for the Gillenwater et al (2011)?s parser.", "acronym": "MS", "label": "Marecek and Straka", "ID": "5135"}, {"sentence": "5.3 Tuning multi-phase IR Because MS (2013)?s parser does not distinguish training data from test data, we pos- tulate S 0 = S 1 .", "acronym": "MS", "label": "Marecek and Straka", "ID": "5136"}, {"sentence": "The contribution of Iterated Reranking We compare the quality of the treebank resulted in the end of phase 1 against the quality of the treebank given by the initialier MS (2013).", "acronym": "MS", "label": "Marecek and Straka", "ID": "5137"}, {"sentence": "We then search for what IR (with the MaxEnc op- tion) contributes to the overall performance by com- paring the quality of the treebank resulted in the end of phase 1 against the quality of the treebank given by its initialier, i.e. MS (2013).", "acronym": "MS", "label": "Marecek and Straka", "ID": "5138"}, {"sentence": "The position and shape of N(D i ) is thus deter- mined by two factors: how well P i can fit D i , and k. Intuitively, the lower the fitness is, the more N(D i ) goes far away fromD i ; and the larger k is, the larger 3 MS (2013) did not report any experimental result on the WSJ corpus.", "acronym": "MS", "label": "Marecek and Straka", "ID": "5139"}, {"sentence": "Naseem and Barzilay (2011), Tu and Honavar (2012), Spitkovsky et al (2012), Spitkovsky et al (2013), and MS (2013) employ ex- tensions of the DMV but with different learning strategies.", "acronym": "MS", "label": "Marecek and Straka", "ID": "5140"}, {"sentence": "e conjunctions (or a punctuation fulfilling its role); in the Moscow family, the con- juncts form a chain where each node in the chain depends on the previous (or following) node; in the Stanford family, the conjuncts are siblings ex- cept for the first (or last) conjunct, which is the 10Names are chosen purely as a mnemonic device, so that Prague Dependency Treebank belongs to the Prague family, MS belongs to the Moscow family, and Stanford parser style belongs to the Stanford family.", "acronym": "MS", "label": "Mel?c?uk style", "ID": "5141"}, {"sentence": "taient ADJ pendant environ 2 mois, tandis que les corpus de test devaient ?", "acronym": "ADJ", "label": "disponibles", "ID": "5142"}, {"sentence": "Il  s'agit  de cr?er et rendre ADJ de nouvelles ressources aux chercheurs  en  TAL,  d'une  part  et  de  d'?quiper  les  langues  africaines  de  ressources num?riques nouvelles et indispensable ?", "acronym": "ADJ", "label": "disponibles", "ID": "5143"}, {"sentence": "et une interface d??dition sp?cifique pour contribuer directement aux dictionnaires ADJ sur la plate-forme.", "acronym": "ADJ", "label": "disponibles", "ID": "5144"}, {"sentence": "lectroniques ADJ sont rares, mal distribu?es, voire inexistantes.", "acronym": "ADJ", "label": "disponibles", "ID": "5145"}, {"sentence": "Les dictionnaires seront donc ADJ sur Internet d'ici la fin de l'ann?e 2012 sous licence Creative Commons. ?", "acronym": "ADJ", "label": "disponibles", "ID": "5146"}, {"sentence": "Les sous-objets de cet objet sont alors  ADJ pour de nouvelles ilff6rences.", "acronym": "ADJ", "label": "disponibles", "ID": "5147"}, {"sentence": "We now justify each of the choices: why ADJ, why clustering, and why shallow features.", "acronym": "ADJ", "label": "adjectives", "ID": "5148"}, {"sentence": "Our aim is to estab- lish semantic classes for ADJ in Catalan by means of clustering, using only shallow syntactic evidence.", "acronym": "ADJ", "label": "adjectives", "ID": "5149"}, {"sentence": "8 08003 Barcelona eloi@iua.upf.es Abstract In this paper, we present a clustering exper- iment directed at the acquisition of semantic classes for ADJ in Catalan, using only shallow distributional features.", "acronym": "ADJ", "label": "adjectives", "ID": "5150"}, {"sentence": "In this paper, we con- centrate on ADJ, which have received less at- tention (see though Hatzivassiloglou and McKeown (1993) and Lapata (2001)).", "acronym": "ADJ", "label": "adjectives", "ID": "5151"}, {"sentence": "We define a broad-coverage classification for ADJ based on Ontological Semantics.", "acronym": "ADJ", "label": "adjectives", "ID": "5152"}, {"sentence": "pronouns 12 count of quote tokens 13 count of 1st person plural pronouns 14 count of 2nd person singular pronouns 15 count of quote positive words 16 count of quote negative words 17 count of nouns 18 count of verbs 19 count of ADJ 20 count of adverbs 21 up to 3-grams extracted from quote Table 1: Common feature set.", "acronym": "ADJ", "label": "adjectives", "ID": "5153"}, {"sentence": "Predicting the semantic ori-  entation of ADJ.", "acronym": "ADJ", "label": "adjectives", "ID": "5154"}, {"sentence": "Major type Minor type  Kanji Reading, Writing, Radical, The  order of writing, Classification  Word knowl- edge  Katakana, How to use Kana, Ap- propriate Noun, To fill blanks for  Verb, ADJ, and Adjunct,  Synonym, Antonym, Particle,  Conjunction, Onomatopoeia, Po- lite Expression, Punctuation mark Reading com- prehension  Who, What, When, Where, How,  Why question, Extract specific  phrases, Progress order of a story,  Prose and Verse   Composition Constructing sentence, How to  write composition  Table 1.", "acronym": "ADJ", "label": "Adjective", "ID": "5155"}, {"sentence": "1,2 V, N Sem Generation Spelling correction 1,2,3 Any Syn/Sem Generation ADJ ordering 1,2 Adj Sem Generation Compound bracketing 1,2 N Syn Analysis Compound interpret.", "acronym": "ADJ", "label": "Adjective", "ID": "5156"}, {"sentence": "ADJs which have an event component in their meaning (event adjectives for short) denote a state that is directly dependent on an event, be it simultaneous or previous to the state.", "acronym": "ADJ", "label": "Adjective", "ID": "5157"}, {"sentence": "ADJs are predicates, equivalent to verbs when appearing in predicative environments.", "acronym": "ADJ", "label": "Adjective", "ID": "5158"}, {"sentence": "This parameter can again be used for basic tasks such as POS-tagging: ADJ-noun ambiguity is notoriously the most difficult one to solve, and the ordering restrictions on the classes of adjectives can help to reduce it.", "acronym": "ADJ", "label": "Adjective", "ID": "5159"}, {"sentence": "701,3   Acquisition of Semantic Classes for ADJs from Distributional Evidence Gemma Boleda GLiCom Universitat Pompeu Fabra La Rambla 30-32 08002 Barcelona gemma.boleda@upf.edu Toni Badia GLiCom Universitat Pompeu Fabra La Rambla 30-32 08002 Barcelona toni.badia@upf.edu Eloi Batlle Audiovisual Institute Universitat Pompeu Fabra Pg.", "acronym": "ADJ", "label": "Adjective", "ID": "5160"}, {"sentence": "Some words,  such as  =oat ADJ   and adverbs ,  do not bu i ld  s t ruc tures  but ra ther  modi fy   s t ruc tures  bu i l t  by other  words.", "acronym": "ADJ", "label": "ad jec t ives", "ID": "5161"}, {"sentence": "I f  John is n o t  s ing le ,  then John is  married,   j?\"his kind of r e l a t i o n  seems t o  hold pr imari ly  between two ADJ  o r   two adverbs belonging t o  t h e  same pr imi t ive  concept.", "acronym": "ADJ", "label": "ad jec t ives", "ID": "5162"}, {"sentence": "The arguments can apparently be verbs, ADJ  o r   Rouns.", "acronym": "ADJ", "label": "ad jec t ives", "ID": "5163"}, {"sentence": "This c lass  covers  most ADJ  and adverbs .", "acronym": "ADJ", "label": "ad jec t ives", "ID": "5164"}, {"sentence": "Action verbs and adjectives can appear in imperative  sentences but  non-action verbs and ADJ  cannot.", "acronym": "ADJ", "label": "ad jec t ives", "ID": "5165"}, {"sentence": "This c lass is  qu i te   substant ia l ,  Inc luding many du l l  nouns and near ly  a l l   ADJ  and adverbs .", "acronym": "ADJ", "label": "ad jec t ives", "ID": "5166"}, {"sentence": "8 08003 Barcelona eloi@iua.upf.es Abstract In this paper, we present a clustering exper- iment directed at the acquisition of semantic classes for adJJs in Catalan, using only shallow distributional features.", "acronym": "JJ", "label": "jective", "ID": "5167"}, {"sentence": "We define a broad-coverage classification for adJJs based on Ontological Semantics.", "acronym": "JJ", "label": "jective", "ID": "5168"}, {"sentence": "To that end, we  devised a more obJJ test, useful only for  scoring the subset of referents that are names  of people.", "acronym": "JJ", "label": "jective", "ID": "5169"}, {"sentence": "pronouns 12 count of quote tokens 13 count of 1st person plural pronouns 14 count of 2nd person singular pronouns 15 count of quote positive words 16 count of quote negative words 17 count of nouns 18 count of verbs 19 count of adJJs 20 count of adverbs 21 up to 3-grams extracted from quote Table 1: Common feature set.", "acronym": "JJ", "label": "jective", "ID": "5170"}, {"sentence": "701,3   Acquisition of Semantic Classes for AdJJs from Distributional Evidence Gemma Boleda GLiCom Universitat Pompeu Fabra La Rambla 30-32 08002 Barcelona gemma.boleda@upf.edu Toni Badia GLiCom Universitat Pompeu Fabra La Rambla 30-32 08002 Barcelona toni.badia@upf.edu Eloi Batlle Audiovisual Institute Universitat Pompeu Fabra Pg.", "acronym": "JJ", "label": "jective", "ID": "5171"}, {"sentence": "Predicting the semantic ori-  entation of adJJs.", "acronym": "JJ", "label": "jective", "ID": "5172"}, {"sentence": "It can  be a Chinese character, a sub-string of English  words, a Korean Hangual or a JJi or  several Japanese Katakanas.", "acronym": "JJ", "label": "Japanese Kanj", "ID": "5173"}, {"sentence": "Site?ID (h) Japanese transliterated to JJi Figure 2: Accuracy in top-1, F -score and MRR for standard runs.", "acronym": "JJ", "label": "Japanese Kanj", "ID": "5174"}, {"sentence": "0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8Accuracy in top-10 0.10.2 0.30.4 0.50.6 0.70.8 0.91 F-score English to ChineseEnglish to HindiEnglish to TamilEnglish to KannadaEnglish to RussianEnglish to KoreanEnglish to JapaneseJapanese transliterated to JJi Figure 4: Accuracy in top-1 vs. F -score for dif- ferent tasks.", "acronym": "JJ", "label": "Japanese Kanj", "ID": "5175"}, {"sentence": "glish Tamil Microsoft Research India 7,974 987 1,000 EnTa English Kannada Microsoft Research India 7,990 968 1,000 EnKa English Russian Microsoft Research India 5,977 943 1,000 EnRu English Chinese Institute for Infocomm Research 31,961 2,896 2,896 EnCh English Korean Hangul CJK Institute 4,785 987 989 EnKo English Japanese Katakana CJK Institute 23,225 1,492 1,489 EnJa Japanese name (in English) JJi CJK Institute 6,785 1,500 1,500 JnJk Table 1: Source and target languages for the shared task on transliteration.", "acronym": "JJ", "label": "Japanese Kanj", "ID": "5176"}, {"sentence": "Site?ID (h) Japanese transliterated to JJi Figure 3: MAPref , MAP10 and MAPsys scores for standard runs.", "acronym": "JJ", "label": "Japanese Kanj", "ID": "5177"}, {"sentence": "First is previous Korean  Hanja, JJi (Chinese characters in  Japanese language) and Chinese Pinyin input  methods, the second one is English-Korean  transliteration.", "acronym": "JJ", "label": "Japanese Kanj", "ID": "5178"}, {"sentence": "Therefore, in our scenario we would like to place a sparsifying ` 1 regularizer over the contextual (interaction) features while still leveraging the squared ` 2 -norm penalty for the standard BOW (BoW) features.", "acronym": "BOW", "label": "bag-of-words", "ID": "5179"}, {"sentence": "For text input, we use raw BOW as features, with different vocabularies for the state side and action side.", "acronym": "BOW", "label": "bag-of-words", "ID": "5180"}, {"sentence": "The system is basically an information re- trieval system which selects a sentence, instead  of a document, based on the BOW  method.", "acronym": "BOW", "label": "bag-of-words", "ID": "5181"}, {"sentence": "Shown are the micro-averaged F 1 scores for a BOW baseline, a system trained on Polyglot embeddings, the two strongest systems of Hermann and Blunsom (2014), and our Joint w/ Aux system with ` 1 and ` 2 regularization.", "acronym": "BOW", "label": "bag-of-words", "ID": "5182"}, {"sentence": "The obvious approach to our unsupervised ver- sion of the problem would be to segment the text  (if necessary), represent each of the resulting units  of text as a BOW, and then use clustering  algorithms to find natural clusters.", "acronym": "BOW", "label": "bag-of-words", "ID": "5183"}, {"sentence": "Shown are histograms with smoothed kernel density estimates of differences in recall and precision between the baseline BOW based approach and each feature space/method (one per row).", "acronym": "BOW", "label": "bag-of-words", "ID": "5184"}, {"sentence": "We frame the classification task as follows: The input features to the classifier are the words in the blog post i.e each blog post is treated as a BOW and the output variable is the binary comment polarity computed in the previ- 14 SentiWordNet PMI Blog SVM sLDA SVM sLDA cb 0.56 0.58 0.79 0.79 dk 0.61 0.64 0.75 0.77 my 0.67 0.59 0.87 0.87 rs 0.53 0.55 0.74 0.76 rwn 0.57 0.59 0.90 0.90 Table 3: Accuracy: Using blog posts to predict comment sentiment polarity ous section.", "acronym": "BOW", "label": "bag of words", "ID": "5185"}, {"sentence": "Context word features record the presence of a word within a fixed window around the tar- get word (BOW); collocational features capture the syntactic environment of the target word and are usu- ally represented by a small number of words and/or part- of-speech tags to the left or right of the target word.", "acronym": "BOW", "label": "bag of words", "ID": "5186"}, {"sentence": "Making the most of BOW: Sentence regularization with alter- nating direction method of multipliers.", "acronym": "BOW", "label": "bag of words", "ID": "5187"}, {"sentence": "Near queries use Altavista?s NEAR operator to ex- pand the n-gram; a NEAR b means that a has to oc- cur in the same ten word window as b; the window is treated as a BOW (e.g., history changes expands to \"history\" NEAR \"changes\").", "acronym": "BOW", "label": "bag of words", "ID": "5188"}, {"sentence": "Each blog post is then represented as a BOW from the post.", "acronym": "BOW", "label": "bag of words", "ID": "5189"}, {"sentence": "We are also interested in studying how to conduct queries not as a BOW but bind by syntactic relations (Wilson et al, 2005).", "acronym": "BOW", "label": "bag of words", "ID": "5190"}, {"sentence": "The dotted line cor- responds to a random choice out of fourteen items and to the perplexity of a HM trained on the corpus.", "acronym": "HM", "label": "histogram model", "ID": "5191"}, {"sentence": "HM  (1999) argued that indexing in question answering  should be based on 1)aragraphs.", "acronym": "HM", "label": "Harabagiu and Maiorano", "ID": "5192"}, {"sentence": "HM (2000) and Postolache et al (2006) projected English corpora to Roma- nian to bootstrap human annotation, either manu- ally or via automatic alignments.", "acronym": "HM", "label": "Harabagiu and Maiorano", "ID": "5193"}, {"sentence": "HM (2000) designed a CR system for English-Romanian bitexts while Mitkov and Barbu (2003) focused on the English-French language pair.", "acronym": "HM", "label": "Harabagiu and Maiorano", "ID": "5194"}, {"sentence": "Also, studies by  Harabagiu, Moldovan, and Maiorano (HM 1996; Harabagiu and  Moldovan 1999) show that cohesion can be used to determine rhetorical relations that  hold between smaller discourse constituents as well.", "acronym": "HM", "label": "Harabagiu and Maiorano", "ID": "5195"}, {"sentence": "SWIZZLE is a multilingual en-  hancement of COCKTAIL (HM,  1999), a coreference r solution system that operates  on a mixture of heuristics that combine semantic  and textual cohesive information 2.", "acronym": "HM", "label": "Harabagiu and Maiorano", "ID": "5196"}, {"sentence": "There are also systems aimed at extracting par- tial knowledge from texts, by either filling seman- tic templates (Hobbs et al, 1996) or by generating a set of linguistic patterns for information extrac- tion (HM, 2000), to name but a few.", "acronym": "HM", "label": "Harabagiu and Maiorano", "ID": "5197"}, {"sentence": "Proper HM: Two proper names have the same head, and they do not contain numeric or location pre-modifiers.", "acronym": "HM", "label": "head match", "ID": "5198"}, {"sentence": "Compatible HM: Two mentions have the same head, and the pre-modifiers of the anaphor are a subset of the pre-modifiers of the antecedent.", "acronym": "HM", "label": "head match", "ID": "5199"}, {"sentence": "The Stanford system is the winner of the CoNLL2011 shared 650 OntoNotes-Dev Same speaker > Compatible HM > Substring > String match > Proper HM > Acronym ACE2004-nwire Compatible HM > Substring > Proper HM > String match > Demonym > Apposition > Same speaker > Role apposition > Relative pronoun > Acronym > Predicate nominative Table 3: The resulting ranking of informativeness scores on different data sets.", "acronym": "HM", "label": "head match", "ID": "5200"}, {"sentence": "The routing information (context vector,  Boolean terms and match threshold) is stored in a  table along with the UID of the route.", "acronym": "UID", "label": "user ID", "ID": "5201"}, {"sentence": "URL harvesting: social network traversal, obvious spam and non-text documents filter- ing, optional spell check of the short message to see if it could be English text, optional record of UIDs for later crawls.", "acronym": "UID", "label": "user ID", "ID": "5202"}, {"sentence": "In the re- sulting list of tokens, all UIDs and web URLs are replaced with placeholders.2 Any remaining punctu- ation is stripped from the tokens and empty tokens are deleted.", "acronym": "UID", "label": "user ID", "ID": "5203"}, {"sentence": "The request includes the source sen- tence, source and target language, and optionally a UID.", "acronym": "UID", "label": "user ID", "ID": "5204"}, {"sentence": "kish 1,415 7.2 German 1,289 6.6 Spanish 954 4.9 French 703 3.6 Italian 658 3.4 Portuguese 357 1.8 Arabic 263 1.3 Table 2: 10 most frequent languages of spell- check-filtered URLs gathered on FriendFeed 4.2 identi.ca The results of the two strategies followed on identi.ca led to a total of 1,113,783 URLs checked for redirection, which were collected in about a week (the deep crawler reached 37,485 UIDs).", "acronym": "UID", "label": "user ID", "ID": "5205"}, {"sentence": "Among others, we defined: a) userID, which is matched against a list of known UIDs to select a UM profile for answer extraction (see Section 5); b) the current query, which is used to dynamically up- date the stack of recent user questions used by the clar- ification request detection module to perform reference resolution; c) the topic of conversation, i.e. the keywords of the last question issued by the user which received an an- swer.", "acronym": "UID", "label": "user ID", "ID": "5206"}, {"sentence": "The second is the UID hypothesis (Jaeger and Levy, 2006; Jaeger, 2010), as it applies to syntactic structure.", "acronym": "UID", "label": "Uniform Information Density", "ID": "5207"}, {"sentence": "However, we observe that priors based on UID (e.g the ?", "acronym": "UID", "label": "user identity", "ID": "5208"}, {"sentence": "Comparison with it allows us to check the usefulness of UID in the task.", "acronym": "UID", "label": "user identity", "ID": "5209"}, {"sentence": "Our model is a principled generative latent variable model which captures three important factors: view- point specific topic preference, UID and user interactions.", "acronym": "UID", "label": "user identity", "ID": "5210"}, {"sentence": "They assume these two concepts are orthogonal and they do not con- sider UID.", "acronym": "UID", "label": "user identity", "ID": "5211"}, {"sentence": "The novelty of our approach is to leverage UID, allowing us to construct a corpus of language-labeled Twitter messages without using automated tools to determine the languages of the messages.", "acronym": "UID", "label": "user identity", "ID": "5212"}, {"sentence": "4 Analysis While the main goal of this paper is to document the AAWD corpus, we also performed several sta- tistical analyses of authority and alignment, in or- der to demonstrate the relevance of these social acts as markers of UID and social dynamics within our corpus.", "acronym": "UID", "label": "user identity", "ID": "5213"}, {"sentence": "SA, the biggest oil pro- ducer in the world, was once a sup- porter of Osama bin Laden and his associates who led attacks against the United States.?", "acronym": "SA", "label": "Saudi Arabia", "ID": "5214"}, {"sentence": "Most Compositional labour union, tax authority, health council, market counterparty, employment policy Most Lexical study design, family motto, wood shaving, avoidance behaviour, smash hit Most Lexical (including Proper Nouns) Vo Quy, Bonito Oliva, Mamur Zapt, Evander Holyfield, SA Table 5: Top lexical and compositional nouns for the BIN-CMPD model using Ilex(c) ent impact of both constituents.", "acronym": "SA", "label": "Saudi Arabia", "ID": "5215"}, {"sentence": "Arabic language is   the official language in all Arab nations as  Egypt, SA and Algeria.", "acronym": "SA", "label": "Saudi Arabia", "ID": "5216"}, {"sentence": "The Arabic Online Commentary dataset that we created was based on reader commentary from the online versions of three Arabic newspapers: Al- Ghad from Jordan, Al-Riyadh from SA, and Al-Youm Al-Sabe?", "acronym": "SA", "label": "Saudi Arabia", "ID": "5217"}, {"sentence": "He collected Arabizi words from tweets and trained a character-level language 52 Reference Year Location Participants Data Size of Data Arabizi English Arabic (Keong et al, 2015) 2015 Malysia 20 Arab Post Graduates SMS 200 Messages 35% 50% 10% (Bies et al, 2014) 2014 Egypt 26 Native Arabic Speakers SMS 101,292 Messages 77% - 23% (Alabdulqader et al, 2014) 2014 SA 61 Students and Non-students SMS, BBM, and Whatsapp 3236 Messages 15% 8% 74% (Bianchi, 2012) 2012 Jordan - Online Forum 460,220 Posts 35.5% 17.5% 32% (Al-Khatib and Sabbah, 2008) 2008 Jordan 46 Students SMS 181 Messages 37% 54% 9% Table 1: Percentage of Arabizi Usage in Related Work model and a statistical sequence labelling algo- rithm.", "acronym": "SA", "label": "Saudi Arabia", "ID": "5218"}, {"sentence": "SA is the world?s biggest oil exporter.?", "acronym": "SA", "label": "Saudi Arabia", "ID": "5219"}, {"sentence": "2.2 Sentiment Analysis SA in computational linguistics has focused on examining what textual features (lexi- cal, syntactic, punctuation, etc) contribute to affec- tive content of text and how these features can be detected automatically to derive a sentiment metric for a word, sentence or whole text.", "acronym": "SA", "label": "Sentiment analysis", "ID": "5220"}, {"sentence": "Semeval 2013 task 2: SA in twitter.", "acronym": "SA", "label": "Sentiment analysis", "ID": "5221"}, {"sentence": "SA: SA deter- mines positive or negative opinions expressed on  topics (Liu, 2012; Pang and Lee, 2008).", "acronym": "SA", "label": "Sentiment analysis", "ID": "5222"}, {"sentence": "SA is often  used to extract the opinions in blog pages.", "acronym": "SA", "label": "Sentiment analysis", "ID": "5223"}, {"sentence": "A sentimental educa- tion: SA using subjectivity summa- rization based on minimum cuts.", "acronym": "SA", "label": "Sentiment analysis", "ID": "5224"}, {"sentence": "c?2012 Association for Computational Linguistics Social Event Radar: A Bilingual Context Mining and SA  Summarization System      Wen-Tai Hsieh Chen-Ming Wu  Department of IM,   National Taiwan University  Institute for Information Industry  wentai@iii.org.tw cmwu@iii.org.tw     Tsun Ku Seng-cho T. Chou  Institute for Information Industry Department of IM,   National Taiwan University  cujing@iii.org.tw chou@im.ntu.edu.tw          Abstract  Social Event Radar is a new social  net", "acronym": "SA", "label": "Sentiment Analysis", "ID": "5225"}, {"sentence": "1331  Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and SA, pages 99?103, Jeju, Republic of Korea, 12 July 2012.", "acronym": "SA", "label": "Sentiment Analysis", "ID": "5226"}, {"sentence": "c?2012 Association for Computational Linguistics Multimodal SA (Abstract of Invited Talk) Rada Mihalcea Department of Computer Science and Engineering University of North Texas P. O. Box 311366 Denton, TX 76203-6886, U.S.A. rada@cs.unt.edu Abstract With more than 10,000 new videos posted online every day on social websites such as YouTube and Facebook, the internet is be- coming an almost infinite source of informa- tion.", "acronym": "SA", "label": "Sentiment Analysis", "ID": "5227"}, {"sentence": "A Sentimental Education:  SA Using Subjectivity  Summarization Based on Minimum Cuts.", "acronym": "SA", "label": "Sentiment Analysis", "ID": "5228"}, {"sentence": "c?2012 Association for Computational Linguistics Multimodal SA (Abstract of Invited Talk) Rada Mihalcea Department of Computer Science and Engineering University of North Texas P. O. Box 311366 Denton, TX 76203-6886, U.S.A. rada@cs.unt.edu Abstract With more than 10,000 new videos posted online every day on social w", "acronym": "SA", "label": "Sentiment Analysis", "ID": "5229"}, {"sentence": "35  Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and SA, page 1, Jeju, Republic of Korea, 12 July 2012.", "acronym": "SA", "label": "Sentiment Analysis", "ID": "5230"}, {"sentence": "Recognizing Contextual Polarity in Phrase- Level SA.", "acronym": "SA", "label": "Sentiment Analysis", "ID": "5231"}, {"sentence": "In the past, Dragon had focused primarily on  speaker dependent and SA recognition , so  that speaker independent research was a new departure  for us in this past year.", "acronym": "SA", "label": "speaker adaptive", "ID": "5232"}, {"sentence": "The 1290 ex- cluded words are OOVs to both the word and hybrid 4We use the IBM system with SA training based on maximum likelihood with no discriminative training.", "acronym": "SA", "label": "speaker adaptive", "ID": "5233"}, {"sentence": "In this paper, we describe the SA capabilities of the BBN BYBLOS continuous  speech recognition system.", "acronym": "SA", "label": "speaker adaptive", "ID": "5234"}, {"sentence": "Definition of procedures for evaluation of vocabu-  lary/SA systems.", "acronym": "SA", "label": "speaker adaptive", "ID": "5235"}, {"sentence": "We also  anticipate moving from speaker dependent recognition to a  SA mode which will require far less training  data for new speakers.", "acronym": "SA", "label": "speaker adaptive", "ID": "5236"}, {"sentence": "The SA transforms are then applied to the future sentences.", "acronym": "SA", "label": "speaker adaptive", "ID": "5237"}, {"sentence": "In Proceedings of the 1st international CIKM workshop on Topic-SA for mass opinion, pages 45?52.", "acronym": "SA", "label": "sentiment analysis", "ID": "5238"}, {"sentence": "Sentiws: a publicly available german- language resource for SA.", "acronym": "SA", "label": "sentiment analysis", "ID": "5239"}, {"sentence": "Indeed, the main goal of our work is to extract and organize the major opinions about a topic that are buried in many scattered opinionated sources rather than perform deeper understanding of opin- ions (e.g., distinguishing positive from negative opinions), which can be done by using any exist- ing SA technique as an orthogonal post-processing step after applying our method.", "acronym": "SA", "label": "sentiment analysis", "ID": "5240"}, {"sentence": "In CIKM workshop on Topic-SA for mass opinion, pages 53?56.", "acronym": "SA", "label": "sentiment analysis", "ID": "5241"}, {"sentence": "Sentiwordnet 3.0: An enhanced lexical resource for SA and opinion mining.", "acronym": "SA", "label": "sentiment analysis", "ID": "5242"}, {"sentence": "There has been a growing interest in the NLP treatment of subjectivity and SA ?", "acronym": "SA", "label": "sentiment analysis", "ID": "5243"}, {"sentence": "formation patterns) and SA.", "acronym": "SA", "label": "statistical analysis", "ID": "5244"}, {"sentence": "The approach is hybrid: it combines morpho- syntactic filters for extraction of term candidates,  and SA that ranks term candidates  according to their termhood.", "acronym": "SA", "label": "statistical analysis", "ID": "5245"}, {"sentence": "This kind of SA as independently sug-  gested in (Resnik, 1991) can be made with LTAGs be-  cause of their extended omain of locality but also be-  cause of their lexiealized property.", "acronym": "SA", "label": "statistical analysis", "ID": "5246"}, {"sentence": "To sum up, the results from the machine learning method are consistent with that from the  multivariate SA in Section 3.1.", "acronym": "SA", "label": "statistical analysis", "ID": "5247"}, {"sentence": "A SA of the TREC-3 data.", "acronym": "SA", "label": "statistical analysis", "ID": "5248"}, {"sentence": "In non-parametric SA, one com-  pares the rank of data sets when the qualitative be-  haviour is similar but the absolute quantities are un-  reliable.", "acronym": "SA", "label": "statistical analysis", "ID": "5249"}, {"sentence": "Similarity between Words  Computed by SA on an English Dictionary  Hideki Kozima  Course in Computer Science  . . . .", "acronym": "SA", "label": "Spreading Activation", "ID": "5250"}, {"sentence": "Word Sense Disambiguation with SA Networks Generated from Thesauri.", "acronym": "SA", "label": "Spreading Activation", "ID": "5251"}, {"sentence": "A Parallel Constraint Satisfaction and  SA Model for Resolving Syntactic Ambiguity.\"", "acronym": "SA", "label": "Spreading Activation", "ID": "5252"}, {"sentence": "Appendix B. Function of Paradigme  SA Rules  Each node Pi of the semantic network Paradigme  computes its activity value vi(T+ 1) at time T+I  as  follows:  v'(T+ l) = ? (", "acronym": "SA", "label": "Spreading Activation", "ID": "5253"}, {"sentence": "Similarity between Words  Computed by SA on an English Diction-  ary.", "acronym": "SA", "label": "Spreading Activation", "ID": "5254"}, {"sentence": "Order Logic Programming for Semantic Interpretation of Coordinate Con- structs (ACL95), S. Kulick 72 9511001 Countability and Number in Japanese-to-English Machine Translation (COLING94), F. Bond et al 442 Computational Linguistics Volume 28, Number 4 73 9511006 Disambiguating Noun Groupings with Respect to WordNet Senses (ACL95 Workshop), P. Resnik 74 9601004 Similarity between Words Computed by SA on an English Dictionary (EACL93), H. Kozima, T. Furugori 75 9604019 Magic for Filter Optimization in Dynamic Bottom-up Processing (ACL96), G. Minnen 76 9604022 Unsupervised Learning of Word-Category Guessing Rules (ACL96), A. Mikheev 77 9605013 Learning Dependencies between Case Frame Slots (COLING96), H. Li, N. Abe 78 9605014 Clustering Words with the MDL Principle (COLING96", "acronym": "SA", "label": "Spreading Activation", "ID": "5255"}, {"sentence": "The function first gathers AO of D that have the persis- tent part of description S (Gp and Sp), and if there is already a domain composed by these objects, its salience is increased such that it is the most salient (line 4).", "acronym": "AO", "label": "all objects", "ID": "5256"}, {"sentence": "The information displayed is not a complete his-  tory including the creation, deformations and  deletion of AO, but rather only a record  of the construction steps (dependencies) of the  stored objects.", "acronym": "AO", "label": "all objects", "ID": "5257"}, {"sentence": "is an environment composed of E, the universe of AO and V , the set of ground predicates that hold in the environment.", "acronym": "AO", "label": "all objects", "ID": "5258"}, {"sentence": "The tree is recursively expanded by selecting the best test for each sub- set and so on, until AO of the current subset belong to the same class.", "acronym": "AO", "label": "all objects", "ID": "5259"}, {"sentence": "The root node is a RD whose ground is AO of the room.", "acronym": "AO", "label": "all objects", "ID": "5260"}, {"sentence": "i) its POS tag is in the RM-D entry of Table 1, ii) its dependency head is inside of the verbal block, and iii) is the right-most object among AO of the verbal block.", "acronym": "AO", "label": "all objects", "ID": "5261"}, {"sentence": "Only the most frequent relations are shown (a key for the English relations is given in Table 2; in German the relations are SB (Subject), OA (AO), CJ (Conjunct), DA (Dative), CD (Coordinator), MO (Modifier), RE (Subordinate Clause), RS (Reported Speech), OC (Object Clausal), OP (Object Prepositional), NK (Noun Kernel), and CVC (Collocational Verb Construction).", "acronym": "AO", "label": "Object Accusative", "ID": "5262"}, {"sentence": "TACL, 3:211?225.", "acronym": "TACL", "label": "Transactions of the ACL", "ID": "5263"}, {"sentence": "In TACL.", "acronym": "TACL", "label": "Transactions of the ACL", "ID": "5264"}, {"sentence": "TACL, 1:49?62.", "acronym": "TACL", "label": "Transactions of the ACL", "ID": "5265"}, {"sentence": "To ap- pear in TACL.", "acronym": "TACL", "label": "Transactions of the ACL", "ID": "5266"}, {"sentence": "TACL.", "acronym": "TACL", "label": "Transactions of the ACL", "ID": "5267"}, {"sentence": "While phrasal alignments are important and have 219 TACL, 2 (2014) 219?230.", "acronym": "TACL", "label": "Transactions of the Association for Computational Linguistics", "ID": "5268"}, {"sentence": "In contrast to this trend, research in knowledge acquisition is now heading towards the seamless in- 231 TACL, 2 (2014) 231?244.", "acronym": "TACL", "label": "Transactions of the Association for Computational Linguistics", "ID": "5269"}, {"sentence": "TACL, 2:27?40.", "acronym": "TACL", "label": "Transactions of the Association for Computational Linguistics", "ID": "5270"}, {"sentence": "TACL, 1:1?12.", "acronym": "TACL", "label": "Transactions of the Association for Computational Linguistics", "ID": "5271"}, {"sentence": "1977), though EM does not 105 TACL, 2 (2014) 105?118.", "acronym": "TACL", "label": "Transactions of the Association for Computational Linguistics", "ID": "5272"}, {"sentence": "Our NEWSSPIKE-RE system encapuslates these intuitions in a novel graphical model making 117 TACL, vol.", "acronym": "TACL", "label": "Transactions of the Association for Computational Linguistics", "ID": "5273"}, {"sentence": "the semantic description pointed out is coded  in the LKB as a definitional  graph.", "acronym": "LKB", "label": "lexical knowledge base", "ID": "5274"}, {"sentence": "Methods based on manually built LKBs, such as WordNet, compute the shortest path be- tween two concepts in the knowledge base and/or look at word overlap in the glosses (see Budan- itsky and Hirst (2006) for an overview).", "acronym": "LKB", "label": "lexical knowledge base", "ID": "5275"}, {"sentence": "Building and using a LKB of near-synonym differences.", "acronym": "LKB", "label": "lexical knowledge base", "ID": "5276"}, {"sentence": "The more evident problem with WORDNET is that  it is a LKB for English, and so it  is not usable for other languages.", "acronym": "LKB", "label": "lexical knowledge base", "ID": "5277"}, {"sentence": "4.1 Semant ic  verb  classi f icat ion in the   LKB  The LKB is based on a hierarchical  representation f French verbs.", "acronym": "LKB", "label": "lexical knowledge base", "ID": "5278"}, {"sentence": "In LREC 2002 Workshop on Ontologies and LKB.. J. Pustejovsky.", "acronym": "LKB", "label": "Lexical Knowledge Bases", "ID": "5279"}, {"sentence": "\"Models for LKB\".", "acronym": "LKB", "label": "Lexical Knowledge Bases", "ID": "5280"}, {"sentence": "In Patrick Saint-Dizier,  editor, Predicative Forms in Natural  Language and LKB.", "acronym": "LKB", "label": "Lexical Knowledge Bases", "ID": "5281"}, {"sentence": "LREC 2002 Workshop on Ontologies  and LKB.", "acronym": "LKB", "label": "Lexical Knowledge Bases", "ID": "5282"}, {"sentence": "LREC 2002 Workshop on Ontologies and  LKB (2002)  22.", "acronym": "LKB", "label": "Lexical Knowledge Bases", "ID": "5283"}, {"sentence": "5.3 Towards integrated polymorphic or multiple-view  LKB  Our system can be viewed as a facet of a broader  'environment for an encyclop:edic discovery' with other  mcxles of activity: sell-review, semi-tutored lessons, where  character thematic 'collections' would drive the discovery.", "acronym": "LKB", "label": "Lexical Knowledge Bases", "ID": "5284"}, {"sentence": "\"Models for LKBs\".", "acronym": "LKB", "label": "Lexical Knowledge Base", "ID": "5285"}, {"sentence": "es  Abstract  This paper explores the automatic onstruction  of a multilingual LKB from  pre-existing lexical resources.", "acronym": "LKB", "label": "Lexical Knowledge Base", "ID": "5286"}, {"sentence": "LREC 2002 Workshop on Ontologies  and LKBs.", "acronym": "LKB", "label": "Lexical Knowledge Base", "ID": "5287"}, {"sentence": "LREC 2002 Workshop on Ontologies and  LKBs (2002)  22.", "acronym": "LKB", "label": "Lexical Knowledge Base", "ID": "5288"}, {"sentence": "A Project for the Con- struction of an Italian LKB in the  Framework of WordNet, IRST Technical Report #  9406-15.", "acronym": "LKB", "label": "Lexical Knowledge Base", "ID": "5289"}, {"sentence": "5.3 Towards integrated polymorphic or multiple-view  LKBs  Our system can be viewed as a facet of a broader  'environment for an encyclop:edic discovery' with other  mcxles of activity: sell-review, semi-tutored lessons, where  character thematic 'collections' would drive the discovery.", "acronym": "LKB", "label": "Lexical Knowledge Base", "ID": "5290"}, {"sentence": "Inference in our model is however not as straightforward and we propose an efficient MCMC sampling scheme.", "acronym": "MCMC", "label": "Markov chain Monte Carlo", "ID": "5291"}, {"sentence": "LogLin(M,F,w,Y(x i )) 3.2 Inference To perform inference in this model, we adopt a common MCMC estimation procedure for IBPs (G?or?ur et al, 2006; Navarro and Griffiths, 2007).", "acronym": "MCMC", "label": "Markov chain Monte Carlo", "ID": "5292"}, {"sentence": "Bayesian probabilistic matrix factorization using MCMC.", "acronym": "MCMC", "label": "Markov chain Monte Carlo", "ID": "5293"}, {"sentence": "generated using MCMC meth- ods.", "acronym": "MCMC", "label": "Markov chain Monte Carlo", "ID": "5294"}, {"sentence": "5 Inference Schemes In this section we give a high level description of a MCMC sampling based inference scheme for the hierarchical Pitman- Yor language model.", "acronym": "MCMC", "label": "Markov chain Monte Carlo", "ID": "5295"}, {"sentence": "Probabilistic inference using MCMC methods.", "acronym": "MCMC", "label": "Markov chain Monte Carlo", "ID": "5296"}, {"sentence": "MCMC: Stochastic Simulation for Bayesian Inference.", "acronym": "MCMC", "label": "Markov Chain Monte Carlo", "ID": "5297"}, {"sentence": "Bayesian inference for PCFGs via MCMC.", "acronym": "MCMC", "label": "Markov Chain Monte Carlo", "ID": "5298"}, {"sentence": "Stochastic techniques such as MCMC are exact in the limit of infinite runtime, but tend to be too slow for large problems.", "acronym": "MCMC", "label": "Markov Chain Monte Carlo", "ID": "5299"}, {"sentence": "MCMC in Practice.", "acronym": "MCMC", "label": "Markov Chain Monte Carlo", "ID": "5300"}, {"sentence": "We also obtained a probability distribution over each co- efficient by MCMC sampling using the R package lme4 version 0.99 (Bates, 2005).", "acronym": "MCMC", "label": "Markov Chain Monte Carlo", "ID": "5301"}, {"sentence": "In Proceedings of the 6th Interna- tional Workshop on TAG and Re- lated Frameworks, pages 101?106, Venice, Italy.", "acronym": "TAG", "label": "Tree Adjoining Grammar", "ID": "5302"}, {"sentence": "An efficient parsing algorithm for TAGs.", "acronym": "TAG", "label": "Tree Adjoining Grammar", "ID": "5303"}, {"sentence": "Feature Structures Based TAGs.", "acronym": "TAG", "label": "Tree Adjoining Grammar", "ID": "5304"}, {"sentence": "Coordi- nation in TAGs: Formal- ization and implementation.", "acronym": "TAG", "label": "Tree Adjoining Grammar", "ID": "5305"}, {"sentence": "Some computational properties of TAGs.", "acronym": "TAG", "label": "Tree Adjoining Grammar", "ID": "5306"}, {"sentence": "For the latter case, the  linguistic realizer is based on TAG  \\[Joshi 87\\].", "acronym": "TAG", "label": "Tree Adjoining Grammar", "ID": "5307"}, {"sentence": "Pre\fx probabilities from stochas- tic TAGs.", "acronym": "TAG", "label": "tree adjoining grammar", "ID": "5308"}, {"sentence": "Conditions on consistency of proba- bilistic TAGs.", "acronym": "TAG", "label": "tree adjoining grammar", "ID": "5309"}, {"sentence": "Using descriptions of trees in a TAG.", "acronym": "TAG", "label": "tree adjoining grammar", "ID": "5310"}, {"sentence": "Parsing strategies with lexicalized grammars: Appli- cation to TAGs.", "acronym": "TAG", "label": "tree adjoining grammar", "ID": "5311"}, {"sentence": "B.: D-ltag system - discourse parsing with a lexi- calized TAG.", "acronym": "TAG", "label": "tree adjoining grammar", "ID": "5312"}, {"sentence": "Incremental seman- tic role labeling with TAG.", "acronym": "TAG", "label": "tree adjoining grammar", "ID": "5313"}, {"sentence": "Sentiment  Classification of MR using Contextual  Valence Shifters.", "acronym": "MR", "label": "Movie Reviews", "ID": "5314"}, {"sentence": "Emotional   Lexicon  Method  News Corpus MR  Pr.", "acronym": "MR", "label": "Movie Reviews", "ID": "5315"}, {"sentence": "Sentiment  Classification of MR Using Contextual  Valence Shifters.", "acronym": "MR", "label": "Movie Reviews", "ID": "5316"}, {"sentence": "Pang et al, 2002) tested various ma- chine learning algorithms on MR.", "acronym": "MR", "label": "Movie Reviews", "ID": "5317"}, {"sentence": "5.2 Sentiment Prediction in MR The first two experiments concern the prediction of the sentiment of movie reviews in the Stanford Sentiment Treebank (Socher et al, 2013b).", "acronym": "MR", "label": "Movie Reviews", "ID": "5318"}, {"sentence": "Features Method  News Corpus MR  Pr.", "acronym": "MR", "label": "Movie Reviews", "ID": "5319"}, {"sentence": "For exam- 1520 Rule-MR-Sp Rule-SpSent Maxent Rel micro-avg 2.77 ?", "acronym": "MR", "label": "Majority Rule", "ID": "5320"}, {"sentence": "4.1 Bias-Correcting MRs We first want to explore the following idea: If a given annotator annotates most items with 0, then we might want to assign less significance to that 542 choice for any particular item.4 That is, if an an- notator appears to be biased towards a particular category, then we might want to try to correct for this bias during aggregation.", "acronym": "MR", "label": "Majority Rule", "ID": "5321"}, {"sentence": "Beta 00.20.40.60.81 Majority Weighte d Rule-b ased (a) Corpus-reproduction metrics 5.38 3.26 4.58 2.98 3.15 1.31 2.07 1.24 Token s Types TTR 0123456 Origin al Weigh ted MR-ba sed 0.640 .69 0.", "acronym": "MR", "label": "Majority Rule", "ID": "5322"}, {"sentence": "The user?s question was analyzed and a MR corresponding to the question was encoded in a knowledge representation formalism.", "acronym": "MR", "label": "meaning representation", "ID": "5323"}, {"sentence": "5 Related Work and Discussion Several semantic parsing methods use a domain- independent MR derived from the combinatory categorial grammar (CCG) parses (e.g., (Cai and Yates, 2013; Kwiatkowski et al, 2013; Reddy et al, 2014)).", "acronym": "MR", "label": "meaning representation", "ID": "5324"}, {"sentence": "Most state-of-the-art approaches to KB-QA are based on semantic pars- ing, where a question (utterance) is mapped to its formal MR (e.g., logical form) and then translated to a KB query.", "acronym": "MR", "label": "meaning representation", "ID": "5325"}, {"sentence": "In NLP a very common paradigm for word MR is the use of the Distributional hypothesis.", "acronym": "MR", "label": "meaning representation", "ID": "5326"}, {"sentence": "is that two different types of MRs are learned, reflecting the tendency for state texts to describe scenes and action texts to describe potential actions from the user.", "acronym": "MR", "label": "meaning representation", "ID": "5327"}, {"sentence": "The system transforms texts  into a formal MR language based on cases.", "acronym": "MR", "label": "meaning representation", "ID": "5328"}, {"sentence": "Average MRe I  4 e J i i i ' i i = ?", "acronym": "MR", "label": "Match Rat", "ID": "5329"}, {"sentence": "80 q?gram MRio M ea n  R ec ip ric al  S co re SoftTFIDF?CRF SoftTFIDF?Lev CRF q?gram Best Levenstein TFIDF Figure 1: Precision, Recall, F-measure and Mean Reciprocal Rank comparisions for each string simi- larity metric across different \u0005 -gram match ratios.", "acronym": "MR", "label": "Match Rat", "ID": "5330"}, {"sentence": "80 q?gram MRio M ea n  R ec ip ric al  S co re SoftTFIDF?CRF SoftTFIDF?Lev CRF q?gr", "acronym": "MR", "label": "Match Rat", "ID": "5331"}, {"sentence": "8 q?gram MRio Pr ec is io n SoftTFIDF?CRF SoftTFIDF?Lev CRF q?gram Best Levenstein TFIDF 0.2 0.3 0.4 0.5 0.6 0.", "acronym": "MR", "label": "Match Rat", "ID": "5332"}, {"sentence": "80 q?gram MRio M ea n  R ec ip ric al  S co re SoftTFIDF?CRF SoftTFIDF?Lev CRF q?gram Best Levenstein TFIDF Figure 1: Precision, Recall, F-measure and Mean Reciprocal Rank comparisions for each string simi- larity metric across different \u0005 -", "acronym": "MR", "label": "Match Rat", "ID": "5333"}, {"sentence": "80 q?gram MRio R ec al l SoftTFIDF?CRF SoftTFIDF?Lev CRF q?gram Best Levenstein TFIDF 0.2 0.3 0.4 0.5 0.6 0.", "acronym": "MR", "label": "Match Rat", "ID": "5334"}, {"sentence": "80 q?gram MRio F?", "acronym": "MR", "label": "Match Rat", "ID": "5335"}, {"sentence": "0.35  0.4  0.45  0.5  0.55  0.6  0.65  0.7  0.75  0  0.2  0.4  0.6  0.8  1 Pr ec is io n q-gram MRio  0.7  0.71  0.72  0.73  0.74  0.75  0.76  0.77  0.78  0  0.2  0.4  0.6  0.8  1 R ec al l q-gram MRio Jaro JaroWinkler SmithWaterman TFIDF UnsmoothedJS Jaccard Figure 1: Performance of Approximate String Matching for Gene Normalization.", "acronym": "MR", "label": "Match Rat", "ID": "5336"}, {"sentence": "A generative model for parsing natural language to MR.", "acronym": "MR", "label": "meaning representations", "ID": "5337"}, {"sentence": "is that two different types of MR are learned, reflecting the tendency for state texts to describe scenes and action texts to describe potential actions from the user.", "acronym": "MR", "label": "meaning representations", "ID": "5338"}, {"sentence": "Joint learning of words and MR for open-text semantic parsing.", "acronym": "MR", "label": "meaning representations", "ID": "5339"}, {"sentence": "For varied selections of thermodynamical exercises, the  system has derived correct MR.", "acronym": "MR", "label": "meaning representations", "ID": "5340"}, {"sentence": "Mimicking an important aspect of human language learning, it ac- quires MR for indi- vidual words from descriptions of visual scenes.", "acronym": "MR", "label": "meaning representations", "ID": "5341"}, {"sentence": "-calculus formulas and their semantic models, corresponding to the semantic or MR, are directly obtained from known semantic representations which were provided with the data or learned before.", "acronym": "MR", "label": "meaning representations", "ID": "5342"}, {"sentence": "We also note that the  MR of Method 2 is much larger than  that of Method 1.", "acronym": "MR", "label": "maximum recall", "ID": "5343"}, {"sentence": "This approach of combining the requirements of MR and maximum precision demands sophisticated syntactic knowl- edge of Czech.", "acronym": "MR", "label": "maximum recall", "ID": "5344"}, {"sentence": "We found that by using the synonyms in WordNet synsets2 as our substitution lexicon, we could achieve a MR of 53% when using an oracle to select the correct synonyms.", "acronym": "MR", "label": "maximum recall", "ID": "5345"}, {"sentence": "Topline recall is the MR possible  for that number of layers.", "acronym": "MR", "label": "maximum recall", "ID": "5346"}, {"sentence": "However, using the synonyms from RT as the substitution lex- icon led to a MR of 85%.", "acronym": "MR", "label": "maximum recall", "ID": "5347"}, {"sentence": "The MR score attainable with our phrases is 84.64% for the development data set.", "acronym": "MR", "label": "maximum recall", "ID": "5348"}, {"sentence": "statistics about the relations and co-reference deci- sions predicted by SERIF, character-based edit  distance, and token SSTs.", "acronym": "SST", "label": "subset tree", "ID": "5349"}, {"sentence": "Syntactic Tree Kernel (STK), also known as a SST kernel (Collins and Duffy, 2002), maps objects in the space of all possible tree fragments constrained by the rule that the sibling nodes from their parents cannot be separated.", "acronym": "SST", "label": "subset tree", "ID": "5350"}, {"sentence": "S shatters an input tree into its subparts (e. g., subtrees, SSTs, or partial trees as described in Section 3).", "acronym": "SST", "label": "subset tree", "ID": "5351"}, {"sentence": "We tuned the regulariza- tion parameter (-c) on the dev set in the same manner as described above, providing 4 GB of memory to the kernel cache (-m 4000).6 We used SST kernels, which compute the similarity between two trees by implicitly enu- merating all possible fragments of the trees (in contrast with subtree kernels, where all frag- ments fully extend to the leaves).", "acronym": "SST", "label": "subset tree", "ID": "5352"}, {"sentence": "o Configuration 2 includes Configuration 1 fea- tures with the addition of string similarity (edit  distance, token SSTs) algorithms for the  name variation stage.", "acronym": "SST", "label": "subset tree", "ID": "5353"}, {"sentence": "We analyze phrase-structures using the notion of tree fragments (referred to as SSTs by Collins and Duffy, 2002).", "acronym": "SST", "label": "subset tree", "ID": "5354"}, {"sentence": "The parser is trained and evaluated with the  SST, which is a nego-  tiation situation, in which two subjects have to  decide on time and place for a meeting.", "acronym": "SST", "label": "Spontaneous Scheduling Task", "ID": "5355"}, {"sentence": "FeasPar was trained, tested and evaluated with  the SST, and compared  with a handmodeled LR-parser.", "acronym": "SST", "label": "Spontaneous Scheduling Task", "ID": "5356"}, {"sentence": "2013 cite) have recently used RNTN to classify sentences into positive/negative categories.", "acronym": "RNTN", "label": "recursive neural tensor networks", "ID": "5357"}, {"sentence": "Can RNTN learn logical reasoning?", "acronym": "RNTN", "label": "recursive neural tensor networks", "ID": "5358"}, {"sentence": "To address this need, we introduce the Stanford Sentiment Treebank and a powerful RNTN that can accurately predict the compositional semantic effects present in this new corpus.", "acronym": "RNTN", "label": "Recursive Neural Tensor Network", "ID": "5359"}, {"sentence": "2012) that represents every word as both a vector and a ma- trix, or RNTNs (Socher et al.,", "acronym": "RNTN", "label": "Recursive Neural Tensor Network", "ID": "5360"}, {"sentence": "To address them, we introduce the RNTN.", "acronym": "RNTN", "label": "Recursive Neural Tensor Network", "ID": "5361"}, {"sentence": "3  190  ~ure of this language is to adopt CU  instead of normal unification.", "acronym": "CU", "label": "constraint unification", "ID": "5362"}, {"sentence": "There are also several disjunctive unification algorithms that  exploit independence, such as CU (Hasida 1986; Nakano 1991), con-  texted unification (Maxwell and Kaplan 1989), and unification based on disjunctive  feature logic (D6rre and Eisele 1990).", "acronym": "CU", "label": "constraint unification", "ID": "5363"}, {"sentence": "A logical CU sys-  tern (Nakano 1991) is used in the planners.", "acronym": "CU", "label": "constraint unification", "ID": "5364"}, {"sentence": "In cu-Prolog, user defined  constraints can be directly added to a program clause  (constraint added Horn clause), and the constraint  unification \\[12, 8\\] 1 is adopted instead of the nor-  1 In these earlier papers, \"CU\" was called  \"conditioned unification.\"", "acronym": "CU", "label": "constraint unification", "ID": "5365"}, {"sentence": "Using CU, the inference rule of  cu-Prolog is as follows:  Q, R; C. , Q' : -S ;  D.,  0 = mgu(Q, Q'), B = my(co,  DO)  $0, R6; B  (Q is an atomic formula.", "acronym": "CU", "label": "constraint unification", "ID": "5366"}, {"sentence": "cu-Prolog adopts CU i stead of  the normal Prolog unification.", "acronym": "CU", "label": "constraint unification", "ID": "5367"}, {"sentence": "The  franle is normal i zed  and  p laced  together  accord -   ing to the spec i f i ca t ion  of the compounder  to fo rm a CUt,  and  this p rocess  of normal i za t ion  and  abutt ing can  be  recurs ive ly  re-   peated.", "acronym": "CU", "label": "compound uni", "ID": "5368"}, {"sentence": "y. y (a+ x) 3 Three computational paradigms Compositional semantics associates meanings to utterances by assigning meanings to atomic items, and by giving rules that allows to compute the meaning of a CUt from the meanings of its parts.", "acronym": "CU", "label": "compound uni", "ID": "5369"}, {"sentence": "The score -2 is assigned to idioms and CUts with gaps.", "acronym": "CU", "label": "compound uni", "ID": "5370"}, {"sentence": "The score -1 is as- signed to CUts.", "acronym": "CU", "label": "compound uni", "ID": "5371"}, {"sentence": "7 A(unit) : awareness level <= -2 -1 >= 0 Mode of alert always emphasis by mouse-over none Score -2 -1 0 1 C(unit) : composition CUt with gap CUt single word D(unit) : difficulty technical term general term elementary term S(unit) : speciality specified domain general domain R(unit) : resource type encyclopaedia dictionary Table 1: Awareness levels and the scores of each characteristic are optimal from the point of view of the user?s search behaviour.", "acronym": "CU", "label": "compound uni", "ID": "5372"}, {"sentence": "MERT in statistical machine translation.", "acronym": "MERT", "label": "Minimum error rate training", "ID": "5373"}, {"sentence": "MERT for statistical machine translation.", "acronym": "MERT", "label": "Minimum error rate training", "ID": "5374"}, {"sentence": "MERT in Statistical Machine Translation.", "acronym": "MERT", "label": "Minimum Error Rate Training", "ID": "5375"}, {"sentence": "F. J. Och, 2003, MERT in Statis- tical Machine Translation.", "acronym": "MERT", "label": "Minimum Error Rate Training", "ID": "5376"}, {"sentence": "MERT in  Statistical Machine Translation.", "acronym": "MERT", "label": "Minimum Error Rate Training", "ID": "5377"}, {"sentence": "MERT in Statis- tical Machine Translation.", "acronym": "MERT", "label": "Minimum Error Rate Training", "ID": "5378"}, {"sentence": "MERT in Sta- tistical Machine Translation.", "acronym": "MERT", "label": "Minimum Error Rate Training", "ID": "5379"}, {"sentence": "Effi- cient MERT and minimum bayes- risk decoding for translation hypergraphs and lattices.", "acronym": "MERT", "label": "minimum error rate training", "ID": "5380"}, {"sentence": "We set the feature weights by optimizing the Bleu score directly using MERT (Och, 2003) on the de- velopment set.", "acronym": "MERT", "label": "minimum error rate training", "ID": "5381"}, {"sentence": "We tuned the weights of each model (non-augmented base- line, unigram-augmented, and unigram-augmented- filtered) with a separate MERT.", "acronym": "MERT", "label": "minimum error rate training", "ID": "5382"}, {"sentence": "Feature weights were set with MERT (Och, 2003) on a tuning set using BLEU (Papineni et al, 2002) as the objective function.", "acronym": "MERT", "label": "minimum error rate training", "ID": "5383"}, {"sentence": "Unfortunately, MERT can- not be directly used to optimize feature weights of max-translation decoding because Eq. (", "acronym": "MERT", "label": "minimum error rate training", "ID": "5384"}, {"sentence": "tation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al, 2003) ap- proaches and recent state-of-the-art machine learn- ing approaches such as maximum entropy (Max- Ent) (Xue and Shen, 2003), support vector ma- chine (SVM) (Kudo and Matsumoto, 2001), con- ditional random fields (CRF) (Peng and McCallum, 2004), and MERT (Gao et al, 2004).", "acronym": "MERT", "label": "minimum error rate training", "ID": "5385"}, {"sentence": "The complex- ity of this step is O(n2), as the number of mini- mal phrases is bounded by the minimum of the num- ber of MP in either language.", "acronym": "MP", "label": "monolingual phrases", "ID": "5386"}, {"sentence": "Marcu and Wong (2002) address point 2 with a lexi- con constraint; MP that are above a length threshold or below a frequency threshold are excluded from the lexicon.", "acronym": "MP", "label": "monolingual phrases", "ID": "5387"}, {"sentence": "In this work, we only require representations for MP that are relatively short.", "acronym": "MP", "label": "monolingual phrases", "ID": "5388"}, {"sentence": "null, Pnull1 ) where the base distributions, PP1 and P null 1 , range over phrase pairs or MP in either language, respectively.", "acronym": "MP", "label": "monolingual phrases", "ID": "5389"}, {"sentence": "0.51 2014 Avg 0.55 0.63 0.71 0.55 0.59 0.52 0.73 0.54 forums 0.35 0.42 0.55 0.48 0.50 0.45 0.66 0.31 students 0.66 0.66 0.73 0.73 0.65 0.69 0.77 0.63 headline 0.64 0.60 0.79 0.64 0.73 0.66 0.76 0.62 belief 0.46 0.71 0.68 0.67 0.48 0.61 0.77 0.41 images 0.52 0.71 0.75 0.62 0.67 0.56 0.82 0.68 2015 Avg 0.53 0.63 0.70 0.63 0.59 0.60 0.76 0.53 SICK 0.53 0.62 0.66 0.57 0.63 0.54 0.72 0.66 results with MP, while J bi per- forms better with bilingual examples.", "acronym": "MP", "label": "monolingual phrases", "ID": "5390"}, {"sentence": "Euclidean dis- tance was consistently chosen for bilingual sen- tences and MP, while cosine sim- ilarity was chosen for bilingual phrases.", "acronym": "MP", "label": "monolingual phrases", "ID": "5391"}, {"sentence": "61   Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 833?840 Manchester, August 2008 Prediction of MP for Semantic Role Labeling Weiwei Sun ?", "acronym": "MP", "label": "Maximal Projection", "ID": "5392"}, {"sentence": "833 Figure 1: A sentence from WSJ test corpus of CoNLL-2005 shared task 2 MP and Its Government of Arguments 2.1 MP Principle and parameters theory is a framework of generative grammar.", "acronym": "MP", "label": "Maximal Projection", "ID": "5393"}, {"sentence": "First, \"Della Noce\" is the last name of a Cana-  dian MP.", "acronym": "MP", "label": "Member of Parliament", "ID": "5394"}, {"sentence": "Two MWTD are merged if they are syn- onymic variants obtained by derivation or conversion.", "acronym": "TD", "label": "T candidates", "ID": "5395"}, {"sentence": "The MWTD secr?tion insulinique (insulin secretion) and hypers?cr?tion insulinique (insulin 1http://www.sciences.univ-nantes.fr/ info/perso/permanents/daille/ and release LINUX.", "acronym": "TD", "label": "T candidates", "ID": "5396"}, {"sentence": "2.2 Semantic Role Features Ideally, we want to use features based on the true semantic roles of the MTD.", "acronym": "TD", "label": "T candidates", "ID": "5397"}, {"sentence": "All MWTD linked by derivational morphol- ogy or by variations inducing semantic variations are clustered.", "acronym": "TD", "label": "T candidates", "ID": "5398"}, {"sentence": "The MWTD produit de la fore?t, produit agroforestier, non-produit agro- forestier, and sous-produit forestier, sous-produit de la fore?t have also been iden- tified.", "acronym": "TD", "label": "T candidates", "ID": "5399"}, {"sentence": "For exam- ple, the following MWTD constitutes a cluster of MWTs: produit forestier/produit de la fore?t, produit non forestier, non-produit agroforestier, produit agroforestier, sous-produit forestier/sous-produit de la fore?t, produit alimentaire forestier andproduit forestier.", "acronym": "TD", "label": "T candidates", "ID": "5400"}, {"sentence": "Syntactical variants that induce se- mantic discrepancies are retrieved from the set of the candidate variants and new MWTD are created.", "acronym": "TD", "label": "T candidates", "ID": "5401"}, {"sentence": "The pa- per describes so called TD, a set of rules designed for a direct transcrip- tion of a certain category of source language words into a target language.", "acronym": "TD", "label": "transducing dictionary", "ID": "5402"}, {"sentence": "Two main bottlenecks of full-fledged  transfer-based systems are:  8  - complexity of the syntactic dictionary  - relative unreliability of the syntactic  analysis of the source language  Even a relatively simple component  (TD) was equally complex  for English-to-Czech and Czech-to-Russian  translation  Limited text domains do not exist in real life,  it is necessary to work with a high coverage  dictionary at least for the source language.", "acronym": "TD", "label": "transducing dictionary", "ID": "5403"}, {"sentence": "It was generally  assumed that for the pair Czech/Russian the  TD would be able to profit  from a substantially greater number of productive  rules.", "acronym": "TD", "label": "transducing dictionary", "ID": "5404"}, {"sentence": "At the time when the work was terminated in  1990, the system had a main translation  dictionary of about 8000 words, accompanied by  so called TD covering another  2000 words.", "acronym": "TD", "label": "transducing dictionary", "ID": "5405"}, {"sentence": "So far, the appl-  ication of TD in the framework of our experiment  has benn considered.", "acronym": "TD", "label": "transducing dictionary", "ID": "5406"}, {"sentence": "The TD was  based on the original idea described in Kirschner  (1987).", "acronym": "TD", "label": "transducing dictionary", "ID": "5407"}, {"sentence": "A TD Bank for Trans-  lators (TEAM).\"", "acronym": "TD", "label": "Terminology Data", "ID": "5408"}, {"sentence": "(5) Brinkmann, Karl-Heinz, TD Banks as a Basis for High-Quality  Translation, in: COLING80 (Tokyo, 1980).", "acronym": "TD", "label": "Terminology Data", "ID": "5409"}, {"sentence": "TD Categories - ISO 12620  ?", "acronym": "TD", "label": "Terminology Data", "ID": "5410"}, {"sentence": "Carl, M., Langlais, P.: An intelligent TDbase as a pre-processor for Statistical Machine Translation.", "acronym": "TD", "label": "Terminology Data", "ID": "5411"}, {"sentence": "License details: http://creativecommons.org/licenses/by/4.0/  66 guage throughout the whole island of Ireland, researchers at Fiontar began development of the Nation- al TDbase for Irish, focal.ie (focal ?", "acronym": "TD", "label": "Terminology Data", "ID": "5412"}, {"sentence": "Categories of Systems  There are three broad categories of \"computerized  translation tools\" (the differences hinging on how  ambitious the system is intended to be): Machine  Translation (MT), Machine-aided Translation (MAT),  and TDbanks.", "acronym": "TD", "label": "Terminology Data", "ID": "5413"}, {"sentence": "2012) than their TD peers.", "acronym": "TD", "label": "typically developing", "ID": "5414"}, {"sentence": "Instances of perseveration on a partic- ular topic in the spontaneous spoken language of children with ASD, however, are not typically ex- plicitly counted in a clinical setting, making com- parisons with TD children diffi- cult to quantify.", "acronym": "TD", "label": "typically developing", "ID": "5415"}, {"sentence": "However, adults with developmental  disorders who have successfully acquired syntax  typically have mental ages of at least 6 or 7, an age  at which TD children also have  well-structured language.", "acronym": "TD", "label": "typically developing", "ID": "5416"}, {"sentence": "Also in class is Charlie, a TD child.", "acronym": "TD", "label": "typically developing", "ID": "5417"}, {"sentence": "ast as expressive of changes in language development as the Index of Productive Syntax (Scarborough, 1990), an empirically validated metric based on an inventory of grammatical structures derived from the child language literature; and (2) these parse tree features can be used to model language development without the use of an inventory of specific structures, assuming only the knowledge that in TD children the level of language development is correlated with age.", "acronym": "TD", "label": "typically developing", "ID": "5418"}, {"sentence": "Currently there are not many op- portunities for children with severe disabilities to play independently and to interact on equal terms with TD children.", "acronym": "TD", "label": "typically developing", "ID": "5419"}, {"sentence": "String alignment with  substitution, insertion, TD, squashing~ and  expansion operations.", "acronym": "TD", "label": "deletion", "ID": "5420"}, {"sentence": "The main problem identified by the authors was boundary TD: a problem which impacts the summarization task.", "acronym": "TD", "label": "deletion", "ID": "5421"}, {"sentence": "In order to take  this fact into account, the penalty for a gap can be  calculated as a function of its length, rather than as  a simple sum of individual TDs.", "acronym": "TD", "label": "deletion", "ID": "5422"}, {"sentence": "quence of substitution and TD as compression  is unsatisfactory because it cannot be distinguished  from an actual sequence of substitution and dele-  tion.", "acronym": "TD", "label": "deletion", "ID": "5423"}, {"sentence": "6 The set of operations con-  tains insertions/TDs, substitutions, and expan-  sions/compressions.", "acronym": "TD", "label": "deletion", "ID": "5424"}, {"sentence": "often ex-  tended to cover pairs consisting of a segment and  the null character, which correspond to the opera-  288  Algorithm Calculation Calculation Dynamic Phonological  of alignment of distance programming features  Covington (1996)  Somers (1998)  Nerbonne and Heeringa (1997)  Gildea and Jurafsky (1996)  explicit  explicit  implicit  explicit  Table 1: Comparison of  lions of insertion and TD (also called indels).", "acronym": "TD", "label": "deletion", "ID": "5425"}, {"sentence": "In  addition, we define two new features: 1) the num- ber of lexical words in a rule to control the model?s  preference for lexicalized rules over un-lexicalized  rules and 2) the average TD in a rule to bal- ance the usage of hierarchical rules and flat rules.", "acronym": "TD", "label": "tree depth", "ID": "5426"}, {"sentence": "We do average-case analysis below because the TD (height) for a sentence of n words is a random variable: in the worst-case it can be linear in n (degenerated into a linear-chain), but we assume this adversarial situation does not happen frequently, and the average TD is O(log n).", "acronym": "TD", "label": "tree depth", "ID": "5427"}, {"sentence": "0  5  10  15  20  25  0  10  20  30  40  50 Tr ee  D ep th  d (n) Sentence Length (n) Avg Depth Variance 3.5 log n Figure 6: Mean and variance of TD vs. sentence length.", "acronym": "TD", "label": "tree depth", "ID": "5428"}, {"sentence": "We first verify the assumptions we made in Sec- tion 3.3 in order to prove the theorem that TD (as a random variable) is normally-distributed with O(logn) mean and variance.", "acronym": "TD", "label": "tree depth", "ID": "5429"}, {"sentence": "Qualitatively, we veri- fied that for most n, TD d(n) does look like a normal distribution.", "acronym": "TD", "label": "tree depth", "ID": "5430"}, {"sentence": "Our TunDiaWN construction  approach is founded, in one hand, on a  corpus based method to analyze and ex- tract TD words.", "acronym": "TD", "label": "Tunisian dialect", "ID": "5431"}, {"sentence": "2013) introduced  a lexicon for the TD in order to  adapt an existing morphological analyzer initial- ly designed for Standard Arabic.", "acronym": "TD", "label": "Tunisian dialect", "ID": "5432"}, {"sentence": "3 Challenges  In the last years, TD is widely used  in new written media and web 2.0, especially in  social networks, blogs, forums, weblogs, etc.,", "acronym": "TD", "label": "Tunisian dialect", "ID": "5433"}, {"sentence": "fr                    Abstract    In this paper, we propose TunDiaWN  (TD Wordnet) a lexical re- source for the dialect language spoken in  Tunisia.", "acronym": "TD", "label": "Tunisian dialect", "ID": "5434"}, {"sentence": "c?2014 Association for Computational Linguistics TD Wordnet creation and enrichment   using web resources and other Wordnets  Rihab Bouchlaghem   LARODEC, ISG de Tunis  2000 Le Bardo, Tunisie  rihab.bouchlaghem@isg.rnu.tn                        Aymen Elkhlifi                Paris-Sorbonne University,             28 Rue Serpente, Paris, France   Aymen.Elkhlifi@paris.sorbonne.fr                    Abstract    In this paper, we propos", "acronym": "TD", "label": "Tunisian dialect", "ID": "5435"}, {"sentence": "that dealt with the automatic  processing of TD are based on spo- ken dialogue corpus.", "acronym": "TD", "label": "Tunisian dialect", "ID": "5436"}, {"sentence": "2013)  presented a method that aims to construct bilin- gual dictionary using explicit knowledge about  the relation between TD and Stand- ard Arabic.", "acronym": "TD", "label": "Tunisian dialect", "ID": "5437"}, {"sentence": "ect Wordnet creation and enrichment   using web resources and other Wordnets  Rihab Bouchlaghem   LARODEC, ISG de Tunis  2000 Le Bardo, Tunisie  rihab.bouchlaghem@isg.rnu.tn                        Aymen Elkhlifi                Paris-Sorbonne University,             28 Rue Serpente, Paris, France   Aymen.Elkhlifi@paris.sorbonne.fr                    Abstract    In this paper, we propose TunDiaWN  (TD Wordnet) a lexical re- source for the dialect language spoken in  Tunisia.", "acronym": "TD", "label": "Tunisian dialect", "ID": "5438"}, {"sentence": "A clustering  technique is adapted and applied to mine  the possible relations existing between  the TD extracted words and  to group them into meaningful groups.", "acronym": "TD", "label": "Tunisian dialect", "ID": "5439"}, {"sentence": "lkhlifi                Paris-Sorbonne University,             28 Rue Serpente, Paris, France   Aymen.Elkhlifi@paris.sorbonne.fr                    Abstract    In this paper, we propose TunDiaWN  (TD Wordnet) a lexical re- source for the dialect language spoken in  Tunisia.", "acronym": "TD", "label": "Tunisian dialect", "ID": "5440"}, {"sentence": "A clustering  technique is adapted and applied to mine  the possible relations existing between  the TD extracted words and  to group them into meaningful gro", "acronym": "TD", "label": "Tunisian dialect", "ID": "5441"}, {"sentence": "The aTD of metaphor and metonymy comprehension in children with autism.", "acronym": "TD", "label": "typical development", "ID": "5442"}, {"sentence": "Given the vocabulary differences seen here, we expect to find not only that children with ASD are aban- doning the topic of the source narrative more fre- quently than children with TD but also that the topics they choose to pursue are related to their own individual specific interests.", "acronym": "TD", "label": "typical development", "ID": "5443"}, {"sentence": "In this paper, we explore the  behaviour of a connectionist network, since these  systems have been widely applied to phenomena  within cognitive and language development  (Elman et al, 1996) and more recently to capturing  both aTD and acquired deficits in  adults (Thomas & Karmiloff-Smith, 2002, 2003).", "acronym": "TD", "label": "typical development", "ID": "5444"}, {"sentence": "We find that the words used by children with TD tend to be used by other children with typ- ical development, while the words used by children with autism overlap less with those used by children with typical devel- opment and even less with those used by other children with autism.", "acronym": "TD", "label": "typical development", "ID": "5445"}, {"sentence": "A TD model involves a parser which generates candidate anal- yses, and human annotators who manually iden- tify the desired tree structure.", "acronym": "TD", "label": "typical development", "ID": "5446"}, {"sentence": "Autism, context/noncontext information processing, and aTD.", "acronym": "TD", "label": "typical development", "ID": "5447"}, {"sentence": "Im- proved algorithms for TD environment in a hyperlinked.", "acronym": "TD", "label": "topic distillation", "ID": "5448"}, {"sentence": "Im- proved algorithms for TD in a hyper- linked environment.", "acronym": "TD", "label": "topic distillation", "ID": "5449"}, {"sentence": "Improved algo- rithms for TD in a hyperlinked environ- ment.", "acronym": "TD", "label": "topic distillation", "ID": "5450"}, {"sentence": "The first state is always the  initial state \"1\", the second depemts on the type of  stern and its ending throughout the conjugation (dig-  its or character strings can be used indifferently for  labelling the states), l~br example, for the first verb  conjugation, whose INF end in \"-at,\" the sec-  ond states are spread out among 10 different states.", "acronym": "INF", "label": "infinitives", "ID": "5451"}, {"sentence": "der fiberlegene Paul  Zu-INF (marginally) and dad-clauses are possible in  lieu of the prepositional.", "acronym": "INF", "label": "infinitives", "ID": "5452"}, {"sentence": "The complement clauses we  consider are daft (that) clauses, ob (whether) clauses, and  infinitive clauses (pure INF and infinitive clauses in-  troduced by zu (to)).", "acronym": "INF", "label": "infinitives", "ID": "5453"}, {"sentence": "This implies that deep annotations as, for instance, have been derived so far from PennTreeBank/PropBank, in which ei- ther all syntactic nodes of the annotation are kept (as in (Bohnet et al, 2010)) or only certain syntac- tic nodes are removed (as THAT complementizers and TO INF in the shared task 2011 on sur- face realization (Belz et al, 2011)) still fall short of a genuine semantic annotation.", "acronym": "INF", "label": "infinitives", "ID": "5454"}, {"sentence": "Tile lexicon mainly contains verb INF in an encoded form.", "acronym": "INF", "label": "infinitives", "ID": "5455"}, {"sentence": "Like this grammar, it contains around 1300 elementary trees and covers auxiliaries, copula, raising and small clause constructions, topicalization, relative clauses, INF, gerunds, passives, adjuncts, ditransitives and datives, ergatives, it-clefts, wh- clefts, PRO constructions, noun-noun modifica- tion, extraposition, sentential adjuncts, impera- tives and resultatives.", "acronym": "INF", "label": "infinitives", "ID": "5456"}, {"sentence": "Sentences with INFs  I. The teacher wanted Kathy to  hurry.", "acronym": "INF", "label": "Inf init ive", "ID": "5457"}, {"sentence": "IF IGEN ( INF Generator)  is another   ra ther  s t ra ight fo rward  F in i te  State Pat tern   Matcher (developed by Gunnel K~llgren).", "acronym": "INF", "label": "Inf init ive", "ID": "5458"}, {"sentence": "We present some experiments il-  lustrating the accuracy of the method and note  that with this INF added, our pronoun  resolution method achieves 84.2% accuracy.", "acronym": "INF", "label": "information", "ID": "5459"}, {"sentence": "The first piece of useful INF we con-  sider is the distance between the pronoun  and the candidate antecedent.", "acronym": "INF", "label": "information", "ID": "5460"}, {"sentence": "The second half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of English words, INF that is  itself used in the pronoun resolution program.", "acronym": "INF", "label": "information", "ID": "5461"}, {"sentence": "This program differs  from earlier work in its almost complete lack of  hand-crafting, relying instead on a very small  corpus of Penn Wall Street Journal Tree-bank  text (Marcus et al, 1993) that has been marked  with co-reference INF.", "acronym": "INF", "label": "information", "ID": "5462"}, {"sentence": "is INF added, our pronoun  resolution method achieves 84.2% accuracy.", "acronym": "INF", "label": "information", "ID": "5463"}, {"sentence": "The second  experiment investigates a method for unsuper-  vised learning of gender/number/animaticity  INF.", "acronym": "INF", "label": "information", "ID": "5464"}, {"sentence": "Our first experiment  shows the relative contribution of each source  Of INF and demonstrates a uccess rate  of 82.9% for all sources combined.", "acronym": "INF", "label": "information", "ID": "5465"}, {"sentence": "urnal Tree-bank  text (Marcus et al, 1993) that has been marked  with co-reference INF.", "acronym": "INF", "label": "information", "ID": "5466"}, {"sentence": "For MUC-5, the following event-related thematic roles were handled : agent, patient, EXP, recipient , beneficiary, source, and location .", "acronym": "EXP", "label": "experiencer", "ID": "5467"}, {"sentence": "18 See Verma nd Mohanan (1991) for an extensive survey of EXP subject constructions i  different  languages.", "acronym": "EXP", "label": "experiencer", "ID": "5468"}, {"sentence": "is an  EXP of the verb upset.", "acronym": "EXP", "label": "experiencer", "ID": "5469"}, {"sentence": "(Yi, 2007) shows agent and EXP as ARG0  accounts for 93% in all ARG0s in Propbank.", "acronym": "EXP", "label": "experiencer", "ID": "5470"}, {"sentence": "the semantic case or  cases that may be used in that slot  ('age,in, \"EXP', etc.),", "acronym": "EXP", "label": "experiencer", "ID": "5471"}, {"sentence": "425  Computational Linguistics Volume 17, Number 4  The sentences in 34 illustrate the various syntactic onsequences of metonymy and  coercion involving EXP verbs, while those in 35 show the different metonymic  extensions possible from the causing event in a killing.", "acronym": "EXP", "label": "experiencer", "ID": "5472"}, {"sentence": "EXP mention of speakers.", "acronym": "EXP", "label": "Explicit", "ID": "5473"}, {"sentence": "Although the four genes are con- EXP renaming PMID 15767583 : Genetic analysis of ykvJKLM mu- tants in Acinetobacter confirmed that each was essen- tial for queuosine biosynthesis, and the genes were re- named queCDEF .", "acronym": "EXP", "label": "Explicit", "ID": "5474"}, {"sentence": "EXP re- naming relations occur in 261 sentences, synonymy- like relations in 349 sentences, biological proof- based relations in 76 sentences.", "acronym": "EXP", "label": "Explicit", "ID": "5475"}, {"sentence": "As expected, the overall accuracy of identify- ing contingency and expansion relations is lower, Task All relations EXP relations only Comparison 91.28% (76.54%) 97.23% (69.72%) Contingency 84.44% (76.81%) 93.99% (79.73%) Temporal 94.79% (86.54%) 95.4% (79.98%) Expansion 77.51% (55.67%) 97.61% (65.16%) Table 2: Decision tree classification accuracy us- ing only the presence of connectives as binary fea- tures.", "acronym": "EXP", "label": "Explicit", "ID": "5476"}, {"sentence": "Atkins, Beryl S., Judy Kegl, Beth Levin (1986)  \"EXP and Implicit Information in Dictionaries\",  in Advances in Lexicology.", "acronym": "EXP", "label": "Explicit", "ID": "5477"}, {"sentence": "EXP renaming relation is the easiest positive case to identify.", "acronym": "EXP", "label": "Explicit", "ID": "5478"}, {"sentence": "Although the four genes are con- EXP renaming PMID 15767583 : Genetic analys", "acronym": "EXP", "label": "Explicit", "ID": "5479"}, {"sentence": "Class EXP (%) Implicit (%) Total Comparison 5590 (69.05%) 2505 (30.95%) 8095 Contingency 3741 (46.75%) 4261 (53.25%) 8002 Temporal 3696 (79.55%) 950 (20.45%) 4646 Expansion 6431 (42.04%) 8868 (57.96%) 15299 Table 1: Discourse relation distribution in seman- tic and explicit/implicit classes in the PDTB 3 Distribution and ambiguity of connectives Table 1 shows the distribution of discourse rela- tio", "acronym": "EXP", "label": "Explicit", "ID": "5480"}, {"sentence": "t Genre = Email Lexical Stop-words Stylistic Character n-grams Lexical Stop-words Stylistic Character n-grams Sex Discrimination 5.85 5.57 1.67 10.33 2.24 7.29 8.86 9.72 Legalization of Marijuana 7.86 7.76 1.57 12.19 2.91 3.32 5.21 7.39 Catholic Church 6.24 8.76 6.24 14.33 2.41 4.48 3.59 5.22 Privacy Rights 5.9 4.66 1.9 14.05 2.97 6.45 4.6 10.06 War in Iraq 8.1 7.95 3.48 15.57 3.96 7.58 2.99 7.79 GM 7.19 5.85 7.19 10.29 2.57 4.31 1.98 6.82 Table 5: Average performance gain from adding an additional topic as training data across different initial topics on dataset 1.", "acronym": "GM", "label": "Gay Marriage", "ID": "5481"}, {"sentence": "Test Topic Lexical Stop-words Stylistic Character n-grams Stop-words + Character n-grams Previous Work Sex Discrimination 66.67 76.19 33.33 95.24 95.24 95 Catholic Church 76.19 95.24 38.10 95.24 100 95 GM 80.95 80.95 42.86 90.48 90.48 95 Legalization of Marijuana 52.38 66.67 33.33 95.24 100 100 Privacy Rights 42.86 52.38 28.57 95.24 90.48 100 War in Iraq 57.14 71.43 38.10 100 100 81 Average 62.7 73.81 35.72 95.24 96.03 94.33 Table 7: Comparing performance of our work with previous work in the same training/testing setting.", "acronym": "GM", "label": "Gay Marriage", "ID": "5482"}, {"sentence": "The first corpus contains communication samples from 21 authors in six genres (Email, Essay, Blog, Chat, Phone Interview, and Discussion) on six topics (Catholic Church, GM, War in Iraq, Legalization of Marijuana, Privacy Rights, and Sex Discrimination), which we call dataset 1.", "acronym": "GM", "label": "Gay Marriage", "ID": "5483"}, {"sentence": "7 Model Evaluation 7.1 Qualitative Evaluation Tables 4 and 5 present the inferred topic- viewpoints words, i.e. arguing expressions, by JTV for the Obama Healthcare and GM data sets, respectively.", "acronym": "GM", "label": "Gay Marriage", "ID": "5484"}, {"sentence": "GM?", "acronym": "GM", "label": "Gay Marriage", "ID": "5485"}, {"sentence": "v. Capitalism 214 62% 2.97 46% 55% 70% 0.67 0.66 0.68 53% 0.49 0.48 0.49 Death Penalty 331 60% 2.40 45% 56% 35% 0.31 0.29 0.34 55% 0.46 0.48 0.44 Evolution 818 66% 3.74 53% 58% 82% 0.78 0.78 0.79 56% 0.49 0.48 0.50 Existence Of God 852 76% 4.16 51% 56% 75% 0.73 0.70 0.76 52% 0.49 0.47 0.51 Firefox v. IE 233 38% 1.27 15% 79% 76% 0.47 0.44 0.49 72% 0.33 0.34 0.33 GM 560 56% 2.01 28% 65% 84% 0.77 0.74 0.81 60% 0.43 0.43 0.44 Gun Control 135 59% 2.08 45% 63% 37% 0.24 0.21 0.27 53% 0.24 0.30 0.20 Healthcare 112 79% 3.11 53% 55% 73% 0.71 0.69 0.72 60% 0.49 0.56 0.44 Immigration 78 58% 1.95 33% 54% 33% 0.21 0.23 0.19 53% 0.39 0.48 0.33 Iphone v. Blackberry 25 44% 1.14 14% 67% 88% 0.80 0.86 0.75 71% 0.46 0.60 0.38 Israel v. Palestine 64 33% 3.37 53% 58", "acronym": "GM", "label": "Gay Marriage", "ID": "5486"}, {"sentence": "The first one is a dedi- cated webpage using the GM JavaScript API (see Figure 2).", "acronym": "GM", "label": "Google Maps", "ID": "5487"}, {"sentence": "2 Related work Mobile applications such as Siri, GM Navigation, Sygic, etc.", "acronym": "GM", "label": "Google Maps", "ID": "5488"}, {"sentence": "2 The application is re- 1 http://code.google.com/apis/kml/documentation/ 2 In order to run it, start Google Earth with KML: http://press.jrc.it/geo?type=event&format=kml&language=en 147 Figure 2: Event visualisation with GM stricted to displaying at most half the globe, but it allows expanding overlaid events.", "acronym": "GM", "label": "Google Maps", "ID": "5489"}, {"sentence": "Templates exist for referring to multimedia content, external websites, news stories, scientific sources, other on-line repositories (such as the Internet Movie Database (IMDB), medical clas- sification systems (ICD9 and ICD10), coordinates on GM, etc.", "acronym": "GM", "label": "Google Maps", "ID": "5490"}, {"sentence": "The video (dated 6/2005) shows exam- ples of two web browsing tasks, one as an exam- ple of navigating the New York Times web site, the other using GM to select and zoom in on a target area.", "acronym": "GM", "label": "Google Maps", "ID": "5491"}, {"sentence": "2 Related work Mobile apps such as Siri, GM Naviga- tion, Sygic, etc.", "acronym": "GM", "label": "Google Maps", "ID": "5492"}, {"sentence": "We show in detail our findings about syntactic levels (how often GM helped assign a relation between two clauses, a verb and its arguments, or a noun and its modifier) and about the accuracy of the suggestion.", "acronym": "GM", "label": "graph matching", "ID": "5493"}, {"sentence": "Robust textual inference via GM.", "acronym": "GM", "label": "graph matching", "ID": "5494"}, {"sentence": "Biometric identification based on the eye movements and GM techniques.", "acronym": "GM", "label": "graph matching", "ID": "5495"}, {"sentence": "Information retrieval with conceptual GM.", "acronym": "GM", "label": "graph matching", "ID": "5496"}, {"sentence": "Syntactic-semantic GM is used to produce a list of candidate assign- ments for 63.75% of the pairs analysed, and in 57% of situations the correct rela- tions is one of the system?s suggestions; in 19.6% of situations it suggests only the correct relation.", "acronym": "GM", "label": "graph matching", "ID": "5497"}, {"sentence": "Con- ceptual GM: a flexible algorithm and ex- periments.", "acronym": "GM", "label": "graph matching", "ID": "5498"}, {"sentence": "2 Background 2.1 GMs Factor graphs (Kschischang et al 2001) succinctly represent the joint distribution over random vari- ables by a product of factors that make the depen- dencies between the random variables explicit.", "acronym": "GM", "label": "Graphical Model", "ID": "5499"}, {"sentence": "Frey B.J. 1998, GMs for Machine  Learning and Digital Communication,  Cambridge, MA, MIT Press  Gildea D., Jurafsky D. 2002, Automatic labeling of  semantic roles, Computational Linguistics,  28(3):245-288.", "acronym": "GM", "label": "Graphical Model", "ID": "5500"}, {"sentence": "The GMs Toolkit: An open source software system for speech and time-series processing.", "acronym": "GM", "label": "Graphical Model", "ID": "5501"}, {"sentence": "An example of the model defined over 5 m2 m1 m3 m5 m4 1 1 1 1 y 12 y 23 y 13 y 45 Figure 3: GM for Entity Resolution: defined over 5 mentions, with the setting of the vari- ables resulting in 2 entities.", "acronym": "GM", "label": "Graphical Model", "ID": "5502"}, {"sentence": "k, `] 2.2 Formulation as a GM We score triples (a, pi, ?)", "acronym": "GM", "label": "Graphical Model", "ID": "5503"}, {"sentence": "In M. I. Jordan, editor, Learning in GMs.", "acronym": "GM", "label": "Graphical Model", "ID": "5504"}, {"sentence": "For the sole transition-based parsers trained with the selected features, we obtain for Chinese, Hungar- ian and Russian higher labeled and UAS.", "acronym": "UAS", "label": "unlabeled accuracy scores", "ID": "5505"}, {"sentence": "Furthermore, when we inspect the UAS (not shown here), we see that the unlabeled attachment score for idafa also decreases.", "acronym": "UAS", "label": "unlabeled accuracy scores", "ID": "5506"}, {"sentence": "However, it is possible to compute approximate UAS by training the constituent parsers on the NP-patched (Vadas and Curran, 2007) version of the data and then running the test output through just the first conversion script?that is, the modified version of Johansson and Nugues? (", "acronym": "UAS", "label": "unlabeled accuracy scores", "ID": "5507"}, {"sentence": "nt to remember that an UAS of 75.8% corresponds to a word-to-word score of 82.7%, which puts Turkish on a par with languages like Czech, Dutch and Spanish.", "acronym": "UAS", "label": "unlabeled attachment score", "ID": "5508"}, {"sentence": "The first consists of determiners and particles, which have an UAS over 80% and which are found within a distance of 1?1.4 IGs from their head.7 The second group mainly contains subjects, objects and different kinds of adjuncts, with a score in the range 60?80% and a distance of 1.8?5.2 IGs to their head.", "acronym": "UAS", "label": "unlabeled attachment score", "ID": "5509"}, {"sentence": "It is then important to remember that an UAS of 75.8% corresponds to a word-to-word score of 82.7%, which puts Turkish on a par with languages like Czech, Dutch and Spanish.", "acronym": "UAS", "label": "unlabeled attachment score", "ID": "5510"}, {"sentence": "The UASs of the converted dependencies are shown as the accuracies in Table 5, since most bunsetsu-based dependency parsers out- put only unlabeled structure.", "acronym": "UAS", "label": "unlabeled attachment score", "ID": "5511"}, {"sentence": "Moreover, when we break down the results according to whether the head of a dependency is part of a multiple-IG word or a complete (single-IG) word, we observe a highly significant difference in accuracy, with only 53.2% UAS for multiple-IG heads versus 83.7% for single-IG heads.", "acronym": "UAS", "label": "unlabeled attachment score", "ID": "5512"}, {"sentence": "Benefiting from the rich features selected in the tree kernel space, our model achieved the best reported UAS of 93.72 without using any additional resource.", "acronym": "UAS", "label": "unlabeled attachment score", "ID": "5513"}, {"sentence": "We tested several evaluation measures that compute the results of each model, that are LAS [labeled attachment score], LUMP 8 [(labeled attachment score + UAS + mor- phology accuracy + part-of-speech accuracy)/4] and PMLAS 9 [labeled attachment score, morphology accuracy and part-of-speech accuracy].", "acronym": "UAS", "label": "unlabeled attachment score", "ID": "5514"}, {"sentence": "For the sole transition-based parsers trained with the selected features, we obtain for Chinese, Hungar- ian and Russian higher labeled and UASs.", "acronym": "UAS", "label": "unlabeled accuracy score", "ID": "5515"}, {"sentence": "Furthermore, when we inspect the UASs (not shown here), we see that the unlabeled attachment score for idafa also decreases.", "acronym": "UAS", "label": "unlabeled accuracy score", "ID": "5516"}, {"sentence": "However, it is possible to compute approximate UASs by training the constituent parsers on the NP-patched (Vadas and Curran, 2007) version of the data and then running the test output through just the first conversion script?that is, the modified version of Johansson and Nugues? (", "acronym": "UAS", "label": "unlabeled accuracy score", "ID": "5517"}, {"sentence": "0.006 (0.004) max -0.03 (0.001) -0.01 (0.003) -0.04 (0.001) -0.02 (0.003) mult 0.16 (0.002) 0.11 (0.006) 0.21 (0.002) 0.03 (0.006) min 0.13 (0.001) 0.11 (0.007) 0.10 (0.001) 0.09 (0.007) gm 0.14 (0.001) 0.18 (0.005) 0.12 (0.001) 0.16 (0.005) dp -0.03 (0.002) -0.09 (0.007) -0.04 (0.002) -0.09 (0.007) Table 7: Means and SEs for Increases in Cosine with respect to the hd Baseline for Proposed Higher-Order Dependency Based Approach.", "acronym": "SE", "label": "Standard Error", "ID": "5518"}, {"sentence": "Corpus Predictability Model Performance SE baseline model 81.98% unigram model 82.86% \u0006 0.93 Read bigram predictability model 84.41% \u0006 1.10 unigram+bigram model 85.03% \u0006 1.04 baseline model 70.03% unigram model 72.22% \u0006 0.62 Spontaneous bigram model 74.46% \u0006 0.30 unigram+bigram model 77.43% \u0006 0.51 Table 4: Ripper Results for Accent Status Prediction Model Predictability Total Accented Word Not Accented Accentability unigram", "acronym": "SE", "label": "Standard Error", "ID": "5519"}, {"sentence": "Bootstrap Methods for SEs, Confidence Intervals, and Other Measures of Statistical Accuracy.", "acronym": "SE", "label": "Standard Error", "ID": "5520"}, {"sentence": "Method % Wins SE Game only 45.7 ?", "acronym": "SE", "label": "Standard Error", "ID": "5521"}, {"sentence": "i Decision Trees Rule Sets  Features Accuracy SE Accuracy SE  1.", "acronym": "SE", "label": "Standard Error", "ID": "5522"}, {"sentence": "324 Shutova, Teufel, and Korhonen Statistical Metaphor Processing Table 4 Examples of seed SE by the system.", "acronym": "SE", "label": "set expansion", "ID": "5523"}, {"sentence": "This is because the work not only adopts the same candidate SE strategy mentioned previously, but also uti- lizes monolingual information when selecting NE pairs (only a simple bigram model is used, however).", "acronym": "SE", "label": "set expansion", "ID": "5524"}, {"sentence": "c?2011 Association for Computational Linguistics HITS-based Seed Selection and Stop List Construction for Bootstrapping Tetsuo Kiso Masashi Shimbo Mamoru Komachi Yuji Matsumoto Graduate School of Information Science Nara Institute of Science and Technology Ikoma, Nara 630-0192, Japan {tetsuo-s,shimbo,komachi,matsu}@is.naist.jp Abstract In bootstrapping (seed SE), select- ing good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human super- vision.", "acronym": "SE", "label": "set expansion", "ID": "5525"}, {"sentence": "Furthermore, Category (V) errors (Expansion Limitation, 5%) are caused by the problem that the desired candidate (i.e., reference) is excluded during the candidate 250 Chen, Zong, and Su A Joint Model to Identify and Align Bilingual Named Entities SE stage.", "acronym": "SE", "label": "set expansion", "ID": "5526"}, {"sentence": "To apply Espresso for this task, we reformulate the task to be that of seed SE, and not classification.", "acronym": "SE", "label": "set expansion", "ID": "5527"}, {"sentence": "Additional guidance, 325 Computational Linguistics Volume 39, Number 2 Table 5 Examples of seed SE by the baseline.", "acronym": "SE", "label": "set expansion", "ID": "5528"}, {"sentence": "0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Fraction of test data used Accu racy     Semiparametric Landwehr et al Landwehr et al (TA) 0 50 100 150 200 2500 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Number of individuals R Accu racy     Landwehr et al (T) Holland & K. (unweighted) Holland & K. (weighted) Figure 3: Multiclass accuracy over number of test observations (left) and number of individuals R (right) with SEs.", "acronym": "SE", "label": "standard error", "ID": "5529"}, {"sentence": "The Landwehr et al model and variants also assign a 592 0 0.2 0.4 0.6 0.8 10 0.2 0.4 0.6 0.8 1 Fraction of test data used Acc urac y Figure 6: Multiclass accuracy over number of test observations with SEs for Semiparametric variants.", "acronym": "SE", "label": "standard error", "ID": "5530"}, {"sentence": "one SE?.", "acronym": "SE", "label": "standard error", "ID": "5531"}, {"sentence": "ber of individuals R (right) with SEs.", "acronym": "SE", "label": "standard error", "ID": "5532"}, {"sentence": "one SE?", "acronym": "SE", "label": "standard error", "ID": "5533"}, {"sentence": "SE.", "acronym": "SE", "label": "standard error", "ID": "5534"}, {"sentence": "Note that the error bars shown in this  section show the SE.", "acronym": "SE", "label": "standard error", "ID": "5535"}, {"sentence": "The first one is fired whenever a free SE pronoun is spotted; the second one takes the results of the first submodule and checks for nominal anaphora.", "acronym": "SE", "label": "sentence external", "ID": "5536"}, {"sentence": "gogo-nara yamada-ga i-ru noda  afternoon-cond PN-nom be-pres aux-pres  (If you mean) the afternoon, Yamada will be here  Figure 3: Discourse relations with and without  anaphoric force  Among discourse relations with sentence xter-  nal anaphoric binding there are two types: those  whose antecedent part is bound sentence xter-  nally and those whose conclusion part is bound  SEly.", "acronym": "SE", "label": "sentence external", "ID": "5537"}, {"sentence": "The system uses two resolution submodules which work in sequence: the first one is fired whenever a free SE pronoun is spotted; the second one takes the results of the first submodule and checks for nominal anaphora.", "acronym": "SE", "label": "sentence external", "ID": "5538"}, {"sentence": "MEMs for Fra- meNet Classification.", "acronym": "MEM", "label": "Maximum Entropy Model", "ID": "5539"}, {"sentence": "MEMing  Toolkit for Python and C++.", "acronym": "MEM", "label": "Maximum Entropy Model", "ID": "5540"}, {"sentence": "A MEM for Prepositional Phrase Attachment.", "acronym": "MEM", "label": "Maximum Entropy Model", "ID": "5541"}, {"sentence": "224   CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 203?207 Manchester, August 2008 Parsing Syntactic and Semantic Dependencies with Two Single-Stage MEMs ?", "acronym": "MEM", "label": "Maximum Entropy Model", "ID": "5542"}, {"sentence": "A MEM for Preposi-  tional Phrase Attachment.", "acronym": "MEM", "label": "Maximum Entropy Model", "ID": "5543"}, {"sentence": "A MEM  for Part-Of-Speech Tagging.", "acronym": "MEM", "label": "Maximum Entropy Model", "ID": "5544"}, {"sentence": "wever, recent theoretical and experimental re- sults in (La\u000berty et al 2001) have highlighted problems with the parameter estimation method for MEMs.", "acronym": "MEM", "label": "ME model", "ID": "5545"}, {"sentence": "However, recent theoretical and experimental re- sults in (La\u000berty et al 2001) have highlighted problems with the parameter estimation method for MEMs.", "acronym": "MEM", "label": "ME model", "ID": "5546"}, {"sentence": "La\u000berty et al 2001) give exper- imental results suggesting that CRFs can per- form signi\fcantly better than MEMs.", "acronym": "MEM", "label": "ME model", "ID": "5547"}, {"sentence": "MEMs have the advantage of being quite  exible in the features that can be incorporated in the model.", "acronym": "MEM", "label": "ME model", "ID": "5548"}, {"sentence": "For system development, we used  MEGA model optimization package6, an imple- mentation of MEMs.", "acronym": "MEM", "label": "ME model", "ID": "5549"}, {"sentence": "Although MEMs provide a nice framework for  incorporating arbitrary knowledge sources that can  be encoded as a large set of constraints, training and  using MEMs is extremely computationally  expensive.", "acronym": "MEM", "label": "ME model", "ID": "5550"}, {"sentence": "Automatic identifi- cation of non-compositional multi-word expressions using LSA.", "acronym": "LSA", "label": "latent semantic analysis", "ID": "5551"}, {"sentence": "Auto- matic identification of non-compositional multi-word expressions using LSA.", "acronym": "LSA", "label": "latent semantic analysis", "ID": "5552"}, {"sentence": "Using LSA to improve access to textual infor-mation.", "acronym": "LSA", "label": "latent semantic analysis", "ID": "5553"}, {"sentence": "One con- clusion of our research is that formality variation is omnipresent in natural corpora, but it does not follow that the identification of these differences on the lexical level is a trivial one; nevertheless, 90 we are able to make significant progress using the methods presented here, in particular the applica- tion of LSA to blog corpora.", "acronym": "LSA", "label": "latent semantic analysis", "ID": "5554"}, {"sentence": "Contextual spelling correction using LSA.", "acronym": "LSA", "label": "latent semantic analysis", "ID": "5555"}, {"sentence": "Unsupervised learning by proba- bilistic LSA.", "acronym": "LSA", "label": "latent semantic analysis", "ID": "5556"}, {"sentence": "Researchers are begin- ning to discuss the limits of structured instruments in terms of which language impairments they tap into and how well they do so, and are advocating the po- tential benefits of LSA ?", "acronym": "LSA", "label": "language sample analysis", "ID": "5557"}, {"sentence": "A solution to Plato?s problem: The LSA the- ory of the acquisition, induction, and representation of knowledge.", "acronym": "LSA", "label": "Latent Semantic Analysis", "ID": "5558"}, {"sentence": "2008), divides the problem in two parts: first the continuous representation is obtained by an adapta- tion of the LSA; then a Gaus- sian mixture model is learned using this continu- ous representation and included in a hidden Markov model.", "acronym": "LSA", "label": "Latent Semantic Analysis", "ID": "5559"}, {"sentence": "They also pro- posed to use LSA to compute the association strength with seed words.", "acronym": "LSA", "label": "Latent Semantic Analysis", "ID": "5560"}, {"sentence": "Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007) 0.74 (reimplemented in (Yeh et al 2009)) 0.71 Compact Hierarchical ESA (Liberman and Markovitch, 2009) 0.71 Hyperlink Graph (Milne and Witten, 2008) 0.69 Graph Traversal (Agirre et al 2009)) 0.66 Distributional Similarity (Agirre et al 2009)) 0.65 LSA (Finkelstein et al 2002) 0.56 Random Graph Walk (Hughes and Ramage, 2007) 0.55 Normalized Path-length (lch) (Strube and Ponzetto, 2006) 0.55 cPMId(? :", "acronym": "LSA", "label": "Latent Semantic Analysis", "ID": "5561"}, {"sentence": "A solution to Plato?s problem: The LSA theory of the acquisition, induction, and representation of knowledge.", "acronym": "LSA", "label": "Latent Semantic Analysis", "ID": "5562"}, {"sentence": "Baldwin et al, 2003) use WordNet::Similarity to provide an evaluation tool for multiword expressions that are identified via LSA. (", "acronym": "LSA", "label": "Latent Semantic Analysis", "ID": "5563"}, {"sentence": "LSA as a tool  to improve automatic speech recognition perfor- mance.", "acronym": "LSA", "label": "Latent semantic analysis", "ID": "5564"}, {"sentence": "LSA for Bulgarian literature.", "acronym": "LSA", "label": "Latent semantic analysis", "ID": "5565"}, {"sentence": "LSA for Russian literature investigation.", "acronym": "LSA", "label": "Latent semantic analysis", "ID": "5566"}, {"sentence": "LSA of stream A might also be usefully employed here.", "acronym": "LSA", "label": "Latent semantic analysis", "ID": "5567"}, {"sentence": "LSA for text-based research.", "acronym": "LSA", "label": "Latent semantic analysis", "ID": "5568"}, {"sentence": "LSA.", "acronym": "LSA", "label": "Latent semantic analysis", "ID": "5569"}, {"sentence": "Using umls CUIs (cuis) for word sense disam- biguation in the biomedical domain.", "acronym": "CUI", "label": "concept unique identifier", "ID": "5570"}, {"sentence": "A typical entry for a concept is:    ID Course  Label Course  Subclassof Work    Table 1 A concept    where ID is the CUI, label is  the readable name of the concept,  subclassof indicates  the relation to another class.", "acronym": "CUI", "label": "concept unique identifier", "ID": "5571"}, {"sentence": "trm278 CUI L0024669 S0059711 2003AC trm656 C0000726 L0000726 S0414154 2003AC . . . . . . . . . . . . . . .", "acronym": "CUI", "label": "C0024881", "ID": "5572"}, {"sentence": "What U.S. company did Sony purchase to form Sony Pic- tures ENT (SPE)??", "acronym": "ENT", "label": "Entertainment", "ID": "5573"}, {"sentence": "Sony Pictures ENT (SPE)?;", "acronym": "ENT", "label": "Entertainment", "ID": "5574"}, {"sentence": "Proceedings 5th Interna- tional Conference ENT Computing,  Cambridge, UK.", "acronym": "ENT", "label": "Entertainment", "ID": "5575"}, {"sentence": "s top politician on Tuesday evening, making way for Conservative 1 http://code.google.com/p/evbcorpus/ 2 http://sourceforge.net/apps/mediawiki/opennlp/ 3 http://cogcomp.cs.illinois.edu/page/software view/4 4 http://nlp.stanford.edu/ner/index.shtml 5 http://alias-i.com/lingpipe/index.html 87 Table 2: Number of files and sentences for each topic Topic File Sentence Economy 125 4,326 ENT 11 365 Health 336 21,107 Politics 141 4,253 Science 34 1,692 Social 110 3,699 Sport 22 838 Technology 104 2,609 Misc 117 117 Total 1,000 45,531 Figure 2: Architecture of building EVNECorpus from EVBCorpus leader David Cameron.? :", "acronym": "ENT", "label": "Entertainment", "ID": "5576"}, {"sentence": "For example, for  organization entities, systems distinguish between  Media and ENT organizations.", "acronym": "ENT", "label": "Entertainment", "ID": "5577"}, {"sentence": "In Proceedings of the AI, ALife and ENT Workshop, Mon- treal, CA.", "acronym": "ENT", "label": "Entertainment", "ID": "5578"}, {"sentence": "<K> ENT, sports, and games 12.", "acronym": "ENT", "label": "Entertainment", "ID": "5579"}, {"sentence": "We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named ENTy recognition was used for identifying proper names, e.g., ?", "acronym": "ENT", "label": "entit", "ID": "5580"}, {"sentence": "Characters in chil- dren?s stories can either be human or non-human ENTies, i.e., animals and non-living objects, ex- hibiting anthropomorphic traits.", "acronym": "ENT", "label": "entit", "ID": "5581"}, {"sentence": "3.2 Identification of Story Characters The second step is identifying candidate charac- ters (i.e., ENTies) that appear in the stories under analysis.", "acronym": "ENT", "label": "entit", "ID": "5582"}, {"sentence": "This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) named ENTy recognition, (vi) dependency parsing, and (vii) co-reference analysis.", "acronym": "ENT", "label": "entit", "ID": "5583"}, {"sentence": "Two broad approaches for the iden- tification of story characters were followed: (i) named ENTy recognition, and (ii) identification of character nominals, e.g., ?", "acronym": "ENT", "label": "entit", "ID": "5584"}, {"sentence": "The more fre-  quently an ENTy is repeated, the more likely it  is to be the topic of the story and thus to be  a candidate for pronominalization.", "acronym": "ENT", "label": "entit", "ID": "5585"}, {"sentence": "The log-OR behaves similarly for our basic algorithm, but appears to be more robust to other partitioning algorithms or tuning (see Section 6), so, for simplicity, we present it here as well.", "acronym": "OR", "label": "odds ratio", "ID": "5586"}, {"sentence": "Specifically, we prune links between pairs of mentions that are of men- tion distance more than 100, as well as values for ai that fall below a particular OR threshold with respect to the best setting of that ai in the BASIC model; that is, those for which log ( PBASIC (ai|x) maxj PBASIC (ai = j|x) ) is below a cutoff ?.", "acronym": "OR", "label": "odds ratio", "ID": "5587"}, {"sentence": "Examples include OR (Moham- mad and Hirst, 2006) and Turney?s (2001) IR-based pointwise mutual information (PMI-IR).", "acronym": "OR", "label": "odds ratio", "ID": "5588"}, {"sentence": "Alongside each feature, we show the words with the highest and lowest log- ORs with respect to the feature.", "acronym": "OR", "label": "odds ratio", "ID": "5589"}, {"sentence": "We first identified the most informative unigrams and bigrams using the information gain measure (Yang and Pedersen 1997), and then selected only the positive outcome predictors using OR (Mladenic and Grobelnik 1999).", "acronym": "OR", "label": "odds ratio", "ID": "5590"}, {"sentence": "For both benchmarks, the scoring scheme for measuring interaction set accuracy is in the form of a log OR of gene pairs either sharing anno- tations or physically interacting.", "acronym": "OR", "label": "odds ratio", "ID": "5591"}, {"sentence": "Anderberg)  4) [M14] Third Sokal-Sneath,       [M15] Sokal-Michiner,      [M16] Rogers-Tanimoto, [M17] Hamann  5) [M18] OR, [M19] Yule?s ,?", "acronym": "OR", "label": "Odds ratio", "ID": "5592"}, {"sentence": "OR ad bc   M19.", "acronym": "OR", "label": "Odds ratio", "ID": "5593"}, {"sentence": "c .565 .453 .546  1/bc .502 .532 .502  [M18] OR .443 .567 .456  Table 9.", "acronym": "OR", "label": "Odds ratio", "ID": "5594"}, {"sentence": "Feature OR  SE p # turns per Q 10.411 0.787 0.003 # clarifications  1.043 0.033 0.024 # no input  2.001 0.176 <0.001 Table 2: User satisfaction regression 5 Conclusion Our results demonstrate the viability of conduct-ing survey interviews of the sort from which im-portant national statistics are derived with spoken dialog systems.", "acronym": "OR", "label": "Odds ratio", "ID": "5595"}, {"sentence": "As an example, the following  3 AMs: OR, Yule?s ?", "acronym": "OR", "label": "Odds ratio", "ID": "5596"}, {"sentence": "3.1 ACM Classifications We tested TFM on corpora representing genres from academic publications to Usenet postings, 2OR is defined as .0/ 1\u000f2\u0004354%/fi68794$/ 1\u000f2\u00043:.0/;6 , where p is Pr(k|C), the probability that term k is present given category C, and q is Pr(k|!C).", "acronym": "OR", "label": "Odds ratio", "ID": "5597"}, {"sentence": "If one of two entity candidates causing crossing or type conflict has a verified lOR, the other can be regarded as an error and removed.", "acronym": "OR", "label": "ocator", "ID": "5598"}, {"sentence": "OR Most entities are often mentioned with geographic entities where they are lo- cated, especially when they are not familiar to general readers.", "acronym": "OR", "label": "ocator", "ID": "5599"}, {"sentence": "4.2 Gazetteer with LOR Most entities are often mentioned with geographic entities where they are lo- cated, especially when they are not familiar to general readers.", "acronym": "OR", "label": "ocator", "ID": "5600"}, {"sentence": "For instance, in the OUP-Hachette  English French dictionary, under bark we find the label  Bot(anical) attached to one meaning and the collOR  (of dog) associated with the other one.", "acronym": "OR", "label": "ocator", "ID": "5601"}, {"sentence": "We would need to investigate further how to make better  use of dictionary information such as collORs, etc.", "acronym": "OR", "label": "ocator", "ID": "5602"}, {"sentence": "The lOR information was also collected from the Probert Encyclopedia.", "acronym": "OR", "label": "ocator", "ID": "5603"}, {"sentence": "We can say that the lOR information is a special case of the co-occurrence information.", "acronym": "OR", "label": "ocator", "ID": "5604"}, {"sentence": "To extract the bigram features, we used a twitter-specific tokenizer (Potts, 2011), which marked uniform resource lORs (URLs), emoti- cons, and repeated characters, and which lowercased words that began with capital letters followed by lowercase letters (but left words in all capitals).", "acronym": "OR", "label": "ocator", "ID": "5605"}, {"sentence": "It filters incorrect word pairs by looking into outside of synonym words and context words in the other text (we call this OR the ?", "acronym": "OR", "label": "outside region", "ID": "5606"}, {"sentence": "We also demonstrate that the OR model achieves performance similar to that of the syllable bigram model using the local minima strategy.", "acronym": "OR", "label": "onset-rhyme", "ID": "5607"}, {"sentence": "Crucially, both the OR and the syllable bigram models achieve levels of perfor-mance that surpass the monosyllabic baseline.", "acronym": "OR", "label": "onset-rhyme", "ID": "5608"}, {"sentence": "There are, of course, other possible  strategies of segmentation, including division at syllable  boundaries and division based on the OR  structure within the syllable (for brange, br + angel  Evaluation of these alternative methods must await fur-  ther experimentation.", "acronym": "OR", "label": "onset-rhyme", "ID": "5609"}, {"sentence": "This research project is funded by the DE  Research Center (DFG).", "acronym": "DE", "label": "German", "ID": "5610"}, {"sentence": "Clemens LANGO  Computational Design, University of Wuppertal  Hofaue 35-39  D-42103 Wuppertal, DEy  lango@code.uni-wuppertal.de  Abstract  We understand knowledge as an infinite sequence  of associations between what we know and what  we are in the process of acquiring as knowledge.", "acronym": "DE", "label": "German", "ID": "5611"}, {"sentence": "Interactive Multimedia Navigation  Prof. Dr. Dr. Mihai NADIN  Computational Design, University of Wuppertal  Hofaue 35-39  D-42103 Wuppertal, DEy  nadin @ code.uni-wuppertal.de  Dipl.", "acronym": "DE", "label": "German", "ID": "5612"}, {"sentence": "c?2006 Association for Computational Linguistics The Benefit of Stochastic PP Attachment to a Rule-Based Parser Kilian A. Foth and Wolfgang Menzel Department of Informatics Hamburg University D-22527 Hamburg DEy foth|menzel@nats.informatik.uni-hamburg.de Abstract To study PP attachment disambiguation as a benchmark for empirical methods in nat- ural language processing it has often been reduced to a binary decision problem (be- tween verb or noun attachment) in a par- ticular syntactic configuration.", "acronym": "DE", "label": "German", "ID": "5613"}, {"sentence": "We believe that the approach could be straightforwardly extended to other Indoeuropean languages, such as Spanish, DE or English.", "acronym": "DE", "label": "German", "ID": "5614"}, {"sentence": "In recent research in the field, the main effort has been to infer semantic classes for verbs, in English (Stevenson et al, 1999) and DE (Schulte im Walde and Brew, 2002).", "acronym": "DE", "label": "German", "ID": "5615"}, {"sentence": "156  Coling 2010: Poster Volume, pages 9?17, Beijing, August 2010 DE Based on WordNet for Robust IR Eneko Agirre IXA NLP Group Univ.", "acronym": "DE", "label": "Document Expansion", "ID": "5616"}, {"sentence": "c?2006 Association for Computational Linguistics Language Model Information Retrieval with DE Tao Tao, Xuanhui Wang, Qiaozhu Mei, ChengXiang Zhai Department of Computer Science University of Illinois at Urbana Champaign Abstract Language model information retrieval de- pends on accurate estimation of document models.", "acronym": "DE", "label": "Document Expansion", "ID": "5617"}, {"sentence": "This pa- per presents a novel DE method based on a WordNet-based system to find related concepts and words.", "acronym": "DE", "label": "Document Expansion", "ID": "5618"}, {"sentence": "2 DE Using WordNet Our key insight is to expand the document with related words according to the background infor- mation in WordNet (Fellbaum, 1998), which pro- vides generic information about general vocabu- lary terms.", "acronym": "DE", "label": "Document Expansion", "ID": "5619"}, {"sentence": "2 DE Retrieval Model 2.1 The KL-divergence retrieval model We first briefly review the KL-divergence retrieval model, on which we will develop the document expansion technique.", "acronym": "DE", "label": "Document Expansion", "ID": "5620"}, {"sentence": "2.2 SemEval-2007 Task 1: CLIR Using IR metrics, this disambiguation scheme was evaluated against another competing platform and an algorithm provided by the Task 1 (Agirre et al, Topics All Nouns 1 .393 .467 5 .397 .478 25 .387 .456 200 .359 .420 Table 1: Accuracy on disambiguating words in Sem- Cor Task PUTOP Topic Expansion 0.30 DE 0.15 English Translation 0.17 SensEval 2 0.39 SensEval 3 0.33 Table 2: Performance results on Task 1 2007) organizers.", "acronym": "DE", "label": "Document Expansion", "ID": "5621"}, {"sentence": "August 2010 DE Based on WordNet for Robust IR Eneko Agirre IXA NLP Group Univ.", "acronym": "DE", "label": "Document Expansion", "ID": "5622"}, {"sentence": "The mo- tivation for this is that the closer two tokens occur together, the more likely it is that their relatedness is not acciDE.", "acronym": "DE", "label": "dental", "ID": "5623"}, {"sentence": "3 Tree Search vs. Dynamic  P rogramming  Once an appropriate function for measuring simi-  larity between pairs of segments has been designed,  290  Feature Phonological Numerical  name term value  Place  Manner  High  Back  \\[bilabial\\]  \\[labioDE\\]  \\[DE\\]  \\[alveolar\\]  \\[retroflex\\]  \\[palato-alveolar\\]  \\[palatal\\]  \\[velar\\]  \\[uvular\\]  \\[pharyngeal\\]  \\[glottal\\]  \\[stop\\]  \\[affricate\\]  \\[fricative\\]  \\[approximant\\]  \\[high vowel\\]  \\[mid vowel\\]  \\[low vowel\\]  \\[high\\]  \\[mid\\]  \\[low\\]  \\[front\\]  \\[central\\]  \\[back\\]  1.0  0.95  0.9  0.85  0.8  0.75  0.7  0.6  0.5  0.3  0.1  1.0  0.9  0.8  0.6  0.4  0.2  0.0  1.0  0.5  0.0  1.", "acronym": "DE", "label": "dental", "ID": "5624"}, {"sentence": "InciDEly, Covington's penalties for indels can be  expressed by an affine gap function with r -- 10 and  s= 40.", "acronym": "DE", "label": "dental", "ID": "5625"}, {"sentence": "For ex- ample, the distance between two tokens within a paragraph probably has not such a large effect on whether their relatedness score is reliable or ac- ciDE.", "acronym": "DE", "label": "dental", "ID": "5626"}, {"sentence": "The fact  that Covington's distance function is not a metric is  not an acciDE oversight; rather, it reflects certain  inherent characteristics of phones.", "acronym": "DE", "label": "dental", "ID": "5627"}, {"sentence": "3 Tree Search vs. Dynamic  P rogramming  Once an appropriate function for measuring simi-  larity between pairs of segments has been designed,  290  Feature Phonological Numerical  name term value  Place  Manner  High  Back  \\[bilabial\\]  \\[labioDE\\]  \\[DE\\]  \\[alveolar\\]  \\[retroflex\\]  \\[palato-alveolar\\]  \\[palatal\\]  \\[velar\\]  \\[uvular\\]  \\[pharyngeal\\]  \\[glottal\\]  \\[stop\\]  \\[affricate\\]  \\[fricative\\]  \\[approximant\\]  \\[high vowel\\]  \\[mid vowel\\]  \\[low vowel\\]  \\[high\\]  \\[mid\\]  \\[low\\]  \\[front\\]  \\[central\\]  \\[back\\]  1.0  0.95  0.9  0.85  0.8  0.75  0.7  0.6  0.5  0.3  0.1  1.0  0.9  0.8  0.6  0.4  0.2  0.0  1.0", "acronym": "DE", "label": "dental", "ID": "5628"}, {"sentence": "TTE range (h) FIN NFI FIN+NFI 0 2.58 3.07 8.51 1?4 2.38 2.64 8.71 5?8 3.02 3.08 8.94 9?12 5.20 5.47 6.57 13?24 5.63 5.54 6.09 25?48 13.14 15.59 5.81 49?96 17.20 20.72 6.93 97?144 30.38 41.18 6.97 > 144 55.45 70.08 9.41 Table 3: MAE for the FIN, NFI, and FIN+NFI systems in different TTE ranges.", "acronym": "MAE", "label": "Mean Absolute Error", "ID": "5629"}, {"sentence": "The model thus learned is evaluated using: (a) Error metrics namely, Mean Squared Error estimate, MAE esti- mate and Mean Percentage Error. (", "acronym": "MAE", "label": "Mean Absolute Error", "ID": "5630"}, {"sentence": "18.72 18.79 18.84 20.20 20.20 20.27 20.27 Baseline Mean 27.29 27.29 27.31 27.31 25.49 25.50 25.53 25.55 26.61 26.60 26.63 26.62 Training Median 10.38 10.28 7.68 7.62 11.09 11.04 8.65 8.50 10.61 10.54 8.03 7.99 Training Mean 11.62 11.12 8.73 8.29 12.43 11.99 9.53 9.16 11.95 11.50 9.16 8.76 Coverage 31,221 31,723 32.240 32,740 18,848 19,176 19,734 20,061 52,186 52,919 53,887 54,617 Table 2: Overall MAE for each method: difference in hours between the estimated time to event and the actual time to event, computed separately for the FIN and NFI subsets, and for the combination.", "acronym": "MAE", "label": "Mean Absolute Error", "ID": "5631"}, {"sentence": "332 Table 3: MAEs for the NBER download predictions. ???", "acronym": "MAE", "label": "Mean absolute error", "ID": "5632"}, {"sentence": "The collection was processed as a stream, sentence by sentence, using bigram fea- 233 d 16 32 64 128 256 SLSH 0.2885 0.2112 0.1486 0.1081 0.0769 LSH 0.2892 0.2095 0.1506 0.1083 0.0755 Table 1: MAE when using signatures gener- ated online (StreamingLSH), compared to offline (LSH).", "acronym": "MAE", "label": "Mean absolute error", "ID": "5633"}, {"sentence": "261.6627 (2) Mean AHT 675.74 seconds Median AHT 543 seconds Mode AHT 366 seconds Standard Deviation 487.72 seconds Correlation coefficient 0.3822 MAE 320.2 seconds Root mean squared error 450.64 seconds Total Number of Instances 6175 Table 2: Data statistics and the goodness of the regression model for 6175 AHT data points.", "acronym": "MAE", "label": "Mean absolute error", "ID": "5634"}, {"sentence": "We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers, which still results in high WERs (40%) and show that even from these noisy transcrip- tions of calls we can automatically build a domain model.", "acronym": "WER", "label": "word error rate", "ID": "5635"}, {"sentence": "A neat solution to poor sentence-level evaluation proposed by Kulesza and Shieber (2004) is to use a Support Vector Machine, using features such as WER, to estimate sentence-level translation quality.", "acronym": "WER", "label": "word error rate", "ID": "5636"}, {"sentence": "Figure 1 shows  the document structure in CLSR test collection, two ASR  transcripts are available for this data, in this work we use  the ASRTEXT2004A field provided by IBM research with  a WER of 38%.", "acronym": "WER", "label": "word error rate", "ID": "5637"}, {"sentence": "Current automatic speech recognition technology for telephone calls have moderate to high WERs (Padmanabhan et al, 2002).", "acronym": "WER", "label": "word error rate", "ID": "5638"}, {"sentence": "The ASR module, based on a hybrid speech recognition system that combines Hidden Markov Models with Multi-layer Perceptrons, with an av- erage WER of 24% (Amaral et al, 2007), greatly influences the performance of the subse- quent modules.", "acronym": "WER", "label": "word error rate", "ID": "5639"}, {"sentence": "2 Background Real-time correction must be done within difficult constraints : with typical captioning rates of 130 words per minute, and 5 to 10% WER, the user must correct between 6 and 13 errors per minute.", "acronym": "WER", "label": "word error rate", "ID": "5640"}, {"sentence": "With one-pass prediction we decide on the prediction for each WERly of other deci- sions.", "acronym": "WER", "label": "position independent", "ID": "5641"}, {"sentence": "We also show that this also holds for uni- gram BLEU and the WER error rate (PER) on a slightly augmented variant of CNs which allows for edges to carry multiple symbols.", "acronym": "WER", "label": "position independent", "ID": "5642"}, {"sentence": "The central fixa- tion bias in scene viewing: Selecting an optimal viewing WERly of motor biases and image feature distributions.", "acronym": "WER", "label": "position independent", "ID": "5643"}, {"sentence": "reference errors hypothesis errors Mister#N Mrs#N be#V is#V can#V Table 3: PER errors: actual words which are partic- ipating in the WER word error rate and their corresponding POS classes An illustration of PER errors is given in Table 3.", "acronym": "WER", "label": "position independent", "ID": "5644"}, {"sentence": "The WER n-best list word agree- ment is the average count of n-grams that contain the word e. It is computed as: NA k (e i ) = 1 N ng N ng ?", "acronym": "WER", "label": "position independent", "ID": "5645"}, {"sentence": "The acoustic models are sets of context-dependent(CD),  WER phone models, which include both  intra-word and cross-word contexts.", "acronym": "WER", "label": "position independent", "ID": "5646"}, {"sentence": "Nonterminals 2,284 1,837 Node Array Size 224KB 204KB WER 25.05% 11.91% Recognition Time 13.8xRT 1.7xRT Ambiguity 15.4 1.9 Table 2: Comparison Results in Section 4, we believe that the amount of am- biguity can be a significant factor in recognition performance.", "acronym": "WER", "label": "Word Error Rate", "ID": "5647"}, {"sentence": "0 5 10 15 20 25 30 35 40 45WER600000 800000 1000000 1200000 1400000 1600000 1800000 2000000 2200000 Tok ens tc.10000tc.20000tc.50000tfcf.5tnpd.1 (b) The number of word tokens remaining after pre-processing.", "acronym": "WER", "label": "Word Error Rate", "ID": "5648"}, {"sentence": "Both  Word Full Simple  Backoff Context Content  POS Errors - 1573 1718  POS Error Rate - 2.69 2.94  Word Perplexity 24.8 22.6 42.4  WER 26.0 24.9 28.9  Sentence Error Rate \\] 56.6 55.2 58.1  Table 1: Comparison with Word-Based Model  models were restricted to only looking at the  previous two words (and POS tags) in the con-  text, and hence are trigram models.", "acronym": "WER", "label": "Word Error Rate", "ID": "5649"}, {"sentence": "Figures 3(a) and 3(b) show how the various pre-processing methods affect word type and token 244 0 5 10 15 20 25 30 35 40 45WER0 10000 20000 30000 40000 50000 60000 Typ es tc.10000tc.20000tc.50000tfcf.5tnpd.1 (a) The number of word types remaining after pre-processing.", "acronym": "WER", "label": "Word Error Rate", "ID": "5650"}, {"sentence": "However, 104 we think the observations about the content features used in this paper were not reliable for the following two reasons: the number of training responses was limited (1000 responses), and the ASR system had a relatively high WER (39%).", "acronym": "WER", "label": "Word Error Rate", "ID": "5651"}, {"sentence": "First, our  Word Perplexity  WER  Sentence Error Rate  I Back?ffl Decision Tree I  Word Word Class POS  Table 2: POS, Class and Word-Based Models  word-based ecision tree model outperforms  the word backoff model, giving an absolute  word-error ate reduction of 0.5%, which was  found significant by the Wilcoxon test (Z-score  -3.26).", "acronym": "WER", "label": "Word Error Rate", "ID": "5652"}, {"sentence": "We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers, which still results in high WERes (40%) and show that even from these noisy transcrip- tions of calls we can automatically build a domain model.", "acronym": "WER", "label": "word error rat", "ID": "5653"}, {"sentence": "A neat solution to poor sentence-level evaluation proposed by Kulesza and Shieber (2004) is to use a Support Vector Machine, using features such as WERe, to estimate sentence-level translation quality.", "acronym": "WER", "label": "word error rat", "ID": "5654"}, {"sentence": "Figure 1 shows  the document structure in CLSR test collection, two ASR  transcripts are available for this data, in this work we use  the ASRTEXT2004A field provided by IBM research with  a WERe of 38%.", "acronym": "WER", "label": "word error rat", "ID": "5655"}, {"sentence": "Current automatic speech recognition technology for telephone calls have moderate to high WERes (Padmanabhan et al, 2002).", "acronym": "WER", "label": "word error rat", "ID": "5656"}, {"sentence": "The ASR module, based on a hybrid speech recognition system that combines Hidden Markov Models with Multi-layer Perceptrons, with an av- erage WERe of 24% (Amaral et al, 2007), greatly influences the performance of the subse- quent modules.", "acronym": "WER", "label": "word error rat", "ID": "5657"}, {"sentence": "2 Background Real-time correction must be done within difficult constraints : with typical captioning rates of 130 words per minute, and 5 to 10% WERe, the user must correct between 6 and 13 errors per minute.", "acronym": "WER", "label": "word error rat", "ID": "5658"}, {"sentence": "3.4.3 GMM Features Bin features may be generalized to multi- dimensional kernels by using a Gaussian smoothing window instead of a rectangular window.", "acronym": "GMM", "label": "Gaussian Mixture Model", "ID": "5659"}, {"sentence": "P (F (D, t, j)|D)P (D) where P (F (D, t, j)|D) is the value of the PDF describing D calculated in the point F (D, t, j), P (F (D, t, j)|D) is the value of the PDF describ- ing D, P (D) is the area of the distribution describ- ing D and P (D) is the area of the distribution for D. In order to estimate the parameters describing the PDF of D and D the Expectation Maximization (EM) algorithm for the GMM (Redner and Walker, 1984) is exploited.", "acronym": "GMM", "label": "Gaussian Mixture Model", "ID": "5660"}, {"sentence": "A GMM was trained us- ing an Expectation Maximization method with the classification of instances performed by choosing the category which maximises the probability of fitting either of the Gaussian components.", "acronym": "GMM", "label": "Gaussian Mixture Model", "ID": "5661"}, {"sentence": "It is difficult to select the erroneous utterances to be rejected by using a classifier that 89 distinguishes speech from noise on the basis of the GMM (Lee et al, 2004); such disfluencies and resulting utterance fragments are parts of human speech.", "acronym": "GMM", "label": "Gaussian Mixture Model", "ID": "5662"}, {"sentence": "c?2009 Association for Computational Linguistics On Semi-Supervised Learning of GMMs for Phonetic Classification?", "acronym": "GMM", "label": "Gaussian Mixture Model", "ID": "5663"}, {"sentence": "e of domain experts, who can devise pedagogically valuable reading lists that order doc- Automatic Speech Recognition (ASR) with HMMs Noisy Channel Model Viterbi Decoding for ASR Training ASR Parameters Viterbi Algorithm Dynamic ProgrammingDecoding/Search ProblemHMMs Markov Chains HMM Pronunciation Lexicon  Iterative Parameter Estimation with EM Gaussian Acoustic Model Discrete Fourier Transforms GMMs Phonemes N-gram Language Model Figure 1: A human-authored concept graph excerpt, showing possible concepts related to automatic speech recognition and their concept dependencies.", "acronym": "GMM", "label": "Gaussian Mixture Model", "ID": "5664"}, {"sentence": "In this work we combined textual and prosodic features, using GMMs for the extracted and non- extracted classes.", "acronym": "GMM", "label": "Gaussian mixture model", "ID": "5665"}, {"sentence": "2012), where training data for neural network model is generated by forced decoding with tradi- tional GMMs.", "acronym": "GMM", "label": "Gaussian mixture model", "ID": "5666"}, {"sentence": "We built GMMs over the locations, with 3, 5, 8, 12, 17, and 23 components.", "acronym": "GMM", "label": "Gaussian mixture model", "ID": "5667"}, {"sentence": "Variational Bayes for d-dimensional GMMs.", "acronym": "GMM", "label": "Gaussian mixture model", "ID": "5668"}, {"sentence": "In the current system, we quantize the mean and variance of each GMM dimension to 5 and 3 bits, respectively.", "acronym": "GMM", "label": "Gaussian mixture model", "ID": "5669"}, {"sentence": "We  assume a GMM for the q event vectors V1, V2, ?,", "acronym": "GMM", "label": "Gaussian mixture model", "ID": "5670"}, {"sentence": "Slavonic, Old Czech, Russian, Ukranian, Serbian, Middle  Bulgarian (Slavonic group)  Albanian  Lithuanian (old and modern), Old Prussian, Lettish (Baltic  group)  Old English, Middle English, Danish, Old Icelandic, Old Gut-  niac, Dialectal Norwegian, Swedish, Old High German, Middle  High German, German (modern), Longobard, Gothic, Burgun-  dian (Germanic group)  Ligurian  Oscan, Umbrian  Latin, VL, Medieval Latin, Italian, Dialectal Italian,  Old French, Catalan, Old Spanish, Spanish (Latin and neo-La-  tin group)  Breton, Middle Breton, Old Breton, Cymric, Middle Cymric,  Old Cymric, Old Irish, Modern Irish, Ogamic, Scottish, Gaulish,  Galatian, Ladn-Gaulish, Latin-British, Vam~elais (Celtic group)  Finnish, Vogulian  The reader will notice that not all Indo-European languages", "acronym": "VL", "label": "Vulgar Latin", "ID": "5671"}, {"sentence": "Note that the marginal probability of a specific Italian word con- ditioned on its VL parent is the sum over all possible derivations that generate it.", "acronym": "VL", "label": "Vulgar Latin", "ID": "5672"}, {"sentence": "It is indeed likely that these were generally eliminated in VL.", "acronym": "VL", "label": "Vulgar Latin", "ID": "5673"}, {"sentence": "For example, documents such as the Appendix Probi (Baehrens, 1922) provide in- dications of orthographic confusions which resulted from the growing gap between Classical Latin and VL phonology around the 3rd and 4th cen- turies AD.", "acronym": "VL", "label": "Vulgar Latin", "ID": "5674"}, {"sentence": "A phone that has two dif-  ferent places of articulation, such as labio-VL \\[w\\],  can be close to two phones that are distant from each  other, such as labial \\[b\\] and VL \\[g\\].", "acronym": "VL", "label": "velar", "ID": "5675"}, {"sentence": "The estimation of the threshold is based on the distance, measured in the phonetic Feature Values Type vowel, consonant Vowel length short, long, diphthong, schwa Vowel height high, mid, low Vowel frontness front mid back Lip rounding yes, no Consonant type stop, fricative, affricative, nasal, liquid Place of articulation labial, alveolar, palatal, labio-dental, dental, VL Consonant voicing yes, no Table 1: Phone features.", "acronym": "VL", "label": "velar", "ID": "5676"}, {"sentence": "3 Tree Search vs. Dynamic  P rogramming  Once an appropriate function for measuring simi-  larity between pairs of segments has been designed,  290  Feature Phonological Numerical  name term value  Place  Manner  High  Back  \\[bilabial\\]  \\[labiodental\\]  \\[dental\\]  \\[alveolar\\]  \\[retroflex\\]  \\[palato-alveolar\\]  \\[palatal\\]  \\[VL\\]  \\[uvular\\]  \\[pharyngeal\\]  \\[glottal\\]  \\[stop\\]  \\[affricate\\]  \\[fricative\\]  \\[approximant\\]  \\[high vowel\\]  \\[mid vowel\\]  \\[low vowel\\]  \\[high\\]  \\[mid\\]  \\[low\\]  \\[front\\]  \\[central\\]  \\[back\\]  1.0  0.95  0.9  0.85  0.8  0.75  0.7  0.6  0.5  0.3  0.1  1.0  0.9  0.8  0.6  0.4  0.2  0.0  1.0  0.5  0.0  1.0  0.5  0.0  Table 3: Multivalued features and their values.", "acronym": "VL", "label": "velar", "ID": "5677"}, {"sentence": "the word pairs chosen in this study have connections with both ethnic and regional di- alects: consonant cluster reduction is a feature of African-American English (Green, 2002) and Te- jano and Chicano English (Bayley, 1994; Santa Ana, 1991); th-stopping (as in wit/with) is a feature of African-American English (Green, 2002) as well as several regional dialects (Gordon, 2004; Thomas, 2004); the VL nasal in doin and goin is a property of informal speech.", "acronym": "VL", "label": "velar", "ID": "5678"}, {"sentence": "the re- placement of the VL nasal with the coronal nasal, which has been associated with informal speech in many parts of the English-speaking world.1 The final word pair know/kno does not differ in pronunciation, and is included as a control.", "acronym": "VL", "label": "velar", "ID": "5679"}, {"sentence": "Multimodal interfaces also stand to play a critical  role in the ongoing migration of interaction onto  wireless PI, such as  PDAs and next generation phones, which have  limited screen real estate and no keyboard.", "acronym": "PI", "label": "portable computing devices", "ID": "5680"}, {"sentence": "3.5 ILP-based Post Inference The final semantic role labeling result is gener- ated through an ILP (Integer Linear Programming) based PI method.", "acronym": "PI", "label": "post inference", "ID": "5681"}, {"sentence": "lp solve 5.5 3 is chosen as our ILP problem solver during the PI stage.", "acronym": "PI", "label": "post inference", "ID": "5682"}, {"sentence": "Be- sides adding a predicate identification and a classification stages, our semantic de- pendency parsing simplifies the traditional four stages semantic role labeling into two: a maximum entropy based argument clas- sification and an ILP-based PI.", "acronym": "PI", "label": "post inference", "ID": "5683"}, {"sentence": "Semantic role labeling is achieved us- ing maximum entropy (MaxEnt) model based semantic role classification and integer linear programming (ILP) based PI.", "acronym": "PI", "label": "post inference", "ID": "5684"}, {"sentence": "Some other works paid much attention to the robust SRL (Pradhan et al, 2005b) and PI (Pun- yakanok et al, 2004).", "acronym": "PI", "label": "post inference", "ID": "5685"}, {"sentence": "A simple PI strategy is given for comparison, where the most possible label (including the virtual label ?", "acronym": "PI", "label": "post inference", "ID": "5686"}, {"sentence": "IT WORDNET  has been coupled with a parser and a number of  experiments have been performed to individu-  ate the methodology with the best trade-off b", "acronym": "IT", "label": "Italian", "ID": "5687"}, {"sentence": "it  Abst rac t   We present aprototype of the IT version of  WORDNET, a general computational lexical re-  source.", "acronym": "IT", "label": "Italian", "ID": "5688"}, {"sentence": "Lexical Discrimination with the IT Version of WORDNET  Alessandro  Ar ta le ,  Bernardo  Magn in i  and  Car lo  S t rapparava   IRST, 1-38050 Povo TN, Italy  e-mail: {artalelmagninilstrappa}@irst.", "acronym": "IT", "label": "Italian", "ID": "5689"}, {"sentence": "The ability to drop argu- ments is not correlated with agreement or case features in Urdu, as has been postulated for IT, for example.", "acronym": "IT", "label": "Italian", "ID": "5690"}, {"sentence": "Translating IT connectives into IT Sign Lan- guage.", "acronym": "IT", "label": "Italian", "ID": "5691"}, {"sentence": "Collins bilingual dictionaries for English/IT,  English/French, English/Spanisla, and  English/German  5.", "acronym": "IT", "label": "Italian", "ID": "5692"}, {"sentence": "We prepared fifty seed terms in total: ten terms for each of five genres; natural language processing, Japanese language, IT, current topics, and persons in Japanese history.", "acronym": "IT", "label": "information technology", "ID": "5693"}, {"sentence": "is in Table 1; a Table 2: Experimental Result Evaluation I Evaluation II domain correct incorrect total S F A C R total natural language processing 101 (93%) 8 ( 7%) 109 6 3 14 11 8 43 Japanese language 71 (81%) 17(19%) 88 7 0 19 5 1 32 IT 113 (88%) 15 (12%) 128 10 5 27 13 0 55 current topics 106 (91%) 10 ( 9%) 116 2 0 13 19 5 39 persons in Japanese history 128 (76%) 41 (24%) 169 18 0 23 1 0 42 Total 519 (85%) 91(15%) 610 43 8 96 49 14 210 check mark ?", "acronym": "IT", "label": "information technology", "ID": "5694"}, {"sentence": "For example, if a text discussed inventions in IT, there could be groups of a few discourse segments each talking about inventions by specific companies.", "acronym": "IT", "label": "information technology", "ID": "5695"}, {"sentence": "2 Assistance in Accessing of Information  The assistive technologies played an important role  in the olden days and even today with emerging  IT it does play a significant  role.", "acronym": "IT", "label": "information technology", "ID": "5696"}, {"sentence": "Telephone-linked care for physical ac- tivity: a qualitative evaluation of the use patterns of an IT program for patients.", "acronym": "IT", "label": "information technology", "ID": "5697"}, {"sentence": "In to- tal, we created 50 Web datasets on the topics such as find a good kindergarten, purchase a used car, plan a trip to DC, how to make a cake, find a good wed- ding videographer, write a survey paper for health care systems, find the best deals for a Mother?s day gift, write a survey paper for social network, write a survey paper for EU?s finance, and write a survey paper for IT.", "acronym": "IT", "label": "information technology", "ID": "5698"}, {"sentence": "This research was supported by the IT Center through their grant to the first author.", "acronym": "IT", "label": "Information Technology", "ID": "5699"}, {"sentence": "In International Conference on Computer Science and IT, pages 277?281, Los Alamitos, CA, USA.", "acronym": "IT", "label": "Information Technology", "ID": "5700"}, {"sentence": "c?2006 Association for Computational Linguistics Matching Syntactic-Semantic Graphs for Semantic Relation Assignment Vivi Nastase1 and Stan Szpakowicz1,2 1 School of IT and Engineering, University of Ottawa, Ottawa, Canada 2 Institute of Computer Science, Polish Academy of Sciences, Warsaw, Poland {vnastase,szpak}@site.uottawa.ca Abstract We present a graph-matching algorithm for semantic relation assignment.", "acronym": "IT", "label": "Information Technology", "ID": "5701"}, {"sentence": "c?2010 Association for Computational Linguistics Features for Detecting Hedge Cues Nobuyuki Shimizu IT Center The University of Tokyo shimizu@r.dl.itc.u-tokyo.ac.jp Hiroshi Nakagawa IT Center The University of Tokyo n3@dl.itc.u-tokyo.ac.jp Abstract We present a sequential labeling approach to hedge cue detection submitted to the bi- ological portion of task 1 for the CoNLL- 2010 shared task.", "acronym": "IT", "label": "Information Technology", "ID": "5702"}, {"sentence": "c?2007 Association for Computational Linguistics Near-Synonym Choice in an Intelligent Thesaurus Diana Inkpen School of IT and Engineering, University of Ottawa 800 King Edward, Ottawa, ON, Canada, K1N 6N5 diana@site.uottawa.ca Abstract An intelligent thesaurus assists a writer with alternative choices of words and or- ders them by their suitability in the writ- ing context.", "acronym": "IT", "label": "Information Technology", "ID": "5703"}, {"sentence": "Transliteration Equivalence using CCA.", "acronym": "CCA", "label": "Canonical Correlation Analysis", "ID": "5704"}, {"sentence": "Briefly, given a multi-view data set, CCA is a tech- nique to find the projection directions in each view so that the objects when projected along these di- 408 rections are maximally aligned (Hotelling, 1936).", "acronym": "CCA", "label": "Canonical Correlation Analysis", "ID": "5705"}, {"sentence": "A nice geometric interpretation of these processes is proposed in (Gaussier et al, 2004), which fur- thermore introduces variants based on Fisher ker- nels, CCA and a com- bination of them, leading to an improvement of the F1-score of 2% (from 0.14 to 0.16) when con- sidering the top 20 candidates.", "acronym": "CCA", "label": "Canonical Correlation Analysis", "ID": "5706"}, {"sentence": "We formulate the 1257 problem of learning hash functions as an opt- mization problem whose relaxation can be solved using CCA.", "acronym": "CCA", "label": "Canonical Correlation Analysis", "ID": "5707"}, {"sentence": "CCA: An Overview with Application to Learning Methods.", "acronym": "CCA", "label": "Canonical Correlation Analysis", "ID": "5708"}, {"sentence": "Some of such embeddings are based on CCA (Hardoon et al, 2004) e.g. (Gong et al, 2014; Klein et al, 2015; Plummer et al, 2015), linear models with ranking loss (Frome et al, 2013; Karpathy and Fei-Fei, 2015; Socher et al, 2014; Weston et al, 2011) or non-linear deep learning models (Kiros et al, 2014; Mao et al, 2015; Ngiam et al, 2011).", "acronym": "CCA", "label": "Canonical Correlation Analysis", "ID": "5709"}, {"sentence": "Word embed- dings can be learned from large-scale unlabeled texts through context-predicting models (e.g., neu- ral network language models) or spectral methods (e.g., CCA) in an unsu- pervised setting.", "acronym": "CCA", "label": "canonical correlation analysis", "ID": "5710"}, {"sentence": "Gener- alized CCA with missing val- ues.", "acronym": "CCA", "label": "canonical correlation analysis", "ID": "5711"}, {"sentence": "Multi- view regression via CCA.", "acronym": "CCA", "label": "canonical correlation analysis", "ID": "5712"}, {"sentence": "We show that it is important to scale features by their inverse variance, in a manner that is closely related to methods used in CCA.", "acronym": "CCA", "label": "canonical correlation analysis", "ID": "5713"}, {"sentence": "A bilingual auto-encoder was proposed by Chandar et al (2014), while Faruqui and Dyer (2014) applied CCA to parallel data to improve monolingual embeddings.", "acronym": "CCA", "label": "canonical correlation analysis", "ID": "5714"}, {"sentence": "Generalized CCA of matrices with missing rows: a simulation study.", "acronym": "CCA", "label": "canonical correlation analysis", "ID": "5715"}, {"sentence": "recognizers are represented t n  ATNk [ I l l  f o m ,   are q u i t s  s i m p l e ,  and are not described further i n  t h i s  paper .", "acronym": "ATN", "label": "Augmented Transition Networ", "ID": "5716"}, {"sentence": "American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 15  Stuart  C. Shapiro Generalized ATNk Grammars  (A  (BE  (DOG  (IS  LUCY  SAW  SAWI   SEE   SEEN  SWEET  (WAS  (YOUNG  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  DET  v))  N))  v)  NPR  N)  V)  N)  V)  V)  im J   v)  ADJ   ))  ROOT .", "acronym": "ATN", "label": "Augmented Transition Networ", "ID": "5717"}, {"sentence": "Kaplan, R. 1973 ATNks as Psychological  Models of Sentence Comprehension.", "acronym": "ATN", "label": "Augmented Transition Networ", "ID": "5718"}, {"sentence": "Pereira, Fernando C. N.; and Warren, David H. D. 1980 Definite  Clause Grammars for Language Analysis--a Survey of the For-  malism and a Comparison with ATNks.", "acronym": "ATN", "label": "Augmented Transition Networ", "ID": "5719"}, {"sentence": "Definite Clause Grammars for Language Analysis  - A Survey of the Formalism and a Comparison  with ATNks.", "acronym": "ATN", "label": "Augmented Transition Networ", "ID": "5720"}, {"sentence": "A Survey of the Formalism and a Comparison  with ATNks.", "acronym": "ATN", "label": "Augmented Transition Networ", "ID": "5721"}, {"sentence": "recognizers are represented t n  ATN [ I l l  f o m ,   are q u i t s  s i m p l e ,  and are not described further i n  t h i s  paper .", "acronym": "ATN", "label": "Augmented Transition Network", "ID": "5722"}, {"sentence": "American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 15  Stuart  C. Shapiro Generalized ATN Grammars  (A  (BE  (DOG  (IS  LUCY  SAW  SAWI   SEE   SEEN  SWEET  (WAS  (YOUNG  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  CTGY  DET  v))  N))  v)  NPR  N)  V)  N)  V)  V)  im J   v)  ADJ   ))  ROOT .", "acronym": "ATN", "label": "Augmented Transition Network", "ID": "5723"}, {"sentence": "Kaplan, R. 1973 ATNs as Psychological  Models of Sentence Comprehension.", "acronym": "ATN", "label": "Augmented Transition Network", "ID": "5724"}, {"sentence": "Pereira, Fernando C. N.; and Warren, David H. D. 1980 Definite  Clause Grammars for Language Analysis--a Survey of the For-  malism and a Comparison with ATNs.", "acronym": "ATN", "label": "Augmented Transition Network", "ID": "5725"}, {"sentence": "Definite Clause Grammars for Language Analysis  - A Survey of the Formalism and a Comparison  with ATNs.", "acronym": "ATN", "label": "Augmented Transition Network", "ID": "5726"}, {"sentence": "A Survey of the Formalism and a Comparison  with ATNs.", "acronym": "ATN", "label": "Augmented Transition Network", "ID": "5727"}, {"sentence": "6- F.C. PEREIRA & D.H.D. WARREN, Definite clause grammars for language analysis -  a survey of the formalism and comparison with ATN,  Artif icial Intell igence, 13(3), 1980.", "acronym": "ATN", "label": "augmented transition networks", "ID": "5728"}, {"sentence": "Pereira, F.C.N. and Warren, D.H.D. 1980 Definite clause gram-  mars for language analysis - a survey of the formalism and a  comparison with ATN.", "acronym": "ATN", "label": "augmented transition networks", "ID": "5729"}, {"sentence": ",by Woods in  the framework of ATN (Woods 1973).", "acronym": "ATN", "label": "augmented transition networks", "ID": "5730"}, {"sentence": "ars, then they went over to transformational gram-  mars, and then some of them started using augmented phrase structure grammars again, (space  for moral~. Whilst we are in this careful scholarly mode, let us do the same service for computa-  tional linguistics: once upon a time computational linguists (i.e. builders of parsers) used aug-  mented phrase structure grammars, then they went over to ATN, and  then many of them started using augmented phrase structure grammars again, (space for  moral~. There are people who would have you believe in one or other of these stories (e.g.  Chomsky 1983, p65, for the first).", "acronym": "ATN", "label": "augmented transition networks", "ID": "5731"}, {"sentence": "In an early paper by  Woods (1970), alternative algorithms that can be  used with ATN are dis-  cussed, including the bottom-up and Earley algor-  ithms.", "acronym": "ATN", "label": "augmented transition networks", "ID": "5732"}, {"sentence": "It is important o note, however, that the  concept of ATN is a par-  ticular way to represent linguistic knowledge; it does  not require that the program using the networks  operate in top-down fashion.", "acronym": "ATN", "label": "augmented transition networks", "ID": "5733"}, {"sentence": "Testing GC can  be done by using a test suite (cp. (", "acronym": "GC", "label": "grammatical coverage", "ID": "5734"}, {"sentence": "We performed a number of post-hoc analyses to estimate the relative weight on fragmentation of variou s linguistic factors for which the MUC-4 version of ALEMBIC had incomplete GC .", "acronym": "GC", "label": "grammatical coverage", "ID": "5735"}, {"sentence": "Five hundred sen-  tences, of up to ten words in length, falling  within CLARE's current core lexical (1600  root forms) and GC were  taken at random from the LOB corpus.", "acronym": "GC", "label": "grammatical coverage", "ID": "5736"}, {"sentence": "It must be complemented with  lexical depth and GC.", "acronym": "GC", "label": "grammatical coverage", "ID": "5737"}, {"sentence": "But translation quality rests on the  linguistic competence of the MT system which again  is based first and foremost on GC  and lexicon size.", "acronym": "GC", "label": "grammatical coverage", "ID": "5738"}, {"sentence": "This results in high precision (96.7%), but re- call is low (82.3%) due to parse failures caused by lack of GC 2.", "acronym": "GC", "label": "grammatical coverage", "ID": "5739"}, {"sentence": "Combining the generative syntactic model and composite language model (GEN) with equal weight yielded a devtest BLEU score of only 0.4513, while discriminatively train- ing the GC models (GLOBAL) increased the score to 0.7679.", "acronym": "GC", "label": "generative component", "ID": "5740"}, {"sentence": "and Moulines, 2007) and incremental EM (Neal and Hinton, 1998) which they use to update the align- ment models (the GC of SMT) on the fly.", "acronym": "GC", "label": "generative component", "ID": "5741"}, {"sentence": "The last few years have seen most work in language  processing  devoted to the  development of  i n t e g r a t e d  s y s  terns, combining  syntactic, semantic, pragmatic,  and GCs.", "acronym": "GC", "label": "generative component", "ID": "5742"}, {"sentence": "A second step consisted in simplifying the  GC by reducing the rules in favour  of well-formedness conditions, o-called filters.", "acronym": "GC", "label": "generative component", "ID": "5743"}, {"sentence": "1 In t roduct ion   1.1 Historic Origin  Early transformational grammar consisted of a  rather complex GC and an equally  complex and equally imperspicuous transformational  component.", "acronym": "GC", "label": "generative component", "ID": "5744"}, {"sentence": "Generative and  analytical components can be derived from the gram-  mars: the analytical component maps a sentence of the  source language into one or more semantic D-trees;  the GC maps a semantic D-tree into  one or more sentences of the target language.", "acronym": "GC", "label": "generative component", "ID": "5745"}, {"sentence": "ing of SLs,  notably that of the dictionary itself.", "acronym": "SLs", "label": "sublanguages", "ID": "5746"}, {"sentence": "On a more theoretical level, the  automatic lexicon builder will add greatly  to our understanding of SLs,  notably that of the dictionary itself.", "acronym": "SLs", "label": "sublanguages", "ID": "5747"}, {"sentence": "We demonstrated with several  examples that selectional restrictions do not  generalize across SLs, and  acquiring them by hand is often inintuitive  and very time-consuming.", "acronym": "SLs", "label": "sublanguages", "ID": "5748"}, {"sentence": "The very  possibi l i ty of creating a large, general-  269  language lexicon points toward a time when  SLs will be obsolete for many of  the purposes for which they are now used;  but they will still be useful and  interesting for a long time to come, and  the automat ic  lexicon builder gives us a  new tool for analyzing them.", "acronym": "SLs", "label": "sublanguages", "ID": "5749"}, {"sentence": "Two biomedical SLs: a description based on the theories of Zellig Harris.", "acronym": "SLs", "label": "sublanguages", "ID": "5750"}, {"sentence": "The design of the lexicon builder is  inuended to be general enough to make it  useful for others building lexicons for  large natural language processing systems  involving different SLs.", "acronym": "SLs", "label": "sublanguages", "ID": "5751"}, {"sentence": "An integrated interactive nviron-  ment has been built to experiment with these ideas,  which can also be used to define SLs and  select strategies for particular applications.", "acronym": "SLs", "label": "sublanguages", "ID": "5752"}, {"sentence": "In Proceedings of the 7th Work- shop on the Representation and Processing of SLs: Corpus Mining, The 10th In- ternational Conference on Language Resources and Evaluation (LREC 2016), Portoroz, Slovenia.", "acronym": "SLs", "label": "Sign Languages", "ID": "5753"}, {"sentence": "HamNoSys Version 2.0: Hamburg  Notation System for SLs: An Introduc- tory Guide, volume 5 of International Studies on Sign  Language and Communication of the Deaf.", "acronym": "SLs", "label": "Sign Languages", "ID": "5754"}, {"sentence": "Data-Driven Machine Transla- tion for SLs.", "acronym": "SLs", "label": "Sign Languages", "ID": "5755"}, {"sentence": "In LREC Workshop on the Rep- resentation and Processing of SLs: Be- yond the Manual Channel.", "acronym": "SLs", "label": "Sign Languages", "ID": "5756"}, {"sentence": "In Proceedings of the 6thWorkshop on the Representation and Processing of SLs: Beyond the Manual Channel, The 9th International Conference on Language Resources and Evaluation (LREC 2014), Reykjavik, Iceland.", "acronym": "SLs", "label": "Sign Languages", "ID": "5757"}, {"sentence": "Hamburg Notation System for SLs - An Introductory Guide.", "acronym": "SLs", "label": "Sign Languages", "ID": "5758"}, {"sentence": "We further demonstrate how S-MART can be applied to tweet entity linking, an important and challenging task underlying many applications in- cluding product feedback (Asur and Huberman, 2010) and TDT (Math- ioudakis and Koudas, 2010).", "acronym": "TDT", "label": "topic detection and tracking", "ID": "5759"}, {"sentence": "This task is important since resolved event coreference is useful in various tasks such as TDT, information extrac- tion, question answering, textual entailment, and contradiction detection.", "acronym": "TDT", "label": "topic detection and tracking", "ID": "5760"}, {"sentence": "The well-known past experience from  IR ~ that notions of who, what, where, when, why  and how may not make a great contribution to the  TDT task (Allan and Papka,  1998) causes this fact, i.e. a topic and an event are  different from each other 1 .", "acronym": "TDT", "label": "topic detection and tracking", "ID": "5761"}, {"sentence": "2004) is centroid based multi-document sum- marizer which generates summaries using cluster  centroids produced by TDT  system.", "acronym": "TDT", "label": "topic detection and tracking", "ID": "5762"}, {"sentence": "On-line LDA: Adaptive topic models for mining text streams with applications to TDT.", "acronym": "TDT", "label": "topic detection and tracking", "ID": "5763"}, {"sentence": "1 Introduction  Our work aims to the acquisition of deep gram- matical information for nouns, because having in- formation such as countability and complementa- tion is necessary for different applications, espe- cially for deep analysis grammars, but also for  question answering, TDT,  etc.", "acronym": "TDT", "label": "topic detection and tracking", "ID": "5764"}, {"sentence": "TDT: event- based information organization.", "acronym": "TDT", "label": "Topic detection and tracking", "ID": "5765"}, {"sentence": "TDT function is to  cluster the hot events and capture the rela- tionship between the relevant events based on  the collected data from websites (event also  referred as topic in this paper).", "acronym": "TDT", "label": "Topic detection and tracking", "ID": "5766"}, {"sentence": "TDT pi- lot study: Final report.", "acronym": "TDT", "label": "Topic detection and tracking", "ID": "5767"}, {"sentence": "TDT: event-based in- formation organization Kluwer Academic Publish- ers, pages 197?224.", "acronym": "TDT", "label": "Topic detection and tracking", "ID": "5768"}, {"sentence": "TDT: event-based information organization.", "acronym": "TDT", "label": "Topic detection and tracking", "ID": "5769"}, {"sentence": "VSMs These text similarity measures project texts onto high-dimensional vec- tors which are then compared.", "acronym": "VSM", "label": "Vector Space Model", "ID": "5770"}, {"sentence": "3.1 VSMs of Semantics In this section, we describe several methods for producing the semantic vectors associated with each event head or argument; i.e., the function sem.", "acronym": "VSM", "label": "Vector Space Model", "ID": "5771"}, {"sentence": "c?2008 Association for Computational Linguistics Sentiment VSM for   Lyric-based Song Sentiment Classification      Yunqing Xia Linlin Wang  Center for Speech and language Tech.", "acronym": "VSM", "label": "Vector Space Model", "ID": "5772"}, {"sentence": "In this article we will discuss in detail sev- eral experiments of morphological cue induction for lexical classification (C?avar et al, 2004a) and (C?avar et al, 2004b) using VSMs for category induction and subsequent rule for- mation.", "acronym": "VSM", "label": "Vector Space Model", "ID": "5773"}, {"sentence": "In Proceedings of EACL 2014, Workshop on Contin- uous VSMs and their Compositional- ity (CVSC).", "acronym": "VSM", "label": "Vector Space Model", "ID": "5774"}, {"sentence": "A Systematic Study of Semantic VSM Parameters.", "acronym": "VSM", "label": "Vector Space Model", "ID": "5775"}, {"sentence": "From fre- quency to meaning: VSMs of seman- tics.", "acronym": "VSM", "label": "vector space model", "ID": "5776"}, {"sentence": "It is also done in  the VSM, so we again represent the  snippets by vectors.", "acronym": "VSM", "label": "vector space model", "ID": "5777"}, {"sentence": "Much of the work in this study is based on that by  Bagga and Baldwin (1998), where they presented a  successful cross-document coreference resolution  algorithm to resolve ambiguities between people having  the same name using the VSM.", "acronym": "VSM", "label": "vector space model", "ID": "5778"}, {"sentence": "Incremental vector space          Our intent with the incremental VSM  is to approximate the work reported by Bagga and  Baldwin (1998).", "acronym": "VSM", "label": "vector space model", "ID": "5779"}, {"sentence": "To test this, we conducted a small study in which we compared the relatedness scores obtained by NGD and the semantic VSM to the human ratings compiled by Finkelstein et al (2002).", "acronym": "VSM", "label": "vector space model", "ID": "5780"}, {"sentence": "The system then  computes the similarity of that summary with each of  the other summaries using the VSM.", "acronym": "VSM", "label": "vector space model", "ID": "5781"}, {"sentence": "VSMls These text similarity measures project texts onto high-dimensional vec- tors which are then compared.", "acronym": "VSM", "label": "Vector Space Mode", "ID": "5782"}, {"sentence": "3.1 VSMls of Semantics In this section, we describe several methods for producing the semantic vectors associated with each event head or argument; i.e., the function sem.", "acronym": "VSM", "label": "Vector Space Mode", "ID": "5783"}, {"sentence": "c?2008 Association for Computational Linguistics Sentiment VSMl for   Lyric-based Song Sentiment Classification      Yunqing Xia Linlin Wang  Center for Speech and language Tech.", "acronym": "VSM", "label": "Vector Space Mode", "ID": "5784"}, {"sentence": "In this article we will discuss in detail sev- eral experiments of morphological cue induction for lexical classification (C?avar et al, 2004a) and (C?avar et al, 2004b) using VSMls for category induction and subsequent rule for- mation.", "acronym": "VSM", "label": "Vector Space Mode", "ID": "5785"}, {"sentence": "In Proceedings of EACL 2014, Workshop on Contin- uous VSMls and their Compositional- ity (CVSC).", "acronym": "VSM", "label": "Vector Space Mode", "ID": "5786"}, {"sentence": "A Systematic Study of Semantic VSMl Parameters.", "acronym": "VSM", "label": "Vector Space Mode", "ID": "5787"}, {"sentence": "On the Shortest Spanning Subtree of a Graph and the TSP.", "acronym": "TSP", "label": "Traveling Salesman Problem", "ID": "5788"}, {"sentence": "If each sen- tence in the source document set has one concept (i.e. Table 2 is a diagonal matrix), Eq.2 becomes the Prize Collecting TSP (Balas, 1989).", "acronym": "TSP", "label": "Traveling Salesman Problem", "ID": "5789"}, {"sentence": "By  viewing edge costs as log probabilities, we can cast the TSP  as one of optimizing P(e), that is, of finding the best source word order in Model 1  decoding.", "acronym": "TSP", "label": "Traveling Salesman Problem", "ID": "5790"}, {"sentence": "c?2016 Association for Computational Linguistics AMR-to-text generation as a TSP Linfeng Song1, Yue Zhang3, Xiaochang Peng1, Zhiguo Wang2 and Daniel Gildea1 1Department of Computer Science, University of Rochester, Rochester, NY 14627 2IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 3Singapore University of Technology and Design Abstract The task of AMR-to-text generation is to gen- erate grammatical text that sustains the seman- tic mean", "acronym": "TSP", "label": "Traveling Salesman Problem", "ID": "5791"}, {"sentence": "If word pairs have probabilities attached  to them, then word ordering resembles the finding the least-cost circuit, also known as the  TSP.", "acronym": "TSP", "label": "Traveling Salesman Problem", "ID": "5792"}, {"sentence": "This problem can be formulated as the TSP and its variants.", "acronym": "TSP", "label": "Traveling Salesman Problem", "ID": "5793"}, {"sentence": "Finding the permutation with the high- est probability in the graph formulation is equal to finding the shortest tour in the graph or, equally, solving the TSP.", "acronym": "TSP", "label": "Travelling Salesman Problem", "ID": "5794"}, {"sentence": "Horvat and Byrne (2014) models the search for the highest prob- ability permutation of words under an n-gram model as a TSP; however, direct comparisons to existing works are not provided.", "acronym": "TSP", "label": "Travelling Salesman Problem", "ID": "5795"}, {"sentence": "TSP ?", "acronym": "TSP", "label": "Travelling Salesman Problem", "ID": "5796"}, {"sentence": "3It is worth mentioning that Cutting Plane Algorithms have been successfully applied for solving very large instances of the TSP, a problem essentially equivalent to the decoding in IBM Model 4.", "acronym": "TSP", "label": "Travelling Salesman Problem", "ID": "5797"}, {"sentence": "6 Related Research Snow et al(2008) and Sorokin and Forsyth (2008) showed that AMT use in providing non-expert annotations for NLP tasks.", "acronym": "AMT", "label": "Amazon?s MechanicalTurk", "ID": "5798"}, {"sentence": "AMT (MTurk) is a virtual marketplace that allows anyone to create and post tasks to be completed by human workers around the globe.", "acronym": "AMT", "label": "Amazon?s Mechanical Turk", "ID": "5799"}, {"sentence": "c?2010 Association for Computational Linguistics Using Mechanical Turk to Annotate Lexicons for Less Commonly Used Languages Ann Irvine and Alexandre Klementiev Computer Science Department Johns Hopkins University Baltimore, MD 21218 {anni,aklement}@jhu.edu Abstract In this work we present results from using AMT (MTurk) to an- notate translation lexicons between English and a large set of less commonly used lan- guages.", "acronym": "AMT", "label": "Amazon?s Mechanical Turk", "ID": "5800"}, {"sentence": "72  Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with AMT, pages 108?113, Los Angeles, California, June 2010.", "acronym": "AMT", "label": "Amazon?s Mechanical Turk", "ID": "5801"}, {"sentence": "In Proceedings of the NAACL HLT Workshop on Creating Speech and Language Data With AMT, pages 108?113.", "acronym": "AMT", "label": "Amazon?s Mechanical Turk", "ID": "5802"}, {"sentence": "of Computer Science and The Center for Language and Speech Processing Johns Hopkins University Baltimore, MD 21218, USA ozaidan@cs.jhu.edu Abstract The past few years have seen an increasing interest in using AMT for purposes of collecting data and perform- ing annotation tasks.", "acronym": "AMT", "label": "Amazon?s Mechanical Turk", "ID": "5803"}, {"sentence": "In Proceedings of the NAACL HLT Workshop on Creating Speech and Language Data With AMT, pages 208?211.", "acronym": "AMT", "label": "Amazon?s Mechanical Turk", "ID": "5804"}, {"sentence": "ge Data with AMT, pages 108?113, Los Angeles, California, June 2010.", "acronym": "AMT", "label": "Amazon?s Mechanical Turk", "ID": "5805"}, {"sentence": "We then describe an empirical study us- ing AMT for evaluating gener- ated referring expressions.", "acronym": "AMT", "label": "Amazon Mechanical Turks", "ID": "5806"}, {"sentence": "We ask 5 in- dependent annotators on AMT to read each p and then determine whether h is true, false, or unclear given p.7 We take the majority an- swer as the true label.", "acronym": "AMT", "label": "Amazon Mechanical Turk", "ID": "5807"}, {"sentence": "This evaluation is performed us- ing a set of human-corrected sentences gathered via AMT, an online service where workers are paid to perform a short task, and further filtered for correctness by an undergraduate research assistant.", "acronym": "AMT", "label": "Amazon Mechanical Turk", "ID": "5808"}, {"sentence": "In order to  investigate how WSI could be accomplished using  AMT, 50 words were random- ly sampled from the 270, and their definitions were  extracted from the Longman Dictionary of Con- temporary English (LDOCE) and the Cambridge  Advanced Learner's Dictionary (CALD).", "acronym": "AMT", "label": "Amazon Mechanical Turk", "ID": "5809"}, {"sentence": "We then utilized the AMT Service1.", "acronym": "AMT", "label": "Amazon Mechanical Turk", "ID": "5810"}, {"sentence": "c?2010 Association for Computational Linguistics Clustering dictionary definitions using AMT   Gabriel Parent Maxine Eskenazi  Language Technologies Institute  Carnegie Mellon University  5000 Forbes Avenue  15213 Pittsburgh, USA    {gparent,max}@cs.cmu.edu        Abstract  Vocabulary tutors need word sense disambig- uation (WSD) in order to provide exercises  and assessments that match the sense of words  being taught.", "acronym": "AMT", "label": "Amazon Mechanical Turk", "ID": "5811"}, {"sentence": "AMT (MTurk) has been  used for the purpose of word sense disambiguation  (Snow et al 2008).", "acronym": "AMT", "label": "Amazon Mechanical Turk", "ID": "5812"}, {"sentence": "= {p1, p3, p4} 3 Matching Score 3.1 NA: Frequency-based Translation (FB) A naive solution for map translation is to use co- occurrence of multilingual tags.", "acronym": "NA", "label": "Naive Approach", "ID": "5813"}, {"sentence": "3 QA System Extensions for the Web 3.1 A NA to Web-based QA The simplest approach to turning InSicht into a web QA system would be to collect German web pages and work with the resulting document col- lection as described in Sect.", "acronym": "NA", "label": "Naive Approach", "ID": "5814"}, {"sentence": "3 A Probabilistic Model  3.1 A NA m Finite State Tagging  It is useful to note that a (W, T) pair can be represented  as a tagged sentence wl/t l ,  w2/t2, ...w,/tn where T =  tl, t2...tn is the sequence of tags denoting the semantic  type for each word in the sentence.", "acronym": "NA", "label": "Naive Approach", "ID": "5815"}, {"sentence": "k=1 logP (w k |w k?1 k?N+1 ) (1) 2.1 NA A naive approach to finding the permutation with the highest probability is to enumerate all permu- tations, compute their probabilities using Equa- tion 1, and choose the permutation with the highest probability as the solution.", "acronym": "NA", "label": "Naive Approach", "ID": "5816"}, {"sentence": "IE is a  pre-specified and autonomous task with a NA  domain of focus, where all the information of interest  is represented in the extraction template.", "acronym": "NA", "label": "narrow", "ID": "5817"}, {"sentence": "Each arc is represented by a three segment polygon  (larger arcs are above the NAer, for readibility  reason).", "acronym": "NA", "label": "narrow", "ID": "5818"}, {"sentence": "As it follows from Definition 3, each satura- tion of a terminal DV-structure \u0018 has the same set of nodes and a strictly NAer set of valencies.", "acronym": "NA", "label": "narrow", "ID": "5819"}, {"sentence": "2012) 1 applied in the experiments described in the next sections, which does not enable creat- ing a word-document matrix and organizing word occurrences by documents or NAly specified topics.", "acronym": "NA", "label": "narrow", "ID": "5820"}, {"sentence": "content are aggre- gated and serve as valid candidates for attribution, 2) if multiple characters and pronouns exist, then they are mapped (if possible) via co-reference res- olution in order to NA down the list of attri- bution candidates, and 3) the quote is attributed to the nearest quote character (or pronoun).", "acronym": "NA", "label": "narrow", "ID": "5821"}, {"sentence": "and submitted to Google to NA the results to texts with exact phrases.", "acronym": "NA", "label": "narrow", "ID": "5822"}, {"sentence": "3.4 NA The aim of extracting social networks from nov- els is to turn a complex object (the novel) into a schematic representation of the core structure of the novel.", "acronym": "NA", "label": "Network Analysis", "ID": "5823"}, {"sentence": "Using Topic  Discovery to Segment Large Communication Graphs  for Social NA, In Proceedings of the  IEEE/WIC/ACM International Conference on Web  Intelligence, 95-99.", "acronym": "NA", "label": "Network Analysis", "ID": "5824"}, {"sentence": "Social NA While many previous studies considered the effect of social dynamics for social media analysis, most relied on an explic- itly available social network structure or consid- ered dialogues and speech acts for which opinion holders are given (Tan et al, 2011; Hu et al, 2013; Li et al, 2014; West et al, 2014; Krishnan and Eisenstein, 2015).", "acronym": "NA", "label": "Network Analysis", "ID": "5825"}, {"sentence": "A  Semantic NA on the Iraq War Blogs.", "acronym": "NA", "label": "Network Analysis", "ID": "5826"}, {"sentence": "2 B i l ingua l  Head Transduct ion   2.1 Bilingual Head Transducers  A head transducer M is a FSMs as-  sociated with a pair of words, a source word w  and a target word v. In fact, w is taken from the  set V1 consisting of the source language vocab-  ulary augmented by the \"empty word\" e, and v  is taken from V~, the target language vocabulary  augmented with e. A head transducer reads from  a pair of source sequences, a left source sequence  L1 and a right source sequence", "acronym": "FSMs", "label": "finite state machine", "ID": "5827"}, {"sentence": "We learn weighted edit distance in a probabilistic FSMs (pFSM) model, where state transitions corre- spond to edit operations.", "acronym": "FSMs", "label": "finite state machine", "ID": "5828"}, {"sentence": "In (Wright et al, 1997) phrase level significance was obtained for noisy transcribed data where the phrases are clustered and combined into FSMss.", "acronym": "FSMs", "label": "finite state machine", "ID": "5829"}, {"sentence": "The jump sequence bK1 can be described by a deterministic FSMs. ?(", "acronym": "FSMs", "label": "finite state machine", "ID": "5830"}, {"sentence": "TESTS FOR UD AND UDP  By treating FSMss as encoders and decoders, tests for  UD and UDF can be converted into tests for IL and ILF (S. EVEN, 1965;  Z. KOHAVr, 1970).", "acronym": "FSMs", "label": "finite state machine", "ID": "5831"}, {"sentence": "This algorithm is based on the  state-table of the inverse of the FSMs which is taken as  the encoder device 4,7.", "acronym": "FSMs", "label": "finite state machine", "ID": "5832"}, {"sentence": "Weighted FSMs have seen a variety of use in NLP (Mohri, 1997).", "acronym": "FSMs", "label": "Finite State Machines", "ID": "5833"}, {"sentence": "Figure 2: Framework of speech retrieval through subword indexing 6 3.1 Subword FSMs as Speech Indices We construct a full index that can be used to search for a query within all the speech utterances ui, i ?", "acronym": "FSMs", "label": "Finite State Machines", "ID": "5834"}, {"sentence": "More specifically, some other previous work on Ma- chine Translation have used lattices (or more generally Weighted FSMs).", "acronym": "FSMs", "label": "Finite State Machines", "ID": "5835"}, {"sentence": "2 Integrating Typed Feature Structures  and FSMs  The main motivation for developing SProUT  comes from the need to have a system that (i)  allows a flexible integration of different processing  modules and (ii) to find a good trade-off between  processing efficiency and linguistic expressive- ness.", "acronym": "FSMs", "label": "Finite State Machines", "ID": "5836"}, {"sentence": "3 Mapping Tree Adjoining Grammar to FSMs What is crucial for being able to define a map- ping from words to application semantics is a very abstract notion of grammatical function: in devising such a mapping, we are not interested in how English realizes certain syntactic argu- ments, i.e., in the phrase structure of the verbal projection.", "acronym": "FSMs", "label": "Finite State Machines", "ID": "5837"}, {"sentence": "Using FSMs for  Evaluating Spoken Dialog Systems?,", "acronym": "FSMs", "label": "Finite State Machines", "ID": "5838"}, {"sentence": "1 context sensitive (C recursive) PSPACE  1.5 indexed  1.75 mildly context sensitive  2 context free  3 regular  Turing Machine (TM)  Linear Bounded Automata  LBA)  ested Stack Automata   NSA)  mbeded Pushdown  Automata (EPDA)  Pushdown Automata (PDA)  FSMs (FSM)  NP-Complete  n 7  n a  ' linear \" '  Table 1: Models of Grammar and Computation  corrected stringset argument that Dutch licences  a\"b'*c '~ constructions, which are MCS.", "acronym": "FSMs", "label": "Finite State Machines", "ID": "5839"}, {"sentence": "At a high level, we would like to identify query segments that corre- spond to IHs, IMs and OTHER.", "acronym": "OTHER", "label": "Others", "ID": "5840"}, {"sentence": "Th is  0nl, it,y is rcfc:rro(I  to as t, hc backw~ird- looking ceutor ((,'b), Any  other   (mtity appear ing hi all Ill, tCl';tll('O is a. \\[}.)r',var(l-lookillg  center (Cf) which niny I)ccomc a (it) later Oil ill ttic  discourse, Cfs arc ordered by grammatical flmctions  according to the, Jr degrees of salience as follows:  Topic > Subject; > Ob ject /Ob ject2   > OTHER (Oblique, Possessor, etc)  Kmneymn~ showed that  t;he zeroq)ronoun corr(>  Sllonds I,o the (3) in ;lalmnc.se.", "acronym": "OTHER", "label": "Others", "ID": "5841"}, {"sentence": "OTHER are more  abstract, such as whether the preceding word is an  article; whether the preceding word is an adjective;  whether the preceding word is a conjunction; whether  the preceding word is a preposition.", "acronym": "OTHER", "label": "Others", "ID": "5842"}, {"sentence": "In our model, the ordering is as follows:  'l'opic > Subject > Object > Object2 > OTHER  > Subject/Ol~ject/Ol~ject2 of subordinate clause  > OTHER ill Sllbordill~Jt;(!", "acronym": "OTHER", "label": "Others", "ID": "5843"}, {"sentence": "OTHER like ?", "acronym": "OTHER", "label": "Others", "ID": "5844"}, {"sentence": "The double complementation in the definitions of these conditional operators, and  also in several OTHER to be introduced later, constitutes an idiom for ex-  pressing universal quantification.", "acronym": "OTHER", "label": "other expressions", "ID": "5845"}, {"sentence": "If agents can  have attitudes toward sentences of thought language, why  shouldn't hey have attitudes toward OTHER of  the same thought language?", "acronym": "OTHER", "label": "other expressions", "ID": "5846"}, {"sentence": "Linguistic Expressions epistemic hedges OTHER non?phrasal lexical phrasal 1st person singular other conditionals ... ... ... We do not claim this separation of hedging markers can fully account for pragmatic and se- mantic analysis of hedging in web forums, but we are confident this classification supports reliable annotation for quantificational assessment of cer- tainty and hedging in this informal domain.", "acronym": "OTHER", "label": "other expressions", "ID": "5847"}, {"sentence": "For notational convenience, we let the overbar in these extensions and in  the OTHER below stand for the complement relative to 7r* as opposed to the  more usual ~* x ~*:  _ _  m  If-P-then-S(R1,R2) = ~* - R1R2 = RIR2  m  If-S-then-P(R1, R2) = 7r* - R1R2 = RIR2  The conditions for a simple context restriction rule are then modeled by the following  relation:  Restrict(% A, p) = If- S-then-P (Tr* A, T ~r* ) n If-P-then- S ( Tr*T , p~r* )  The first component ensur", "acronym": "OTHER", "label": "other expressions", "ID": "5848"}, {"sentence": "Type-based meth- ods frequently exploit the fact that idioms have 754 a number of properties which differentiate them from OTHER.", "acronym": "OTHER", "label": "other expressions", "ID": "5849"}, {"sentence": "False negation cues: Some negation words may be also used in OTHER with- out constituting a negation, as in sentence (3).", "acronym": "OTHER", "label": "other expressions", "ID": "5850"}, {"sentence": "Using ran- dom guesses, the BL accuracy is 0", "acronym": "BL", "label": "baseline", "ID": "5851"}, {"sentence": "For the exact match scheme, the obtained performance is higher7 than the BL (random guess) that equals to 0.250.", "acronym": "BL", "label": "baseline", "ID": "5852"}, {"sentence": "The BL accuracy equals 0.31 given that random guesses are used.", "acronym": "BL", "label": "baseline", "ID": "5853"}, {"sentence": "Using random guesses, the BL precision is 0.010 and 0.333 for quote-to-speaker attribution and gender estimation, respectively.", "acronym": "BL", "label": "baseline", "ID": "5854"}, {"sentence": "t-test wrt BL).", "acronym": "BL", "label": "baseline", "ID": "5855"}, {"sentence": "Using ran- dom guesses, the BL accuracy is 0.33.", "acronym": "BL", "label": "baseline", "ID": "5856"}, {"sentence": "At first glance Princi-  ple 2 is exemplified linguistically by subject-  empathy( 'ident' in her term ) sharing \\[5\\],  or by the combination of preference for Cb  (BL Center ) continuing and  Cf Ranking \\[8\\].", "acronym": "BL", "label": "Backward Looking", "ID": "5857"}, {"sentence": "Among them, the so-called BACKWARD-LOOKING CENTER (CB), defined as follows: BL Center (CB) CB(U i+1 ), the BACKWARD-LOOKING CENTER of utter- ance U i+1 , is the highest ranked element of CF(U i ) that is realized in U i+1 .", "acronym": "BL", "label": "Backward Looking", "ID": "5858"}, {"sentence": "BL Function which characterizes  how an utterance relates to the previous dis-  course.", "acronym": "BL", "label": "Backward Looking", "ID": "5859"}, {"sentence": "BL/Forward Looking Fea- tures.", "acronym": "BL", "label": "Backward Looking", "ID": "5860"}, {"sentence": "Using ran- dom guesses, the BLe accuracy is 0", "acronym": "BL", "label": "baselin", "ID": "5861"}, {"sentence": "For the exact match scheme, the obtained performance is higher7 than the BLe (random guess) that equals to 0.250.", "acronym": "BL", "label": "baselin", "ID": "5862"}, {"sentence": "The BLe accuracy equals 0.31 given that random guesses are used.", "acronym": "BL", "label": "baselin", "ID": "5863"}, {"sentence": "Using random guesses, the BLe precision is 0.010 and 0.333 for quote-to-speaker attribution and gender estimation, respectively.", "acronym": "BL", "label": "baselin", "ID": "5864"}, {"sentence": "t-test wrt BLe).", "acronym": "BL", "label": "baselin", "ID": "5865"}, {"sentence": "Using ran- dom guesses, the BLe accuracy is 0.33.", "acronym": "BL", "label": "baselin", "ID": "5866"}, {"sentence": "(3) T.empor~l input from the phonetic leve_l  <voiceless, 0 91A9, C>  <voiced, 91.2, 517.5, C>  <glide, 452.6, 498.2, C>  <occlusive, 0, 35.4, C>  <transient, 34.5, 641.6, C>  <noise, (:~).61, 91.16, C>  <vowellike, 94.29, 392.6, C>  <nasal, 402.9, 518.6, C>  <BL, 20.45, 93.2, C>  <tongue-retracted, 93.21, 392.6, C>  <BL, 392.62, 518.2, C >  (4) Ev?nt invent~)r3~  et: VOI (voiceless, < 0,91.19 > )  e2: VOl(voiced,<91.2,517.5>)  e~: GLt(glide, < 452.6,498.2 >)  e~: OCC(oeclusive,<0,35.4>)  es: TRA(transient,< 34.5,60.6>)  e6: NOl(noise, < 60.61,91.16 > ) eT: VOW(vowellike, <94.29,392.6 >)  es: NAS(nasal, < 402.9,518.6 > ) e~: LAB(BL, < 20.45,93.2 >)  et0: TON(retracted, < 93.21,392.6 > ) eL~: LAB(bil", "acronym": "BL", "label": "bilabial", "ID": "5867"}, {"sentence": "518.6, C>  <BL, 20.45, 93.2, C>  <tongue-retracted, 93.21, 392.6, C>  <BL, 392.62, 518.2, C >  (4) Ev?nt invent~)r3~  et: VOI (voiceless, < 0,91.19 > )  e2: VOl(voiced,<91.2,517.5>)  e~: GLt(glide, < 452.6,498.2 >)  e~: OCC(oeclusive,<0,35.4>)  es: TRA(transient,< 34.5,60.6>)  e6: NOl(noise, < 60.61,91.16 > ) eT: VOW(vowellike, <94.29,392.6 >)  es: NAS(nasal, < 402.9,518.6 > ) e~: LAB(BL, < 20.45,93.2 >)  et0: TON(retracted, < 93.21,392.6 > ) eL~: LAB(BL, < 392.62,518.2 > ) Of particular interest to the phonological parser are the  precedence r lations between those event properties of  the same type and the overlap and temlx~ral inclusion  relations between event properties of differing types.", "acronym": "BL", "label": "bilabial", "ID": "5868"}, {"sentence": "(3) T.empor~l input from the phonetic leve_l  <voiceless, 0 91A9, C>  <voiced, 91.2, 517.5, C>  <glide, 452.6, 498.2, C>  <occlusive, 0, 35.4, C>  <transient, 34.5, 641.6, C>  <noise, (:~).61, 91.16, C>  <vowellike, 94.29, 392.6, C>  <nasal, 402.9, 518.6, C>  <BL, 20.45, 93.2, C>  <tongue-retracted, 93.21, 392.6, C>  <BL, 392.62, 518.2, C >  (4) Ev?nt invent~)r3~  et: VOI (voiceless, < 0,91.19 > )  e2: VOl(voiced,<91.2,517.5>)  e~: GLt(glide, < 452.6,498.2 >)  e~: OCC(oeclusive,<0,35.4>)  es: TRA(transient,< 34.5,60.6>)  e6: NOl(noise, < 60.61,91.16 > ) eT: VOW(vowellike, <94.29,392.6 >)  es: NAS(nasal, < 402.9,518.6 > ) e~: LAB(BL, <", "acronym": "BL", "label": "bilabial", "ID": "5869"}, {"sentence": "It also shows that cer- tain German codas are assimilated by the alveolar sounds /d/ and /s/ from the original BL [m] to an apico-alveolar [n], as in Boden (E: ground, MHG: bodem) or in Besen (E: broom, MHG: be?sem, OHG: pe?samo).", "acronym": "BL", "label": "bilabial", "ID": "5870"}, {"sentence": "<BL, 392.62, 518.2, C >  (4) Ev?nt invent~)r3~  et: VOI (voiceless, < 0,91.19 > )  e2: VOl(voiced,<91.2,517.5>)  e~: GLt(glide, < 452.6,498.2 >)  e~: OCC(oeclusive,<0,35.4>)  es: TRA(transient,< 34.5,60.6>)  e6: NOl(noise, < 60.61,91.16 > ) eT: VOW(vowellike, <94.29,392.6 >)  es: NAS(nasal, < 402.9,518.6 > ) e~: LAB(BL, < 20.45,93.2 >)  et0: TON(retracted, < 93.21,392.6 > ) eL~: LAB(BL, < 392.62,518.2 > ) Of particular interest to the phonological parser are the  precedence r lations between those event properties of  the same type and the overlap and temlx~ral inclusion  relations between event properties of differing types.", "acronym": "BL", "label": "bilabial", "ID": "5871"}, {"sentence": "3 Tree Search vs. Dynamic  P rogramming  Once an appropriate function for measuring simi-  larity between pairs of segments has been designed,  290  Feature Phonological Numerical  name term value  Place  Manner  High  Back  \\[BL\\]  \\[labiodental\\]  \\[dental\\]  \\[alveolar\\]  \\[retroflex\\]  \\[palato-alveolar\\]  \\[palatal\\]  \\[velar\\]  \\[uvular\\]  \\[pharyngeal\\]  \\[glottal\\]  \\[stop\\]  \\[affricate\\]  \\[fricative\\]  \\[approximant\\]  \\[high vowel\\]  \\[mid vowel\\]  \\[low vowel\\]  \\[high\\]  \\[mid\\]  \\[low\\]  \\[front\\]  \\[central\\]  \\[back\\]  1.0  0.95  0.9  0.85  0.8  0.75  0.7  0.6  0.5  0.3  0.1  1.0  0.9  0.8  0.6  0", "acronym": "BL", "label": "bilabial", "ID": "5872"}, {"sentence": "Precision Recall F-score BL 72.66 66.17 69.26 +ALF 78.14 64.36 70.59 Table 3: Confidence-based Alignment Link Filter- ing on C-E Alignment Precision Recall F-score BL 84.43 83.64 84.04 +ALF 88.29 83.14 85.64 Table 4: Confidence-based Alignment Link Filter- ing on A-E Alignment 512 sentence pairs, and the A-E alignment test set is the 200 Arabic-English sentence pairs from NIST MT03 test set.", "acronym": "BL", "label": "Baseline", "ID": "5873"}, {"sentence": "This should be taken with a grain BL Original Query Hybrid Tot Good Top 5 Tot Good Top 5 Tot Good Top 5 Poe 12 6.5 3 10 0.5 0.5 10 5.5 2.5 Romantics 10 0 0 15 0 0 10 3 3 Witch Hunts 10 8 3 14 2 1 10 8 5 US Wars 15 12 2 0 0 0 16 13 4 Sonnets 15 10 5 10 2 0 10 8 4 Presidents 15 2 2 15 0 0 15 2 2 Epics 10 7 4 10 5 3 10 7 4 Dec of Ind 10 2 0 0 0 0 10 5.5 2 Avr.", "acronym": "BL", "label": "Baseline", "ID": "5874"}, {"sentence": "In Proceed- ings of the Fourteenth International Conference BL Original Query Hybrid FPF FPF/Tot FPF FPF/Tot FPF FPF/Tot Poe 1 .08 1 .1 1 .1 Romantics 8 .8 15 1 2 .2 Witch Hunts 2 .2 1 .07 0 0 US Wars 3 .2 3 .19 Sonnets 5 .33 8 .8 2 .2 Presidents 5 .33 10 .67 2 .13 Epics 0 0 0 0 0 0 Dec of Ind 3 .3 2 .2 28.1% 44% 12.8% Table 4: False Positives Containing Figure on Computational Linguistics, Nantes, France, July.", "acronym": "BL", "label": "Baseline", "ID": "5875"}, {"sentence": "For evaluation, 10-fold cross valida- Dataset Relaxed Exact lex pos lex pos BL 0.625 0.250 QUOTES1 0.869 0.883 0.445 0.373 QUOTES2 0.877 0.831 0.450 0.435 BOTH 0.886 0.858 0.464 0.383 Table 4: Age estimation: average accuracy.", "acronym": "BL", "label": "Baseline", "ID": "5876"}, {"sentence": "p BL 0.010 0.333 10 stories (subset of dataset) Scheme 1 0.833 0.780 0.672 0.929 Scheme 2 0.868 0.710 0.759 0.917 Scheme 3 0.835 0.710 0.759 0.917 17 stories (full dataset) Scheme 2 0.845 0.688 0.733 0.892 Table 3: Quote attribution and gender estimation.", "acronym": "BL", "label": "Baseline", "ID": "5877"}, {"sentence": "2.3 MWEss As shown in Eryigit et al. (", "acronym": "MWEs", "label": "Multi Word Expression", "ID": "5878"}, {"sentence": "Mining Complex Predicates  In Hindi Using Parallel Hindi-English Corpus, ACL- IJCNLP, Workshop on MWEs,  Singapore.", "acronym": "MWEs", "label": "Multi Word Expression", "ID": "5879"}, {"sentence": "Patterns of German Particle Verbs and their Impact on Lexical Semantics Stefan Bott Sabine Schulte im Walde Institut f?ur Maschinelle Sprachverabeitung Universit?at Stuttgart Pfaffenwaldring 5b, 70569 Stuttgart, Germany {stefan.bott,schulte}@ims.uni-stuttgart.de Abstract German particle verbs, like anblicken (to gaze at) combine a base verb (blicken) with a particle (an) to form a special kind of MWEs.", "acronym": "MWEs", "label": "Multi Word Expression", "ID": "5880"}, {"sentence": "5 83.30 81.37 AI-KU 2 78.57 85.12 83.25 76.35 83.24 81.35 Baseline 77.67 84.6 82.36 75.82 83.20 80.88 Table 9: Results on Swedish Gold Predicted Precision Recall F1 Precision Recal F1 Best System 99.41 99.38 99.39 81.68 79.97 80.81 AI-KU 1 99.41 99.38 99.39 74.47 71.51 72.96 AI-KU 2 99.38 99.36 99.37 74.34 71.51 72.89 MaltOptimizer Baseline 98.77 99.18 99.26 72.64 68.09 70.29 Table 10: Results of MWEss on French 82 4.2 Experiment II The second approach is discretizing the real valued vectors.", "acronym": "MWEs", "label": "Multi Word Expression", "ID": "5881"}, {"sentence": "One example is (Sharoff, 2004), where shallow parsing is used for the identification of preposi- tional MWEss in Russian, with the following explanation of reasons for performing some language-dependent processing: ?", "acronym": "MWEs", "label": "Multi Word Expression", "ID": "5882"}, {"sentence": "of the ACL 2003 Workshop on MWEs, 49?56.", "acronym": "MWEs", "label": "Multiword Expressions", "ID": "5883"}, {"sentence": "In Proceedings of the ACL-07 Workshop on A Broader Perspective on MWEs.", "acronym": "MWEs", "label": "Multiword Expressions", "ID": "5884"}, {"sentence": "In Proceedings of the  ACL-04 Workshop on MWEs:  Integrating Processing, p. 1?8.", "acronym": "MWEs", "label": "Multiword Expressions", "ID": "5885"}, {"sentence": "The second was a presentation on the same workshop by Aravind Joshi and Owen Rambow of their en- coding of DG in TAG, and the third was a talk by Charles Fillmore titled MWEs: An Extremist Approach.", "acronym": "MWEs", "label": "Multiword Expressions", "ID": "5886"}, {"sentence": "XDG solving is efficient at least for the smaller- scale example grammars tested so far, but these good results hinge substantially on the assump- Second ACL Workshop on MWEs: Integrating Processing, July 2004, pp.", "acronym": "MWEs", "label": "Multiword Expressions", "ID": "5887"}, {"sentence": "2009 ACL and AFNLP A Cohesion Graph Based Approach for Unsupervised Recognition of Literal and Non-literal Use of MWEs Linlin Li and Caroline Sporleder Saarland University Postfach 15 11 50 66041 Saarbr?ucken Germany {linlin,csporled}@coli.uni-saarland.de Abstract We present a graph-based model for rep- resenting the lexical cohesion of a dis- course.", "acronym": "MWEs", "label": "Multiword Expressions", "ID": "5888"}, {"sentence": "MWEs: A pain in the neck for NLP.", "acronym": "MWEs", "label": "Multiword expressions", "ID": "5889"}, {"sentence": "MWEs: A  pain in the neck for NLP.", "acronym": "MWEs", "label": "Multiword expressions", "ID": "5890"}, {"sentence": "2028  MWEs as dependency subgraphs Ralph Debusmann Programming Systems Lab Saarland University Postfach 15 11 50 66041 Saarbru?cken, Germany rade@ps.uni-sb.de Abstract We propose to model multiword expres- sions as dependency subgraphs, and re- alize this idea in the grammar formal- ism of Extensible Dependency Gram- mar (XDG).", "acronym": "MWEs", "label": "Multiword expressions", "ID": "5891"}, {"sentence": "MWEs in the wild?", "acronym": "MWEs", "label": "Multiword expressions", "ID": "5892"}, {"sentence": "MWEs: a pain in the neck for NLP.", "acronym": "MWEs", "label": "Multiword expressions", "ID": "5893"}, {"sentence": "In Proceedings of the ACL workshop on MWEs.", "acronym": "MWEs", "label": "Multiword expressions", "ID": "5894"}, {"sentence": "KX architecture is the same across all languages, except for the module selecting MWEs, that is based on PoS tags (this is the only language-dependent part of the system).", "acronym": "MWEs", "label": "multiword expressions", "ID": "5895"}, {"sentence": "20 3 KX configuration for the DEFT 2012 task As introduced before (Section 2.2), to port KX to the French language and, in particular, to adapt it to the DEFT 2012 task, the Morfette morphological analyzer (Chrupala et al, 2008) has been integrated into the system, to select as MWEs only the n-grams matching certain lexical patterns (i.e. part-of-speech).", "acronym": "MWEs", "label": "multiword expressions", "ID": "5896"}, {"sentence": "Alignment-based extraction of MWEs.", "acronym": "MWEs", "label": "multiword expressions", "ID": "5897"}, {"sentence": "We distinguish between the task of identifying candidate MWEs 2http://en.wiktionary.org Feature Description ?", "acronym": "MWEs", "label": "multiword expressions", "ID": "5898"}, {"sentence": "Then, from the n-gram list a sublist of MWEs (MWE) is derived, i.e. combinations of words expressing a unitary concept, for example ?", "acronym": "MWEs", "label": "multiword expressions", "ID": "5899"}, {"sentence": "636 bine evidence from multiple languages, we develop a novel boosting algorithm tailored to the task of ranking MWEs by their degree of id- iomaticity.", "acronym": "MWEs", "label": "multiword expressions", "ID": "5900"}, {"sentence": "PASSly?", "acronym": "PASS", "label": "passive", "ID": "5901"}, {"sentence": "Given a sentence that includes one or more quotes, the respective PASS characters were not considered as candidate speakers.", "acronym": "PASS", "label": "passive", "ID": "5902"}, {"sentence": "is a PASS character.", "acronym": "PASS", "label": "passive", "ID": "5903"}, {"sentence": "The PASS char- acters were identified via the following relations extracted by dependency parsing: nsubjpass (PASS nominal subject) and pobj (object of a preposition).", "acronym": "PASS", "label": "passive", "ID": "5904"}, {"sentence": "d voice: if the syntactic head of p is be and p is not ended with -ing, then p is PASS.", "acronym": "PASS", "label": "passive", "ID": "5905"}, {"sentence": "PASS-aggressive (PA) algorithm (Crammer et al 2006) was proposed for nor- mal multi-class classification and can be easily extended to structure learning (Crammer et al.,", "acronym": "PASS", "label": "Passive", "ID": "5906"}, {"sentence": "(ii) PASS  E<0,1>  %Yl \\[ (the (automaton))  (Ixl~*psub~ ~accept ) ) (Y ) ) \\ ] ) \\ ]   *en(accept) %So\\[ (the(automaton))  (Ix I\\[ (*psubj (x)) (s) \\]) \\]  / ~  *psubJ the(automaton) /  be/ acc~epted Jy thS  ~u~tomaton  where *ene E<<0,1>,<0,1,1>>,  *psubj E E<<0,O>,i>.", "acronym": "PASS", "label": "Passive", "ID": "5907"}, {"sentence": "PASS-aggressive sequence labeling with discrim- inative post-editing for recognising person entities in tweets.", "acronym": "PASS", "label": "Passive", "ID": "5908"}, {"sentence": "(4) PASS Mode.", "acronym": "PASS", "label": "Passive", "ID": "5909"}, {"sentence": "736 PASS-Aggressive Learning We train our model with a structured version of the PASS-Aggressive (PA) algorithm (Crammer et al.,", "acronym": "PASS", "label": "Passive", "ID": "5910"}, {"sentence": "1 Introduction GREs has been stud- ied for the last two decades.", "acronym": "GRE", "label": "Generation of referring expression", "ID": "5911"}, {"sentence": "GREs: Managing structural ambiguities.", "acronym": "GRE", "label": "Generation of referring expression", "ID": "5912"}, {"sentence": "GREs: Assessing the Incremental Algorithm.", "acronym": "GRE", "label": "Generation of referring expression", "ID": "5913"}, {"sentence": "GREs: Assessing the incremental algorithm.", "acronym": "GRE", "label": "Generation of referring expression", "ID": "5914"}, {"sentence": "GREs.", "acronym": "GRE", "label": "Generation of referring expression", "ID": "5915"}, {"sentence": "7.1 Measurement Axis Descriptor GREs (noun phrases) is one of the key problems explored within the natural language generation literature.", "acronym": "GRE", "label": "Generation of referring expression", "ID": "5916"}, {"sentence": "Com- putational GRE: A survey.", "acronym": "GRE", "label": "generation of referring expressions", "ID": "5917"}, {"sentence": "Efficient context-sensitive GRE.", "acronym": "GRE", "label": "generation of referring expressions", "ID": "5918"}, {"sentence": "Passonneau (1996) proposes an algorithm for the  GRE and Walker (1996a)  integrates centering into a cache model of attentional  state.", "acronym": "GRE", "label": "generation of referring expressions", "ID": "5919"}, {"sentence": "lexicalisation, aggre- gation and GRE.", "acronym": "GRE", "label": "generation of referring expressions", "ID": "5920"}, {"sentence": "Its applications are not limited to  the GRE.", "acronym": "GRE", "label": "generation of referring expressions", "ID": "5921"}, {"sentence": "This paper proposes a method that allows effi-  cient GRE, through  a unification grammar, at the cost of some ini-  tial effort in tailoring the phrase-structure rules  to the current knowledge base.", "acronym": "GRE", "label": "generation of referring expressions", "ID": "5922"}, {"sentence": "Find- ing planted partitions in nearly linear time using ar- rested SPEC.", "acronym": "SPEC", "label": "spectral clustering", "ID": "5923"}, {"sentence": "We consider two forms of SPEC: EigenCluster (Cheng et al, 2006), a method origi- nally designed to cluster snippets for search results into semantically related categories, and GSpec (Ng et al, 2001), a method that directly clusters a collo- cation graph.", "acronym": "SPEC", "label": "spectral clustering", "ID": "5924"}, {"sentence": "But instead of learning representations of pivots in source and target domains the authors used SPEC to align domain-specific and domain-independent words into a set of feature- clusters.", "acronym": "SPEC", "label": "spectral clustering", "ID": "5925"}, {"sentence": "Name disambiguation in author citations using a K-way SPEC method.", "acronym": "SPEC", "label": "spectral clustering", "ID": "5926"}, {"sentence": "Kernel k- means: SPEC and normalized cuts.", "acronym": "SPEC", "label": "spectral clustering", "ID": "5927"}, {"sentence": "This indicates that the MRW model with SPEC is more robust than that with the baseline, k-means, with respect to the differ- ent number of clusters.", "acronym": "SPEC", "label": "spectral clustering", "ID": "5928"}, {"sentence": "SPEC is a trans- formation of the original sentences into a set of or- thogonal eigenvectors.", "acronym": "SPEC", "label": "Spectral clustering", "ID": "5929"}, {"sentence": "SPEC partitions objects relying on their similarity matrix.", "acronym": "SPEC", "label": "Spectral clustering", "ID": "5930"}, {"sentence": "SPEC for German verbs.", "acronym": "SPEC", "label": "Spectral clustering", "ID": "5931"}, {"sentence": "3.2 Classification SPEC (Shi and Malik, 2000) is used in this work, since we found it to outperform other clustering approaches such as k-means and Gaus- sian mixture models.", "acronym": "SPEC", "label": "Spectral clustering", "ID": "5932"}, {"sentence": "SPEC refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters with high intra-cluster similarity and low inter-cluster similarity.", "acronym": "SPEC", "label": "Spectral clustering", "ID": "5933"}, {"sentence": "SPEC can be viewed in abstract terms as the partitioning of a graph G over a set of words W. The weights on the edges of G are the similarities Sij.", "acronym": "SPEC", "label": "Spectral clustering", "ID": "5934"}, {"sentence": "In addition, tf-idf (Term-Frequency IDF) weighting was used when training LSI topic models.", "acronym": "IDF", "label": "Inverse Document Frequency", "ID": "5935"}, {"sentence": "IDF Numeric fea- tures that compare source and target word frequen- cies.", "acronym": "IDF", "label": "Inverse Document Frequency", "ID": "5936"}, {"sentence": "Why IDF?", "acronym": "IDF", "label": "Inverse Document Frequency", "ID": "5937"}, {"sentence": "The weight function is a combination of similarity measure between t and si and IDF (idf) of t. The next two subsections explain the calculation of the similarity measure and the idf in detail.", "acronym": "IDF", "label": "Inverse Document Frequency", "ID": "5938"}, {"sentence": "5.1.2 IDF If f number of documents in corpus Q contain a term t and the total number of documents in Q is N, the IDF (idf) of t is idf(t) = log N f (3) Combining the similarity measure and the idf of t in the corpus, we define the weight function ?(", "acronym": "IDF", "label": "Inverse Document Frequency", "ID": "5939"}, {"sentence": "In order to integrate the strengths of tradi- tional IR models, the IDFy idf is considered, which measures the general im- portance of a term for predicting the content of a document.", "acronym": "IDF", "label": "inverse document frequenc", "ID": "5940"}, {"sentence": "For example, word fre- quencies have typically been weighted by IDFies (tf ?", "acronym": "IDF", "label": "inverse document frequenc", "ID": "5941"}, {"sentence": "We gather term frequen- cies and IDFies across the whole corpus.", "acronym": "IDF", "label": "inverse document frequenc", "ID": "5942"}, {"sentence": "tf(wj , dk)idf(wj), where tf(wi, dk) is the term frequency for wi in dk and idf(wi) is the IDFy weight for wi.", "acronym": "IDF", "label": "inverse document frequenc", "ID": "5943"}, {"sentence": "wh?WH idf(wh) (3) where idf(w) is the IDFy of the word w. For sake of comparison, we consider also the corresponding more classical version that does not apply the IDFy s2(T,H) = ?", "acronym": "IDF", "label": "inverse document frequenc", "ID": "5944"}, {"sentence": "Since we represent word to- kens rather than word types in the cohesion graph, we do not need to model the term frequency tf separately, instead we set salience to the log value of the IDFy idf : salience(t i ) = log |D| |{d : t i ?", "acronym": "IDF", "label": "inverse document frequenc", "ID": "5945"}, {"sentence": "In Table 10 we show the normalized performance  (D) of MEAD, for the six clusters at nine  CompR.", "acronym": "CompR", "label": "compression rates", "ID": "5946"}, {"sentence": "Note that for  the largest cluster, Cluster D, MEAD outperformed  Lead at all CompR.", "acronym": "CompR", "label": "compression rates", "ID": "5947"}, {"sentence": "Secondly, even though sentence co-selection metrics have been widely used for evaluating summaries of other genres, different CompR, different gold standards, and availability of naturally occurring competitive baselines (e.g., lead baseline in newswire summarization) make fair comparison difficult.", "acronym": "CompR", "label": "compression rates", "ID": "5948"}, {"sentence": "In scientific ar- ticles, however, the CompR have to be much higher: Shortening a 20-page journal article to a half-page summary requires a compression to 2.5% of the original.", "acronym": "CompR", "label": "compression rates", "ID": "5949"}, {"sentence": "As discussed above, the CompR on news texts are far lower: there are fewer sentences from which to choose, making it easier to agree on which ones to select.", "acronym": "CompR", "label": "compression rates", "ID": "5950"}, {"sentence": "The toolkit implements multiple summariza- tion algorithms (at arbitrary CompR) such as position-based, TF*IDF, largest common subsequence, and keywords.", "acronym": "CompR", "label": "compression rates", "ID": "5951"}, {"sentence": "The test corpus is used in the evaluation  in such a way that each cluster is summarized at 9  different CompR, thus giving nine times as  many sample points as one would expect from the  size of the corpus.", "acronym": "CompR", "label": "compression rates", "ID": "5952"}, {"sentence": "In scientific ar- ticles, however, the CompRs have to be much higher: Shortening a 20-page journal article to a half-page summary requires a compression to 2.5% of the original.", "acronym": "CompR", "label": "compression rate", "ID": "5953"}, {"sentence": "Figure 2: After user has added all of the documents to be summarized, they select the CompR and then submit for summarization.", "acronym": "CompR", "label": "compression rate", "ID": "5954"}, {"sentence": "The average length of a story is 3,333 tokens and the target CompR expressed in the number of sentences is 94%.", "acronym": "CompR", "label": "compression rate", "ID": "5955"}, {"sentence": "As discussed above, the CompRs on news texts are far lower: there are fewer sentences from which to choose, making it easier to agree on which ones to select.", "acronym": "CompR", "label": "compression rate", "ID": "5956"}, {"sentence": "The toolkit implements multiple summariza- tion algorithms (at arbitrary CompRs) such as position-based, TF*IDF, largest common subsequence, and keywords.", "acronym": "CompR", "label": "compression rate", "ID": "5957"}, {"sentence": "Since the abstractive summaries had already been created, summary size was de- termined by their size (which means creating sum- maries using a CompR of around 10% of the original size).", "acronym": "CompR", "label": "compression rate", "ID": "5958"}, {"sentence": "We demonstrate that a combina- tion of instance, annotator and annotation task characteristics are important for developing an accurate estimator, and argue that both corre- lation coefficient and RMSE should be used for evaluating annotation cost estimators.", "acronym": "RMSE", "label": "root mean square error", "ID": "5959"}, {"sentence": "Their performances, reported in Table 4, were as- sessed using five measures: the multiple correlation ratio (R), the accuracy (acc), the adjacent accuracy 7 (adjacc), the RMSE (rmse) and the mean absolute error (mae).", "acronym": "RMSE", "label": "root mean square error", "ID": "5960"}, {"sentence": "rmulae 5 72.3% 0.32 SLALEX 16 68.1% 0.29 SLASYN 14 71.2% 0.28 SLALEX + SLASYN 30 82.3% 0.23 BEST10SYN 10 69.9% 0.28 All Syntactic Features 25 75.3% 0.27 BEST10LEX 10 82.4% 0.22 All Lexical Features 19 86.7% 0.20 BEST10ALL 10 89.7% 0.18 All features 46 93.3% 0.15 Table 5: Classification results for WeeBit Corpus 5.1 Evaluation Metrics We report our results in terms of classification accu- racy and RMSE.", "acronym": "RMSE", "label": "root mean square error", "ID": "5961"}, {"sentence": "Some ap- plications might need components with high re- call, for example; others, high precision or high F- measure or low RMSE.", "acronym": "RMSE", "label": "root mean squared error", "ID": "5962"}, {"sentence": "Root mean squared error of the extrapolated curves at the three anchor sizes The RMSEs obtained by extrap- olating the learning curve are much lower than those obtained by prediction of translation accuracy using the monolingual corpus only (see Table 4), which is expected given that more direct evidence is avail- able in the former case .", "acronym": "RMSE", "label": "root mean squared error", "ID": "5963"}, {"sentence": "Table 6 reports the RMSE at the three anchor sizes from the combined curves.", "acronym": "RMSE", "label": "root mean squared error", "ID": "5964"}, {"sentence": "Initial Points 10K 75K 500K 1K-5K-10K 0.005 0.017 0.042 5K-10K-20K 0.002 0.015 0.034 1K-5K-10K-20K 0.002 0.008 0.019 Table 5: Root mean squared error of the extrapolated curves at the three anchor sizes The RMSEs obtained by extrap- olating the learning curve are much lower than those obtained by prediction of translation accuracy using the monolingual corpus only (see Table 4), which is expected given that more direct evidence is avail- able in the former case .", "acronym": "RMSE", "label": "root mean squared error", "ID": "5965"}, {"sentence": "In Table 5, one can also see that the RMSE for the sets 1K- 5K-10K and 5K-10K-20K are quite close for anchor 9The 10K point is not an extrapolation point but lies within the range of the set of initial points.", "acronym": "RMSE", "label": "root mean squared error", "ID": "5966"}, {"sentence": "In Table 5, one can also see that the RMSE for the sets 1K- 5K-10K and 5K-10K-20K are quite close for", "acronym": "RMSE", "label": "root mean squared error", "ID": "5967"}, {"sentence": "par- allel corpus is available, we introduced an extrapola- tion method and a combined method yielding high- precision predictions: using models trained on up to 20K sentence pairs we can predict performance on a given test set with a RMSE in the order of 1 BLEU point at 75K sentence pairs, and in the order of 2-4 BLEU points at 500K. Consider- ing that variations in the order of 1 BLEU point on a same test dataset can be observed simply due to the instability of the standard MERT parameter tun- ing algorithm (Foster and Kuhn, 2009; Clark et al, 2011), we believe our results to be close to what can be achieve", "acronym": "RMSE", "label": "root mean squared error", "ID": "5968"}, {"sentence": "7.2 Extrapolated Learning Curves As explained in Section 5, we evaluate the accuracy of predictions from the extrapolated curve using the RMSE (see Eq.", "acronym": "RMSE", "label": "root mean squared error", "ID": "5969"}, {"sentence": "This setup leads to a Mean Aver- age Error of 0.62 and a RMSE of 0.78.", "acronym": "RMSE", "label": "Root Mean Squared Error", "ID": "5970"}, {"sentence": "RMS: RMSE, MAE: Mean Absolute Error, R: Correlation Coefficient.", "acronym": "RMSE", "label": "Root Mean Squared Error", "ID": "5971"}, {"sentence": "162  In EFF, we use this probability information to  identify the topic of the segment with the belief  that the topic is more likely to be referred to by  a pronoun.", "acronym": "EFF", "label": "effect", "ID": "5972"}, {"sentence": "Using these base features, we then evaluated the EFFs of feature combinations by repeatedly training the system and selecting feature combinations that in- creased the performance on a heldout set.", "acronym": "EFF", "label": "effect", "ID": "5973"}, {"sentence": "Although this success rate  overstates the EFF, it is a clear indication that  knowledge of a referent's gender and animatic-  ity is essential to anaphora resolution.", "acronym": "EFF", "label": "effect", "ID": "5974"}, {"sentence": "The precision score computed over all phrases  containing any of the target honorifics are 66.0%  l In EFF, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes across different  observations.", "acronym": "EFF", "label": "effect", "ID": "5975"}, {"sentence": "Second, we analyze the EFFs of partic- ular choices we made when building our system, especially the feature combinations and learning methods.", "acronym": "EFF", "label": "effect", "ID": "5976"}, {"sentence": "he EFFs of feature combinations by repeatedly training the system and selecting feature combinations that in- creased the performance on a heldout set.", "acronym": "EFF", "label": "effect", "ID": "5977"}, {"sentence": "in EFF, this means that higher-ranked  constraints have priority in eliminating candidates.", "acronym": "EFF", "label": "effect", "ID": "5978"}, {"sentence": "EFFs of age and gender on blog- ging.", "acronym": "EFF", "label": "Effect", "ID": "5979"}, {"sentence": "TP FP FN P (%) R (%) F1 (%) 1 647 79 143 89.12 81.90 85.36 2 647 80 143 89.00 81.90 85.30 1,2 647 81 143 88.87 81.90 85.24 Table 3: EFFs of removing features (1) or (2), or both Table 3 shows the effect of removing (1), (2), or both (1) and (2), showing that they overfit the training data.", "acronym": "EFF", "label": "Effect", "ID": "5980"}, {"sentence": "On the EFFiveness of the Skew  Divergence for Statistical Language Analysis,  Technical Report, Department of Computer Science,  Cornell University, 2001.", "acronym": "EFF", "label": "Effect", "ID": "5981"}, {"sentence": "EFF of Eliminating Feature Classes  on 10K Training Set  6 MSR Paraphrase Corpus is nearly twice that of  the 10K training set, AER performance is meas- urably degraded.", "acronym": "EFF", "label": "Effect", "ID": "5982"}, {"sentence": "6.2 EFFiveness of Large Corpus  The large corpus of newspapers and the Web are  used effectively in many different cases.", "acronym": "EFF", "label": "Effect", "ID": "5983"}, {"sentence": "The evaluation metrics were precision P (the number of true pos- K TP FP FN P (%) R (%) F1 (%) 10 641 80 149 88.90 81.14 84.84 20 644 79 146 89.07 81.52 85.13 30 644 80 146 88.95 81.52 85.07 40 645 81 145 88.84 81.65 85.09 50 645 80 145 88.97 81.65 85.15 Table 2: EFFs of K in Bayes Point Machines itives divided by the total number of elements la- beled as belonging to the positive class) recall R (the number of true positives divided by the to- tal number of elements that actually belong to the positive class) and their harmonic mean, the F1 score (F1 = 2PR/(P + R)).", "acronym": "EFF", "label": "Effect", "ID": "5984"}, {"sentence": "The set of NEG is then extended with the unlabeled instances classified as negative by h?i.", "acronym": "NEG", "label": "negative instances", "ID": "5985"}, {"sentence": "In this paper, P is dynamically assigned ac- cording to different argument since different heu- ristics could produce different proportion of posi- tive and NEG used to training data.", "acronym": "NEG", "label": "negative instances", "ID": "5986"}, {"sentence": "It can be explained that when  using heuristics, the proportion of positive and  NEG in dataset are adjusted reasona- bly to improve the model.", "acronym": "NEG", "label": "negative instances", "ID": "5987"}, {"sentence": "2.3 Pruning Comparisons During Training A potential drawback of including all the negative examples as in Bengston and Roth (2008) is that the NEG far outnumber the positive ones, which is challenging for training a classifier.", "acronym": "NEG", "label": "negative instances", "ID": "5988"}, {"sentence": "Also a set of NEG is formed by paring the anaphor and each of the intervening candidates.", "acronym": "NEG", "label": "negative instances", "ID": "5989"}, {"sentence": "By contrast, Soon et al(2001) reduce the number of NEG by using only mentions between the mention and its closest coreferent pair as neg- ative examples.", "acronym": "NEG", "label": "negative instances", "ID": "5990"}, {"sentence": "For instance, the number of constituents labeled to  arguments (positive instances) is much less than  the number of the rest (NEG).", "acronym": "NEG", "label": "negative instances", "ID": "5991"}, {"sentence": "Four problems render vector space model  (VSM)-based text classification approach in- effective: 1) Many words within song lyrics  actually contribute little to sentiment; 2)  Nouns and verbs used to express sentiment are  ambiguous; 3) NEG and modifiers  around the sentiment keywords make particu- lar contributions to sentiment; 4) Song lyric is  usually very short.", "acronym": "NEG", "label": "Negations", "ID": "5992"}, {"sentence": "NEG and modifiers are helpful to  determine the unique meaning of the sentiment  words within certain context window, e.g. 3 pre- ceding words and 3 succeeding words in our case.", "acronym": "NEG", "label": "Negations", "ID": "5993"}, {"sentence": "(3) NEG and modifiers are included in the s- VSM model to reflect the functions of invers- ing, strengthening and weakening.", "acronym": "NEG", "label": "Negations", "ID": "5994"}, {"sentence": "NEG  Because of  2  T3= G l l , l l (p lq l ) (p lq l )~ r-~q 1  410  the 2-placed FV G 2 - 11,11 can be interpreted ee   presupposition-rejecting negation.", "acronym": "NEG", "label": "Negations", "ID": "5995"}, {"sentence": "System Description Precision Relevance  Without DIRT 0.6876 0.54 %  Without WordNet 0.6800 1.63 %  Without Acronyms 0.6838  1.08 %  Without BK 0.6775 2.00 %  Without NEG 0.6763 2.17 %  Without NEs 0.5758 16.71 %  Table 4: Components relevance  7 Conclusions  The system?s core algorithm is based on the tree  edit distance approach, however, focused on trans- forming the hypothesis.", "acronym": "NEG", "label": "Negations", "ID": "5996"}, {"sentence": "Length of the undirected path between the two 3.3.2 NEG There are no sentences in our corpus with more than one negation.", "acronym": "NEG", "label": "Negations", "ID": "5997"}, {"sentence": "In the same annotation process we have identi- fied NLP problems in these passages which must be solved to identify the facts correctly including: syn- onym and hyponym substitution, coreference reso- lution, NEG handling, and the incorporation of knowledge from within the full text and the domain.", "acronym": "NEG", "label": "negation", "ID": "5998"}, {"sentence": "We show that this protein is phosphorylated during mitosis in human cells and that this requires active cdc2-cyclin B. (Intro) Table 5: Example instances with cataphora and event anaphora 6 Negated Expressions To quantify the importance of lexical and logical NEGs we have annotated each instance involv- ing one or more negated expressions that must be resolved to derive the fact.", "acronym": "NEG", "label": "negation", "ID": "5999"}, {"sentence": "ksi \ba\u0001zka), two neuter genders n1 (dziecko), n2 (okno), and three plurale tantum genders p1 (wujostwo), p2 (drzwi), p3 (okulary); \u0000 person: pri , sec , ter; \u0000 degree: pos , comp , sup; \u0000 aspect : imperf , perf ; \u0000 NEG: aff , neg; \u0000 accentability (Pol.:", "acronym": "NEG", "label": "negation", "ID": "6000"}, {"sentence": "Finally, for task 3 participants were asked to extract NEGs and speculations regarding events.", "acronym": "NEG", "label": "negation", "ID": "6001"}, {"sentence": "The current libraries include anal- yses of major constituent word order (SOV, SVO, etc), sentential NEG, coordination, and yes-no question formation.", "acronym": "NEG", "label": "negation", "ID": "6002"}, {"sentence": "Lexical adverbs, including manner, time, and loca- tion, and adverbs of NEG, which vary by clause type (declarative, imperative, or interrogative) ?", "acronym": "NEG", "label": "negation", "ID": "6003"}, {"sentence": "For com- puting the counts of positive and NEG words (Feature 15 and 16) we used the General Inquirer database (Stone et al.,", "acronym": "NEG", "label": "negative", "ID": "6004"}, {"sentence": "pronouns 12 count of quote tokens 13 count of 1st person plural pronouns 14 count of 2nd person singular pronouns 15 count of quote positive words 16 count of quote NEG words 17 count of nouns 18 count of verbs 19 count o", "acronym": "NEG", "label": "negative", "ID": "6005"}, {"sentence": "NEG?,", "acronym": "NEG", "label": "negative", "ID": "6006"}, {"sentence": "In order to align the existing annotations to our three-class scheme the following mapping5 was adopted: (i) AN, DI, FE, SA were mapped to NEG affect, (ii) NE was mapped to neutral affect, and (iii) HA was mapped to positive affect.", "acronym": "NEG", "label": "negative", "ID": "6007"}, {"sentence": "pronouns 12 count of quote tokens 13 count of 1st person plural pronouns 14 count of 2nd person singular pronouns 15 count of quote positive words 16 count of quote NEG words 17 count of nouns 18 count of verbs 19 count of adjectives 20 count of adverbs 21 up to 3-grams extracted from quote Table 1: Common feature set.", "acronym": "NEG", "label": "negative", "ID": "6008"}, {"sentence": "NEG surprise? (", "acronym": "NEG", "label": "negative", "ID": "6009"}, {"sentence": "4 count of 1st person singular pronouns 5 count of NEG particles 6 count of numbers 7 count of prepositions 8 count of pronouns 9 count of ?", "acronym": "NEG", "label": "negative", "ID": "6010"}, {"sentence": "A main characteristic of question answering in restricted domains is the integration of domain-specific information that is either developed for question answering or that has been developed for other PRPs.", "acronym": "PRP", "label": "purpose", "ID": "6011"}, {"sentence": "As in the previous two tasks 10FCV was applied for evaluation PRPs.", "acronym": "PRP", "label": "purpose", "ID": "6012"}, {"sentence": "Whereas structured knowledge-based QA systems are well adapted to applications managing complex queries in a very structured information environment, the kind of research developed in TREC, CLEF, and NTCIR is probably better suited to broad-PRP generic applications dealing with simple factual ques- tions such as World Wide Web?based question answering.", "acronym": "PRP", "label": "purpose", "ID": "6013"}, {"sentence": "One of the PRPs of the paper is to test whether this hypothesis is right.", "acronym": "PRP", "label": "purpose", "ID": "6014"}, {"sentence": "WordNet-based approaches are unsuitable for our PRPs as they only model so-called ?", "acronym": "PRP", "label": "purpose", "ID": "6015"}, {"sentence": "Alternatively, having made a selection of texts, one may opt to be presented with a word cloud of its most salient terms, for exploratory PRPs.", "acronym": "PRP", "label": "purpose", "ID": "6016"}, {"sentence": "To do this, we treat  the omitted NP as an anaphor which, like Sidner's  treatment of full definite NP's and PRP  nouns, co-specifies an element recorded by the fo-  cusing algorithm.", "acronym": "PRP", "label": "personal pro-", "ID": "6017"}, {"sentence": "Pronouns,  Types and  Tokens  Incidence of PRP nouns, number of pronouns  per noun phrase, types and  tokens.", "acronym": "PRP", "label": "personal pro-", "ID": "6018"}, {"sentence": "These features have similar weights to the character n-gram features and for the most part seem to represent ungrammatical constructions (e.g., the first feature indicates that a PRP noun followed by an uninflected verb predicts Chi- nese).", "acronym": "PRP", "label": "personal pro-", "ID": "6019"}, {"sentence": "Impersonal verbs with obligatory short ac- cusative personal pronoun, short dative per- sonal pronoun or short dative PRP noun + se (e.g., marzi me ?", "acronym": "PRP", "label": "personal pro-", "ID": "6020"}, {"sentence": "\u0000 In transitive VPCs unstressed PRP nouns must precede the particle (e.g. They ate it up but not *They ate up it).", "acronym": "PRP", "label": "personal pro-", "ID": "6021"}, {"sentence": "Based on the correctness of scratch pad annotations aggregated over several translation ex- ercises, the system gives feedback in the form of a simple message, such as King Alfred is pleased with your work on strong nouns and PRP nouns, or King Alfred suggests that you should re- view weak verbs.", "acronym": "PRP", "label": "personal pro-", "ID": "6022"}, {"sentence": "13ul PRP, in par-  ticular 1st and 2nd person pronouns, also show  a much higher frequency, and this can hardly be  attrihutcd to subject matter, rather to different  communicative functi(ms of feuillet(mistie writ-  ing and say economic news.", "acronym": "PRP", "label": "personal pronouns", "ID": "6023"}, {"sentence": "From both corpora, in order to use  \"'semantically relevant\" tokens for the HMM bigrams,  we retained all nouns, verbs, adverbs, and adjectives and  deleted all function words except prepositions, commas,  final stops, PRP and interrogative adverbs.", "acronym": "PRP", "label": "personal pronouns", "ID": "6024"}, {"sentence": "Verbs whose subjects are common nouns account for 57.8% of all verbs that have subjects (verbs with different types of subjects, most of which are PRP, are not considered here, since these subjects are not part of the noun classifier).", "acronym": "PRP", "label": "personal pronouns", "ID": "6025"}, {"sentence": "Two stage classifier with First-person pro- nouns + Seed words/trigrams (FP+SE2): A 5 PRP, 3rd person singular words, family words, human words, sexual words, etc 1991 Method Acc G F 1 M F 1 H F 1 Avg F 1 LDA 49.2 0.00 0.65 0.05 0.23 MedLDA 43.3 0.41 0.52 0.09 0.34 LIWC 49.2 0.34 0.61 0.18 0.38 BOW+ 54.1 0.50 0.59 0.15 0.41 SEED 54.4 0.52 0.60 0.14 0.42 ASUM 56.6 0.32 0.70 0.38 0.47 SDTM?", "acronym": "PRP", "label": "personal pronouns", "ID": "6026"}, {"sentence": "Noun phrases are proper nouns and PRP.", "acronym": "PRP", "label": "personal pronouns", "ID": "6027"}, {"sentence": "PRP?i.e.,", "acronym": "PRP", "label": "personal pronouns", "ID": "6028"}, {"sentence": "Note that this model is DIR: Each target word (observation) can be aligned to at most one source word (hidden state), whereas a source word could be used multiple times.", "acronym": "DIR", "label": "directional", "ID": "6029"}, {"sentence": "However, we show that it is much better to train two DIR models concurrently, coupling their posterior distributions over alignments to approximately agree.", "acronym": "DIR", "label": "directional", "ID": "6030"}, {"sentence": "We propose two such constraints: (i) bijectivity: one word should not translate to many words; and (ii) symmetry: DIR alignments should agree.", "acronym": "DIR", "label": "directional", "ID": "6031"}, {"sentence": "There are several problems with the model that arise from its DIRity, however.", "acronym": "DIR", "label": "directional", "ID": "6032"}, {"sentence": "This leads to the common practice of post-processing heuristics for intersecting DIR alignments to produce nearly bijective and symmetric results (Koehn, Och, and Marcu 2003).", "acronym": "DIR", "label": "directional", "ID": "6033"}, {"sentence": "Let the DIR models be defined as: ??", "acronym": "DIR", "label": "directional", "ID": "6034"}, {"sentence": "3.4 Symmetry Constraints The DIR nature of the generative models used to recover word alignments con- flicts with their interpretation as translations.", "acronym": "DIR", "label": "directional", "ID": "6035"}, {"sentence": "Define z to range over the union of all possible 491 Computational Linguistics Volume 36, Number 3 DIR alignments ??", "acronym": "DIR", "label": "directional", "ID": "6036"}, {"sentence": "CO-   Note that a(w, w') has DIR (from w to w'), so  that a(w, w') may not be equal to a(w', w):  a(films, theatre) = 0.178988 ,  o ( theat re ,  films) ---- 0.068927.", "acronym": "DIR", "label": "direction", "ID": "6037"}, {"sentence": "The START multimedia information system: Current technology and future DIRs.", "acronym": "DIR", "label": "direction", "ID": "6038"}, {"sentence": "We minimize E and find optimum locations for points separating each segment us- ing Powell?s conjugate DIR method, deter- mined the most effective for this task.", "acronym": "DIR", "label": "direction", "ID": "6039"}, {"sentence": "The present paper at-  tempts to be a small step in this DIR.", "acronym": "DIR", "label": "direction", "ID": "6040"}, {"sentence": "DIR?.", "acronym": "DIR", "label": "direction", "ID": "6041"}, {"sentence": "We replace this radical to a placeholder and generate a candidate rule with the corresponding DIR by the radical position in this character.", "acronym": "DIR", "label": "direction", "ID": "6042"}, {"sentence": "References:  I. \"A Theory of Linguistic MDls MEANING -- TEXT\", Moscow,  1972 (in Russian).", "acronym": "MD", "label": "Mode", "ID": "6043"}, {"sentence": "2 A Probabi l i s t ic  MD l   There are many factors, both syntactic and se-  mantic, upon which a pronoun resolution sys-  tem relies. (", "acronym": "MD", "label": "Mode", "ID": "6044"}, {"sentence": "to apply to either a structure y or its parts r. The Markov assumption for factoring labels lets us use the Viterbi algorithm (much like a Hidden Markov MDl) in order to find y = argmaxy? (", "acronym": "MD", "label": "Mode", "ID": "6045"}, {"sentence": "Frey B.J. 1998, Graphical MDls for Machine  Learning and Digital Communication,  Cambridge, MA, MIT Press  Gildea D., Jurafsky D. 2002, Automatic labeling of  semantic roles, Computational Linguistics,  28(3):245-288.", "acronym": "MD", "label": "Mode", "ID": "6046"}, {"sentence": "MDrate performance was achieved for the QUOTES3 and QUOTES4 datasets, 0.426 and 0.411, respectively.", "acronym": "MD", "label": "Mode", "ID": "6047"}, {"sentence": "In this case  Probability  MDl  P(dH)  P(plwa)  P(w lh, t,l)  P(alm.)", "acronym": "MD", "label": "Mode", "ID": "6048"}, {"sentence": "Simulation-based RL allows to explore unseen actions which are not in the data, and thus less initial data is needed (MD and Lemon, 2008b).", "acronym": "MD", "label": "Rieser", "ID": "6049"}, {"sentence": "Verena MD and Oliver Lemon.", "acronym": "MD", "label": "Rieser", "ID": "6050"}, {"sentence": "689 We are currently collecting data in targeted Wizard-of-Oz experiments, to derive a fully data- driven training environment and test the learnt policy with real users, following (MD and Lemon, 2008b).", "acronym": "MD", "label": "Rieser", "ID": "6051"}, {"sentence": "For example, presenting differing numbers of at- tributes to the user, and making the user more or less likely to choose an item, as shown by (MD and Lemon, 2008b) for multimodal interaction.", "acronym": "MD", "label": "Rieser", "ID": "6052"}, {"sentence": "In extending this previous work we treat NLG as a statistical sequential planning problem, anal- ogously to current statistical approaches to Dia- logue Management (DM), e.g. (Singh et al, 2002; Henderson et al, 2008; MD and Lemon, 2008a) and ?", "acronym": "MD", "label": "Rieser", "ID": "6053"}, {"sentence": "c?2009 Association for Computational Linguistics Natural Language Generation as Planning Under Uncertainty for Spoken Dialogue Systems Verena MD School of Informatics University of Edinburgh vrieser@inf.ed.ac.uk Oliver Lemon School of Informatics University of Edinburgh olemon@inf.ed.ac.uk Abstract We present and evaluate a new model for Natural Language Generation (NLG) in Spoken Dialogue Systems, based on statis- tical planning, given noisy feedback from the current generation context (e.g. a user and a surface realiser).", "acronym": "MD", "label": "Rieser", "ID": "6054"}, {"sentence": "A maximum entropy/MD translation model.", "acronym": "MD", "label": "minimum divergence", "ID": "6055"}, {"sentence": "If we wish to find the estimate which has MD to an infinite distribution P (T ), we use the same for- mula, but the counts become expected counts: P (X ?", "acronym": "MD", "label": "minimum divergence", "ID": "6056"}, {"sentence": "In alternative, if q 0 is not uniform then p is called a MD model (according to (Berger and Printz, 1998)).", "acronym": "MD", "label": "minimum divergence", "ID": "6057"}, {"sentence": "While a uni- form prior on the set of futures results in a max- imum entropy model, choosing other priors out- put a MD models.", "acronym": "MD", "label": "minimum divergence", "ID": "6058"}, {"sentence": "A comparison of criteria for maximum entropy / MD feature selec- tion.", "acronym": "MD", "label": "minimum divergence", "ID": "6059"}, {"sentence": "Incorporating position informa- tion into a maximum entropy/MD translation model.", "acronym": "MD", "label": "minimum divergence", "ID": "6060"}, {"sentence": "MD.", "acronym": "MD", "label": "Mutual dependency", "ID": "6061"}, {"sentence": "And  there  are  three  types  o f  un i f i ca -   t ions  as  fo l lows ;   *MD: compos i t ion  o f  t ran -   s i t i ve  verb  w i th  d i rec t  ob jec t ,  and  com-   pos i t ion  o f  d i rec t  ob jec t  w i th  ind i rec t   ob jec t .", "acronym": "MD", "label": "Mutual dependency", "ID": "6062"}, {"sentence": "3.6 MD Elements of a mutual dependency set are mutually confirming.", "acronym": "MD", "label": "Mutual dependency", "ID": "6063"}, {"sentence": "MDity and Time features have been added in order to implement fusion strategies at dialogue level.", "acronym": "MD", "label": "Modal", "ID": "6064"}, {"sentence": "3 Improving Multi-MD Representations Figure 1 illustrates how our system computes multi-modal semantic representations.", "acronym": "MD", "label": "Modal", "ID": "6065"}, {"sentence": "Improving Multi-MD Representa- tions Using Image Dispersion: Why Less is Some- times More.", "acronym": "MD", "label": "Modal", "ID": "6066"}, {"sentence": "c?2014 Association for Computational Linguistics Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-MD Semantics Douwe Kiela ?", "acronym": "MD", "label": "Modal", "ID": "6067"}, {"sentence": "2 Related work 2.1 Multi-MD Distributional Semantics Multi-modal models are motivated by parallels with human concept acquisition.", "acronym": "MD", "label": "Modal", "ID": "6068"}, {"sentence": "The codes characterize some  inherent features (such as \"MD\"), control proper?", "acronym": "MD", "label": "Modal", "ID": "6069"}, {"sentence": "Reranking and SELF for parser adaptation.", "acronym": "SELF", "label": "self-training", "ID": "6070"}, {"sentence": "Of course, the SELF strategy is orthogonal to the improvements we have made.", "acronym": "SELF", "label": "self-training", "ID": "6071"}, {"sentence": "In this paper we use SELF", "acronym": "SELF", "label": "self-training", "ID": "6072"}, {"sentence": "They have success- fully played in three types of constraints for our experiments: PR penalty (Our model), decoding constraints in SELF (STS) and virtual evi- dences (VES).", "acronym": "SELF", "label": "self-training", "ID": "6073"}, {"sentence": "SELF?)", "acronym": "SELF", "label": "self-training", "ID": "6074"}, {"sentence": "This is the first time that SELF with small labeled datasets is applied suc- cessfully to these tasks.", "acronym": "SELF", "label": "self-training", "ID": "6075"}, {"sentence": "Verbal imagery is think-  ing ~In words or SELF, while visual imagery is thinking  in p~ ctures.", "acronym": "SELF", "label": "talking to oneself", "ID": "6076"}, {"sentence": "In Proceedings of the 17th International Congress on Computer Assisted RAD Surgery (CARS), pages 299?304, London, UK, June.", "acronym": "RAD", "label": "Radiology and", "ID": "6077"}, {"sentence": "1The captions were extracted from RAD Ra- diographics c?", "acronym": "RAD", "label": "Radiology and", "ID": "6078"}, {"sentence": "For British-English a laboratory version of the  interactive mode is available for applications in RAD  Pathology.", "acronym": "RAD", "label": "Radiology and", "ID": "6079"}, {"sentence": "c?2007 Association for Computational Linguistics Machine Learning Based Semantic Inference: Experiments and Ob- servations at RTE-3  Baoli Li1, Joseph Irwin1, Ernest V. Garcia2, and Ashwin Ram1    1 College of Computing  Georgia Institute of Technology  Atlanta, GA 30332, USA  baoli@gatech.edu  gtg519g@mail.gatech.edu  ashwin@cc.gatech.edu    2 Department of RAD  School of Medicine, Emory University  Atlanta, GA 30322, USA  Ernest.Garcia@emoryhealthcare.org    Abstract  Textual Entailment Recognition is a se- mantic inference task that is required in  many natural language processing (NLP)  applications.", "acronym": "RAD", "label": "Radiology", "ID": "6080"}, {"sentence": "A Machine Learning Approach for Identi- fying Anatomical Locations of Actionable Findings in RAD Reports.", "acronym": "RAD", "label": "Radiology", "ID": "6081"}, {"sentence": "Professional Lan- guage in Swedish RAD Reports ?", "acronym": "RAD", "label": "Radiology", "ID": "6082"}, {"sentence": "A Novel Hybrid  Approach to Automated Negation Detection in Clin- ical RAD Reports.", "acronym": "RAD", "label": "Radiology", "ID": "6083"}, {"sentence": "The British Journal of RAD, 73(873):999?1001.", "acronym": "RAD", "label": "Radiology", "ID": "6084"}, {"sentence": "Improved Identification of Noun Phrases in Clinical RAD Reports Using a High-Performance Statistical Natural Language Parser Augmented with the UMLS Specialist Lex- icon.", "acronym": "RAD", "label": "Radiology", "ID": "6085"}, {"sentence": "La- tent semantic WSI and disambigua- tion.", "acronym": "WSI", "label": "word sense induction", "ID": "6086"}, {"sentence": "Samuel Brody  et al (2009) adopt a novel Bayesian approach  and formalize the WSI problem  in a generative model.", "acronym": "WSI", "label": "word sense induction", "ID": "6087"}, {"sentence": "The goal of this paper is to inves- tigate different options available to crowdsource a  clustering task and evaluate their efficiency in the  concrete application of WSI.", "acronym": "WSI", "label": "word sense induction", "ID": "6088"}, {"sentence": "Semeval-2007 task 02: evaluating WSI and discrim- ination systems.", "acronym": "WSI", "label": "word sense induction", "ID": "6089"}, {"sentence": "In work on WSD and other tasks related to pol- ysemy, such as WSI, sense alter- nations are treated as word-specific.", "acronym": "WSI", "label": "word sense induction", "ID": "6090"}, {"sentence": "The results from this tech- nique on our WSI problem are  shown in the next section.", "acronym": "WSI", "label": "word sense induction", "ID": "6091"}, {"sentence": "Figure 2: The general WSI Model: models extract distributional data from contexts and induce senses by clustering the extracted informa- tion.", "acronym": "WSI", "label": "Word Sense Induction", "ID": "6092"}, {"sentence": "pervised systems known as Ensemble Clustering to unsupervised NLP systems and focus on the fully unsupervised task of WSI.", "acronym": "WSI", "label": "Word Sense Induction", "ID": "6093"}, {"sentence": "We propose applying a new and more gen- eral framework for combining unsupervised systems known as Ensemble Clustering to unsupervised NLP systems and focus on the fully unsupervised task of WSI.", "acronym": "WSI", "label": "Word Sense Induction", "ID": "6094"}, {"sentence": "We propose evaluating various unsupervised en- sembles when applied to the unsupervised task of WSI with a framework for combining diverse feature spaces and cluster- ing algorithms.", "acronym": "WSI", "label": "Word Sense Induction", "ID": "6095"}, {"sentence": "The task of WSI extends the problem of Word Sense Disambiguation by simply assuming that a model must first learn and define a sense inventory before disambiguating multi-sense words.", "acronym": "WSI", "label": "Word Sense Induction", "ID": "6096"}, {"sentence": "Since WSI is fundamentally a clustering problem, with many vari- ations, it serves well as a NLP case study for Ensem- ble Clustering.", "acronym": "WSI", "label": "Word Sense Induction", "ID": "6097"}, {"sentence": "3 WSI Models WSI models define word senses in terms of the distributional hypothesis, whereby the meaning of a word can be defined by the surround- ing", "acronym": "WSI", "label": "Word Sense Induction", "ID": "6098"}, {"sentence": "Since WSI is fundamentally a clustering problem, with many vari- ations, it serves well as a NLP case study", "acronym": "WSI", "label": "Word Sense Induction", "ID": "6099"}, {"sentence": "c?2012 Association for Computational Linguistics Evaluating Unsupervised Ensembles when applied to WSI Keith Stevens1,2 1University of California Los Angeles; Los Angeles , California, USA 2Lawrence Livermore National Lab; Livermore, California, USA?", "acronym": "WSI", "label": "Word Sense Induction", "ID": "6100"}, {"sentence": "In the present settings, only 42 of the 95 French source words remained, 38 of which kept exactly one En- glish candidate; among these, 27 are the ET, and 1 is an adjective derived from the ET (estomac/gastric).", "acronym": "ET", "label": "expected translation", "ID": "6101"}, {"sentence": "For the evaluation, we computed the rank of the ET of each test word and syn- thesized them as a percentile rank distribution.", "acronym": "ET", "label": "expected translation", "ID": "6102"}, {"sentence": "Instead of re- ranking the translations provided by the component systems, we search for the hypothesis with the min- imum ET error among all the pos- sible finite-length strings in the target language.", "acronym": "ET", "label": "expected translation", "ID": "6103"}, {"sentence": "For instance, for the French word chirurgie whose ET is surgery, we have as top ranked words pain, breast, desmoplasia, pro- cedure, metastatic..., and for m?decine (medicine), we have information, clinician, article, medical.... For common words like, e.g., analyse/analysis and sang/blood, we have girdle, sample, statistic... for analysis and output, collection, calorimetry... for blood as best ranked translations.", "acronym": "ET", "label": "expected translation", "ID": "6104"}, {"sentence": "The ET  quality score for the phrase pair (e,f) is defined as  )|,()|,(),( ** effBleufeeBleufeB +=  (8)  where *e is the human translation of the source  phrase f, and *f is the human translation of the  target phrase e. These human translations are  obtained from hand alignment of some parallel  sentences.", "acronym": "ET", "label": "expected translation", "ID": "6105"}, {"sentence": "We then pro- duced a ranked list of the top translational equiv- alents and tested whether the ET can be differentiated from other well-known domain words.", "acronym": "ET", "label": "expected translation", "ID": "6106"}, {"sentence": "Coor- dinating ETs are commata and coordinating conjunctions.", "acronym": "ET", "label": "element", "ID": "6107"}, {"sentence": "Hence  P(pla, W) = P(plw,~)  Since I~\" is a vector, we need to normal-  ize P(ff'lh, t,l, a) to obtain the probability of  each ET in the vector.", "acronym": "ET", "label": "element", "ID": "6108"}, {"sentence": "The representation for sentence 1 states that the first ET of the 5-gram (-3; third word to the left of the adjective) is empty (because the second ET is a phrase boundary marker), that the sec- ond ET is a clause delimiter (conjunction that), the third one (-1; word preceding the adjective) is a definite determiner, and the fourth one (+1; word following the adjective) is a common noun.", "acronym": "ET", "label": "element", "ID": "6109"}, {"sentence": "The evaluation metrics were precision P (the number of true pos- K TP FP FN P (%) R (%) F1 (%) 10 641 80 149 88.90 81.14 84.84 20 644 79 146 89.07 81.52 85.13 30 644 80 146 88.95 81.52 85.07 40 645 81 145 88.84 81.65 85.09 50 645 80 145 88.97 81.65 85.15 Table 2: Effects of K in Bayes Point Machines itives divided by the total number of ETs la- beled as belonging to the positive class) recall R (the number of true positives divided by the to- tal number of ETs that actually belong to the positive class) and their harmonic mean, the F1 score (F1 = 2PR/(P + R)).", "acronym": "ET", "label": "element", "ID": "6110"}, {"sentence": "5.4 Global and Local Reordering Model In order to show the advantages of explicitly mod- eling global phrase reordering, we implemented a different reordering model where the reordering pattern is classified into three values: monotone adjacent, RA and neutral.", "acronym": "RA", "label": "reverse adjacent", "ID": "6111"}, {"sentence": "On two corpora, namely, Medstract and the  new 100-Medline corpus, 100% RA was achieved.", "acronym": "RA", "label": "recognition accuracy", "ID": "6112"}, {"sentence": "All three methods produce similar speech RA.", "acronym": "RA", "label": "recognition accuracy", "ID": "6113"}, {"sentence": "The use of MFCDCN and SCA  improves RA by 35.0 percent overall, and the  ratio of overaU error rates for the C2 and P0 conditions i  1.49, as  in Spoke 5.", "acronym": "RA", "label": "recognition accuracy", "ID": "6114"}, {"sentence": "Mildly and moderately dysarthric speakers can attain a RA of 80% in dictation systems, breath exercises and phonation training improve performance (Young and Mihai- lidis, 2010).", "acronym": "RA", "label": "recognition accuracy", "ID": "6115"}, {"sentence": "2 we describe these compensation procedures in detail,  and we examine their effect on RA in Secs.", "acronym": "RA", "label": "recognition accuracy", "ID": "6116"}, {"sentence": "PERFORMANCE OF ALGORITHMS IN  DEVELOPMENTAL TESTING  In this and the following section we describe the results of a series  of experiments that compare the RA of the vari-  ous algorithms described in Sec.", "acronym": "RA", "label": "recognition accuracy", "ID": "6117"}, {"sentence": "By  comparing the C2 and C3 results using the CLSTLK microphone  with the P0 and S1 results using the Audio-Technica mic, we note  that very little degradation is observed in the 20-dB condition but  that RA is quite low for the 0-riB condition, even  when the signal is compensated.", "acronym": "RA", "label": "recognition accuracy", "ID": "6118"}, {"sentence": "By collaps- ing MG and reverse gap into neutral, it can be thought of as a local reordering model sim- ilar to the block orientation bigram (Tillmann and Zhang, 2005).", "acronym": "MG", "label": "monotone gap", "ID": "6119"}, {"sentence": "Since non-local reorderings such as MG and reverse gap are more frequent in Japanese to English translations, they are worth modeling ex- plicitly in this reordering model.", "acronym": "MG", "label": "monotone gap", "ID": "6120"}, {"sentence": "c?2013 Association for Computational Linguistics Distributions on MG Derivations Tim Hunter Department of Linguistics Cornell University 159 Central Ave.,", "acronym": "MG", "label": "Minimalist Grammar", "ID": "6121"}, {"sentence": "On Formal Properties of MGs.", "acronym": "MG", "label": "Minimalist Grammar", "ID": "6122"}, {"sentence": "Pittsburgh, PA, 15213 cdyer@cs.cmu.edu Abstract We present three ways of inducing proba- bility distributions on derivation trees pro- duced by MGs, and gi", "acronym": "MG", "label": "Minimalist Grammar", "ID": "6123"}, {"sentence": "MGs are feature-driven, meaning features of lexical items determine which operations can occur and when.", "acronym": "MG", "label": "Minimalist Grammar", "ID": "6124"}, {"sentence": "4 MGs If correct, the ERH would explain the increasing difficulty across the AH in terms of greater or lesser uncertainty about intermediate parser states.", "acronym": "MG", "label": "Minimalist Grammar", "ID": "6125"}, {"sentence": "A MG is a five- tuple G = ??,", "acronym": "MG", "label": "Minimalist Grammar", "ID": "6126"}, {"sentence": "Pittsburgh, PA, 15213 cdyer@cs.cmu.edu Abstract We present three ways of inducing proba- bility distributions on derivation trees pro- duced by MGs, and give their maximum likelihood estimators.", "acronym": "MG", "label": "Minimalist Grammar", "ID": "6127"}, {"sentence": "nguistics Distributions on MG Derivations Tim Hunter Department of Linguistics Cornell University 159 Central Ave.,", "acronym": "MG", "label": "Minimalist Grammar", "ID": "6128"}, {"sentence": "purpose  of enhancing the recall rate, FASTR includes a  MG used to generate term variant rules from  term rules.", "acronym": "MG", "label": "metagrammar", "ID": "6129"}, {"sentence": "The MG goes multilin- gual: A cross-linguistic look at the V2-phenomenon.", "acronym": "MG", "label": "metagrammar", "ID": "6130"}, {"sentence": "In this experiment, he MG consists of  positive paradigmatic metarules (e.g. (11)) and filtering  negative metarules rejecting the spurious variations  extracted by the positive ones (e.g. (12)).", "acronym": "MG", "label": "metagrammar", "ID": "6131"}, {"sentence": "These for- malisms take the MG approach, where the basic units are tree descriptions (i.e., formulas denoting sets of trees) rather than trees.", "acronym": "MG", "label": "metagrammar", "ID": "6132"}, {"sentence": "With the purpose  of enhancing the recall rate, FASTR includes a  MG used to generate term variant rules from  term rules.", "acronym": "MG", "label": "metagrammar", "ID": "6133"}, {"sentence": "(2) Rule : NI ---> (N2 ---> N3 N4) N 5  <N I label> = 'XRD'  <N I metaLabel> = 'XX'  <N I lexicalization> = 'Ns'  <N 3 lenuna> = 'X'  <N 3 inflection> = 7  <N 4 lemma> = 'ray'  <N 4 inflection> = I  <Ns lemma> = 'diffraction'  <N5 inflection> = I.  The third level of the formalism consists of a  MG.", "acronym": "MG", "label": "metagrammar", "ID": "6134"}, {"sentence": "The formalism of FASTR is organized into three  levels : a single word lexicon, a terminological grammar  and a MG for term variations.", "acronym": "MG", "label": "metagrammar", "ID": "6135"}, {"sentence": "This has implications for role text containing appositives, the second situation in which a comma appears within a single RS.", "acronym": "RS", "label": "role span", "ID": "6136"}, {"sentence": "Semantic roles                                                    6 http://nlp.cs.berkeley.edu/Main.html  799 can cover sentential constituents of arbitrary  length, and simply using word alignments for  projection is likely to result in wrong RSs.", "acronym": "RS", "label": "role span", "ID": "6137"}, {"sentence": "Note that there is no obvious random baseline for the complex task of predicting RSs and their labels, however.", "acronym": "RS", "label": "role span", "ID": "6138"}, {"sentence": "We observe that if a RS does include a predicate, resulting questions are often ungrammatical due to the con- jugation of that predicate.", "acronym": "RS", "label": "role span", "ID": "6139"}, {"sentence": "SVM classifiers were trained 2 with the LIBLINEAR library (Fan et al, 2008) and learned to predict the frame name, RSs, and role labels.", "acronym": "RS", "label": "role span", "ID": "6140"}, {"sentence": "We believe that our current work reduces the barrier for semantic MT evaluation for RS languages sufficiently so that semantic MT evaluation can be applied to most other languages.", "acronym": "RS", "label": "resource scarce", "ID": "6141"}, {"sentence": "The re- quirement of a large training corpus renders these  algorithms unsuitable for RS languag- es.", "acronym": "RS", "label": "resource scarce", "ID": "6142"}, {"sentence": "Introduction Development of language applications for local languages in Africa requires innovative approaches since many of these languages are RS.", "acronym": "RS", "label": "resource scarce", "ID": "6143"}, {"sentence": "duate School,  Harbin Institute of Technology, Shenzhen 518055  2Department Of Computing, the Hong Kong Polytechnic University  guilin.nlp@gmail.com, xuruifeng@hitsz.edu.cn, csluqin@comp.polyu.edu.hk, xujun@hitsz.edu.cn,  csjxu@comp.polyu.edu.hk,{bliu,wangxl}@insun.hit.edu.cn    Abstract  Transfer learning has been used in opin- ion analysis to make use of available lan- guage resources for other RS  languages.", "acronym": "RS", "label": "resource scarce", "ID": "6144"}, {"sentence": "How- ever, since our work focuses on RS lan- guages we did not want to incur the additional cost of using a development set.", "acronym": "RS", "label": "resource scarce", "ID": "6145"}, {"sentence": "In this paper we study the challenges of  named entity recognition for RS  languages among South Asian languages.", "acronym": "RS", "label": "resource scarce", "ID": "6146"}, {"sentence": "RS.", "acronym": "RS", "label": "Reference Standard", "ID": "6147"}, {"sentence": "where qtf is the frequency of term t in the query q, and w(t,d)  is the RS of a document d for the query term t,  given by:  ) 5.0 1log() )1( 1(),( 2 + +??", "acronym": "RS", "label": "relevance score", "ID": "6148"}, {"sentence": "Following (Zhai, 2008), the RS between question q and user u can be estimated by the negative Kullback-Leibler divergence between ?", "acronym": "RS", "label": "relevance score", "ID": "6149"}, {"sentence": "Statisti- cal methods for calculating the RS of each fragment can be categorized into sev- eral classes: cue-based (Edmundson, 1969), key- word- or frequency-based (Luhn, 1958; Edmund- son, 1969; Neto et al, 2000; Steinberger and Jezek, 2004; Kallel et al, 2004; Vanderwende et al.,", "acronym": "RS", "label": "relevance score", "ID": "6150"}, {"sentence": "The RS for a sentence is calculated as a sum of its domination scores over all paths.", "acronym": "RS", "label": "relevance score", "ID": "6151"}, {"sentence": "Using the In_expC2 model, the RS of a  document d for a query q is given by the formula:                    (3) ?", "acronym": "RS", "label": "relevance score", "ID": "6152"}, {"sentence": "interests, which is helpful to derive RSs.", "acronym": "RS", "label": "relevance score", "ID": "6153"}, {"sentence": "1% 59.5% 36.3%     + Proper Nouns 36.5% 56.8% 44.4%  Named Entities 48.4% 49.1% 48.7%  All Combined 21.1% 65.0% 31.9%  Manual Scoring 67.0% 75.0% 70.8%      Single word SVM 19.0% 30.0% 23.3%  + Stemming 22.0% 30.2% 25.5%     + Proper Nouns 46.3% 54.0% 49.9%  Named Entities 60.1% 41.5% 49.1%  All Combined 20.3% 65.7% 31.0%  Manual Scoring 47.0% 62.0% 53.5%  Figure 1: Results on Daily Kos (top) and RS  (bottom) data.", "acronym": "RS", "label": "Red State", "ID": "6154"}, {"sentence": "dailykos.com) and RS  (www.redstate.com).", "acronym": "RS", "label": "Red State", "ID": "6155"}, {"sentence": "RS is a conserva- tive political blog whereas Daily Kos is a liberal  political blog.", "acronym": "RS", "label": "Red State", "ID": "6156"}, {"sentence": "We collected a  total of 100,000 blog posts from Daily Kos and  70,000 blog posts from RS and a total of  787,780 tags across both blogs (an average of 4.63  tags per post).", "acronym": "RS", "label": "Red State", "ID": "6157"}, {"sentence": "5 Results and Evaluation  For evaluating our methods, we used 2,681 posts  from Daily Kos and 571 posts from RS.", "acronym": "RS", "label": "Red State", "ID": "6158"}, {"sentence": "We collected a  total of 100,000 blog posts from Daily Kos and  70,000 blog posts from RS and a to", "acronym": "RS", "label": "Red State", "ID": "6159"}, {"sentence": "We collected a  total of 100,000 blog posts from Daily Kos and  70,000 blog posts from RS and a total of  787,780 tags across bot", "acronym": "RS", "label": "Red State", "ID": "6160"}, {"sentence": "3 Data  We collected data from two major political blogs,  Daily Kos (www.dailykos.com) and RS  (www.redstate.com).", "acronym": "RS", "label": "Red State", "ID": "6161"}, {"sentence": "Maximum entropy classifier is thus closely related to logistic regression model.1560 PF: - Position from the beginning of listing - Position to the end of listing Orthographic Features: - Identity of the current word - Current word contains a digit - Current word contains only digits - Current word is capitalized - Current word begins with a capitalized letter followed by all non-cap letters.", "acronym": "PF", "label": "Position Features", "ID": "6162"}, {"sentence": "pos(wi)] / |s|  3.4 Sentence PF  In our study, another type of position features,  which model sentence position information, is  defined for comparison with the word position  features.", "acronym": "PF", "label": "Position Features", "ID": "6163"}, {"sentence": "3.2 Word PF  With the above model, word position features  are defined to represent the word position  information and are then incorporated into the  model.", "acronym": "PF", "label": "Position Features", "ID": "6164"}, {"sentence": "3.3 Incorporating the PF   To incorporate the position features into the  word-based summarization model, we use them  to adjust the importance of the word appearance.", "acronym": "PF", "label": "Position Features", "ID": "6165"}, {"sentence": "+b 3 ) Embeddings PF     &       Path Features Input Features Position features Path features Softmax  Layer concatenate Dropout  Layer ?", "acronym": "PF", "label": "Position Features", "ID": "6166"}, {"sentence": "3.5.3 Relative PF  We define three types of position features  which depict the relative structures between  the two entities, including Nested, Adjacent  and Separated.", "acronym": "PF", "label": "Position Features", "ID": "6167"}, {"sentence": "To c a p t u r e  t h e  dependence o f  syntax on semantic con ten t  a n d   b o c j a l  c o n t e x t ,  t h e  sentence  gene ra to r  uses f u n c t i o n - l i k e  grammar  rules o f  t h e  form  ( R u l  erame C a t  Variables PFs ) .", "acronym": "PF", "label": "Predicate Form", "ID": "6168"}, {"sentence": "In general, a predicate is determined 85 Name Note Baseline Features PF and POS of the predi- cate Noun Form and POS of the head- word of the candidate phrase Particle Form and POS of the particle of the candidate phrase Path Dependency relation between the predicate and the candi- date phrase Passive Passive auxiliary verbs that the predicate contains PhPosit Relative phrase position be- tween the predicate and the candidate phrase SentPosit Relative s", "acronym": "PF", "label": "Predicate Form", "ID": "6169"}, {"sentence": "of distinct aspect terms identified by the method, ordered by decreasing PF.", "acronym": "PF", "label": "predicted frequency", "ID": "6170"}, {"sentence": "Syntactic features (14): part-of-speech tags (POS) of: first phrase word, last phrase word, 229 word immediately before phrase and word im- mediately after phrase; syntactic paths from word to verb: all paths, only paths for words before verb and only paths for words after verb; phrase label, label of phrase parent, subcate- gorisation of verb parent, PF from PropBank, voice, head preposition for preposi- tional phrases and same parents flag.", "acronym": "PF", "label": "predicate frame", "ID": "6171"}, {"sentence": "In the end, neither the ar- gument lemma, nor the PF improved the performance.", "acronym": "PF", "label": "predicate frame", "ID": "6172"}, {"sentence": "We tried to represent the chain using various subsets of the following elements: the argument lemma and part-of-speech, the PF and part- of-speech, the parts-of-speech and syntactic de- pendencies of the intermediate words linking the argument to the predicate.", "acronym": "PF", "label": "predicate frame", "ID": "6173"}, {"sentence": "In order to allow access to addi- tional useful information, such as subsumption,  property inheritance, PFs from other  sources, links to instances, and so on, our goal is to  link the senses to an ontology.", "acronym": "PF", "label": "predicate frame", "ID": "6174"}, {"sentence": "4.3 Data Distribution The total amount of data prepared for the 14 verbs are divided into three non-overlapping sets in a bal- anced form in terms of both the number of the target PFs, and the relevant instances of each frame.", "acronym": "PF", "label": "predicate frame", "ID": "6175"}, {"sentence": "Parsing consists of four steps: predicate sense disambiguation, argument identification, argument classification and PF constraint satis- faction.", "acronym": "PF", "label": "predicate frame", "ID": "6176"}, {"sentence": "39 That-COMP are optional words that in- troduce sentential complements in English.", "acronym": "COMP", "label": "complementizers", "ID": "6177"}, {"sentence": "Label numRef numSys numCorr F1 CP-NoneL 1723 1724 1715 0.995 IP-NoneL 3874 3875 3844 0.992 VP-NoneR 660 633 597 0.923 IP-NoneM 440 432 408 0.936 VP-NoneL 135 107 105 0.868 Table 7: 5 most frequent labels carrying pseudo tags and their performances COMP for subordinate clauses.", "acronym": "COMP", "label": "complementizers", "ID": "6178"}, {"sentence": "This implies that deep annotations as, for instance, have been derived so far from PennTreeBank/PropBank, in which ei- ther all syntactic nodes of the annotation are kept (as in (Bohnet et al, 2010)) or only certain syntac- tic nodes are removed (as THAT COMP and TO infinitives in the shared task 2011 on sur- face realization (Belz et al, 2011)) still fall short of a genuine semantic annotation.", "acronym": "COMP", "label": "complementizers", "ID": "6179"}, {"sentence": "For example, there are no empty COMP annotated in the CTB while English does not allow dropped pronouns.", "acronym": "COMP", "label": "complementizers", "ID": "6180"}, {"sentence": "The poss ib le  COMP are*  ]CHAT Mother said t h a t  Mike should move,  FORT0 Mother told Mike t o  move.", "acronym": "COMP", "label": "complementizers", "ID": "6181"}, {"sentence": "We extract HLDS- based quasi logical form graphs from the CCG- bank and semantically empty function words such as COMP, infinitival-to, expletive subjects, and case-marking prepositions are adjusted to reflect their purely syntactic status.", "acronym": "COMP", "label": "complementizers", "ID": "6182"}, {"sentence": "4 Evaluation We compared the Gibbs sampling COMP (GS) against a version of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM).", "acronym": "COMP", "label": "compressor", "ID": "6183"}, {"sentence": "At the same time, the satisfaction mean score is 3.23 over 5, whereas the same users attribute to human COMPs a satisfaction mean score of 3.7, really not so much more.", "acronym": "COMP", "label": "compressor", "ID": "6184"}, {"sentence": "EFFECTS OF THE STRUCTURE OF TASK-ORIENTED  DIALOGUES  Task-oriented conversations have a specific goal to be  achieved: the performance of a task (e.g., the air  COMP assembly in Grosz (1977)).", "acronym": "COMP", "label": "compressor", "ID": "6185"}, {"sentence": "For example, Grosz collected  and studied dialogues in which an expert guides an  apprentice in the assembly of an air COMP.", "acronym": "COMP", "label": "compressor", "ID": "6186"}, {"sentence": "3 COLIN?s COMP: System architecture and implementation 3.1 Architecture We assume we have a raw text as an input, which may be the output of an extract summarizer, and we have to produce a compressed version of it, by reducing as many sentences as we can, without deleting a single one.", "acronym": "COMP", "label": "compressor", "ID": "6187"}, {"sentence": "157 \"heart attack\" compressed encoding RBM trained encoder original vector Figure 2: Using a RBM trained COMP to generate a compressed encoding of ?", "acronym": "COMP", "label": "compressor", "ID": "6188"}, {"sentence": ", i , - , - , l )  t\\[,\\] --* t\\[rlr\\]  COMP of the material below the root node ~r of an initial tree  allows for the completion of the node at which substitution occurred.", "acronym": "COMP", "label": "Completion", "ID": "6189"}, {"sentence": "4.1 COMP Time Figure 3 shows the time that it took for our HITs for 37 languages to be completed on MTurk.", "acronym": "COMP", "label": "Completion", "ID": "6190"}, {"sentence": "COMP rate is calculated as the number of completed calls divided by the total num- ber of calls.", "acronym": "COMP", "label": "Completion", "ID": "6191"}, {"sentence": "COMP of items (moving of the dot from left to right over a nonterminal)  breaks up into several cases, depending on which production type is being completed.", "acronym": "COMP", "label": "Completion", "ID": "6192"}, {"sentence": "c?2009 Association for Computational Linguistics Cross-lingual Alignment and COMP of Wikipedia Templates Gosse Bouma Information Science University of Groningen g.bouma@rug.nl Sergio Duarte Information Science University of Groningen sergio.duarte@gmail.com Zahurul Islam Information Science University of Groningen zaisdb@gmail.com Abstract For many languages, the size of Wikipedia is an order of magnitude smaller than the En- glish Wikipedia.", "acronym": "COMP", "label": "Completion", "ID": "6193"}, {"sentence": "4.4 Selection of Feature Templates of the Graph-based COMP Model The graph-based completion model re-scores the beam incrementally and leads to a higher accuracy.", "acronym": "COMP", "label": "Completion", "ID": "6194"}, {"sentence": "He observes that this approach suffers from an acute data sparseness problem and goes on to obtain counts for candidate COMPs through web searches, thus achieving a translation accu- racy of 86?87%.", "acronym": "COMP", "label": "compound", "ID": "6195"}, {"sentence": "Then we investi- gate the generality of the web-based approach by apply- ing it to a range of analysis and generations tasks, involv- ing both syntactic and semantic knowledge: (c) ordering of prenominal adjectives, (d) COMP noun bracketing, (e) COMP noun interpretation, and (f) noun count- ability detection.", "acronym": "COMP", "label": "compound", "ID": "6196"}, {"sentence": "Ad-adjectival adjectives are special forms of adjectives used in COMPs like angielsko- polski ?", "acronym": "COMP", "label": "compound", "ID": "6197"}, {"sentence": "on the assump- tion that the COMP exerts similar attractions as the base noun.", "acronym": "COMP", "label": "compound", "ID": "6198"}, {"sentence": "Termhood  is a numeric estimation of the degree to which a  given linguistic unit (a multiword COMP) is  related to a domain-specific concept.", "acronym": "COMP", "label": "compound", "ID": "6199"}, {"sentence": "Grefenstette translates COMPs from German and Spanish into English, and uses BNC frequencies as a filter for candidate translations.", "acronym": "COMP", "label": "compound", "ID": "6200"}, {"sentence": "We will call the set of composable fragments at a certain step in the stochastic process the COMP set at that step.", "acronym": "COMP", "label": "competition", "ID": "6201"}, {"sentence": "f'?CS  P(f') P( f)  (3) CP(f | CS)  = Bod & Kaplan give three definitions of increasing complexity for the COMP set: the first definition groups all fra", "acronym": "COMP", "label": "competition", "ID": "6202"}, {"sentence": "i CP(f i | CSi) where the COMP probability  CP(f | CS) is expressed in terms of fragment probabilities P( f): ?", "acronym": "COMP", "label": "competition", "ID": "6203"}, {"sentence": "f'?CS  P(f') P( f)  (3) CP(f | CS)  = Bod & Kaplan give three definitions of increasing complexity for the COMP set: the first definition groups all fragments that only satisfy the Category-matching condition of the composition operation; the second definition groups all fragments which satisfy both", "acronym": "COMP", "label": "competition", "ID": "6204"}, {"sentence": "ote the probability of choosing a fragment f from a COMP set CS containing f, then the probability of a derivation D = < f1, f2 ... fk> is (2) P(< f1, f2 ... fk>)  =  ?", "acronym": "COMP", "label": "competition", "ID": "6205"}, {"sentence": "The required efforts discouraged us from building a middle ontology between the BioNLP and the PIKB data models, especially given the time lim- itations for the present task COMP.", "acronym": "COMP", "label": "competition", "ID": "6206"}, {"sentence": "Let CP( f | CS) denote the probability of choosing a fragment f from a COMP set CS containing f, then the probability of a derivation D = < f1, f2 ... fk> is (2) P(< f1, f2 ... fk>)  =  ?", "acronym": "COMP", "label": "competition", "ID": "6207"}, {"sentence": "f'?CS  P(f') P( f)  (3) CP(f | CS)  = Bod & Kaplan give three definitions of increasing complexity for the COMP set: the first definition groups all fragments that only satisfy the Category-matching condition of the composition operation; the second definition groups all fragments which satisfy both Category- matching and Uniqueness; and the third definition groups all fragments which satisfy Category- matching, Uniqueness and Coherence.", "acronym": "COMP", "label": "competition", "ID": "6208"}, {"sentence": "As noted by Lewis and Sparck-Jones (1996), the results of large-scale document-retrieval COMPs such as the Text Retrieval Conference (TREC, 2000) do not necessarily reflect the experience many users have with retrieval systems.", "acronym": "COMP", "label": "competition", "ID": "6209"}, {"sentence": "We will call the set of composable fragments at a certain step in the stochastic process the COMPn set at that step.", "acronym": "COMP", "label": "competitio", "ID": "6210"}, {"sentence": "f'?CS  P(f') P( f)  (3) CP(f | CS)  = Bod & Kaplan give three definitions of increasing complexity for the COMPn set: the first definition groups all fra", "acronym": "COMP", "label": "competitio", "ID": "6211"}, {"sentence": "i CP(f i | CSi) where the COMPn probability  CP(f | CS) is expressed in terms of fragment probabilities P( f): ?", "acronym": "COMP", "label": "competitio", "ID": "6212"}, {"sentence": "f'?CS  P(f') P( f)  (3) CP(f | CS)  = Bod & Kaplan give three definitions of increasing complexity for the COMPn set: the first definition groups all fragments that only satisfy the Category-matching condition of the composition operation; the second definition groups all fragments which satisfy both", "acronym": "COMP", "label": "competitio", "ID": "6213"}, {"sentence": "ote the probability of choosing a fragment f from a COMPn set CS containing f, then the probability of a derivation D = < f1, f2 ... fk> is (2) P(< f1, f2 ... fk>)  =  ?", "acronym": "COMP", "label": "competitio", "ID": "6214"}, {"sentence": "The required efforts discouraged us from building a middle ontology between the BioNLP and the PIKB data models, especially given the time lim- itations for the present task COMPn.", "acronym": "COMP", "label": "competitio", "ID": "6215"}, {"sentence": "Let CP( f | CS) denote the probability of choosing a fragment f from a COMPn set CS containing f, then the probability of a derivation D = < f1, f2 ... fk> is (2) P(< f1, f2 ... fk>)  =  ?", "acronym": "COMP", "label": "competitio", "ID": "6216"}, {"sentence": "f'?CS  P(f') P( f)  (3) CP(f | CS)  = Bod & Kaplan give three definitions of increasing complexity for the COMPn set: the first definition groups all fragments that only satisfy the Category-matching condition of the composition operation; the second definition groups all fragments which satisfy both Category- matching and Uniqueness; and the third definition groups all fragments which satisfy Category- matching, Uniqueness and Coherence.", "acronym": "COMP", "label": "competitio", "ID": "6217"}, {"sentence": "As noted by Lewis and Sparck-Jones (1996), the results of large-scale document-retrieval COMPns such as the Text Retrieval Conference (TREC, 2000) do not necessarily reflect the experience many users have with retrieval systems.", "acronym": "COMP", "label": "competitio", "ID": "6218"}, {"sentence": "2 TSGs TSGs (Joshi and Schabes, 1997) generalize context-free grammars by allow- ing nonterminals to rewrite as tree fragments of ar- bitrary size, instead of as only a sequence of one or 217 .S .NP .VP .VBD .said .NP .SBAR .. Figure 1: A Tree Substitution Grammar fragment.", "acronym": "TSG", "label": "Tree substitution grammar", "ID": "6219"}, {"sentence": "4 As an example,  4 The tree substitution grammar is lexicalized inthe sense that each of the trees has an associated anchor,  442  van Noord Efficient Head-Corner Parsing  nt5:I nt0:a ntl:4 nt2:3  / \\  r  ntO man home  nt3:6 / \\   at nt2  nt4:5 nt6:l  /\\ /\\  4 nt3 nt5 7 /\\ /\\  nt0 man see nt4  nt6:2 / \\   1 nt3 / \\   nt5 7 / \\   see nt l   Figure 5  TSG that derives each of the two derivation trees of the sentence I see a  man at home, for the grammar of Billot and Lang (1989).", "acronym": "TSG", "label": "Tree substitution grammar", "ID": "6220"}, {"sentence": "The best accuracy is achieved by excluding fea- 137 Feature Accuracy (1) Word 1,2-gram 0.8075 (2) POS 2,3-gram 0.5555 (3) POS,Function 2,3-gram 0.7080 (4) Chracter 2,3-gram 0.6678 (5) Dependency 0.7236 (6) TSG 0.6455 (7) 1 + 2 0.7825 (8) 1 + 3 0.7913 (9) 1 + 4 0.7953 (10) 1 + 5 0.8020 (11) 1 + 6 0.7999 (12) 1 + 2 + 3 0.7849 (13) 1 + 2 + 3 + 4 0.8000 (14) 1 + 2 + 3 + 4 + 5 0.8097 (15) ALL 0.8088 Table 5: 10-fold cross validation results for each feature tures whose NLF is 1 or 11.", "acronym": "TSG", "label": "Tree substitution grammar", "ID": "6221"}, {"sentence": "In TSGs, for instance, there may be many ways of combining elementary trees to pro- duce the same output tree; in machine translation, many different elementary phrases or elementary tree pairs might produce the same output string.", "acronym": "TSG", "label": "tree substitution grammar", "ID": "6222"}, {"sentence": "In- cremental TSG for pars- ing and word prediction.", "acronym": "TSG", "label": "tree substitution grammar", "ID": "6223"}, {"sentence": "P ) be a TSG (regular tree grammar) in normal form that recognizesL (i.e.", "acronym": "TSG", "label": "tree substitution grammar", "ID": "6224"}, {"sentence": "Like description TSGs, but unlike multi-component TAGs, it allows trees to be partitioned into any desired set of contiguous components during composition, 2.", "acronym": "TSG", "label": "tree substitution grammar", "ID": "6225"}, {"sentence": "Like multi-component TAGs, but unlike descrip- tion TSGs, it allows the speci\fcation of particular insertion sites within elementary trees, and 3.", "acronym": "TSG", "label": "tree substitution grammar", "ID": "6226"}, {"sentence": "Bayesian learning of a TSG.", "acronym": "TSG", "label": "tree substitution grammar", "ID": "6227"}, {"sentence": "Since the new SVM QP is convex, it is no harder to op- timize than the standard SVM objective.", "acronym": "QP", "label": "quadratic program", "ID": "6228"}, {"sentence": "2.2.1 Optimization as a Binary SVM We could solve the optimization problem in (4) directly using a QPming solver.", "acronym": "QP", "label": "quadratic program", "ID": "6229"}, {"sentence": "Building on this efficient framework, we incorporate variance regularization into the SVM?s QP.", "acronym": "QP", "label": "quadratic program", "ID": "6230"}, {"sentence": "2 Three Multi-Class SVM Models We describe three max-margin multi-class classi- fiers and their corresponding QPs.", "acronym": "QP", "label": "quadratic program", "ID": "6231"}, {"sentence": "For VAR-SVM, we solve the primal form of the QP directly in CPLEX (2005), a general optimization package.", "acronym": "QP", "label": "quadratic program", "ID": "6232"}, {"sentence": "We refer to a CS-SVM trained using the variance minimization QP as the VAR-SVM.", "acronym": "QP", "label": "quadratic program", "ID": "6233"}, {"sentence": "IE Index Case  Insensitive Corpus Multi-level  TemplateQuestion Keyword  IndexKeyword indexing InfoXtract Asking PointIdentification Feature RankingSnippet Retrieval Snippet-levelAnswer QP Text Processing Answer Ranking Case Restoration InfoXtract   Figure 2:  Architecture of QA Based on NLP/IE    Snippet Retrieval    Snippet retrieval generates the top n (we chose  200) most relevant sentence-level candidate  answer snippets based on the question processing  results.", "acronym": "QP", "label": "Question Processing", "ID": "6234"}, {"sentence": "First, the keywords extracted by the QP module can be enhanced with concepts from the topic signatures to produce a ranked list of paragraphs, resulting from the Pas- sage Retrieval Module.", "acronym": "QP", "label": "Question Processing", "ID": "6235"}, {"sentence": "If the QP component detects an  Asking Point CE Link, the system first attempts to  retrieve snippets that contain the corresponding CE  relationship.", "acronym": "QP", "label": "Question Processing", "ID": "6236"}, {"sentence": "QP: The query is classified and the two top expected answer types are estimated; it is then submitted to the underlying search engine; ?", "acronym": "QP", "label": "Question Processing", "ID": "6237"}, {"sentence": "This system consists of three components:  (i) QP, (ii) Text Processing, and  (iii) Answer Ranking.", "acronym": "QP", "label": "Question Processing", "ID": "6238"}, {"sentence": "i  Figure 2: Textract/QA 1.0 Prototype Architecture  The general algorithm for question  answering is as follows:  168  Process Question  Shallow parse question  Determine Asking Point  Question expansion (using word lists)  Process Documents  Tokenization, POS tagging, NE Indexing  Shallow Parsing (not yet utilized)  Text Matcher  Intersect search engine results with NE  rank answers  2.1 QP  The QP results are a list of  keywords plus the information for asking point.", "acronym": "QP", "label": "Question Processing", "ID": "6239"}, {"sentence": "A QP pro- cedure.", "acronym": "QP", "label": "quadratic programming", "ID": "6240"}, {"sentence": "N = length(S); m = length(f); F = [f;D]; pc = min(pc,(D-Ds)/N); % Adjust pc, if Ds is close to D. % Solve a QP problem to find an initial 350 Li and Church Sketch for Estimating Associations % guess of the MLE that minimizes the 2-norm with respect to % the MF estimation and satisfies the constraints.", "acronym": "QP", "label": "quadratic programming", "ID": "6241"}, {"sentence": "2.2.1 Optimization as a Binary SVM We could solve the optimization problem in (4) directly using a QP solver.", "acronym": "QP", "label": "quadratic programming", "ID": "6242"}, {"sentence": "SVM is trained by solving a  dual QP problem.", "acronym": "QP", "label": "quadratic programming", "ID": "6243"}, {"sentence": "An algorithm for QP.", "acronym": "QP", "label": "quadratic programming", "ID": "6244"}, {"sentence": "It is formu- lated as a QP problem: min ?", "acronym": "QP", "label": "quadratic programming", "ID": "6245"}, {"sentence": "The experiment has been carried out on 60 sentences  with 1201 different lectures, and formed by using seven  verbs (wr~te, eat, smell, corrode, buy, receive, assocza~e)  coupled with fifty common ouns and two PNs.", "acronym": "PN", "label": "proper noun", "ID": "6246"}, {"sentence": "For nouns, number and gender information is needed, as well as infor- mation as to whether it is a common or PN.", "acronym": "PN", "label": "proper noun", "ID": "6247"}, {"sentence": "NNP Noun phrases (e.g., PNs) ex- tracted from comment texts.", "acronym": "PN", "label": "proper noun", "ID": "6248"}, {"sentence": "Noun phrases are PNs and personal pronouns.", "acronym": "PN", "label": "proper noun", "ID": "6249"}, {"sentence": "PN?", "acronym": "PN", "label": "proper noun", "ID": "6250"}, {"sentence": "The Urdu morphol- ogy provides the following analysis for the PN nAdyA. (3) nAdyA +Noun +Name +Fem The tags provide the information that it is a noun, in particular a type of PN (Name), and is fem- inine.", "acronym": "PN", "label": "proper noun", "ID": "6251"}, {"sentence": "An extreme alternative is to have a first pass  where only referring expressions which look like  anaphors are marked up, such as pronouns, def-  inite NPs and reduced forms of PN.", "acronym": "PN", "label": "proper names", "ID": "6252"}, {"sentence": "We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named entity recognition was used for identifying PN, e.g., ?", "acronym": "PN", "label": "proper names", "ID": "6253"}, {"sentence": "2In case of PN, there exist many dedicated algo- rithms and systems for finding them in texts, often developed within the Message Understanding Conference series.", "acronym": "PN", "label": "proper names", "ID": "6254"}, {"sentence": "2) a set of part-of-speech patterns was used for the extraction of human and non-human characters that were not represented by PN, e.g., ?", "acronym": "PN", "label": "proper names", "ID": "6255"}, {"sentence": "2 A Flexemic Tagset for Polish The tagset presented in this section is based on the following design assumptions: \u0000 what is being tagged is a single orthographic word or, in some well-defined cases, a part thereof; multi-word constructions, even those sometimes considered to be morphological formations (so-called analytic forms) or dic- tionary entries (PN), should be considered by a different level of process- ing;2 cf.", "acronym": "PN", "label": "proper names", "ID": "6256"}, {"sentence": "The analysis also explains why PN can  never be modified restrictively.", "acronym": "PN", "label": "proper names", "ID": "6257"}, {"sentence": "Utterance-length number of words in the sentence Part-of-speech (compare) POS-auto POS tag of the misrecognized word au- tomatically assigned on a transcript POS-guess POS tag of the misrecognized word guessed by a user PN PN all bigrams and trigrams of POS tags in a sentence Syntactic Dependency Dep-tag dependency tag of the misrecognized word automatically assigned on a tran- script Dep-pair dependency tags of all (parent, child) pairs in the sentence Parent-POS POS tag of the syntactic parent of the misrecognized word Semantic Sem-role semantic role of the misrecognized word Sem-presence all se", "acronym": "PN", "label": "POS ngrams", "ID": "6258"}, {"sentence": "Overall, the most discriminating features for both INFORM and CONVENTIONAL are mostly word ngrams, while those for REQUEST-ACTION and REQUEST-INFORMATION are mostly PN.", "acronym": "PN", "label": "POS ngrams", "ID": "6259"}, {"sentence": "semble tags PDS ART PDS PDS ART POS context (CRF) ADV:PROAV:VAFIN:APPR, PROAV:VAFIN:APPR:PDS, ... POS context (tree) PROAV:VAFIN:APPR, VAFIN:APPR:ART, ... word form with POS context (CRF) PROAV:VAFIN:APPR:der, VAFIN:APPR:der:APPR, ... word form with POS context (CRF:tree) PROAV:VAFIN:APPR:der, ..., der:APPR:ART:NN, ... extended features I: universal POS universal ensemble tags P D P P D universal PN P:D, P:P, P:P, ..., P:P:P:D, P:D:P:P:D universal POS context (CRF) ADV:P:VF:ADP, P:VF:ADP:P, ... word form with universal POS context (CRF) P:VF:ADP:der, VF:ADP:der:ADP, ADP:der:ADP:D, ... word form with universal POS context (CRF:tree) VF:VF:ADP:ADP:der, ADP:ADP:der:ADP:ADP, ... extended features II: brown clusters brown cluster for word form 110111011111 brown cluster with universal P", "acronym": "PN", "label": "POS ngrams", "ID": "6260"}, {"sentence": "BOWn=word ngrams; CHAR3=char trigrams; POSn=PN; DEP/DEPL=syntactic dependecies.", "acronym": "PN", "label": "POS ngrams", "ID": "6261"}, {"sentence": "0.727 +0.1% less PN 72.8% ?", "acronym": "PN", "label": "POS ngrams", "ID": "6262"}, {"sentence": "Utterance-length number of words in the sentence Part-of-speech (compare) POS-auto POS tag of the misrecognized word au- tomatically assigned on a transcript POS-guess POS tag of the misrecognized word guessed by a user PN PN all bigrams and trigrams of POS tags in a sentence Syntactic Dependency Dep-tag dependency tag of the misrecognized word automatically assigned on a tran- script Dep-pair dependency tags of all (parent, child) pairs in the sentence Parent-POS POS tag of the syntactic parent of the misrecognized word Semantic Sem-role semantic role of the misrecognized word Sem-presence all semantic role", "acronym": "PN", "label": "POS ngrams", "ID": "6263"}, {"sentence": "An extreme alternative is to have a first pass  where only referring expressions which look like  anaphors are marked up, such as pronouns, def-  inite NPs and reduced forms of PNs.", "acronym": "PN", "label": "proper name", "ID": "6264"}, {"sentence": "tag as it is a part of a PN?", "acronym": "PN", "label": "proper name", "ID": "6265"}, {"sentence": "We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named entity recognition was used for identifying PNs, e.g., ?", "acronym": "PN", "label": "proper name", "ID": "6266"}, {"sentence": "2In case of PNs, there exist many dedicated algo- rithms and systems for finding them in texts, often developed within the Message Understanding Conference series.", "acronym": "PN", "label": "proper name", "ID": "6267"}, {"sentence": "2) a set of part-of-speech patterns was used for the extraction of human and non-human characters that were not represented by PNs, e.g., ?", "acronym": "PN", "label": "proper name", "ID": "6268"}, {"sentence": "2 A Flexemic Tagset for Polish The tagset presented in this section is based on the following design assumptions: \u0000 what is being tagged is a single orthographic word or, in some well-defined cases, a part thereof; multi-word constructions, even those sometimes considered to be morphological formations (so-called analytic forms) or dic- tionary entries (PNs), should be considered by a different level of process- ing;2 cf.", "acronym": "PN", "label": "proper name", "ID": "6269"}, {"sentence": "c?2007 Association for Computational Linguistics Towards Robust Unsupervised PN Disambiguation  Ying Chen  Center for Spoken Language Research University of Colorado at Boulder  yc@colorado.edu  James Martin  Department of Computer Science  University of Colorado at Boulder  James.Martin@colorado.edu      Abstract  The increasing use of large open-domain  document sources is exacerbating the  problem of ambiguity in named entities.", "acronym": "PN", "label": "Personal Name", "ID": "6270"}, {"sentence": "Unsuper- vised PN Disambiguation In Proceedings  of the seventh conference on Natural language learn- ing at HLT-NAACL, pages 33-40.", "acronym": "PN", "label": "Personal Name", "ID": "6271"}, {"sentence": "PolyUHK:A Robust Information Extraction  System for Web PNs.", "acronym": "PN", "label": "Personal Name", "ID": "6272"}, {"sentence": "\u000f\u0002\u0004\u000f \u000f\u0002\u000f \u000f\u0002\u000f\u000f \u000f\u0002\u0005 \u000f\u0002\u0005\u000f \u0001 \u000f\u0001\u0001 \u0007\u0001\u0001\u0001 \u0007\u000f\u0001\u0001 \u0003\u0001\u0001\u0001 \u0003\u000f\u0001\u0001 \u000e\u0001\u0001\u0001 \u000e\u000f\u0001\u0001 \u0004\u0001\u0001\u0001 \u0004\u000f\u0001\u0001 \u000f\u0001\u0001\u0001   Figure 8: MSR test error curve for TuneUp  272    DLUT: Chinese PN Disambiguation with Rich  Features  Dongliang Wang  Department of Computer Science  and Engineering, Dalian University  of Technology  wdl129@163.com  Degen Huang  Department of Computer Science  and Engineering, Dalian University  of Technology  huangdg@dlut.edu.cn    Abstract  In this paper we describe a person clus- tering system for a given document set  and report the results we", "acronym": "PN", "label": "Personal Name", "ID": "6273"}, {"sentence": "A Study of PN Disam- biguation.", "acronym": "PN", "label": "Personal Name", "ID": "6274"}, {"sentence": "Ex- tracting PNs from Email: Applying  Named Entity Recognition to Informal Text, Proc.", "acronym": "PN", "label": "Personal Name", "ID": "6275"}, {"sentence": "Lexical Token n-gram Token n-grams in a 2 word window around the preposition POS n-gram POS n-grams in a 2 word window around the preposition HEAD PREC VP The head verb in the preceding verb phrase HEAD PREC NP The head noun in the PN HEAD FOLLOW NP The head noun in the following noun phrase Parsing HEAD Head of the preposition HEAD POS POS of the head COMP Complement of the preposition COMPLEMENT POS POS of the complement HEAD RELATION Prep-Head relation name COMPLEMENT RELATION Prep-Comp relation name Phrase Structure PARENT TAG TAG of the preposition?s parent GRANDPARENT TAG TAG of the preposition?s gra", "acronym": "PN", "label": "preceding noun phrase", "ID": "6276"}, {"sentence": "Nasukawa lso finds  (similarly to (\\[Mitkov 93\\])) that the frequency of  PNs with the same lemma as  the candidate noun phrase may be an indication  for preference.", "acronym": "PN", "label": "preceding noun phrase", "ID": "6277"}, {"sentence": "<- N18  5.7 Conjunction Construction  In Persian, there is a construction to modify the  PN with an adjective clause  which we have named the Conjunction construc- tion.", "acronym": "PN", "label": "preceding noun phrase", "ID": "6278"}, {"sentence": "Each noun phrase is  compared to all PNs.", "acronym": "PN", "label": "preceding noun phrase", "ID": "6279"}, {"sentence": "Earlier work \\[11 \\] on PP-attachment for verb phrases (whether  the PP attaches to the PN or to the verb  phrase) used statistics on co-occurences of two bigrams: the  main verb (V) and preposition (P) bigram and the main noun  in the object noun phrase (N1) and preposition bigram.", "acronym": "PN", "label": "preceding noun phrase", "ID": "6280"}, {"sentence": "For  each noun phrase NPj encountered, consider each  PN NPi.", "acronym": "PN", "label": "preceding noun phrase", "ID": "6281"}, {"sentence": "Mod- eling interestingness with DNNs.", "acronym": "DNN", "label": "deep neural network", "ID": "6282"}, {"sentence": "In the future, we would like to investigate the advantages and disadvantages between tree- based models and other non-linear models such as DNNs or recurrent neural net- works.", "acronym": "DNN", "label": "deep neural network", "ID": "6283"}, {"sentence": "The in- teraction function could be an inner product, a bi- linear operation, or a nonlinear function such as a DNN.", "acronym": "DNN", "label": "deep neural network", "ID": "6284"}, {"sentence": "Some recent work on relation classification has focused on the use of DNNs with the aim of reducing the number of handcrafted fea- tures (Socher et al, 2012; Zeng et al, 2014; Yu et al.,", "acronym": "DNN", "label": "deep neural network", "ID": "6285"}, {"sentence": "The DRRN uses separate DNNs to map state and action text strings into embedding vectors, from which ?", "acronym": "DNN", "label": "deep neural network", "ID": "6286"}, {"sentence": "Context-dependent pre-trained DNNs for large-vocabulary speech recognition.", "acronym": "DNN", "label": "deep neural network", "ID": "6287"}, {"sentence": "We consequently introduce one-class classification problem and develop a One-Class DNN.", "acronym": "DNN", "label": "Deep Neural Network", "ID": "6288"}, {"sentence": "2016 Association for Computational Linguistics Bi-Transferring DNNs for Domain Adaptation Guangyou Zhou 1 , Zhiwen Xie 1 , Jimmy Xiangji Huang 2 , and Tingting He 1 1 School of Computer, Central China Normal University, Wuhan 430079, China 2 School of Information Technology, York University, Toronto, Canada {gyzhou,xiezhiwen,tthe}@mail.ccnu.edu.cn jhuang@yorku.ca Abstract Sentiment classification aims to automati- cally predict sentiment polar", "acronym": "DNN", "label": "Deep Neural Network", "ID": "6289"}, {"sentence": "Relation Classification via  Convolutional DNN.", "acronym": "DNN", "label": "Deep Neural Network", "ID": "6290"}, {"sentence": "Joint Opinion Relation Detection Using One-Class DNN Liheng Xu, Kang Liu and Jun Zhao National Laboratory of Pattern Recognition Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China {lhxu, kliu, jzhao}@nlpr.ia.ac.cn Abstract Detecting opinion relation is a crucial step for fine-gained opinion summarization.", "acronym": "DNN", "label": "Deep Neural Network", "ID": "6291"}, {"sentence": "DNN Approach for the Dialog State Tracking Challenge.", "acronym": "DNN", "label": "Deep Neural Network", "ID": "6292"}, {"sentence": "ConjVs in consecutive sequence  of Complex Predicates (CPs) have  been identified.", "acronym": "ConjVs", "label": "conjunct verbs", "ID": "6293"}, {"sentence": "System  achieves F-Scores of 75.73%, and  77.92% for compound verbs and  89.90% and 89.66% for ConjVs  respectively on two types of Bengali  corpus.", "acronym": "ConjVs", "label": "conjunct verbs", "ID": "6294"}, {"sentence": "rty        Sivaji Bandyopadhyay  Department of Computer Science and Engineering  Jadavpur University  dipankar.dipnil2005@gmail.com,  santanupersonal1@gmail.com,  tapabratamondal@gmail.com, its_tanmoy@yahoo.co.in,  sivaji_cse_ju@yahho.com      Abstract  This paper presents the automatic ex- traction of Complex Predicates (CPs)  in Bengali with a special focus on  compound verbs (Verb + Verb) and  ConjVs (Noun /Adjective +  Verb).", "acronym": "ConjVs", "label": "conjunct verbs", "ID": "6295"}, {"sentence": "1 Introduction  Complex Predicates (CPs) contain [verb] +  verb (compound verbs) or [noun/  adjective/adverb] +verb (ConjVs)  combinations in South Asian languages (Hook,  1974).", "acronym": "ConjVs", "label": "conjunct verbs", "ID": "6296"}, {"sentence": "Compound verbs  follow the pattern of verb-verb (V-V) combination  while ConjVs are formed with either noun- verb (N-V) or adjective-verb (A-V) combinations.", "acronym": "ConjVs", "label": "conjunct verbs", "ID": "6297"}, {"sentence": "Jadavpur University  dipankar.dipnil2005@gmail.com,  santanupersonal1@gmail.com,  tapabratamondal@gmail.com, its_tanmoy@yahoo.co.in,  sivaji_cse_ju@yahho.com      Abstract  This paper presents the automatic ex- traction of Complex Predicates (CPs)  in Bengali with a special focus on  compound verbs (Verb + Verb) and  ConjVs (Noun /Adjective +  Verb).", "acronym": "ConjVs", "label": "conjunct verbs", "ID": "6298"}, {"sentence": "They are used to handle special constructions like ConjVs (ex:- prashna kiyaa (question did)), coordinating conjunc- tions and ellipses.", "acronym": "ConjVs", "label": "conjunct verbs", "ID": "6299"}, {"sentence": "Lexical scopes of compound and  ConjVs in consecutive sequence  of Complex Predicates (CPs) have  been identified.", "acronym": "ConjVs", "label": "conjunct verbs", "ID": "6300"}, {"sentence": "u@yahho.com      Abstract  This paper presents the automatic ex- traction of Complex Predicates (CPs)  in Bengali with a special focus on  compound verbs (Verb + Verb) and  ConjVs (Noun /Adjective +  Verb).", "acronym": "ConjVs", "label": "conjunct verbs", "ID": "6301"}, {"sentence": "The lexical patterns of com- pound and ConjVs are extracted  based on the information of shallow  morphology and available seed lists of  verbs.", "acronym": "ConjVs", "label": "conjunct verbs", "ID": "6302"}, {"sentence": "ahho.com      Abstract  This paper presents the automatic ex- traction of Complex Predicates (CPs)  in Bengali with a special focus on  compound verbs (Verb + Verb) and  conConjVs (Noun /Adjective +  Verb).", "acronym": "ConjVs", "label": "junct verbs", "ID": "6303"}, {"sentence": "System  achieves F-Scores of 75.73%, and  77.92% for compound verbs and  89.90% and 89.66% for conConjVs  respectively on two types of Bengali  corpus.", "acronym": "ConjVs", "label": "junct verbs", "ID": "6304"}, {"sentence": "onConjVs in consecutive sequence  of Complex Predicates (CPs) have  been identified.", "acronym": "ConjVs", "label": "junct verbs", "ID": "6305"}, {"sentence": "1 Introduction  Complex Predicates (CPs) contain [verb] +  verb (compound verbs) or [noun/  adjective/adverb] +verb (conConjVs)  combinations in South Asian languages (Hook,  1974).", "acronym": "ConjVs", "label": "junct verbs", "ID": "6306"}, {"sentence": "Compound verbs  follow the pattern of verb-verb (V-V) combination  while conConjVs are formed with either noun- verb (N-V) or adjective-verb (A-V) combinations.", "acronym": "ConjVs", "label": "junct verbs", "ID": "6307"}, {"sentence": "They are used to handle special constructions like conConjVs (ex:- prashna kiyaa (question did)), coordinating conjunc- tions and ellipses.", "acronym": "ConjVs", "label": "junct verbs", "ID": "6308"}, {"sentence": "Sivaji Bandyopadhyay  Department of Computer Science and Engineering  Jadavpur University  dipankar.dipnil2005@gmail.com,  santanupersonal1@gmail.com,  tapabratamondal@gmail.com, its_tanmoy@yahoo.co.in,  sivaji_cse_ju@yahho.com      Abstract  This paper presents the automatic ex- traction of Complex Predicates (CPs)  in Bengali with a special focus on  compound verbs (Verb + Verb) and  conConjVs (Noun /Adjective +  Verb).", "acronym": "ConjVs", "label": "junct verbs", "ID": "6309"}, {"sentence": "Lexical scopes of compound and  conConjVs in consecutive sequence  of Complex Predicates (CPs) have  been identified.", "acronym": "ConjVs", "label": "junct verbs", "ID": "6310"}, {"sentence": "avpur University  dipankar.dipnil2005@gmail.com,  santanupersonal1@gmail.com,  tapabratamondal@gmail.com, its_tanmoy@yahoo.co.in,  sivaji_cse_ju@yahho.com      Abstract  This paper presents the automatic ex- traction of Complex Predicates (CPs)  in Bengali with a special focus on  compound verbs (Verb + Verb) and  conConjVs (Noun /Adjective +  Verb).", "acronym": "ConjVs", "label": "junct verbs", "ID": "6311"}, {"sentence": "The lexical patterns of com- pound and conConjVs are extracted  based on the information of shallow  morphology and available seed lists of  verbs.", "acronym": "ConjVs", "label": "junct verbs", "ID": "6312"}, {"sentence": "Our motivation that within a search set, reviews tend to resemble one another rather than differ is reminiscent of intuitions underlying the use of PRF (PF) in IR (Ruthven and Lalmas, 2003, Section 3.5).", "acronym": "PRF", "label": "pseudo relevance feedback", "ID": "6313"}, {"sentence": "Explicit and PRF (RF) techniques (Ruthven and Lalmas, 2003; Baeza- Yates and Ribeiro-Neto, 1999; Manning et al, 2008) are more related to Bobo in the sense that they do not build long-term profiles.", "acronym": "PRF", "label": "pseudo relevance feedback", "ID": "6314"}, {"sentence": "4.2 Quality of Seeds As in PRF, quality of seeds plays an critical role in search performance.", "acronym": "PRF", "label": "pseudo relevance feedback", "ID": "6315"}, {"sentence": "This formula has been proposed in the setting of PRF, where expansion terms are chosen based on the top documents re- trieved for the original query.", "acronym": "PRF", "label": "pseudo relevance feedback", "ID": "6316"}, {"sentence": "Ex- pansion terms are extracted from these definition clusters using PRF: we first retrieve the definition clusters which are most re- lated to the user query, and then extract the most relevant terms from these definition clusters to ex- pand the query.", "acronym": "PRF", "label": "pseudo relevance feedback", "ID": "6317"}, {"sentence": "We apply the technique of PRF to obtain expansion terms from definition clusters.", "acronym": "PRF", "label": "pseudo relevance feedback", "ID": "6318"}, {"sentence": "I e -~ ~3~ \\[o:m\\]  Itnow it looks as if they they both  .32 ~ iml l l lh~ l l l~   131 AR,,,R~VIil IJ3 \\[~\\] I l lthink that we ,muslr l ' l  wor ry  too +,+much A,~m~UT TH'ISl  134 Ilwe we limake it ~+perfectly ~lear that :,papers must be in on the ~first of  : .MXYI  '~-*  135 l~:ml  L~6 ,\\[mlltlh~\\]I,  31   Coling 2010: Poster Volume, pages 54?62, Beijing, August 2010 Query Expansion based on PRF from Definition Clusters Delphine Bernhard LIMSI-CNRS Delphine.Bernhard@limsi.fr Abstract Query expansion consists in extending user queries with related terms in order to solve the lexical gap problem in Infor- mation Retrieval and Question Answer- ing.", "acronym": "PRF", "label": "Pseudo Relevance Feedback", "ID": "6319"}, {"sentence": "Inspired by the successful use of PRF (Tao and Zhai, 2006) techniques in Information Retrieval and the cosine similarity measure (Salton, 1989) in Data Mining, we design a novel learning model which takes the multistream prediction feedback that is initially re- turned from seed samples 1 and uses a mean cosine similarity measure to calculate the distance between the new instance and prominent seed data point", "acronym": "PRF", "label": "Pseudo Relevance Feedback", "ID": "6320"}, {"sentence": "Collins and Koo Discriminative Reranking for NLP   PRF Method Based on Taylor Expansion of Re- trieval Function in NTCIR-3 Patent Retrieval Task   Kazuaki KISHIDA  Faculty of Cultural Information Resources  Surugadai University  698 Azu, Hanno, Saitama 357-8555 JAPAN  kishida@surugadai.ac.jp      Abstract  Pseudo relevance feedback is empirically  known as a useful method for enhancing  retrieval performance.", "acronym": "PRF", "label": "Pseudo Relevance Feedback", "ID": "6321"}, {"sentence": "For example, for  the PRF problem, Qin  et al (2008) defined Equation 2 as the similari- ties between any two documents in their CRF- based model.", "acronym": "PRF", "label": "Pseudo Relevance Feedback", "ID": "6322"}, {"sentence": "We adapted the classic PRF  algorithm (Qiu, 1993), which has been so far applied  only to document retrieval tasks, to similarity  computation in a straightforward way and also tried  several variations of if (not described here due to  lack of space).", "acronym": "PRF", "label": "Pseudo Relevance Feedback", "ID": "6323"}, {"sentence": "epartment of Computer Science, National Tsing-Hua University,   2Institute of Information Science, Academia Sinica,   3Department of Computer Science & Engineering, Yuan Ze University  hongjie@iis.sinica.edu.tw  s951416@mail.yzu.edu.tw   thtsai@saturn.yzu.edu.tw  hsu@iis.sinica.edu.tw  223 though one of the documents is not as relevant  to the given query as the other; this problem is  similar to PRF (Kwok,  1984).", "acronym": "PRF", "label": "Pseudo Relevance Feedback", "ID": "6324"}, {"sentence": "with a CM. ?", "acronym": "CM", "label": "case marker", "ID": "6325"}, {"sentence": "wa, CM) and ??? (", "acronym": "CM", "label": "case marker", "ID": "6326"}, {"sentence": "Furthermore, Arabic distinguishes between definite and indefinite CMs.", "acronym": "CM", "label": "case marker", "ID": "6327"}, {"sentence": "For the gold input, apart from LEMMA that pro- vides around 0.7 points, the most useful feature is 4NORK2, NOR1 and NMG are auxiliaries CMs.", "acronym": "CM", "label": "case marker", "ID": "6328"}, {"sentence": "5.3 Case markers Turkish, being a fairly scrambling language, uses CMs to denote the syntactic functions of nouns and noun groups.", "acronym": "CM", "label": "case marker", "ID": "6329"}, {"sentence": "Chinese language does not have inflection, con- jugation, or CMs (Li and Thompson, 1989).", "acronym": "CM", "label": "case marker", "ID": "6330"}, {"sentence": "(< synt~_~c-t~escriptioh> <semandCMgscfi'ption> ) ) ) )  A morphological descriodon contaM~ both contextual and  context-free inf6i'mafion.", "acronym": "CM", "label": "cM", "ID": "6331"}, {"sentence": "CM,wtM ) (1) We applied Bayes?", "acronym": "CM", "label": "cM", "ID": "6332"}, {"sentence": "lhe stru~ure ofthese atoms is presented belo~  (<root> (<lemma>  (< para~CMesc0\"pfi'on-sclector >  < fiaorph~logic .-~:xi. \"", "acronym": "CM", "label": "cM", "ID": "6333"}, {"sentence": "The atoms generated in  this situation have the foll6wing structure:  (UNKNOWN < unknown-word>  ( < ixm~'ble-root > < morphologiCMe.scri'pdon >'~*)  The unknown word is associated with aql legal segmentations  and for each of them the morphological inf6i'rnation deduced  from the identified en \"dings i  prOcidex\\[  Lexical synthesis i the reverse of lexical analysk The process  interface nsures conversion of a morpho-lexical tom sefluence  into a word _Sg:luence.", "acronym": "CM", "label": "cM", "ID": "6334"}, {"sentence": "The development set with no  morphologiCM processing was used for these tests.", "acronym": "CM", "label": "cM", "ID": "6335"}, {"sentence": "Note also that these lexi-  cal rules can be interpreted statically as well as dy-  namiCMly.", "acronym": "CM", "label": "cM", "ID": "6336"}, {"sentence": "We use the MIRA and Support Vector Machines for experiments on Arabic, Dari, English, and Pashto, and provide a detailed analysis of our results.", "acronym": "MIRA", "label": "Margin-Infused Relaxed Algorithm", "ID": "6337"}, {"sentence": "MIRA for Moses.", "acronym": "MIRA", "label": "Margin Infused Relaxed Algorithm", "ID": "6338"}, {"sentence": "In order to incorporate second-order features (specifically, sibling features), McDonald et al pro- posed a dependency parser based on the Eisner algo- rithm (MIRA, 2006).", "acronym": "MIRA", "label": "McDonald and Pereira", "ID": "6339"}, {"sentence": "Unlike the training procedure employed by McDonald et al (2005b) and MIRA (2006), we provide positive and negative examples in the training data.", "acronym": "MIRA", "label": "McDonald and Pereira", "ID": "6340"}, {"sentence": "2005a) 90.9 MIRA (2006) 91.5 Huang and Sagae (2010) 92.1 Koo and Collins (2010) 93.04 Zhang and Nivre (2011) 92.9 Martins et al. (", "acronym": "MIRA", "label": "McDonald and Pereira", "ID": "6341"}, {"sentence": "We adopt the second-order graph- based model of MIRA (2006), which casts the problem as finding an optimal tree from a fully-connect directed graph and factors the score of a dependency tree into scores of pairs of sibling dependencies.", "acronym": "MIRA", "label": "McDonald and Pereira", "ID": "6342"}, {"sentence": "We ex- perimented with popular feature sets previously used for named entity (McCallum and Li, 2003) and gene (MIRA, 2005) recognition including orthographic, part-of-speech (POS), shallow parsing and gazetteers.", "acronym": "MIRA", "label": "McDonald and Pereira", "ID": "6343"}, {"sentence": "However, online learning for NLP typically involves expensive inference on each example for 10 or more passes over millions of examples, which of- ten makes training too slow in practice; for example systems such as the popular (2nd-order) MST parser (MIRA, 2006) usually require the order of days to train on the Treebank on a com- modity machine (McDonald et al 2010).", "acronym": "MIRA", "label": "McDonald and Pereira", "ID": "6344"}, {"sentence": "MIRA for moses.", "acronym": "MIRA", "label": "Margin infused relaxed algorithm", "ID": "6345"}, {"sentence": "verb part- of-speech; and INC aspect.", "acronym": "INC", "label": "incompletive", "ID": "6346"}, {"sentence": "Since Identifinder occasionally tags an  entity INCly, we manually went through each  selection to filter out entities that were not people?s  names.", "acronym": "INC", "label": "incorrect", "ID": "6347"}, {"sentence": "Two patterns are grammatically INC due to gender dis- agreement, namely wydawa?a sie?", "acronym": "INC", "label": "incorrect", "ID": "6348"}, {"sentence": "Fur- thermore, from a half to two thirds of the INC responses were specifically about the subject the query wanted to exclude, displaying little or no understanding of excision alternative phrases.", "acronym": "INC", "label": "incorrect", "ID": "6349"}, {"sentence": "Consider again the  three high-salience words to which our unsuper-  vised learning program assigned INC gen-  der: \"husband\", \"wife\", and \"years.\"", "acronym": "INC", "label": "incorrect", "ID": "6350"}, {"sentence": "in the gold standard, 1285 are attached correctly by the parser, while 607 receive an INC regent.", "acronym": "INC", "label": "incorrect", "ID": "6351"}, {"sentence": "981 correct parsed unparsed average INC ambiguity Existing 50% 8% 42% 10.62 vocab w/added 76% 8% 14% 12.56 vocab Table 1: Grammar performance on held-out data evaluation was carried out only on the remaining 72 sentences.", "acronym": "INC", "label": "incorrect", "ID": "6352"}, {"sentence": "On the other hand, they could prove to be invaluable when training a NER tag- ger for UGC, which is known to be noisy and fragmented.", "acronym": "UGC", "label": "User Generated Content", "ID": "6353"}, {"sentence": "The French Social Media Bank: a Treebank of Noisy UGC.", "acronym": "UGC", "label": "User Generated Content", "ID": "6354"}, {"sentence": "impressme : The Language of Motivation in UGC.", "acronym": "UGC", "label": "User Generated Content", "ID": "6355"}, {"sentence": "A Hierarchical Entity-based Approach to Structuralize UGC in Social Media: A Case of Yahoo!", "acronym": "UGC", "label": "User Generated Content", "ID": "6356"}, {"sentence": "Mining Refinements to Online Instructions from UGC Gregory Druck Yahoo!", "acronym": "UGC", "label": "User Generated Content", "ID": "6357"}, {"sentence": "UGC has been investigated in the context of machine translation in recent work dealing specifically with spelling correc- tion (Bertoldi et al.,", "acronym": "UGC", "label": "User-generated content", "ID": "6358"}, {"sentence": "UGC on the social media rep- resents the views of the users and hence, may be opinion-bearing.", "acronym": "UGC", "label": "User-generated content", "ID": "6359"}, {"sentence": "1 Introduction UGC ?", "acronym": "UGC", "label": "User-generated content", "ID": "6360"}, {"sentence": "2 Related work 2.1 Text normalization UGC on the web is notoriously low-quality, containing slang, abbreviations, inconsis- tent grammar and spelling.", "acronym": "UGC", "label": "User-generated content", "ID": "6361"}, {"sentence": "2014 Association for Computational Linguistics Mining Lexical Variants from Microblogs: An Unsupervised Multilingual Approach Alejandro Mosquera University of Alicante San Vicente del Raspeig s/n - 03690 Alicante, Spain amosquera@dlsi.ua.es Paloma Moreda University of Alicante San Vicente del Raspeig s/n - 03690 Alicante, Spain moreda@dlsi.ua.es Abstract UGC has become a re- current resource for NLP tools and ap- plications, hence many efforts have been made lately in order to handle the noise present in short social media texts.", "acronym": "UGC", "label": "User-generated content", "ID": "6362"}, {"sentence": "Preliminary experiments with the HAC algorithm showed that the number of clusters used did not have a big impact on translation quality, 1 so we will only present results that use the same num- ber of clusters as in the manual partitions (10 for Arabic and 17 for Chinese).", "acronym": "HAC", "label": "hierarchical agglomerative clustering", "ID": "6363"}, {"sentence": "Compared with other possible clustering approaches, such as HAC (Kartsaklis et al.,", "acronym": "HAC", "label": "hierarchical agglomerative clustering", "ID": "6364"}, {"sentence": "Bottom-up clustering is also  called HAC,  which is more popular than top-down clustering.", "acronym": "HAC", "label": "hierarchical agglomerative clustering", "ID": "6365"}, {"sentence": "In this paper the following clustering methods  are used: a genetic algorithm with integers, vec- tor quantization networks trained by a genetic  algorithm, HAC  with different metrics.", "acronym": "HAC", "label": "hierarchical agglomerative clustering", "ID": "6366"}, {"sentence": "A den- dogram of the similarity between the classifiers is shown in Figure 2, derived using maximum linkage HAC.", "acronym": "HAC", "label": "hierarchical agglomerative clustering", "ID": "6367"}, {"sentence": "attack bang hit knock walkout find attack - 4 18 7 0 0 bang - 38 43 2 0 0 hit - 44 2 29 knock - 2 0 walkout - 0 find - We used the CLUTO clustering toolkit (Karypis, 2002) to induce a HAC on the vectors for Ws.", "acronym": "HAC", "label": "hierarchical agglomerative clustering", "ID": "6368"}, {"sentence": "And we try to  use HAC al- gorithm to help raise the recall score.", "acronym": "HAC", "label": "Hierarchical Agglomerative Clustering", "ID": "6369"}, {"sentence": "HAC can be applied to both feature vectors and collocation graphs.", "acronym": "HAC", "label": "Hierarchical Agglomerative Clustering", "ID": "6370"}, {"sentence": "2 Combining Clustering and Machine Learning We use HAC (see e.g. Everitt (1993)) as our clustering method.", "acronym": "HAC", "label": "Hierarchical Agglomerative Clustering", "ID": "6371"}, {"sentence": "We follow (Jur- gens and Stevens, 2010a) and cluster the final cen- troids with HAC, with the average link criteria as suggested by (Ped- ersen and Bruce, 1997).", "acronym": "HAC", "label": "Hierarchical Agglomerative Clustering", "ID": "6372"}, {"sentence": "Yaari Y. (1997) Segmentation of Expository Text by  HAC.", "acronym": "HAC", "label": "Hierarchical Agglomerative Clustering", "ID": "6373"}, {"sentence": "4.3 HAC  In this work we consider hierarchical agglomera- tive binary clustering where we set each utter- ance to one subclass and then we consequently  group classes into pairs until there is only one  ).", "acronym": "HAC", "label": "Hierarchical Agglomerative Clustering", "ID": "6374"}, {"sentence": "Unsupervised learning of SO from a hundred- billion-word corpus (technical report erc-1094).", "acronym": "SO", "label": "semantic orientation", "ID": "6375"}, {"sentence": "Various approaches have been adopted in subjec- tivity detection, SO detection,  review classification and review mining.", "acronym": "SO", "label": "semantic orientation", "ID": "6376"}, {"sentence": "They use the two terms excellent and poor as seed  terms to determine the SO of  other terms.", "acronym": "SO", "label": "semantic orientation", "ID": "6377"}, {"sentence": "SO applied to unsupervised classifi- cation of reviews.", "acronym": "SO", "label": "semantic orientation", "ID": "6378"}, {"sentence": "Deter- mining the SO of terms through  gloss classification.", "acronym": "SO", "label": "semantic orientation", "ID": "6379"}, {"sentence": "Latent variable models for SOs of phrases.", "acronym": "SO", "label": "semantic orientation", "ID": "6380"}, {"sentence": "Acknowledgments We gratefully acknowledge Mike Bada?s help in loading the SO into Prote?ge?.", "acronym": "SO", "label": "Sequence Ontology", "ID": "6381"}, {"sentence": "For example, if a string in text refers to something in the domain of the Gene Ontology, we take it as belonging to a Gene Ontology seman- tic class (using the name of the ontology only for convenience); if a string in text refers to something belonging to the domain of the SO, we take it as belonging to a SO se- mantic class.", "acronym": "SO", "label": "Sequence Ontology", "ID": "6382"}, {"sentence": "We loaded the ConceptMapper with dictionar- ies derived from several ontologies, including the Gene Ontology Cellular Component branch, Cell Type Ontology, BRENDA Tissue Ontology, and the SO.", "acronym": "SO", "label": "Sequence Ontology", "ID": "6383"}, {"sentence": "SO ?", "acronym": "SO", "label": "Sequence Ontology", "ID": "6384"}, {"sentence": "The best decomposition order varies from language to language: right-to-left in SO is best for Chinese-English, right-to-left in target order is best for German-English and left-to-right or right- to-left in target order are best in English-Bulgarian.", "acronym": "SO", "label": "source order", "ID": "6385"}, {"sentence": "get side, for X , the order of the two children follows the SO, while for Y , the or- der follows the inverse.", "acronym": "SO", "label": "source order", "ID": "6386"}, {"sentence": "We compare all four decomposition orders (SO left-to-right and right-to-left, and target order left-to-right and right- to-left).", "acronym": "SO", "label": "source order", "ID": "6387"}, {"sentence": "Likewise SO based mod- els provide distributions over source sentences and unordered target sides of MTUs.", "acronym": "SO", "label": "source order", "ID": "6388"}, {"sentence": "The first clause (monotone, reverse) in- dicates whether the target language translation order follows the SO; the second (adjacent, gap) indicates whether the source phrases are adjacent or separated by an intervening phrase on the target side.", "acronym": "SO", "label": "source order", "ID": "6389"}, {"sentence": "We get around this problem by splitting the  knowledge SOing into two or more orderings that are  partial.", "acronym": "SO", "label": "source order", "ID": "6390"}, {"sentence": "2 Polarization across Dimensions IP models describe probabilistic rela- tionships between observed responses (votes) on a set of items (bills) by a set of responders (legis- lators) who are characterized by continuous latent traits (Fox, 2010).", "acronym": "IP", "label": "Ideal point", "ID": "6391"}, {"sentence": "IP models are often created using Bayesian techniques over large amounts of roll-call data (Clin- ton et al, 2004; Jackman, 2001).", "acronym": "IP", "label": "Ideal point", "ID": "6392"}, {"sentence": "IP models assume all legisla- tors and bills can be plotted as single points in one- dimensional ?", "acronym": "IP", "label": "Ideal point", "ID": "6393"}, {"sentence": "Using music copyright as the paral- lel, consider the situation where an AAC user, Alice say, imports text from a novel under copyright (me- chanical rights) and adapts it by adding other text and other copyright material in order to create her own monologue (IP rights).", "acronym": "IP", "label": "intellectual property", "ID": "6394"}, {"sentence": "The other topic, named C2, talks about IP and contains 10 pairs of paper drafts.", "acronym": "IP", "label": "intellectual property", "ID": "6395"}, {"sentence": "Intellectual Property Rights  Since lexicons, unlike text and speech databases, are  likely to be incorporated (perhaps in derived form) in  commercial HLT products, IP rights  come to center stage.", "acronym": "IP", "label": "intellectual property", "ID": "6396"}, {"sentence": "Moreover, the increased creation of novel utterances and wider opportunity to relay such utterances potentially in- crease IP issues.", "acronym": "IP", "label": "intellectual property", "ID": "6397"}, {"sentence": "150 (a) first draft (b) final draft (c) Revision detection using Hashemi?s approach Figure 1: Fragments of a paper in corpus C2 discussing IP, (c) is Hashemi?s work, green for recognized modifications, blue for insertions and red for deletion For sentence alignment, each sentence in the fi- nal draft is assigned the index of its aligned sen- tence in the original draft.", "acronym": "IP", "label": "intellectual property", "ID": "6398"}, {"sentence": "We know that resources themselves are not openly available, but at least the metadata description should inform the community about their existence, about IP rights and modes of usage.", "acronym": "IP", "label": "intellectual property", "ID": "6399"}, {"sentence": "3.4 Independent Phrase Chunking There is no special rule for IP chunking.", "acronym": "IP", "label": "independent phrase", "ID": "6400"}, {"sentence": "It can be done only through knowledge base that stores the cases where IPs take place.", "acronym": "IP", "label": "independent phrase", "ID": "6401"}, {"sentence": "This result can be attributed to the fact that there are too small number of exceptions for adverb phrases and IPs.", "acronym": "IP", "label": "independent phrase", "ID": "6402"}, {"sentence": "However, phrase-based models cannot effectively conduct long-distance reordering because they are based purely on statis- tics of syntax-IPs.", "acronym": "IP", "label": "independent phrase", "ID": "6403"}, {"sentence": "This ap- proach is a bit like our change regions combined with Moses?s region-IP pairs.", "acronym": "IP", "label": "independent phrase", "ID": "6404"}, {"sentence": "These results were obtained from a  corpus of spoken utterances many of which con-  tained several IPs and sentences.", "acronym": "IP", "label": "independent phrase", "ID": "6405"}, {"sentence": "Since the word w has exactly k different analyses:  k E~ 1P(Ai w) Ei=IP = = \\[ = 1.", "acronym": "IP", "label": "I Pi", "ID": "6406"}, {"sentence": "3.1 Context of current part-of-speech and  current word  Here, we assume:  e(t i I G~) = I P(ti I p~wi)  \\[ P(tl IP)  where  piwi ~ dp  PiWi ~ dp  ~={piwi,piwi3C}+{pi,pi3C } and piwi is a  part-of-speech and word pair existing in the  training data C.   In this case, the current part-of-speech and  word pair is also used as a lexical entry to  determine the current structural chunk tag and  we have a total of about 49563 lexical  entries(\\[ ?", "acronym": "IP", "label": "I Pi", "ID": "6407"}, {"sentence": "dIPttsburgh); the variable ?", "acronym": "IP", "label": "I Pi", "ID": "6408"}, {"sentence": "We estimated the probabilities  P(c IP) and P(c) similarly to Resnik (1993) by us-  ing relative frequencies from the BNC, together with  WordNet (Miller et al, 1990) as a source of taxo-  nomic semantic lass information.", "acronym": "IP", "label": "I Pi", "ID": "6409"}, {"sentence": "e(c IP)\" log P(c IP_______~)  rli P(c)  (4) rli=~-~P(clpi).logP(Cplc;i)  C  In the case of adjective-noun combinations, the se-  lectional association measures the semantic fit of an  adjective and each of the semantic lasses of the  nouns it co-occurs with.", "acronym": "IP", "label": "I Pi", "ID": "6410"}, {"sentence": "We estimated the probabilities  P(c IP) and P(c) similarly to Resnik (1993) by us-  ing relative frequencies from the BNC, together with  W", "acronym": "IP", "label": "I Pi", "ID": "6411"}, {"sentence": "We estimated the probabilities  P(c IP) and P(c) similarly to Resnik (1993) by us-  ing relative frequencies from the BNC, together with  WordNet (Miller", "acronym": "IP", "label": "I Pi", "ID": "6412"}, {"sentence": "TAL et langues anciennes, 50(2):47?71.", "acronym": "TAL", "label": "Traitement automatique des langues", "ID": "6413"}, {"sentence": "TAL et langues an- ciennes, 50(2):201?235, October.", "acronym": "TAL", "label": "Traitement automatique des langues", "ID": "6414"}, {"sentence": "Essai de definition\", Actes du colloque  \"TAL naturelles  et syst~mes documentaires \", Clermont-Ferrand  \\[Milner 1989\\] Milner Jean-Claude (1989),  \"Introduction ~t une science du langage\", Scull,  Paris  \\[Monteil 1990\\] Monteil Marie Gaelle,  P~not Nadine (1990), \"Indexation  Automatique, fonctionnement - Principes  gtntraux\", Note interne HN46464, EDF,  Direction des Etudes et Recherches, Service  IPN, C", "acronym": "TAL", "label": "Traitement automatique des langues", "ID": "6415"}, {"sentence": "TAL, 36(1- 2):23?35.", "acronym": "TAL", "label": "Traitement automatique des langues", "ID": "6416"}, {"sentence": "TAL, 44(2):81?105.", "acronym": "TAL", "label": "Traitement automatique des langues", "ID": "6417"}, {"sentence": "TAL 44(2):81-105.", "acronym": "TAL", "label": "Traitement automatique des langues", "ID": "6418"}, {"sentence": "Approche Probabiliste de l'Analyse  Syntaxique\", TALs, vol.", "acronym": "TAL", "label": "Traitement Automatique des Langue", "ID": "6419"}, {"sentence": "Actes de la 12?me Conf?rence annuelle  sur le TALs Natu- relles, 451-456.", "acronym": "TAL", "label": "Traitement Automatique des Langue", "ID": "6420"}, {"sentence": "In TALs.", "acronym": "TAL", "label": "Traitement Automatique des Langue", "ID": "6421"}, {"sentence": "le 5 f?vrier 2012 sur les principales listes de diffusion dans les domaines des sciences de l?information (ASIS-L), de la fouille de textes (TextAnalytics, KDnuggets), des humanit?s num?riques (DH, Humanist), du TALs et de la linguistique de corpus (Corpora, LN, etc.).", "acronym": "TAL", "label": "Traitement Automatique des Langue", "ID": "6422"}, {"sentence": "Diego Molla, Rolf Schwitter, Michael Hess, Rachel Fournier, 2000, Extrans, an Answer Extraction System, TALs, Hermes Science Publication, 41-2, 495-522.", "acronym": "TAL", "label": "Traitement Automatique des Langue", "ID": "6423"}, {"sentence": "TALs, 46(1):115?140.", "acronym": "TAL", "label": "Traitement Automatique des Langue", "ID": "6424"}, {"sentence": "Fast Training of SV  Machines Using Sequential Minimal Optimization.", "acronym": "SV", "label": "Support Vector", "ID": "6425"}, {"sentence": "Learning to Classify Text  Using SV Machines: Methods, Theory,  and Algorithms.", "acronym": "SV", "label": "Support Vector", "ID": "6426"}, {"sentence": "931  SV Machines for Paraphrase Identification   and Corpus Construction  Chris Brockett and William B. Dolan  Natural Language Processing Group  Microsoft Research  One Microsoft Way, Redmond, WA 98502, U.S.A.  {chrisbkt, billdol}@microsoft.com  Abstract  The lack of readily-available large cor- pora of aligned monolingual sentence  pairs is a major obstacle to the devel- opment of Statist", "acronym": "SV", "label": "Support Vector", "ID": "6427"}, {"sentence": "Learning with Kernels: SV Machines,  Regularization, Optimization, and Beyond.", "acronym": "SV", "label": "Support Vector", "ID": "6428"}, {"sentence": "Advances in  Kernel Methods: SV Learning.", "acronym": "SV", "label": "Support Vector", "ID": "6429"}, {"sentence": "In this  paper, we describe the use of annotated  datasets and SV Machines  to induce larger monolingual para- phrase corpora from a comparable cor- pus of news clusters found on the  World Wide Web.", "acronym": "SV", "label": "Support Vector", "ID": "6430"}, {"sentence": "5.9 SV As happened with Polish, the results for SV (Nivre et al 2006b) are not as good as we could ex- pect; however we believe that the information shown in this paper is useful because MaltOptimizer detects which features are able to outperform the best model found so far and the model trained with MaltParser in default settings by a bit less than 2 points in the predicted scenario and mor", "acronym": "SV", "label": "Swedish", "ID": "6431"}, {"sentence": "5.9 SV As happened with Polish, the results for SV (Nivre et al 2006b) are not as good as we could ex- pect; however we believe that the information shown in this paper is useful because MaltOptimizer detects which features are able to outperform the best model found so far and the model trained with MaltParser in default settings by a bit less than 2 points in the predicted scenario and more than 2 points in the gold scenario.", "acronym": "SV", "label": "Swedish", "ID": "6432"}, {"sentence": "5.8 Polish Polish (S?widzin?ski and Wolin?ski, 2010) is one of the two languages (with SV) in which our model performs with the worst results.", "acronym": "SV", "label": "Swedish", "ID": "6433"}, {"sentence": "85.19 86.30 78.16 79.86 84.93 85.71 German 79.90 81.09 84.85 87.70 7.80 87.32 90.40 76.64 79.98 83.59 86.96 Hebrew 76.78 76.80 79.37 80.17 3.39 79.83 79.83 76.61 76.61 80.03 80.03 Hungarian 70.37 71.11 71.98 81.91 11.54 80.69 80.74 71.27 72.34 82.37 83.14 Korean 87.22 87.22 87.22 88.94 1.72 86.52 90.20 81.69 88.43 83.74 89.39 Polish 75.52 75.58 79.28 80.27 4.75 81.58 81.91 76.64 77.70 79.79 80.49 SV 76.75 76.75 78.91 79.76 3.01 74.85 74.85 75.73 75.73 77.67 77.67 Table 1: Labeled attachment score per phase compared to default settings for all training sets from the Shared Task on PMRLs in the gold scenario on the held-out test set for optimization.", "acronym": "SV", "label": "Swedish", "ID": "6434"}, {"sentence": "77.65 79.33 76.54 77.98 77.56 79.00 German 78.69 79.87 82.58 83.97 5.28 83.39 86.63 74.81 77.81 79.22 82.75 Hebrew 76.29 76.31 79.01 79.67 3.38 73.40 73.40 69.97 69.97 73.01 73.01 Hungarian 68.26 69.12 69.96 78.71 10.45 76.82 77.62 69.08 70.15 79.00 79.63 Korean 80.08 80.08 80.08 81.63 1.55 77.96 83.02 74.87 82.06 75.90 82.65 Polish 74.43 74.49 76.93 78.41 3.98 80.61 80.83 75.29 75.63 79.50 80.49 SV 74.53 74.53 76.51 77.66 3.13 72.90 72.90 73.21 73.21 75.82 75.82 Table 2: Labeled attachment score per phase compared to default settings for all training sets from the Shared Task on PMRLs in the predicted scenario on the held-out test set for optimization.", "acronym": "SV", "label": "Swedish", "ID": "6435"}, {"sentence": "For the Advanced search option we fully acknowledge to emulate the elegant interface to CQL- query building as provided by the SV Spr?akbanken 10 .", "acronym": "SV", "label": "Swedish", "ID": "6436"}, {"sentence": "By means of SV, the partial  SemSpec is linked to the denotation.", "acronym": "SV", "label": "shared variables", "ID": "6437"}, {"sentence": "At the grammatical level (i.e. leaving aside prag-  matic onsiderations),the translation of an InL' formu-  la to a scoped logical formula can be determined by the  specific scoping operator involved (indicated in the  sub-formula) nd by its relation to its semantic argu-  ment (indicated by SV).", "acronym": "SV", "label": "shared variables", "ID": "6438"}, {"sentence": "All the necessary  connections between phrases are made at the com-  position level when lexical entries are instantiated,  through the SV of the sigma projec-  tions.", "acronym": "SV", "label": "shared variables", "ID": "6439"}, {"sentence": "matching the remaining characters in the  word against he surface part of the spelling  pattern, thereby, through SV,  instantiating the characters for the lexical  part to provide a possible root spelling;  ?", "acronym": "SV", "label": "shared variables", "ID": "6440"}, {"sentence": "As a consequence of this mathematical formulation, the  metarules are expressed as couples of  monoadic  predicates with SV, For example, the  metanfle of coordination (3) is described by formula (7).", "acronym": "SV", "label": "shared variables", "ID": "6441"}, {"sentence": "This is because : (i) in InL', the scope of seeping  operators i left undefined ; (ii) SV ex-  press the relation between determiner and restrictor,  and between seeping operators and their semantic  arguments ; (iii) the grammar places constants (i.e.  proper names) in the specified place of the argumental  list of the predicate.", "acronym": "SV", "label": "shared variables", "ID": "6442"}, {"sentence": "chalte  is the FV with inflection (-?", "acronym": "FV", "label": "Full Verb", "ID": "6443"}, {"sentence": "4.2 FV Information  Three more disambiguation strategies condi-  tion the choice of tense on the full verb in  a CVP, viz.", "acronym": "FV", "label": "Full Verb", "ID": "6444"}, {"sentence": "uthe is a FV  with inflection -e ?", "acronym": "FV", "label": "Full Verb", "ID": "6445"}, {"sentence": "Figure 1: Example of an agenda graph for building  guidance domain  Feature Types Features #Size  Word-level   features  unigram 175  bigram 573  trigram 1034  Utterance-level   features  dialog act (DA) 9  main goal (MG) 16  slot FV 8  system act (SA) 26  Discourse-level   features  previous DA 10  previous MG 17  previous SA 27  Table 1: List of feature sets  90 For a set of N dialog examples X={xi|i=1,..,N}, the  binary feature vectors are represented by using a set of  features from the dialog corpus.", "acronym": "FV", "label": "filling status", "ID": "6446"}, {"sentence": "A  user's query may be expressed in a controlled lan-  guage (e.g., a boolean expression of keywords) or,  more desirably, aNL, such as English.", "acronym": "NL", "label": "natural anguage", "ID": "6447"}, {"sentence": "The environ-  ment can facilitate xploration of macro debug-  ging techniques and has relevance for studies  of translation from readable diagram programs  to NL instructions.", "acronym": "NL", "label": "natural anguage", "ID": "6448"}, {"sentence": "Recent develop-  ments in NL text retrieval.", "acronym": "NL", "label": "natural anguage", "ID": "6449"}, {"sentence": "For example, the  lexical atoms extracted by this process from the  CACM corpus (about 1 MB) include \"operating  system\", \"data structure\", \"decision table\", \"data  base\", \"real time\", \"NL\", \"on line\",  \"least squares\", \"numerical integration\", and \"fi-  nite state automaton\", among others.", "acronym": "NL", "label": "natural anguage", "ID": "6450"}, {"sentence": "Massively parallel parsing: A strongly inter-  active model of NL interpretation.", "acronym": "NL", "label": "natural anguage", "ID": "6451"}, {"sentence": "This environment is based on a  set of NL processing compo-  nents, at the morphologic, syntactic and  semantic levels.", "acronym": "NL", "label": "natural anguage", "ID": "6452"}, {"sentence": "1 Introduction Concerted efforts over the past years 1 in the NL language area in Europe, the Netherlands and the northern half of Belgium, Flanders, have yielded a corpus of over 500 million words of richly linguis- tically annotated contemporary written NL, called SoNaR (Oostdijk et al.,", "acronym": "NL", "label": "Dutch", "ID": "6453"}, {"sentence": "Licence details: http://creativecommons.org/licenses/by/4.0/ 1 Funded in large part by the NL Language Union in the STEVIN programme described in the Open Access book ?", "acronym": "NL", "label": "Dutch", "ID": "6454"}, {"sentence": "Essential Speech and Language Technology for NL?", "acronym": "NL", "label": "Dutch", "ID": "6455"}, {"sentence": "2 SoNaR The SoNaR project developed a large scale reference corpus for contemporary, written NL.", "acronym": "NL", "label": "Dutch", "ID": "6456"}, {"sentence": "OpenSoNaR: user-driven development of the SoNaR corpus interfaces Martin Reynaert TiCC / Tilburg University CLST / Radboud Universiteit Nijmegen reynaert@uvt.nl Matje van de Camp De Taalmonsters matje@taalmonsters.nl Menno van Zaanen TiCC / Tilburg University mvzaanen@uvt.nl Abstract OpenSoNaR is an online system that allows for analyzing and searching the large scale NL reference corpus SoNaR. Due to the size of the corpus, accessing the information contained in the dataset has proven to be difficult for less technically inclined researchers.", "acronym": "NL", "label": "Dutch", "ID": "6457"}, {"sentence": "This balanced corpus consists of about 540 million tokens of NL across a wide range of text types, such as books, magazine articles, reports, subtitles, but also data from the ?", "acronym": "NL", "label": "Dutch", "ID": "6458"}, {"sentence": "In J. Vicedo, P. Martnez- Barco, R. Muoz, and M. Saiz Noeda, editors, Ad- vances in NL Processing, volume 3230 of Lecture Notes in Computer Science, pages 82?90.", "acronym": "NL", "label": "Natural Language", "ID": "6459"}, {"sentence": "NL Engineer- ing, 6 (1):15 ?", "acronym": "NL", "label": "Natural Language", "ID": "6460"}, {"sentence": "He  has worked on Machine Learning in the context of NL Processing and  has published papers in several conferences.", "acronym": "NL", "label": "Natural Language", "ID": "6461"}, {"sentence": "Managing fieldwork data with Toolbox and the NL Toolkit.", "acronym": "NL", "label": "Natural Language", "ID": "6462"}, {"sentence": "Proceedings of the Fourteenth Conference on Computational NL Learning: Shared Task, pages 138?143, Uppsala, Sweden, 15-16 July 2010.", "acronym": "NL", "label": "Natural Language", "ID": "6463"}, {"sentence": "Head-Driven Statistical Models for NL Parsing.", "acronym": "NL", "label": "Natural Language", "ID": "6464"}, {"sentence": "The lemmatized 1http://domino.watson.ibm.com/library/CyberDig.nsf (key- word=RC22176) 2http://www.cs.cmu.edu/?alavie/METEOR 182 Table 1: Translation results as increasing amount of training data in IWSLT06 CSTAR track System AER BLEU METEOR 50K NL 0.217 0.158 0.427 lemma 0.199 0.167 0.431 100K NL 0.178 0.182 0.457 lemma 0.177 0.188 0.463 300K NL 0.150 0.223 0.501 lemma 0.132 0.217 0.505 400K NL 0.136 0.231 0.509 lemma 0.102 0.224 0.507 500K NL 0.119 0.235 0.519 lemma 0.104 0.241 0.522 600K NL 0.095 0.238 0.535 lemma 0.069 0.248 0.536 Table 2: Statistical significance test in terms of BLEU: sys1=non-lemma, sys2=lem", "acronym": "NL", "label": "nonlem", "ID": "6465"}, {"sentence": "matized 1http://domino.watson.ibm.com/library/CyberDig.nsf (key- word=RC22176) 2http://www.cs.cmu.edu/?alavie/METEOR 182 Table 1: Translation results as increasing amount of training data in IWSLT06 CSTAR track System AER BLEU METEOR 50K NL 0.217 0.158 0.427 lemma 0.199 0.167 0.431 100K NL 0.178 0.182 0.457 lemma 0.177 0.188 0.463 300K NL 0.150 0.223 0.501 lemma 0.132 0.217 0.505 400K NL 0.136 0.231 0.509 lemma 0.102 0.224 0.507 500K NL 0.119 0.235 0.519 lemma 0.104 0.241 0.522 600K NL 0.095 0.238 0.535 lemma 0.069 0.248 0.536 Table 2: Statistical significance test in terms of BLEU: sys1=non-lemma, sys2=lemma Data size Diff(sys1-sys2) 50K -0.092 [-0.0176,-0.0012] 100K -0.006 [-0.0155,0.0039] 300K 0.0057 [-0.0046,0.0161] 400K 0.0074 [-0.0023,0.0174] 500K -0.0054 [-0.", "acronym": "NL", "label": "nonlem", "ID": "6466"}, {"sentence": "The lemmatized 1http://domino.watson.ibm.com/library/CyberDig.nsf (key- word=RC22176) 2http://www.cs.cmu.edu/?alavie/METEOR 182 Table 1: Translation results as increasing amount of training data in IWSLT06 CSTAR track System AER BLEU METEOR 50K NL 0.217 0.158 0.427 lemma 0.199 0.167 0.431 100K NL 0.178 0.182 0.457 lemma 0.177 0.188 0.463 300K NL 0.150 0.223 0.501 lemma 0.132 0.217 0.505 400K NL 0.136 0.231 0.509 lemma 0.102 0.224 0.507 500K NL 0.119 0.235 0.519 lemma 0.104 0.241 0.522 600K NL 0.095 0.238 0.535 lemma 0.069 0.248 0.536 Table 2: Statistical significance test in terms of BLEU: sys1=non-lemma, sys2=lemma Data size Diff(sys1-sys2) 50K -0.092 [-0.0176,-0.00", "acronym": "NL", "label": "nonlem", "ID": "6467"}, {"sentence": "e/METEOR 182 Table 1: Translation results as increasing amount of training data in IWSLT06 CSTAR track System AER BLEU METEOR 50K NL 0.217 0.158 0.427 lemma 0.199 0.167 0.431 100K NL 0.178 0.182 0.457 lemma 0.177 0.188 0.463 300K NL 0.150 0.223 0.501 lemma 0.132 0.217 0.505 400K NL 0.136 0.231 0.509 lemma 0.102 0.224 0.507 500K NL 0.119 0.235 0.519 lemma 0.104 0.241 0.522 600K NL 0.095 0.238 0.535 lemma 0.069 0.248 0.536 Table 2: Statistical significance test in terms of BLEU: sys1=non-lemma, sys2=lemma Data size Diff(sys1-sys2) 50K -0.092 [-0.0176,-0.0012] 100K -0.006 [-0.0155,0.0039] 300K 0.0057 [-0.0046,0.0161] 400K 0.0074 [-0.0023,0.0174] 500K -0.0054 [-0.0139,0.0035] 600K -0.0103 [-0.0201,-0.0006] translations did not outperform the non-lemmatized ones uniforml", "acronym": "NL", "label": "nonlem", "ID": "6468"}, {"sentence": "The lemmatized 1http://domino.watson.ibm.com/library/CyberDig.nsf (key- word=RC22176) 2http://www.cs.cmu.edu/?alavie/METEOR 182 Table 1: Translation results as increasing amount of training data in IWSLT06 CSTAR track System AER BLEU METEOR 50K NL 0.217 0.158 0.427 lemma 0.199 0.167 0.431 100K NL 0.178 0.182 0.457 lemma 0.177 0.188 0.463 300K NL 0.150 0.223 0.501 lemma 0.132 0.217 0.505 400K NL 0.136 0.231 0.509 lemma 0.102 0.224 0.507 500K NL 0.119 0.235 0.519 lemma 0.104 0.241 0.522 600K NL 0.095 0.238 0.535 lemma 0.069 0.248 0.536 Table 2: Statistical significance test in terms of BLEU: sys1=non-lemma, sys2=lemma Data size Diff(sys1-sys2) 50K -0.092 [-0.0176,-0.0012] 100K -0.006 [-0.0155,0.0039] 300K 0.0057 [-0.0046,", "acronym": "NL", "label": "nonlem", "ID": "6469"}, {"sentence": ".nsf (key- word=RC22176) 2http://www.cs.cmu.edu/?alavie/METEOR 182 Table 1: Translation results as increasing amount of training data in IWSLT06 CSTAR track System AER BLEU METEOR 50K NL 0.217 0.158 0.427 lemma 0.199 0.167 0.431 100K NL 0.178 0.182 0.457 lemma 0.177 0.188 0.463 300K NL 0.150 0.223 0.501 lemma 0.132 0.217 0.505 400K NL 0.136 0.231 0.509 lemma 0.102 0.224 0.507 500K NL 0.119 0.235 0.519 lemma 0.104 0.241 0.522 600K NL 0.095 0.238 0.535 lemma 0.069 0.248 0.536 Table 2: Statistical significance test in terms of BLEU: sys1=non-lemma, sys2=lemma Data size Diff(sys1-sys2) 50K -0.092 [-0.0176,-0.0012] 100K -0.006 [-0.0155,0.0039] 300K 0.0057 [-0.0046,0.0161] 400K 0.0074 [-0.0023,0.0174] 500K -0.0054 [-0.0139,0.0035] 600K -0.0103 [-0.0201,-0.0006] translatio", "acronym": "NL", "label": "nonlem", "ID": "6470"}, {"sentence": "The term 'metaphor' is often used to refer to NL comparisons that  are novel and vivid and that convey ideas that might otherwise be difficult to express (Ortony, 1975).", "acronym": "NL", "label": "nonliteral", "ID": "6471"}, {"sentence": "Most would agree that metaphors are NL similarity comparisons (though not everyone  would agree on how literality should be defined), and that they are typically used for expressive-affective as  opposed to explanatory-predictive purposes.", "acronym": "NL", "label": "nonliteral", "ID": "6472"}, {"sentence": "A clustering approach for the nearly unsupervised recognition of NL language.", "acronym": "NL", "label": "nonliteral", "ID": "6473"}, {"sentence": "Gedigian et alused PropBank annotation (arguments and their semantic pour *NL cluster* wsj04:7878 N As manufacturers get bigger, they are likely to pour more money into the battle for shelf space, raising the ante for new players.", "acronym": "NL", "label": "nonliteral", "ID": "6474"}, {"sentence": "Yet, what is still assumed, rather than demonstrated, is that NL  uses of language are sometimes necessary for accomplishing such goals, rather than merely convenient or  elegant ways of doing so.", "acronym": "NL", "label": "nonliteral", "ID": "6475"}, {"sentence": "A clustering ap- proach for the nearly unsupervised recognition of NL language.", "acronym": "NL", "label": "nonliteral", "ID": "6476"}, {"sentence": "Ring s t ruc tures  a re  a very  f lex ib le  med ium,   su i tab le  fo r  the  organ izat ion  of data on which  NL  operates  and fo rmal i zed  enough to be amenab le  to man ipu la t ion  by  computer  p rograms.", "acronym": "NL", "label": "natura l  language", "ID": "6477"}, {"sentence": "By des ign ing  the log ica l  par t  su f f i c ient ly  genera l ,   one can use i t  even fo r  var ious  NLs,  i f  one  combln~s it with suitable information parts.", "acronym": "NL", "label": "natura l  language", "ID": "6478"}, {"sentence": "The defect in the ex is tent  theor ies  is the lack  of explanat ion of the mechan ism for adjust ing to tile  real world the formal  symbol ic  sys tems used in the  theories;  the only tMng they explain is the relation  between NL and the formal  system.", "acronym": "NL", "label": "natura l  language", "ID": "6479"}, {"sentence": "Natura l   language encompasses  a mul t i tude  of fo rmal  languages  and it is the  complex i t ies  of the memory  s t ruc tures  on which  NL  can  and does  operate  that  account  for the complex i t ies ,  f lex ib i l i ty   and r i chness  of NL.", "acronym": "NL", "label": "natura l  language", "ID": "6480"}, {"sentence": "These  la t te r  give r i se  to the  notor ious  prob lem of ambigu i t ies  in NL ana lys i s .", "acronym": "NL", "label": "natura l  language", "ID": "6481"}, {"sentence": "Tk, with k on the order 104, similar in granularity to the NL topic hierarchy (Kornai et al2003) and reserve T0 to topicless texts or ?", "acronym": "NL", "label": "Northern Light", "ID": "6482"}, {"sentence": "Through a  series of experiments, we found that we could  download highly useful information from Web  search engines such as Google, Yahoo, and  NL by searching ambiguous location  names in the Gazetteer.", "acronym": "NL", "label": "Northern Light", "ID": "6483"}, {"sentence": "He has developed several machine learning based NL  processing systems that are widely used in the computational linguistics community and  in industry and has presented invited talks and tutorials in several major conferences.", "acronym": "NL", "label": "natural language", "ID": "6484"}, {"sentence": "2 Motivation PP attachment disambiguation has often been studied as a benchmark test for empirical meth- ods in NL processing.", "acronym": "NL", "label": "natural language", "ID": "6485"}, {"sentence": "As for documents in NL, the amount of source code on Internet is increasing; facilitating the re-use of all or part of previously implemented programs.1 If no reference to the original work is included, pla- giarism would be committed.", "acronym": "NL", "label": "natural language", "ID": "6486"}, {"sentence": "2002), an open source NL engineering  system.", "acronym": "NL", "label": "natural language", "ID": "6487"}, {"sentence": "Roth has published broadly in machine learning, NL processing,  knowledge representation and reasoning and received several paper, teaching and  research awards.", "acronym": "NL", "label": "natural language", "ID": "6488"}, {"sentence": "Applied to the subtask of syntax analysis, the di- chotomy manifests itself in the existence of learnt and handwritten grammars of NLs.", "acronym": "NL", "label": "natural language", "ID": "6489"}, {"sentence": "He published several papers in NL  processing, machine learning and semantic interpretation.", "acronym": "NL", "label": "natural language", "ID": "6490"}, {"sentence": "Machine learning approaches using DTs  proposed so far have focused on preference selection  criteria directly derived from the decision tree re-  sults.", "acronym": "DTs", "label": "decision trees", "ID": "6491"}, {"sentence": "In  the case of DTs, we do have to provide in-  formation about possible antecedent indicators (syn-  tactic, semantic, and pragmatic features) contained  in the corpus, but the relevance of features for the  resolution task is extracted automatically from the  training data.", "acronym": "DTs", "label": "decision trees", "ID": "6492"}, {"sentence": "(Magerman 95; Jelinek et al 94) describe a  history-based approach which uses DTs to  estimate 7a(T\\[S).", "acronym": "DTs", "label": "decision trees", "ID": "6493"}, {"sentence": "How-  ever, DTs are characterized by an indepen-  dent learning of specific features, i.e., relations be-  tween single attributes cannot be obtained automat-  ically.", "acronym": "DTs", "label": "decision trees", "ID": "6494"}, {"sentence": "Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi- mum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), DTs (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998).", "acronym": "DTs", "label": "decision trees", "ID": "6495"}, {"sentence": "Machine learning approaches using DTs  proposed so far have focused on preference selection  criteria directly deriv", "acronym": "DTs", "label": "decision trees", "ID": "6496"}, {"sentence": "Their Machine  Learning-based Resolver (MLR) is trained us-  ing DTs with 1971 anaphoras (exclud-  ing those referring to multiple discontinuous an-  tecedents) and they report an average success  rate of 74.8%.", "acronym": "DTs", "label": "decision trees", "ID": "6497"}, {"sentence": "5 Visualization of DTs We accompany the above Scala library with a web- based visualization tool that runs the two parsers in parallel and visualizes the two outputs for the same text side by side.", "acronym": "DTs", "label": "Discourse Trees", "ID": "6498"}, {"sentence": "Experiments in Constructing a Corpus of DTs.", "acronym": "DTs", "label": "Discourse Trees", "ID": "6499"}, {"sentence": "37  Experiments in Constructing a Corpus of DTs  Daniel Marcu  Information Sciences Institute and  Department of Computer Science  University of Southern California  4676 Admiralty Way, Suite 1001  Marina del Rey, CA 90292  marcu @isi.", "acronym": "DTs", "label": "Discourse Trees", "ID": "6500"}, {"sentence": "DTs Are Good Indicators  of Importance in Text.", "acronym": "DTs", "label": "Discourse Trees", "ID": "6501"}, {"sentence": "DTs Are Good In- dicators of Importance in Text.", "acronym": "DTs", "label": "Discourse Trees", "ID": "6502"}, {"sentence": "3.1 Generating DTs In Rhetorical Structure Theory, discourse analysis involves two subtasks: (i) discourse segmentation, or breaking the text into a sequence of EDUs, and (ii) discourse parsing, or the task of linking the units (EDUs and larger discourse units) into la- beled discourse trees.", "acronym": "DTs", "label": "Discourse Trees", "ID": "6503"}, {"sentence": "This accounts for the fact that the English PP[in] can have the anchored interpre- tation in a clause with a future auxiliary, such as (12) and (39), or in a clause with a futurate present tense, such as we are leaving in a minute, but not in a clause with a VBD, such as (2), or in a clause with a non-futurate present tense, such as (11).", "acronym": "VBD", "label": "past tense verb", "ID": "6504"}, {"sentence": "For instance, many surnames  are at the same time nouns or plural nouns in  English and thus in both variants can be fol-  lowed by a VBD.", "acronym": "VBD", "label": "past tense verb", "ID": "6505"}, {"sentence": "Three  features had high positive loadings on this dimen- sion:  Frequency of past perfect aspect verbs, fre- quency of VBDs and frequency of 3rd  person singular pronouns.", "acronym": "VBD", "label": "past tense verb", "ID": "6506"}, {"sentence": "A VBD situates the phrase in 2003 differently than one in the future.", "acronym": "VBD", "label": "past tense verb", "ID": "6507"}, {"sentence": "The need for the category of agglutination is a result of the way VBD forms are seg- mented (cf. (", "acronym": "VBD", "label": "past tense verb", "ID": "6508"}, {"sentence": "For example, the triple \\[1\\] 'be'  \\[2\\] adverb \\[3\\] past-tense-verb has been assigned a  scaling factor which downgrades a sequence contain-  ing this triple compared with a competing sequence  of \\[1\\] 'be' \\[2\\] adverb \\[3\\]-past-participle/adjective, on  the basis that after a form of 'be', past participles and  adjectives are more likely than a VBD  (Marshall (1983), p. 146).", "acronym": "VBD", "label": "past tense verb", "ID": "6509"}, {"sentence": "In some cases, such as VBDs or noun plurals, morphological distinctions found in Czech are also found in English.", "acronym": "VBD", "label": "verb past tense", "ID": "6510"}, {"sentence": "Main verb tenses are indefinitely referential, cre-  ating a new temporal entity under constraints  imposed by its type (i.e., past, present, or fu-  ture) in relation to a discourse reference time 2 tR.  For instance, a main VBD introduces a new temporal entity t under the constraint prior-  to(t, tR).", "acronym": "VBD", "label": "verb past tense", "ID": "6511"}, {"sentence": "This allows to generate out-of-vocabulary (OOV) words and phrases, which are not only recogni- tion errors, but also plausible variants of different source phrases that can be translated to one tar- get phrase, e.g., VBD forms or function words.", "acronym": "VBD", "label": "verb past tense", "ID": "6512"}, {"sentence": "{ assert : proposition presup : proposition* 3.2 Alternative Sets The concept of alternative sets VBZ an impor- tant role in the semantics of alternative phrases.", "acronym": "VBZ", "label": "plays", "ID": "6513"}, {"sentence": "Table 1 disVBZ the distribution of entities versus  their occurrences in our corpus.", "acronym": "VBZ", "label": "plays", "ID": "6514"}, {"sentence": "The space reconst ructor  VBZ the role of recon-   strucUng the semant ic  space so that the user  can be  satisfied.", "acronym": "VBZ", "label": "plays", "ID": "6515"}, {"sentence": "His research work studies  the role that an external context, such as the real world or a simulated world, VBZ in  semantic interpretation and learning protocols.", "acronym": "VBZ", "label": "plays", "ID": "6516"}, {"sentence": "and Vicedo Question Answering in Restricted Domains: An Overview it should be noted that some words would still have several senses available and therefore word-sense disambiguation still VBZ a role.", "acronym": "VBZ", "label": "plays", "ID": "6517"}, {"sentence": "When a function is pointed, the plain text section disVBZ the source code.", "acronym": "VBZ", "label": "plays", "ID": "6518"}, {"sentence": "P7-X4---------- RV--7---------- z??ska?n??m (getting) NVBZ7-----A---- NVBZ7-----A---- telefonn??ch (phone) AAFP2----1A---- AAFP2----1A---- linek (lines) NNFP2-----A---- NNFP2-----A---- Figure 1: Annotation error: P7-X4----------, should have been: RV--7---------- strong advantage.", "acronym": "VBZ", "label": "NNS", "ID": "6519"}, {"sentence": "The used patterns are: 1) (DT|CD) (NN|VBZ), 2) DT JJ (NN|VBZ), 3) NN POS (NN|VBZ), and 4) PRP$ JJ (NN|VBZ).", "acronym": "VBZ", "label": "NNS", "ID": "6520"}, {"sentence": "The training set consists of 1674 documents from newswire, MZ articles, broadcast news, broad- cast conversations and webpages, and the develop- ment set consists of 202 documents from the same source.", "acronym": "MZ", "label": "magazine", "ID": "6521"}, {"sentence": "IEEE Circuits and systems MZ, 6(3):21?45.", "acronym": "MZ", "label": "magazine", "ID": "6522"}, {"sentence": "rt meadows pearland laval safeway osu turnt lds hayward westbank harkins parker jammin poutine huskies stillwater angeles temple cal bayou camelback blake mayne boul everett topeka usc murray jose houma mesa cherry katy est seatac sooners chargers menudito swaaaaggg lawd gilbert siiiiim jamming je ducks straighht oc mormon folsom gtf pima coors tsu sherbrooke victoria kc compton gateway roseville MZ dbacks englewood marcos pas beaverton manhattan meadowview megaplex juiced gumbo mcdowell pikes laredo fkn hella boomer rancho lake vallejo buku devils rockies texas centre sounders sooner ventura Table 6: Top 20 features selected for various regions using logistic regression on TWUS with a uniform 5 ?", "acronym": "MZ", "label": "magazine", "ID": "6523"}, {"sentence": "The dataset consists of 2083 documents from a much larger va- riety of genres, such as conversations, MZs, web text, etc.", "acronym": "MZ", "label": "magazine", "ID": "6524"}, {"sentence": "The texts that the templates were filled from were newspaper and technical MZ articles concerned either with joint business ventures or microelectronics fabrication technology.", "acronym": "MZ", "label": "magazine", "ID": "6525"}, {"sentence": "This balanced corpus consists of about 540 million tokens of Dutch across a wide range of text types, such as books, MZ articles, reports, subtitles, but also data from the ?", "acronym": "MZ", "label": "magazine", "ID": "6526"}, {"sentence": "AI MZ, 19(4):25?49.", "acronym": "MZ", "label": "Magazine", "ID": "6527"}, {"sentence": "Even for the `naturally' multi- agent task of soccer commentary, the systems de- scribed in the recent AI MZ special issue on RoboCup (Andr\u0013e et al, 2000) are all single-agent.", "acronym": "MZ", "label": "Magazine", "ID": "6528"}, {"sentence": "Technology and Society MZ, IEEE, 31(4):73?80.", "acronym": "MZ", "label": "Magazine", "ID": "6529"}, {"sentence": "AI MZ, 21(1):57{66, Spring.", "acronym": "MZ", "label": "Magazine", "ID": "6530"}, {"sentence": "AI MZ, pages 73{85, Spring.", "acronym": "MZ", "label": "Magazine", "ID": "6531"}, {"sentence": "by Euromoney and Banker?s MZ.", "acronym": "MZ", "label": "Magazine", "ID": "6532"}, {"sentence": "When human annotators were not sure, they used <OPTIONAL PSB=...> where PSB is a list of possible NE classes.", "acronym": "PSB", "label": "POSSIBILITY", "ID": "6533"}, {"sentence": "iNVESTIGATING THE PSB OF A HICROPROCESSOR-BASED  MACIIINE TRANSLATTON SYSTEM  Haro ld  L. Somers  Centre fo r  Computat iona l  L ingu is t i cs   Un ivers i ty  of  Hanchester  Ins t i tu te  o f  Sc ience and Technology  PO Box RR, Manchester  H60 tqO, England  ABSTRACT  This  paper descr ibes  an on-go in~ research   pro jec t  be ing car r ied  out  by s ta f f  and s tudents  ac  the Centre fo r  Compu", "acronym": "PSB", "label": "POSSIBILITY", "ID": "6534"}, {"sentence": "26 SYNONYMY 0 0 0 0 0 27 ANTONYMY 0 0 0 0 0 28 PROBABILITY 0 0 0 0 0 29 PSB 0 0 0 0 0 30 CERTAINTY 0 0 0 0 0 31 THEME 6.51 1.75 3.30 6.26 9.75 ?", "acronym": "PSB", "label": "POSSIBILITY", "ID": "6535"}, {"sentence": "26 SYNONYMY a word/concept that means the same or nearly the same as another word/concept; (NAME) (Marry is called Minnie); (Sowa 1994) 27 ANTONYMY a word/concept that is the opposite of another word/concept; (empty is the opposite of full); (Sowa 1994) 28 PROBABILITY OF the quality/state of being probable; likelihood EXISTENCE (There is little chance of rain tonight); (Sowa 1994) 29 PSB the state/condition of being possible; (I might go to Opera tonight); (Sowa 1994) 30 CERTAINTY the state/condition of being certain or without doubt; (He definitely left the house this morning); 31 THEME an entity that is changed/involved by the action/event denoted by the predicate; (music lover; John opened the door.); (", "acronym": "PSB", "label": "POSSIBILITY", "ID": "6536"}, {"sentence": "nt to CLAUSE, A doesn't want  CLAUSE(1), B appeals to A to realize CLAUSE),  (NECESSITY-WITH-SOURCE-ENVIROI~ENTAL-CIRCUMSTANCES: A is in-  different to CLAUSE, One realizes CLAUSE(1) if A doesn't  realize CLAUSE, A doesn't want CLAUSE(1)),  (NECESSITY-WITH-SOME-SOURCE: At least one type of necessity  'is given),  (PO~IBILITY-WITH-SOURCE-AGENT: Inner circumstances of A are  complete for CLAUSE),  (PSB-WITH-SOURCE-NON-AGENT: B is superior to A, B  agrees to realize CLAUSE, B realizes CLAUSE(1) if A real-  izes CLAUSE and B doesn't agree to realize CLAUSE, A  doesn't want CLAUSE(I ) ),  ( PSB-WITH-S OURCE-ENVIRONMENTAL -CIRCUMSTANCES : Environ-  mental circumstances are complete for CLAUSE),  (PSB-WITH-ALL-SOURCES: All types of possibilities are  - 238 -  given),  (WIL", "acronym": "PSB", "label": "POSSIBILITY", "ID": "6537"}, {"sentence": "of the PSB, volume 11, pages 100?111, Maui, HI.", "acronym": "PSB", "label": "Pacific Symposium on Biocomputing", "ID": "6538"}, {"sentence": "PSB, pages 652?663.", "acronym": "PSB", "label": "Pacific Symposium on Biocomputing", "ID": "6539"}, {"sentence": "In PSB, pages 451?", "acronym": "PSB", "label": "Pacific Symposium on Biocomputing", "ID": "6540"}, {"sentence": "PSB.", "acronym": "PSB", "label": "Pacific Symposium on Biocomputing", "ID": "6541"}, {"sentence": "In PSB.", "acronym": "PSB", "label": "Pacific Symposium on Biocomputing", "ID": "6542"}, {"sentence": "In Proceedings of the Second PASCAL Chal- lenges Workshop on RTE.", "acronym": "RTE", "label": "Recognizing Textual Entailment", "ID": "6543"}, {"sentence": "intino 31 Torino,10121, Italy kouylekov@celi.it Luca Dini Celi S.R.L. via San Quintino 31 Torino,10121, Italy dini@celi.it Alessio Bosca Celi S.R.L. via San Quintino 31 Torino,10121, Italy alessio.bosca@celi.it Marco Trevisan Celi S.R.L. via San Quintino 31 Torino, Italy trevisan@celi.it Abstract This paper presents CELI?s participation in the SemEval The Joint Student Response Anal- ysis and 8th RTE Challenge (Task7) and Cross-lingual Textual Entailment for Content Synchronization task (Task 8).", "acronym": "RTE", "label": "Recognizing Textual Entailment", "ID": "6544"}, {"sentence": "An Open-Source Package for RTE.", "acronym": "RTE", "label": "Recognizing Textual Entailment", "ID": "6545"}, {"sentence": "The PASCAL RTE Challenge.", "acronym": "RTE", "label": "Recognizing Textual Entailment", "ID": "6546"}, {"sentence": "With the introduction of the MSR alignment cor- pus (Brockett, 2007) developed from the second RTE challenge data (Bar- Haim et al.,", "acronym": "RTE", "label": "Recognizing Textual Entailment", "ID": "6547"}, {"sentence": "3 RTE Recent work in computational seman- tics (Haghighi et al, 2005; Hickl et al, 2006b; MacCartney et al, 2006) has demonstrated the viability of supervised machine learning-based approaches to the recognition of textual en- tailment (TE).", "acronym": "RTE", "label": "Recognizing Textual Entailment", "ID": "6548"}, {"sentence": "In Pro-ceedings of the PASCAL RTE Challenge Workshop.", "acronym": "RTE", "label": "Recognizing Textual En-tailment", "ID": "6549"}, {"sentence": "The fifth pascal RTE chal- lenge.", "acronym": "RTE", "label": "recognizing textual entailment", "ID": "6550"}, {"sentence": "The PASCAL RTE challenge.", "acronym": "RTE", "label": "recognizing textual entailment", "ID": "6551"}, {"sentence": "contains two major types of experimental setups: (i) those for an intrinsic eval- uation allow to evaluate the system performance in an isolated setting by comparing the system results with a human gold standard, and (ii) those for an extrinsic evaluation allow to evaluate the system with respect to a particular task at hand, where text similarity is a means for solving a concrete prob- lem, e.g. RTE.", "acronym": "RTE", "label": "recognizing textual entailment", "ID": "6552"}, {"sentence": "Biutee: A modular open-source system for RTE.", "acronym": "RTE", "label": "recognizing textual entailment", "ID": "6553"}, {"sentence": "Tree edit models for RTEs, paraphrases, and answers to questions.", "acronym": "RTE", "label": "recognizing textual entailment", "ID": "6554"}, {"sentence": "Extrinsic Evaluation Our framework includes two setups for an extrinsic evaluation: detecting text reuse, and RTE.", "acronym": "RTE", "label": "recognizing textual entailment", "ID": "6555"}, {"sentence": "to be carried out by 7 users by interacting with an IM platform, which they were told to be the system interface.", "acronym": "IM", "label": "instant messaging", "ID": "6556"}, {"sentence": "Automatic IM dialogue using statistical models and dialogue acts.", "acronym": "IM", "label": "instant messaging", "ID": "6557"}, {"sentence": "They were all told to use a web application that describes the Desert Scenario (see Section 3) and proposes to un- dertake two IM chats with two human users5.", "acronym": "IM", "label": "instant messaging", "ID": "6558"}, {"sentence": "1 Introduction The increasing prevalence of informal text from which a dialog structure can be reconstructed (e.g., email or IM), raises new challenges if we are to help users make sense of this cacophony.", "acronym": "IM", "label": "instant messaging", "ID": "6559"}, {"sentence": "Arabizi is a non-standard romanization of Arabic  script that is widely adopted for communication  over the Internet (World Wide Web, email) or  for sending messages (IM and  mobile phone text messaging) when the actual  Arabic script alphabet is either unavailable for  technical reasons or otherwise more difficult to  use.", "acronym": "IM", "label": "instant messaging", "ID": "6560"}, {"sentence": "85  Analysis of Intention in Dialogues Using Category Trees and  Its Application to Advertisement Recommendation  Hung-Chi Huang Ming-Shun Lin Hsin-Hsi Chen  Department of Computer Science and Information Engineering   National Taiwan University   Taipei, Taiwan   {hchuang, mslin}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw       Abstract  We propose an intention analysis system  for IM applications.", "acronym": "IM", "label": "instant messaging", "ID": "6561"}, {"sentence": "IMs are generated as follows.", "acronym": "IM", "label": "Interregnum", "ID": "6562"}, {"sentence": "IM to Denver ? ?? ?", "acronym": "IM", "label": "Interregnum", "ID": "6563"}, {"sentence": "Reparandum IM ? ?? ?", "acronym": "IM", "label": "Interregnum", "ID": "6564"}, {"sentence": "Because the paraphrase grammar is de- signed to directly relate surface language to dialogue moves, dialogue moves are generated directly, skip- ping the IM processing.", "acronym": "IM", "label": "Input Manager", "ID": "6565"}, {"sentence": "By contrast, DSMs are trained on large, domain-general corpora.", "acronym": "DSMs", "label": "distributional semantic models", "ID": "6566"}, {"sentence": "This result reinforces the importance of the contextualization procedure for DSMs.", "acronym": "DSMs", "label": "distributional semantic models", "ID": "6567"}, {"sentence": "There are a number of potential advantages that DSMs offer.", "acronym": "DSMs", "label": "distributional semantic models", "ID": "6568"}, {"sentence": "The method is based on DSMs by effectively treating abbreviations and their corresponding def- inition as synonymous, at least in the sense of shar- ing distributional properties.", "acronym": "DSMs", "label": "distributional semantic models", "ID": "6569"}, {"sentence": "A large scale evaluation of DSMs: Pa- rameters, interactions and model selection.", "acronym": "DSMs", "label": "distributional semantic models", "ID": "6570"}, {"sentence": "4 Experiments We trained the DSMs us- ing the Annotated Gigaword corpus (Napoles et al.,", "acronym": "DSMs", "label": "distributional semantic models", "ID": "6571"}, {"sentence": "c?2010 Association for Computational Linguistics DSMs Stefan Evert, University of Osnabr?ck 1.", "acronym": "DSMs", "label": "Distributional Semantic Models", "ID": "6572"}, {"sentence": "Contrasting Syntagmatic and Paradigmatic Relations: Insights from DSMs Gabriella Lapesa 3,1 1 Universit?at Osnabr?uck Institut f?ur Kognitionswissenschaft glapesa@uos.de Stefan Evert 2 2 FAU Erlangen-N?urnberg Professur f?ur Korpuslinguistik stefan.evert@fau.de Sabine Schulte im Walde 3 3 Universit?at Stuttgart Institut f?ur Maschinelle Sprachverarbeitung schulte@ims.uni-stuttgart.de Abstract This paper presents a large-scale evalua- ti", "acronym": "DSMs", "label": "Distributional Semantic Models", "ID": "6573"}, {"sentence": "UNIBA: Combining DSMs and Word Sense Disambiguation for Textual Similarity Pierpaolo Basile and Annalina Caputo and Giovanni Semeraro Department of Computer Science University of Bari Aldo Moro Via, E. Orabona, 4 - 70125 Bari (Italy) {firstname.surname}@uniba.it Abstract This paper describes the UNIBA team participation in the Cross-Level Semantic Similarity task at SemEval 2014.", "acronym": "DSMs", "label": "Distributional Semantic Models", "ID": "6574"}, {"sentence": "4 Conclusions and Future Work Our contribution is in the use of complementary features in order to learn the function of STS, a part of the challenge of building Compositional DSMs.", "acronym": "DSMs", "label": "Distributional Semantic Models", "ID": "6575"}, {"sentence": "2.6 Compositional features In DSMs, given the vector representations of two words, it is always possible to compute their similarity as the cosine of the angle between them.", "acronym": "DSMs", "label": "Distributional Semantic Models", "ID": "6576"}, {"sentence": "2.1 Distributional Semantics Level DSMs (DSM) are an easy way for building geometrical spaces of con- cepts, also known as Semantic (or Word) Spaces, by skimming through huge corpora of text in or- der to learn the context of word usage.", "acronym": "DSMs", "label": "Distributional Semantic Models", "ID": "6577"}, {"sentence": "Difference between the number of PP/ARGP/ADVP/CONJP phrases in the source and target; ?", "acronym": "ARG", "label": "NP/VP/ADJ", "ID": "6578"}, {"sentence": "difference between the number of PP/ARGP/ADVP/CONJP phrases in the source and target; ?", "acronym": "ARG", "label": "NP/VP/ADJ", "ID": "6579"}, {"sentence": "ARG Y (a branching5) and Z (a set of edges) are constraints on the edges that can be part of the solution, A. Edges in Y are required to be in the solution and edges in 5A branching is a subgraph that contains no cycles and no more than one edge directed into each node.", "acronym": "ARG", "label": "Arguments", "ID": "6580"}, {"sentence": "ARG are the complements selected by the  head 4.", "acronym": "ARG", "label": "Arguments", "ID": "6581"}, {"sentence": "In this paper, we present DAVID (Detector of ARG of Verbs with Incompatible Denota- tions), a resource-based system for detecting pref- erence violations.", "acronym": "ARG", "label": "Arguments", "ID": "6582"}, {"sentence": "ARG are assigned to clusters based on their inferred canonical function.", "acronym": "ARG", "label": "Arguments", "ID": "6583"}, {"sentence": "ARG that can be filled multiple times marked with ?", "acronym": "ARG", "label": "Arguments", "ID": "6584"}, {"sentence": "ARG of all but 2 of the 7 available mappings were edited, either to add missing arguments or to correct nonsensi- cal ones.", "acronym": "ARG", "label": "Arguments", "ID": "6585"}, {"sentence": "Binot and K. Jensen A Semantic Expert  Using an On-line Standard Dictionary  Proceedings of the IJCAI Milano, 1987  \\[41 K. Dahlgren and J. McDoweU KT in  Knowledge Reimesentation Proceedings of the  Coling-86 1986  151 Heidorn G.E. \"Augmented Phrase Structure  Grammar\" in \"Theoretical Issues in Natural  Language Processing\" N ash- Webber and  Schank ,eds, ACL 1975  161 J. Katz, P. Postal An Integrated Theory of  Linguistic Descriptions Cambridge, M.LT.", "acronym": "KT", "label": "Kind Types", "ID": "6586"}, {"sentence": "Dahlgren, K. and McDowell, J. 1986a KT in Knowledge  Representation.", "acronym": "KT", "label": "Kind Types", "ID": "6587"}, {"sentence": "Dalflgren, K., and J. McDowell (1986) KT  in Knowledge Representation.", "acronym": "KT", "label": "Kind Types", "ID": "6588"}, {"sentence": "1986 KT in Knowledge  Representation.", "acronym": "KT", "label": "Kind Types", "ID": "6589"}, {"sentence": "Although the KT value forms the  basis for this, it is not in itself sufficient.", "acronym": "KT", "label": "Knowledge Type", "ID": "6590"}, {"sentence": "2 Data, Tool and KTs Interoperability in building pipelined NLP applications is intended ensure the exchange of information between the different NLP tools.", "acronym": "KT", "label": "Knowledge Type", "ID": "6591"}, {"sentence": "These were selected because 0099 contained multiple tie up relationships, and the other contained a single tie up relationship wit h \t KT Words (Stems) Tokens Compounds Idioms Verb categories Nominal categories Grammar Arcs Grammar States Concepts Semantic Mappings Domain Template Core System 14,81 6 18,928 343 88 16 404 273 90 386 0 0 New/ Mod for MUC-5 387 81 1 110 5 0 0 9 5 1 147 1 1 204 some complex coreference phenomena.", "acronym": "KT", "label": "Knowledge Type", "ID": "6592"}, {"sentence": "Dimension Cohen?s Kappa  KT 0.9017  Certainty Level 0.9329  Polarity 0.9059  Manner 0.8944  Source 0.9520  Table 3.", "acronym": "KT", "label": "Knowledge Type", "ID": "6593"}, {"sentence": "Events with  the KT of Observation could corre- spond to new knowledge, but only if they repre- sent observations from the current study, rather  than observations cited from elsewhere.", "acronym": "KT", "label": "Knowledge Type", "ID": "6594"}, {"sentence": "9 .Acknowledgements   Thi~ report is based upon work supported by  the Defense Advanced DARPA  under Grant N00014-9O-J-1851 from the Office  of Naval Research and by the National Science  l:oun(lation under Grant 11H-89-02304.", "acronym": "DARPA", "label": "Research Projects Agency", "ID": "6595"}, {"sentence": "400-81-0030 of the National Institute of Education and by the Advanced DARPA of the  Department of Defense under Contract No.", "acronym": "DARPA", "label": "Research Projects Agency", "ID": "6596"}, {"sentence": "Defense Advanced DARPA,  February.", "acronym": "DARPA", "label": "Research Projects Agency", "ID": "6597"}, {"sentence": "This  work  was  sponsored by the Defense Advanced DARPA and was monitored by the Space and Naval  Warfare Systems Command under  contract  N000-39-86-C-0307  193  The 'stack' (which is far from being a stack in the computer science sense of a f irst- in-f irst-out  list) is a list of partial transcriptions, ordered by the PTE.", "acronym": "DARPA", "label": "Research Projects Agency", "ID": "6598"}, {"sentence": "Acknowledgements    We gratefully acknowledge the support of the Na- tional Science Foundation Grant NSF-0715078,  Consistent Criteria for Word Sense Disambigua- tion, and the GALE program of the Defense Ad- vanced DARPA, Contract No.", "acronym": "DARPA", "label": "Research Projects Agency", "ID": "6599"}, {"sentence": "Acknowledgements  This research was sponsored in part by US West and in part  by the Defense Advanced DARPA (DOD),  Axpa Order No.", "acronym": "DARPA", "label": "Research Projects Agency", "ID": "6600"}, {"sentence": "9 .Acknowledgements   Thi~ report is based upon work supported by  the DARPA  under Grant N00014-9O-J-1851 from the Office  of Naval Research and by the National Science  l:oun(lation under Grant 11H-89-02304.", "acronym": "DARPA", "label": "Defense Advanced Research Projects Agency", "ID": "6601"}, {"sentence": "Acknowledgments  This material is based upon work supported by the  DARPA un- der Contract No.", "acronym": "DARPA", "label": "Defense Advanced Research Projects Agency", "ID": "6602"}, {"sentence": "DARPA,  February.", "acronym": "DARPA", "label": "Defense Advanced Research Projects Agency", "ID": "6603"}, {"sentence": "This  work  was  sponsored by the DARPA and was monitored by the Space and Naval  Warfare Systems Command under  contract  N000-39-86-C-0307  193  The 'stack' (which is far from being a stack in the computer science sense of a f irst- in-f irst-out  list) is a list of partial transcriptions, ordered by the PTE.", "acronym": "DARPA", "label": "Defense Advanced Research Projects Agency", "ID": "6604"}, {"sentence": "Acknowledgments This research was supported in part by the GALE pro- gram of the DARPA, Contract No.", "acronym": "DARPA", "label": "Defense Advanced Research Projects Agency", "ID": "6605"}, {"sentence": "Acknowledgements  This research was sponsored in part by US West and in part  by the DARPA (DOD),  Axpa Order No.", "acronym": "DARPA", "label": "Defense Advanced Research Projects Agency", "ID": "6606"}, {"sentence": "NP repetition is  one simple way of approximately identifying the  TOP.", "acronym": "TOP", "label": "topic", "ID": "6607"}, {"sentence": "162  In effect, we use this probability information to  identify the TOP of the segment with the belief  that the TOP is more likely to be referred to by  a pronoun.", "acronym": "TOP", "label": "topic", "ID": "6608"}, {"sentence": "The more accurately the TOP of a seg-  ment can be identified, the higher the success  rate we expect an anaphora resolution system  can achieve.", "acronym": "TOP", "label": "topic", "ID": "6609"}, {"sentence": "The idea is similar to that used in  the centering approach (Brennan et al, 1987)  where a continued TOP is the highest-ranked  candidate for pronominalization.", "acronym": "TOP", "label": "topic", "ID": "6610"}, {"sentence": "References  by pronouns are closely related to the TOP or  the center of the discourse.", "acronym": "TOP", "label": "topic", "ID": "6611"}, {"sentence": "The more accurately the TOP of a seg-  ment can be identified, the higher the success  rate we expect an anaphora re", "acronym": "TOP", "label": "topic", "ID": "6612"}, {"sentence": "The more fre-  quently an entity is repeated, the more likely it  is to be the TOP of the story and thus to be  a candidate for pronominalization.", "acronym": "TOP", "label": "topic", "ID": "6613"}, {"sentence": "l emma exp lanat ion   o  suushi*  wa  suru   J  f  mo  (  )  nado  nai  aru   kara  koto  dewa  nen  hi  no  comma  period  numerals  TOP  'do'  right angular parenthesis  left angular parenthesis  TOP  left parenthesis  right parenthesis  'so forth'  dash  negative auxiliary  'exist ' , 'be'   ' from'  nominalizer  TOP  'year'   'day'   possessive particle  sible to think of a complex clue in terms of its com-  ponent clues for which a sentence is marked.", "acronym": "TOP", "label": "topic marker", "ID": "6614"}, {"sentence": "3.1 Extraction of Topic Part A phrase whose head word is marked with a TOP wa is extracted as a topic.", "acronym": "TOP", "label": "topic marker", "ID": "6615"}, {"sentence": "In Japanese,  the antecedent part can be syntactically deter-  mined, so far as the topic phrase is expressed with  the TOP.", "acronym": "TOP", "label": "topic marker", "ID": "6616"}, {"sentence": "For fear of noise, we only harvested PASs that have just a predicate and an argument for ga (nominative) with its topic (an NP) explicitly marked by a TOP wa.", "acronym": "TOP", "label": "topic marker", "ID": "6617"}, {"sentence": "7The names Top and Next refer to the TOP of the stack S and the first token in the remaining input I, respectively.", "acronym": "TOP", "label": "token on top", "ID": "6618"}, {"sentence": "In the arc-eager approach introduced by Nivre et al (2006) the possible actions are as follows, with s0 being the TOP of the stack and n0 being the next token in the buffer: ?", "acronym": "TOP", "label": "token on top", "ID": "6619"}, {"sentence": "Given an arbitrary configuration of the parser, there are four possible transitions to the next configuration (where t is the TOP of the stack, n is the next input token, w is any word, and r, r? ?", "acronym": "TOP", "label": "token on top", "ID": "6620"}, {"sentence": "that the Single head constraint is satisfied, while the Reduce transition can only be ap- plied if the TOP of the stack already has a head.", "acronym": "TOP", "label": "token on top", "ID": "6621"}, {"sentence": "In particular, many features involve properties of the two target tokens, the TOP of the stack ?", "acronym": "TOP", "label": "token on top", "ID": "6622"}, {"sentence": "with or with- out adding an arc from the TOP of the stack to the token pushed ?", "acronym": "TOP", "label": "token on top", "ID": "6623"}, {"sentence": "By collating by referent and ABSing away  to the gender classes of pronouns, rather than  individual pronouns, we have the relative fre-  quencies with which a given referent is referred  to by pronouns of each gender class.", "acronym": "ABS", "label": "abstract", "ID": "6624"}, {"sentence": "After several  inquiries, the learner will \"own\" several such  associative maps and will be able to continue the  learning process at the level of generality or  ABSion at which new knowledge can be  generated.", "acronym": "ABS", "label": "abstract", "ID": "6625"}, {"sentence": "The data source used by the system is the set of MEDLINE ABSs, a large bibliographic database that is accessed on-line via PubMed.", "acronym": "ABS", "label": "abstract", "ID": "6626"}, {"sentence": "This would be com-  patible in the longer term with extending the  domain of the task to cover ABS objects  such as events, when they are not described  using an NP.", "acronym": "ABS", "label": "abstract", "ID": "6627"}, {"sentence": "In particular, only Noun Phrases were anno-  tated (thereby circumventing problems of null  anaphora, summation, ABSion, etc.,", "acronym": "ABS", "label": "abstract", "ID": "6628"}, {"sentence": "Demner-Fushman and Lin (2005) operationalize knowledge extraction for populat- ing a database with PICO (Population, Intervention, Comparison, and Outcome) ele- ments from medical ABSs obtained from MEDLINE.", "acronym": "ABS", "label": "abstract", "ID": "6629"}, {"sentence": "STG's add another ABSn to parsing  schemata, namely on the grammar side.", "acronym": "ABS", "label": "abstractio", "ID": "6630"}, {"sentence": "After several  inquiries, the learner will \"own\" several such  associative maps and will be able to continue the  learning process at the level of generality or  ABSn at which new knowledge can be  generated.", "acronym": "ABS", "label": "abstractio", "ID": "6631"}, {"sentence": "STG's constitute a level of ABSn be-  tween grammars and parsing schemata because  they can be used to encode various classes of  grammars, whereas the mechanism for recog-  nizing admissible sequences of subconstituents  by a parsing algorithm is built into the gram-  mar.", "acronym": "ABS", "label": "abstractio", "ID": "6632"}, {"sentence": "Un- like the thread extension, no additional ABSn will be needed.", "acronym": "ABS", "label": "abstractio", "ID": "6633"}, {"sentence": "A parsing schema abstracts  from unimportant algorithmic details and thus,  like STG's, represents a well-defined level of  ABSn between grammars and parsers.", "acronym": "ABS", "label": "abstractio", "ID": "6634"}, {"sentence": "In particular, only Noun Phrases were anno-  tated (thereby circumventing problems of null  anaphora, summation, ABSn, etc.,", "acronym": "ABS", "label": "abstractio", "ID": "6635"}, {"sentence": "SllCtl an ABSn is useflll beCallSe  it; allows to study l)rot)erties of parsing algo-  rithms, and to compare different parsing algo-  rithms, independently of tile prot)erties of an  mtderlying rammar formalism.", "acronym": "ABS", "label": "abstractio", "ID": "6636"}, {"sentence": "STG's add another ABS to parsing  schemata, namely on the grammar side.", "acronym": "ABS", "label": "abstraction", "ID": "6637"}, {"sentence": "After several  inquiries, the learner will \"own\" several such  associative maps and will be able to continue the  learning process at the level of generality or  ABS at which new knowledge can be  generated.", "acronym": "ABS", "label": "abstraction", "ID": "6638"}, {"sentence": "STG's constitute a level of ABS be-  tween grammars and parsing schemata because  they can be used to encode various classes of  grammars, whereas the mechanism for recog-  nizing admissible sequences of subconstituents  by a parsing algorithm is built into the gram-  mar.", "acronym": "ABS", "label": "abstraction", "ID": "6639"}, {"sentence": "Un- like the thread extension, no additional ABS will be needed.", "acronym": "ABS", "label": "abstraction", "ID": "6640"}, {"sentence": "A parsing schema abstracts  from unimportant algorithmic details and thus,  like STG's, represents a well-defined level of  ABS between grammars and parsers.", "acronym": "ABS", "label": "abstraction", "ID": "6641"}, {"sentence": "In particular, only Noun Phrases were anno-  tated (thereby circumventing problems of null  anaphora, summation, ABS, etc.,", "acronym": "ABS", "label": "abstraction", "ID": "6642"}, {"sentence": "SllCtl an ABS is useflll beCallSe  it; allows to study l)rot)erties of parsing algo-  rithms, and to compare different parsing algo-  rithms, independently of tile prot)erties of an  mtderlying rammar formalism.", "acronym": "ABS", "label": "abstraction", "ID": "6643"}, {"sentence": "1997)  and later Harabagiu and Maiorano (HM) (2000)  investigated the acquisition of the lexical concept  SPA using WordNet and have applied their methods  to the Information Extraction task.", "acronym": "SPA", "label": "space", "ID": "6644"}, {"sentence": "In all the experiments, we clustered the whole set of 2283 adjectives, as the set of objects alters the vector SPA and thus the classification results.", "acronym": "SPA", "label": "space", "ID": "6645"}, {"sentence": "V (D) is the version SPA, which is the set of weightswi that classify the training data correctly, and |V (D)| is the size of the version SPA.", "acronym": "SPA", "label": "space", "ID": "6646"}, {"sentence": "As increasing the number of perceptrons re- sults in more thorough exploration of the version SPA V (D), we expect that the performance of the classifier would improve as K increases.", "acronym": "SPA", "label": "space", "ID": "6647"}, {"sentence": "In practice, to explore the version SPA of weights consistent with the training data, BPM trains a few different perceptrons (Collins, 2002) by shuffling the samples.", "acronym": "SPA", "label": "space", "ID": "6648"}, {"sentence": "A Plan-Based Approach to  SPA Recognition.", "acronym": "SPA", "label": "Speech Act", "ID": "6649"}, {"sentence": "ference NE Aliasing Concept Textual Input 1 Textual Input 2 Lexical Alignment Paraphrase Acquisition Alignment Module WWW Training Corpora  Classifier YES NO Features Alignment Dependency Features Paraphrase Features Semantic/ Pragmatic Features Feature Extraction Classification Module Lexico?Semantic PoS/ NER Synonyms/ Antonyms Normalization Syntactic Semantic Temporal Parsing Modality Detection SPA Recognition Pragmatics Factivity Detection Belief Recognition Preprocessing Figure 4: Textual Entailment Architecture.", "acronym": "SPA", "label": "Speech Act", "ID": "6650"}, {"sentence": "Syntax and Seman-  tics, Volume 3: SPAs, Academic Press, pp.", "acronym": "SPA", "label": "Speech Act", "ID": "6651"}, {"sentence": "Syntax and  70  Semantics III: SPAs, (pp.", "acronym": "SPA", "label": "Speech Act", "ID": "6652"}, {"sentence": "SPAs.", "acronym": "SPA", "label": "Speech Act", "ID": "6653"}, {"sentence": "192  An ascription-based approach to SPAs  Abstract:  The two principal areas of natural language processing  research in pragmatics are belief modelling and speech  act processing.", "acronym": "SPA", "label": "Speech Act", "ID": "6654"}, {"sentence": "Our decision to run MBC for MUC-4 was largely motivated by use of the AT metric as the official scorin g metric for MUC-4 .", "acronym": "AT", "label": "All Templates", "ID": "6655"}, {"sentence": "If we had generated a summary score report based on only two templates instead of three, our AT precision would have been 94 .", "acronym": "AT", "label": "All Templates", "ID": "6656"}, {"sentence": "erp-total 230 87 3 \t 2 \t 3 0 \t 2 79 \t 222 \t 351 2 \t 4 \t 9 1 phys-tgt-total 195 68 0 \t 0 \t 3 0 \t 0 65 \t 192 \t 728 0 \t 0 \t 9 6 hum-tgt-total 449 63 20 \t 4 \t 2 0 \t 4 37 \t 423 \t 772 5 \t 35 \t 5 9 Matched/Missing 1259 71 39 \t 13 \t 11 1 \t 7 8 \t 1196 \t 898 4 \t 64 \t 1 1 Matched/Spurious 113 527 39 \t 13 \t 26 1 \t 7 464 \t 50 \t 1277 40 \t 9 \t 8 8 Matched Only 113 71 39 \t 13 \t 11 1 \t 7 8 \t 50 \t 69 40 \t 64 \t 1 1 AT 1259 527 39 \t 13 \t 26 1 \t 7 464 \t 1196 \t 2106 4 \t 9 \t 8 8 Set Fills Only 593 36 26 \t 4 \t 4 0 \t 2 2 \t 559 \t 418 5 \t 78 \t 6 String Fills Only 337 9 5 \t 2 \t 2 0 \t 2 0 \t 328 \t 257 2 \t 67 \t 0 Table 2.", "acronym": "AT", "label": "All Templates", "ID": "6657"}, {"sentence": "With the third template averaged in, our AT precision drops to 76 .", "acronym": "AT", "label": "All Templates", "ID": "6658"}, {"sentence": "Because AT is maximally sensitive to all types of precision loss, it is generall y advantageous to minimize spurious templates for this metric .", "acronym": "AT", "label": "All Templates", "ID": "6659"}, {"sentence": "529 1189 160 63 24 0 23 942 282 718 36 16 79 perp-total 249 687 39 19 41 0 4 588 150 631 19 7 86 phys-tgt-total 255 280 26 12 28 1 10 214 189 1788 12 11 76 hum-tgt-total 594 236 82 42 28 1 38 84 442 2038 17 44 36 Matched/Missing 1627 614 307 136 121 2 75 50 1063 1203 23 61 8 Matched/Spurious 971 2392 307 136 121 2 75 1828 407 4601 39 16 7 6 Matched Only 971 614 307 136 121 2 75 50 407 629 39 61 8 AT 1627 2392 307 136 121 2 75 1828 1063 5175 23 16 7 6 Set Fills Only 778 333 177 35 74 0 7 47 492 538 25 58 1 4 String Fills Only 419 105 50 30 22 1 30 3 317 353 16 62 3 Table 3 .", "acronym": "AT", "label": "All Templates", "ID": "6660"}, {"sentence": "Introduction to the ATs in this Special Section Demner-Fushman and Lin?s article (Answering clinical questions with knowledge-based and statistical techniques) extends previous work by the authors (Demner-Fushman and Lin 2005) on a QA system in the medical domain.", "acronym": "AT", "label": "Article", "ID": "6661"}, {"sentence": "Generating Summaries of Multiple News ATs.", "acronym": "AT", "label": "Article", "ID": "6662"}, {"sentence": "Another example:  \"President and wife came to capital\"  (ATs and pronouns ere dropped to reflect Russian),  This phrase is processed as  \"President roof-country with xhis wife came to capital Eof-  country\".", "acronym": "AT", "label": "Article", "ID": "6663"}, {"sentence": "Auto- matic Paraphrase Acquisition from News ATs.", "acronym": "AT", "label": "Article", "ID": "6664"}, {"sentence": "AT 2.", "acronym": "AT", "label": "Article", "ID": "6665"}, {"sentence": "ATs are in English and come from a variety of sources.", "acronym": "AT", "label": "Article", "ID": "6666"}, {"sentence": "It is use- ful for low-level tasks such as parsing (e.g. for PP-ATment ambiguity within NPs), but also for tasks oriented to semantics, such as the extraction of relationships between individuals or concepts.", "acronym": "AT", "label": "attach", "ID": "6667"}, {"sentence": "Unlike in typical HPSG approaches, the informa- tion about the realized arguments is still exposed in the COMPS and SUBJ lists of this constituent.10 This makes the necessary information available to separately-ATing modifiers (such as ngara- ganaguja (?", "acronym": "AT", "label": "attach", "ID": "6668"}, {"sentence": "The procedure is not based on the deno-  tative meaning of a word, but only on the connota-  tive emotions ATed to the word; it is difficult to  choose the relevant dimensions, i.e. the dimensions  required for the sufficient semantic space.", "acronym": "AT", "label": "attach", "ID": "6669"}, {"sentence": "g.de Abstract To study PP ATment disambiguation as a benchmark for empirical methods in nat- ural language processing it has often been reduced to a binary decision problem (be- tween verb or noun ATment) in a par- ticular syntactic configuration.", "acronym": "AT", "label": "attach", "ID": "6670"}, {"sentence": "c?2006 Association for Computational Linguistics The Benefit of Stochastic PP Attachment to a Rule-Based Parser Kilian A. Foth and Wolfgang Menzel Department of Informatics Hamburg University D-22527 Hamburg Germany foth|menzel@nats.informatik.uni-hamburg.de Abstract To study PP ATment disambiguation as a benchmark for empirical methods in nat- ural language processing it has often been reduced to a binary decision problem (be- tween verb or noun ATment) in a par- ticular syntactic configuration.", "acronym": "AT", "label": "attach", "ID": "6671"}, {"sentence": "We combine the ATment predictions mad", "acronym": "AT", "label": "attach", "ID": "6672"}, {"sentence": "We combine the ATment predictions made by a simple model of lexical attraction with a full-fledged parser of German to de- termine the actual benefit of the subtask to parsing.", "acronym": "AT", "label": "attach", "ID": "6673"}, {"sentence": "We show that the combination of data-driven and rule-based components can reduce the number of all parsing errors by 14% and raise the ATment accuracy for dependency parsing of German to an unprecedented 92%.", "acronym": "AT", "label": "attach", "ID": "6674"}, {"sentence": "nefit of Stochastic PP Attachment to a Rule-Based Parser Kilian A. Foth and Wolfgang Menzel Department of Informatics Hamburg University D-22527 Hamburg Germany foth|menzel@nats.informatik.uni-hamburg.de Abstract To study PP ATment disambiguation as a benchmark for empirical methods in nat- ural language processing it has often been reduced to a binary decision problem (be- tween verb or noun ATment) in a par- ticular syntactic configuration.", "acronym": "AT", "label": "attach", "ID": "6675"}, {"sentence": "are: the AT (ANY),  the restriction (sport), the question target (Jenni- fer Capriati), and the relation (play).", "acronym": "AT", "label": "answer type", "ID": "6676"}, {"sentence": "Where Q and T are sets of the bag-of-words  for the question relation and the triple relation  respectively, Lin(a,b) is a measure for the seman- tic similarity between a and b based on WordNet  (Lin, 1998), and L(x) is the number of elements  in the set x.  The Answer Extraction: this component first  filters out the triples mismatching the expected  AT.", "acronym": "AT", "label": "answer type", "ID": "6677"}, {"sentence": "Fi- nally, when the question word is the WH-word, we check if the paired word belongs to some phrase that has the correct AT using simple rules, such as ?", "acronym": "AT", "label": "answer type", "ID": "6678"}, {"sentence": "However, adding more information like named entity match- ing and AT verification does not seem to help much (Line #5 vs. #4).", "acronym": "AT", "label": "answer type", "ID": "6679"}, {"sentence": "Another interesting finding we have is that while the latent structured model, LCLR, performs better than the other two unstructured models, the difference diminishes after more in- formation, including the enhanced lexical seman- tic knowledge and AT verification, has been incorporated.", "acronym": "AT", "label": "answer type", "ID": "6680"}, {"sentence": "However, the questions in the reading  comprehension don?t limit the ATs to  person and organization, even if the question is  ?", "acronym": "AT", "label": "answer type", "ID": "6681"}, {"sentence": "Currently, we reject all but the parsed S fragments?and NP fragments when expected 8Command-and-control applications have also made use of an AT, which represents activities being carried out by the dialogue-enabled device (Gruenstein, 2002); however, this application currently makes no use of this.", "acronym": "AT", "label": "Activity Tree", "ID": "6682"}, {"sentence": "Items which the system will consider for genera- tion are placed (either directly by the robot, or indi- rectly by the AT) on the ?", "acronym": "AT", "label": "Activity Tree", "ID": "6683"}, {"sentence": "For example, the Dialogue Move Tree can update Salience List, System Agenda, Pend- ing List, and AT, while the AT can update only the System Agenda and send ex- ecution requests to the robot, and it can query the Activity Model (when adding nodes).", "acronym": "AT", "label": "Activity Tree", "ID": "6684"}, {"sentence": "ive]) * Root (1) Root o Command (0) command([go],[param_list([pp_loc(to,arg([np(det([def],the),[n(tower, sg)])]))])]) [[dmtask0] current] + Report report(inform,agent([np([n(uav,sg)])]),curr_activity([command ([take_off])]))[] o Report report(inform,agent([np([n(uav,sg)])]),confirm_activity([command([go], [param_list([pp_loc(to,arg([np(det([def],the),[n(tower,sg)], )]))])])])) [[dmtask0] current] AT * root o [dmtask0] current relation = SEQuential command = go pp = pp_loc(to,Args) np = np(det([def],the),[n(tower,sg)]) + [sim3] current relation = none command = take_off pp = null, np = null Salience List (least salient -- most salient) * [np(det([def],the),[n(tower,sg)])] (speech) * [np(det([def],the),[n(tower,sg)])] (speech) Figure 6: Attachment in the Dialogue Move Classes DMT", "acronym": "AT", "label": "Activity Tree", "ID": "6685"}, {"sentence": "AT?", "acronym": "AT", "label": "Activity Tree", "ID": "6686"}, {"sentence": "which is a shared representation of current and planned activities and their execution status, in- volving temporal and hierarchical ordering (in fact, one can think of the AT as a Hierarchical Task Network for the device).", "acronym": "AT", "label": "Activity Tree", "ID": "6687"}, {"sentence": "Other examples also have a similar significance: (3) for the pair  ACT-Objective, (4) for Manner-Directional.", "acronym": "ACT", "label": "Actor", "ID": "6688"}, {"sentence": "83  Computational Linguistics Volume 21, Number 1  and Sgall, 1994), as well as investigations with native speakers of English, we hypoth-  esize that the SO of some of the main kinds of complementations i  English has the  following shape: 4  Time - ACT - Addressee - Objective - Origin - Effect - Manner - Directional.", "acronym": "ACT", "label": "Actor", "ID": "6689"}, {"sentence": "A1: Jack Lemmon won the Academy Award for Best ACT for Save the Tiger (1973).", "acronym": "ACT", "label": "Actor", "ID": "6690"}, {"sentence": "These include, on the one hand, inner participants or arguments,  such as ACT, Addressee, Objective, and, on the other hand, free modifications, uch  as Locative, Means, Manner, Cause, several temporal and directional modifications,  those of Condition, Regard, Accompaniment, etc.", "acronym": "ACT", "label": "Actor", "ID": "6691"}, {"sentence": "Its lexical scope can be enlarged easily, if the added lexical items are accompanied by  appropriate grammatical data, especially by valency (case) frames specifying the optional and  obligatory arguments (ACT, Addressee, Objective, Origin, and Effect, with verbs).", "acronym": "ACT", "label": "Actor", "ID": "6692"}, {"sentence": "Pret t (yesterday)Time  Most of our symbols (for Indefinite, Preterite, ACT, Addressee, Objective, Di-  rectional) should be self-explanatory; Gener(al Relationship) is the free modification  typical for an adjectival modifier of a noun.", "acronym": "ACT", "label": "Actor", "ID": "6693"}, {"sentence": "A stochastic parser based on an SLM with ACTs.", "acronym": "ACT", "label": "arboreal context tree", "ID": "6694"}, {"sentence": "InterACT Multimedia Navigation  Until now, problems of navigation have in the main  been reduced to the research of interface.", "acronym": "ACT", "label": "active", "ID": "6695"}, {"sentence": "In recent ime,  with the emergence of channels and ACT  desktops, the dynamic haracteristics ofnavigation  tools have again changed.", "acronym": "ACT", "label": "active", "ID": "6696"}, {"sentence": "However, these forms are  interACT in a limited sense only.", "acronym": "ACT", "label": "active", "ID": "6697"}, {"sentence": "The three aspects  that make up the object of our research are 1) the  expression of knowledge through interACT  multimedia; 2) navigation tools corresponding to  the expression of knowledge through interACT  multimedia; nd 3) mechanisms for updating both  the body of knowledge represented through  interACT multimedia methods and the  appropriate navigation tools.", "acronym": "ACT", "label": "active", "ID": "6698"}, {"sentence": "In recent ime,  with the emergence of channels and ACT  desktops, the dynamic haracteristics ofnavigation  tools have again cha", "acronym": "ACT", "label": "active", "ID": "6699"}, {"sentence": "InterACT Multimedia Navigation  Prof. Dr. Dr. Mihai NADIN  Computational Design, University of Wuppertal  Hofaue 35-39  D-42103 Wuppertal, Germany  nadin @ code.uni-wuppertal.de  Dipl.", "acronym": "ACT", "label": "active", "ID": "6700"}, {"sentence": "Associations are almost  always multimedial, i.e., we associate texts,  sounds, pictures, movement e c.  The structure of an interACT, multimedia  encyclopedia that is based on associations includes  1) a knowledge space/domain,  2) an associative search procedure,  3) a function for storing associative traces.", "acronym": "ACT", "label": "active", "ID": "6701"}, {"sentence": "3 Representations We survey the following distributional represen- tations: (i) count vectors reduced by a Singular Value Decomposition (SVD), (ii) word clusters in- duced using the likelihood of a class-based language model, (iii) distributed embeddings trained using a neural network and (iv) ACT, a task-specific representation obtained from an auto- matically tagged corpus.", "acronym": "ACT", "label": "accumulated tag counts", "ID": "6702"}, {"sentence": "SemLink maps Arg0 to the Agent of carve-21.2-2 and Arg1 to the PAT.", "acronym": "PAT", "label": "Patient", "ID": "6703"}, {"sentence": "1996-1997 ABPI Compendium of  PAT Information Leaflets.", "acronym": "PAT", "label": "Patient", "ID": "6704"}, {"sentence": "2.2 Semantic Role Labeling  Semantic role labeling is the task of identifying  semantic roles such as Agent, PAT, Speaker,  or Topic, in a sentence.", "acronym": "PAT", "label": "Patient", "ID": "6705"}, {"sentence": "In our running example, VerbNet specifies +int control and +concrete for the Agent and PAT of carve-21.2-2, respectively.", "acronym": "PAT", "label": "Patient", "ID": "6706"}, {"sentence": "Depression as a Risk Factor for Poor Prog- nosis Among PATs With Acute Coronary Syn- drome: Systematic Review and Recommendations A Scientific Statement From the American Heart Association.", "acronym": "PAT", "label": "Patient", "ID": "6707"}, {"sentence": "Using SemLink, map each PropBank argu- ment name to the corresponding VerbNet the- matic roles in these entries (Agent, PAT, etc.).", "acronym": "PAT", "label": "Patient", "ID": "6708"}, {"sentence": "This likely stems from the variance of  verbs with respect o the thematic roles they allow (e.g.,  Agent, Instrument, PAT, etc.)", "acronym": "PAT", "label": "Patient", "ID": "6709"}, {"sentence": "PATs with depression often have common symptoms of low energy, reduced or intensified psychomotor movements, low concentration, in- decisiveness, and thoughts of death, as well as related symptoms such as fatigue, insomnia, and weight gain.", "acronym": "PAT", "label": "Patient", "ID": "6710"}, {"sentence": "The PAT is applied at the sentence level.", "acronym": "PAT", "label": "pattern", "ID": "6711"}, {"sentence": "The used PATs are: 1) (DT|CD) (NN|NNS), 2) DT JJ (NN|NNS), 3) NN POS (NN|NNS), and 4) PRP$ JJ (NN|NNS).", "acronym": "PAT", "label": "pattern", "ID": "6712"}, {"sentence": "These POS-based PATs are quite generic, al- lowing for the creation of large sets of characters.", "acronym": "PAT", "label": "pattern", "ID": "6713"}, {"sentence": "using syntactic PATs.", "acronym": "PAT", "label": "pattern", "ID": "6714"}, {"sentence": "2) a set of part-of-speech PATs was used for the extraction of human and non-human characters that were not represented by proper names, e.g., ?", "acronym": "PAT", "label": "pattern", "ID": "6715"}, {"sentence": "A hybrid ap- proach is adopted, where PAT-based and statistical methods are used along with utilization of external knowledge sources.", "acronym": "PAT", "label": "pattern", "ID": "6716"}, {"sentence": "The identification of quotes in the story is based on a simple PAT-based ap- proach: the quote boundaries are signified by the respective symbols, e.g., ?", "acronym": "PAT", "label": "pattern", "ID": "6717"}, {"sentence": "The output of grammatical modules is fed then onto the BMe which activates an algorithm for anaphoric binding.", "acronym": "BM", "label": "Binding Modul", "ID": "6718"}, {"sentence": "The output of grammatical modules is fed then onto the BM which activates an algorithm for anaphoric binding.", "acronym": "BM", "label": "Binding Module", "ID": "6719"}, {"sentence": "This work was funded by the Ad- vanced Research and Development Activity's  Advanced Question AQUAINT  Program, National Science Foundation award  IIS-0325646 and a Stanford Graduate Fellow- ship.", "acronym": "AQUAINT", "label": "Answering for Intelligence", "ID": "6720"}, {"sentence": "8 Acknowledgements This work was supported in part by the Advanced Research and Development Activity?s Advanced Question AQUAINT Program.", "acronym": "AQUAINT", "label": "Answering for Intelligence", "ID": "6721"}, {"sentence": "This work was funded by the Ad- vanced Research and Development Activity's  AQUAINT  Program, National Science Foundation award  IIS-0325646 and a Stanford Graduate Fellow- ship.", "acronym": "AQUAINT", "label": "Advanced Question Answering for Intelligence", "ID": "6722"}, {"sentence": "Graphical  representations can range from simple diagrams  47  to ANIMd sequences.", "acronym": "ANIM", "label": "animate", "ID": "6723"}, {"sentence": "Being ANIM, man performs either thematic  role well, allowing the main clause reading to remain  *I thank Christy Doran, Jason Eisner, Jeff Reynar, and  John Trueswell for valuable comments.", "acronym": "ANIM", "label": "animate", "ID": "6724"}, {"sentence": "5 Unsuperv ised  Learn ing  o f  Gender   In fo rmat ion   The importance of gender information as re-  vealed in the previous experiments caused us to  consider automatic methods for estimating the  probability that nouns occurring in a large cor-  pus of English text deonote inANIM, mascu-  line or feminine things.", "acronym": "ANIM", "label": "animate", "ID": "6725"}, {"sentence": "What follows is the complete list of morpholog- ical categories assumed in the proposed tagset: \u0000 number: sg , pl ; \u0000 case: nom , acc , gen , dat , inst , loc , voc; \u0000 gender: masculine personal m1 (facet), mas- culine ANIM m2 (ko?n), masculine inani- mate m3 (st?\u0007), feminine f (kobieta, \u0001zyrafa, 3Segmentation, as understood in the present context, is discussed at length in (Przepi?rkowski and Wolin?ski, 2003).", "acronym": "ANIM", "label": "animate", "ID": "6726"}, {"sentence": "Agent SteaJ Object  Apple:'  Figure 8: Graphs from the sentence \"Un avocat vole  une pomme\"  ample, \"Steal\" needs an ANIMd agent (Figure 9),  therefore graphs with the \"Avocado\" concept can be  removed from the selection.", "acronym": "ANIM", "label": "animate", "ID": "6727"}, {"sentence": "Since van,  which is inANIM, makes a good Theme but a poor  Agent for recognized, the past participial analysis in  2) is reinforced and the main clause (past tense) sup-  pressed.", "acronym": "ANIM", "label": "animate", "ID": "6728"}, {"sentence": "a subset of the STORIES dataset that included 10 stories, the following schemes were used for filter- ing of candidate speakers: (i) Scheme 1: all speak- ers linked with speech verbs, (ii) Scheme 2: speak- ers, who are persons or ANIMs or spiritual entities according to their first WordNet sense, linked with speech verbs , and (iii) Scheme 3: as Scheme 2, 5SU+/?", "acronym": "ANIM", "label": "animal", "ID": "6729"}, {"sentence": "refers to the co- occurrence pattern between a verb and a noun 33 [Sentence pattern] <word1> ga <word2> wo taberu (eat) [Sense relation] agent object [Case particle] ga (nominative) wo (accusative) [Sense identifier] 30f6b0 (human);30f6bf (ANIM) 30f6bf(ANIM);30f6ca(plants); 30f6e5(parts of plants); 3f9639(food and drink); 3f963a(feed) Figure 1: An example of a verb ?", "acronym": "ANIM", "label": "animal", "ID": "6730"}, {"sentence": "A character was retained if any of its hypernyms was found to fall into certain types of WordNet concepts: person, ANIM, plant, artifact, spiritual being, physical entity.", "acronym": "ANIM", "label": "animal", "ID": "6731"}, {"sentence": "Characters in chil- dren?s stories can either be human or non-human entities, i.e., ANIMs and non-living objects, ex- hibiting anthropomorphic traits.", "acronym": "ANIM", "label": "animal", "ID": "6732"}, {"sentence": "There is no doubt that I  claim that all cats are ugly ANIMs.", "acronym": "ANIM", "label": "animal", "ID": "6733"}, {"sentence": "Automatically ac- quiring fine-grained IS distinctions in German.", "acronym": "IS", "label": "information status", "ID": "6734"}, {"sentence": "It con- sists of 50 texts taken from the WSJ portion of the OntoNotes corpus (Weischedel et al 2011) with al- most 11,000 NPs annotated for IS including 663 bridging NPs and their antecedents.", "acronym": "IS", "label": "information status", "ID": "6735"}, {"sentence": "2For German surface realization, Cahill and Riester (2009) show that incorporating IS features based on the linguistics literature improves performance on realization ranking.", "acronym": "IS", "label": "information status", "ID": "6736"}, {"sentence": "Learning IS of discourse entities.", "acronym": "IS", "label": "information status", "ID": "6737"}, {"sentence": "Learning the fine- grained IS of discourse entities.", "acronym": "IS", "label": "information status", "ID": "6738"}, {"sentence": "This is a minimal assignment in the sense that we do not project informativity; instead, we only set informa- tivity for those discourse referents whose realization shows explicit clues as to their IS.", "acronym": "IS", "label": "information status", "ID": "6739"}, {"sentence": "1 In t roduct ion   We present a statIStical method for determin-  ing pronoun anaphora.", "acronym": "IS", "label": "is", "ID": "6740"}, {"sentence": "We incorpo-  rate multiple anaphora resolution factors into  a statIStical framework - -  specifically the dIS-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.", "acronym": "IS", "label": "is", "ID": "6741"}, {"sentence": "We present some experiments il-  lustrating the accuracy of the method and note  that with thIS information added, our pronoun  resolution method achieves 84.2% accuracy.", "acronym": "IS", "label": "is", "ID": "6742"}, {"sentence": "We incorpo-  rate multiple anaphora resolution factors into  a statIStical framework - -  specifically the dIS-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition", "acronym": "IS", "label": "is", "ID": "6743"}, {"sentence": "A StatIStical Approach to Anaphora Resolution  Niyu  Ge, John  Hale and Eugene Charn iak   Dept.", "acronym": "IS", "label": "is", "ID": "6744"}, {"sentence": "brown, edu  Abst ract   ThIS paper presents an algorithm for identi-  fying pronominal anaphora and two experi-  ments based upon thIS algorithm.", "acronym": "IS", "label": "is", "ID": "6745"}, {"sentence": "We incorpo-  rate multiple anaphora resolution factors into  a statIStical framework - -  specif", "acronym": "IS", "label": "is", "ID": "6746"}, {"sentence": "ThIS program differs  from earlier work in its almost complete lack of  hand-craft", "acronym": "IS", "label": "is", "ID": "6747"}, {"sentence": "The second  experiment investigates a method for unsuper-  vISed learning of gender/number/animaticity  information.", "acronym": "IS", "label": "is", "ID": "6748"}, {"sentence": "Our system uses three visual properties to predict IS; we se- lect properties that are known from previous work to help predict whether a landmark will be men- tioned.", "acronym": "IS", "label": "information structure", "ID": "6749"}, {"sentence": "Complex descriptions, however, have a non-trivial IS?", "acronym": "IS", "label": "information structure", "ID": "6750"}, {"sentence": "Freer-word-order languages such as Ger- man also have predictable ISs which have been employed in surface realization systems, but these require a different structural analysis than in English (Zarrie?", "acronym": "IS", "label": "information structure", "ID": "6751"}, {"sentence": "Finally, it makes a the- oretical contribution: By linking the ISs observed in the data to the existing re- 520 search on salience and IS, we show that visually prominent objects are treated as part of common ground despite the lack of pre- vious mention.", "acronym": "IS", "label": "information structure", "ID": "6752"}, {"sentence": "2013), also studying the Wally corpus, demonstrates that visual features affect determiner choice for NPs, but do not study IS.", "acronym": "IS", "label": "information structure", "ID": "6753"}, {"sentence": "Although these aspects of DBG are important in processing written and even transcribed voice messages i n the Air Force and Army messages to which the system has been applied, the MUC-5 application does no t depend heavily on outside information, and is not concerned with evaluation of the information received , and is characterized by more variability in IS across texts .", "acronym": "IS", "label": "information structure", "ID": "6754"}, {"sentence": "J N. and Ratcliff, D., \"Generalized IS  for Log-Linear Models\", The Annals of Mathematical Statis-  tics, VoL 43, pp 1470-1480,1972.", "acronym": "IS", "label": "Iterative Sealing", "ID": "6755"}, {"sentence": "We use a set of 30 word pairs from a study carried out by (MC, 1991).", "acronym": "MC", "label": "Miller and Charles", "ID": "6756"}, {"sentence": "Additionally, numerous studies (Carnine et al, 1984; MC, 1991; McDonald and Ramscar, 2001) have shown that context plays a vital role in defining the mean- ings of words. (", "acronym": "MC", "label": "Miller and Charles", "ID": "6757"}, {"sentence": "the MC study as well as the Rubenstein and Goodenough research.", "acronym": "MC", "label": "Miller and Charles", "ID": "6758"}, {"sentence": "MC, 1991) suggest that the cognitive representation of a word is an abstrac- tion derived from its contexts (encountered by the person).", "acronym": "MC", "label": "Miller and Charles", "ID": "6759"}, {"sentence": "Using the distributional hypothesis (Harris, 1951), and operationalizing similarity of terms (MC, 1991), it became possible to compute term similarities for a large vocabulary (Ruge, 1992).", "acronym": "MC", "label": "Miller and Charles", "ID": "6760"}, {"sentence": "They rank MC (1991)?s set (henceforth ?", "acronym": "MC", "label": "Miller and Charles", "ID": "6761"}, {"sentence": "Thus a leaf with the label  inexpens ive  has the total of 4 cases, one of which  is MC.", "acronym": "MC", "label": "misclassified", "ID": "6762"}, {"sentence": "The best combination re- sults in 74% of the clusters having no MC  definitions.", "acronym": "MC", "label": "misclassified", "ID": "6763"}, {"sentence": "If those MC definitions end  up being used to represent possible sense labels in  WSD, wrong labels might decrease the quality of  the disambiguation stage.", "acronym": "MC", "label": "misclassified", "ID": "6764"}, {"sentence": "(4/1), indicates the number of MC cases.", "acronym": "MC", "label": "misclassified", "ID": "6765"}, {"sentence": "Test (656 cases):  T F ~ f i e d   293 62 \\[Class : T  68 233 IClass : F Errors : 130 (19.8%)  1  Figure 2 An Example of Decision Trees  The two numbers in the brackets denote the  number of cases covered by the branch and the  number of cases being MC respectively:  The results of our experiment will be elaborated  on in future, when we shall also explore the  application of machine learning techniques to  recognizing rhetorical relations on the basis of  discourse markers, and extracting important  sentences from Chinese text.", "acronym": "MC", "label": "misclassified", "ID": "6766"}, {"sentence": "They indicate the number of  cases that reach a particular leaf and also the num-  ber of MC cases.", "acronym": "MC", "label": "misclassified", "ID": "6767"}, {"sentence": "For instance, the idiom break the ice in Example 7 could be MC as lit- eral due to there being a high relatedness score be- tween ice and snow.", "acronym": "MC", "label": "misclassified", "ID": "6768"}, {"sentence": "Figure 2: Typology of Argumentative Structure: Examples of (i) Tree h>1 ; (ii) Chain; (iii) Tree h=1 Feature Group Id Argumentation Feature Description 1 # of Claims AC 2 # of Premises 3,4 # and fraction of sentences containing argument components 5, 6 # and % of supported Claims AR 7, 8 # and % of dangling Claims 9 # of Claims supporting MC 10, 11 # of total Attacks and Attacks against MC 12 # of Argument Chains TS 13 # of Argument Tree h=1 14 # of Argument Tree h>1 Table 1: Argumentation Features or claims.", "acronym": "MC", "label": "Major Claim", "ID": "6769"}, {"sentence": "MC Claim Premise None MC .675 .132 .148 .045 Claim .025 .552 .338 .086 Premise .014 .163 .754 .069 None .012 .123 .204 .660 Table 3: Confusion probability matrix for argument component annotations (Category ?", "acronym": "MC", "label": "Major Claim", "ID": "6770"}, {"sentence": "where the root is a claim and the leaves are premises 550 Figure 2: Typology of Argumentative Structure: Examples of (i) Tree h>1 ; (ii) Chain; (iii) Tree h=1 Feature Group Id Argumentation Feature Description 1 # of Claims AC 2 # of Premises 3,4 # and fraction of sentences containing argument components 5, 6 # and % of supported Claims AR 7, 8 # and % of dangling Claims 9 # of Claims supporting MC 10, 11 # of total Attacks and Attacks against MC 12 # of Argument Chains TS 13 # of Argument Tree h=1 14 # of Argument Tree h>1 Table 1: Argumentation Features or claims.", "acronym": "MC", "label": "Major Claim", "ID": "6771"}, {"sentence": "COR feedback and persistent learning for information extraction.", "acronym": "COR", "label": "Corrective", "ID": "6772"}, {"sentence": "3.2479 1.1383 1.0366 3.1029 1.1298 1.0247 0 0.02 0.04 0.06 0.08 0.10.838 0.84 0.842 0.844 0.846 0.848 0.85 0.852 COR Step O?Lo ss     d=110d=220 (a) O-Loss 0 0.02 0.04 0.06 0.08 0.12.15 2.2 2.25 2.3 2.35 2.4 COR Step S?Lo ss     d=110d=220 (b) S-Loss 0 0.02 0.04 0.06 0.08 0.1 1.02 1.025 1.03 1.035 1.04 1.045 1.05 COR Step H?Lo ss     d=110d=220 (c) H-Loss Figure 2: Impact of COR Step ?", "acronym": "COR", "label": "Corrective", "ID": "6773"}, {"sentence": "0.8591 0.8428 S-Loss 8.5516 2.8921 2.3190 7.8623 2.8449 2.2812 H-Loss 3.2479 1.1383 1.0366 3.1029 1.1298 1.0247 0 0.02 0.04 0.06 0.08 0.10.838 0.84 0.842 0.844 0.846 0.848 0.85 0.852 COR Step O?Lo ss     d=110d=220 (a) O-Loss 0 0.02 0.04 0.06 0.08 0.12.15 2.2 2.25 2.3 2.35 2.4 COR Step S?Lo ss     d=110d=220 (b) S-Loss 0 0.02 0.04 0.06 0.08 0.1 1.02 1.025 1.03 1.035 1.04 1.045 1.05 COR Step H?Lo ss     d=110d=220 (c) H-Loss Figure 2: Impact of COR Step ?", "acronym": "COR", "label": "Corrective", "ID": "6774"}, {"sentence": "view.com/ 409 Table 1: Performance Comparisons (A Smaller Loss Value Means a Better Performance) Metrics Dimensinality=110 Dimensinality=220H-RLS HL-flat HL-SOT H-RLS HL-flat HL-SOT O-Loss 0.9812 0.8772 0.8443 0.9783 0.8591 0.8428 S-Loss 8.5516 2.8921 2.3190 7.8623 2.8449 2.2812 H-Loss 3.2479 1.1383 1.0366 3.1029 1.1298 1.0247 0 0.02 0.04 0.06 0.08 0.10.838 0.84 0.842 0.844 0.846 0.848 0.85 0.852 COR Step O?Lo ss     d=110d=220 (a) O-Loss 0 0.02 0.04 0.06 0.08 0.12.15 2.2 2.25 2.3 2.35 2.4 COR Step S?Lo ss     d=110d=220 (b) S-Loss 0 0.02 0.04 0.06 0.08 0.1 1.02 1.025 1.03 1.035 1.04 1.045 1.05 COR Step H?Lo ss     d=110d=220 (c) H-Loss Figure 2: Impact of COR Step ?", "acronym": "COR", "label": "Corrective", "ID": "6775"}, {"sentence": "cs Dimensinality=110 Dimensinality=220H-RLS HL-flat HL-SOT H-RLS HL-flat HL-SOT O-Loss 0.9812 0.8772 0.8443 0.9783 0.8591 0.8428 S-Loss 8.5516 2.8921 2.3190 7.8623 2.8449 2.2812 H-Loss 3.2479 1.1383 1.0366 3.1029 1.1298 1.0247 0 0.02 0.04 0.06 0.08 0.10.838 0.84 0.842 0.844 0.846 0.848 0.85 0.852 COR Step O?Lo ss     d=110d=220 (a) O-Loss 0 0.02 0.04 0.06 0.08 0.12.15 2.2 2.25 2.3 2.35 2.4 COR Step S?Lo ss     d=110d=220 (b) S-Loss 0 0.02 0.04 0.06 0.08 0.1 1.02 1.025 1.03 1.035 1.04 1.045 1.05 COR Step H?Lo ss     d=110d=220 (c) H-Loss Figure 2: Impact of COR Step ?", "acronym": "COR", "label": "Corrective", "ID": "6776"}, {"sentence": "4.4 Impact of COR Step ?", "acronym": "COR", "label": "Corrective", "ID": "6777"}, {"sentence": "COR analysis refers to the process of  determining whether or not two mentions of entities  refer to the same person (Kibble and Deemter, 2000).", "acronym": "COR", "label": "Coreference", "ID": "6778"}, {"sentence": "Algorithms for  Scoring COR Chains.", "acronym": "COR", "label": "Coreference", "ID": "6779"}, {"sentence": "COR analysis attempts to decide whether  John Smith and Mr. Smith refer to the same person, and  whether John is also the same person.", "acronym": "COR", "label": "Coreference", "ID": "6780"}, {"sentence": "COR as the  Foundations for Link Analysis Over Free Text  Databases.", "acronym": "COR", "label": "Coreference", "ID": "6781"}, {"sentence": "In Proceedings of the  Linguistic COR Workshop at The First  International Conference on Language Resources and  Evaluation (LREC'98), pp563-566, 1998.", "acronym": "COR", "label": "Coreference", "ID": "6782"}, {"sentence": "A Methodology for  Cross-Document COR.", "acronym": "COR", "label": "Coreference", "ID": "6783"}, {"sentence": "239   Cross-Document COR on a Large Scale Corpus        Abstract       In this paper, we will compare and evaluate the  effectiveness of different statistical methods in the  task of cross-document coreference resolution.", "acronym": "COR", "label": "Coreference", "ID": "6784"}, {"sentence": "Introduction       COR analysis refers to the process of  determining whether or not two mentions of entities  refer to the same person (Kibble and Deemter, 2000).", "acronym": "COR", "label": "Coreference", "ID": "6785"}, {"sentence": "c?2014 Association for Computational Linguistics Linguistic and AF for Automatic Identification of Autism Spectrum Disorders in Children?s Narrative Hiroki Tanaka, Sakriani Sakti, Graham Neubig, Tomoki Toda, Satoshi Nakamura Graduate School of Information Science, Nara Institute of Science and Technology {hiroki-tan, ssakti, neubig, tomoki, s-nakamura}@is.naist.jp Abstract Autism spectrum disorders are develop- mental disorders characterised as", "acronym": "AF", "label": "Acoustic Features", "ID": "6786"}, {"sentence": "c?2009 Association for Computational Linguistics On NoMatchs, NoInputs and BargeIns: Do Non-AF Support Anger Detection?", "acronym": "AF", "label": "Acoustic Features", "ID": "6787"}, {"sentence": "4 AF: The Speech   Recognition Process  In the last two decades ignificant advances have  been made in the field of automatic speech  recognition (SR), both in commercial and re-  search domains.", "acronym": "AF", "label": "Acoustic Features", "ID": "6788"}, {"sentence": "Clas- sifying Subject Ratings of Emotional Speech Using AF.", "acronym": "AF", "label": "Acoustic Features", "ID": "6789"}, {"sentence": "75    Automatic Prosodic Labeling with Conditional Random Fields and Rich AF Gina-Anne Levow University of Chicago Department of Computer Science 1100 E. 58th St. Chicago, IL 60637 USA levow@cs.uchicago.edu Abstract Many acoustic approaches to prosodic la- beling in English have employed only lo- cal classifiers, although text-based classifi- cation has employed some sequential mod- els.", "acronym": "AF", "label": "Acoustic Features", "ID": "6790"}, {"sentence": "AF  -mean confidence, pmisrecs%l, pmisrecs%2, pmis-  recs%3, pmisrecs%4  ?", "acronym": "AF", "label": "Acoustic Features", "ID": "6791"}, {"sentence": "Figure 1: Overall ratio of L?SBarT verbs, presence in SBV and symbol coverage 5.2 Nouns We found that 24 % of the noun lemmas in LBL and SBV lacked symbol coverage, and that there was a wide range in AF, varying from 232.84 down to 1.06.", "acronym": "AF", "label": "adjusted frequency", "ID": "6792"}, {"sentence": "Without making any formal categorization, it is clear that the words with highest AF are abstract words, such as sam- band ?", "acronym": "AF", "label": "adjusted frequency", "ID": "6793"}, {"sentence": "A heuristic AF estimate is  proposed that, at least for novel-sized texts, is considerably more accurate.", "acronym": "AF", "label": "adjusted frequency", "ID": "6794"}, {"sentence": "which had an AF of 105.71 in SBV and a relative frequency of 1.03 ?", "acronym": "AF", "label": "adjusted frequency", "ID": "6795"}, {"sentence": "We use a t-score derived from the adjusted frequen-  cies in our corpus to decide whether the prepositional  phrase into Afganistan is attached to the verb (root)  send/V or to the noun (root) soldier/N. In our cor-  pus, soldier/N has an AF of 1488.5, and  send/V has an AF of 1706.5; soldier/N  occurred in 32 distinct preposition contexts, and send/V  in 60 distinct preposition contexts; f(send/V into) = 84,  f(soldier/N into) = 1.5.", "acronym": "AF", "label": "adjusted frequency", "ID": "6796"}, {"sentence": "A phrase-based alignment model for NLI.", "acronym": "NLI", "label": "natural language inference", "ID": "6797"}, {"sentence": "Ambiguity in word meaning In order for our system to be able to make correct NLI, it must be able to handle paraphrasing and deal with hypernymy.", "acronym": "NLI", "label": "natural language inference", "ID": "6798"}, {"sentence": "A large annotated cor- pus for learning NLI.", "acronym": "NLI", "label": "natural language inference", "ID": "6799"}, {"sentence": "We re- lease our implementation as the first open-source monolingual aligner, which we hope to be of ben- efit to other researchers in the rapidly expanding area of NLI.", "acronym": "NLI", "label": "natural language inference", "ID": "6800"}, {"sentence": "Robust, lexical- ized NLI.", "acronym": "NLI", "label": "native language identification", "ID": "6801"}, {"sentence": "Exploiting parse structures for NLI.", "acronym": "NLI", "label": "native language identification", "ID": "6802"}, {"sentence": "Contrastive analysis and NLI.", "acronym": "NLI", "label": "native language identification", "ID": "6803"}, {"sentence": "A report on the first NLI shared task.", "acronym": "NLI", "label": "native language identification", "ID": "6804"}, {"sentence": "Wilks, Y. (1975a) A Preferential Pattern-Seeking Semantics for NLI.", "acronym": "NLI", "label": "Natural Language Inference", "ID": "6805"}, {"sentence": "NLI.", "acronym": "NLI", "label": "Natural Language Inference", "ID": "6806"}, {"sentence": "32   Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 521?528 Manchester, August 2008 Modeling Semantic Containment and Exclusion in NLI Bill MacCartney Stanford University wcmac@cs.stanford.edu Christopher D. Manning Stanford University manning@cs.stanford.edu Abstract We propose an approach to natural lan- guage inference based on a model of nat- ural logic, which identifies valid infer- ences by their lexical and syntactic fea- tures, without full semantic interpretation.", "acronym": "NLI", "label": "Natural Language Inference", "ID": "6807"}, {"sentence": "WilLS, Y.A. (1975) \"A Preferential Pattern-Seeking Semantics for  NLI.\"", "acronym": "NLI", "label": "Natural Language Inference", "ID": "6808"}, {"sentence": "Wilks, Y., A Preferential Pattern-Seeking Semantics  for NLI.", "acronym": "NLI", "label": "Natural Language Inference", "ID": "6809"}, {"sentence": "Wilks, Y.A. 1975 A Preferential Pattern-Seeking Semantics for  NLI.", "acronym": "NLI", "label": "Natural Language Inference", "ID": "6810"}, {"sentence": "2.2 GLMs We follow the framework of Collins (2002; 2004), recently applied to language modeling in Roark et al. (", "acronym": "GLM", "label": "Global Linear Model", "ID": "6811"}, {"sentence": "2.2 A GLM The model used for parsing with this approach is a global linear model.", "acronym": "GLM", "label": "Global Linear Model", "ID": "6812"}, {"sentence": "Biconvex functions and possible ap- plications have been well studied in the optimi- sation literature (Quesada and Grossmann, 1995; 4Note that other loss functions could be used here, such as logistic loss for classification, or more generally bilinear variations of GLMs (Nelder and Wed- derburn, 1972).", "acronym": "GLM", "label": "Generalised Linear Model", "ID": "6813"}, {"sentence": "Yogatama et al(2011) proposed a regulariser for GLM that encourages local temporal smoothness.", "acronym": "GLM", "label": "generalised linear models", "ID": "6814"}, {"sentence": "2 3 Models A common approach to regression arises through the application of GLM.", "acronym": "GLM", "label": "generalised linear models", "ID": "6815"}, {"sentence": "Rosti et al (2007) look at sentence-level com- binations (as well as word- and phrase-level), using reranking of n-best lists and confidence scores derived from GLM with probabilistic features from n-best lists.", "acronym": "GLM", "label": "generalised linear models", "ID": "6816"}, {"sentence": "This was partly done by Maeda et al 1988  by means of DRT.", "acronym": "DRT", "label": "Discourse Representation Theory", "ID": "6817"}, {"sentence": "Examples are Quasi Logical Forms (Al- shawi, 1990), Dynamic Predicate Logic (Groe- nendijk and Stokhof, 1991), and Underspecified DRT (Reyle, 1993).", "acronym": "DRT", "label": "Discourse Representation Theory", "ID": "6818"}, {"sentence": "I. (1998): The dynamic potential of topic and focus: A Praguian approach to DRT.", "acronym": "DRT", "label": "Discourse Representation Theory", "ID": "6819"}, {"sentence": "In Groenindijk and Stokhof, editors, Studies  in DRT and the The-  ory of Generalized Quantifiers.", "acronym": "DRT", "label": "Discourse Representation Theory", "ID": "6820"}, {"sentence": "For the semantic analysis, a version  of DRT is used which  can express underspecification a d take composi-  tionality into account.", "acronym": "DRT", "label": "Discourse Representation Theory", "ID": "6821"}, {"sentence": "Studies in DRT and the  Theory of Generalized Quantifiers, Foris Publica-  tions.", "acronym": "DRT", "label": "Discourse Representation Theory", "ID": "6822"}, {"sentence": "Our treatment of discourse refer- ents and accessibility domains is similar to that of DRT (Kamp and Reyle, 1993).", "acronym": "DRT", "label": "discourse representation theory", "ID": "6823"}, {"sentence": "\\[18\\] Schubert, L. K. and Pelletier, F. J. \"Generically speak-  ing, or, using DRT to in-  terpret generics.\"", "acronym": "DRT", "label": "discourse representation theory", "ID": "6824"}, {"sentence": "An underspacified seg-  meated DRT (US-  DR'I').", "acronym": "DRT", "label": "discourse representation theory", "ID": "6825"}, {"sentence": "From discourse to logic: Introduction to modeltheoretic semantics of natural language, formal logic and DRT.", "acronym": "DRT", "label": "discourse representation theory", "ID": "6826"}, {"sentence": "Seg- mented DRT: Dynamic semantics with discourse structure.", "acronym": "DRT", "label": "discourse representation theory", "ID": "6827"}, {"sentence": "In terms of DRT, e  is tim discourse referent of which the logical form is a  description.", "acronym": "DRT", "label": "discourse representation theory", "ID": "6828"}, {"sentence": "One possibility is that higher ranked retrieved documents are more likely to contain biographical facts, while in later documents it is more likely that AA instances are in fact false positives.", "acronym": "AA", "label": "automatically annotated training", "ID": "6829"}, {"sentence": "Bacchiani and Roark (2003) obtained positive results in unsupervised domain adaptation of language models by using a speech recognition system with an out-of-domain language model to produce an AA cor-pus that is used to adapt the language model us-ing a maximum a posteriori (MAP) adaptation strategy.", "acronym": "AA", "label": "automatically annotated training", "ID": "6830"}, {"sentence": "They AA data for the test target Donald Trump, thus converting the task into weakly supervised seen target stance detection.", "acronym": "AA", "label": "automatically annotated training", "ID": "6831"}, {"sentence": "In (1), so is usually classified as AA  within a sentence, but in (2) so is recognized as  marking a change in message thrust at the  discourse level.", "acronym": "AA", "label": "an adverb", "ID": "6832"}, {"sentence": "till), when is AA of time and no nominal element fol- lows.", "acronym": "AA", "label": "an adverb", "ID": "6833"}, {"sentence": "This may be done by inserting AA into  the sentence and asking the user whether the meaning  remains unchanged.", "acronym": "AA", "label": "an adverb", "ID": "6834"}, {"sentence": "The combination of AA with an adjec-  tive, past participle, or progressive verb is given  score 0.", "acronym": "AA", "label": "an adverb", "ID": "6835"}, {"sentence": "Our concern in this  project is to identify so in the discourse sense as in  (2) in contrast to so used as AA in the  sentential sense as in (1).", "acronym": "AA", "label": "an adverb", "ID": "6836"}, {"sentence": "where the wildcard can be either an ad- jective or AA.", "acronym": "AA", "label": "an adverb", "ID": "6837"}, {"sentence": "The 1 Portions of this article are taken from the paper \"MUC-6 : A Brief History\", in COLING-96, Proc .", "acronym": "MUC-6", "label": "Message Understanding Conference-6", "ID": "6838"}, {"sentence": "MUC-6: A Brief History.", "acronym": "MUC-6", "label": "Message Understanding Conference-6", "ID": "6839"}, {"sentence": "We have annotated two data sets, one from the Brown corpus and one based on data from the MUC-6 (MUC6).", "acronym": "MUC-6", "label": "Message Understanding Conference 6", "ID": "6840"}, {"sentence": "MUC-6.", "acronym": "MUC-6", "label": "Message Understanding Conference 6", "ID": "6841"}, {"sentence": "c?2010 Association for Computational Linguistics Preferences versus Adaptation during REG Martijn Goudbeek University of Tilburg Tilburg, The Netherlands m.b.goudbeek@uvt.nl Emiel Krahmer University of Tilburg Tilburg, The Netherlands e.j.krahmer@uvt.nl Abstract Current REG algorithms rely on domain dependent pref- erences for both content selection and lin- guistic realization.", "acronym": "REG", "label": "Referring Expression Generation", "ID": "6842"}, {"sentence": "A Two-tier User Simulation Model for Reinforcement Learning of Adaptive REG Poli- cies.", "acronym": "REG", "label": "Referring Expression Generation", "ID": "6843"}, {"sentence": "CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 151?158 Manchester, August 2008 Trainable Speaker-Based REG Giuseppe Di Fabbrizio and Amanda J. Stent and Srinivas Bangalore AT&T Labs - Research, Inc. 180 Park Avenue Florham Park, NJ 07932, USA {pino,stent,srini}@research.att.com Abstract Previous work in referring expression gen- eration has explored general purpose tech- niques for attribute selection and surface realization.", "acronym": "REG", "label": "Referring Expression Generation", "ID": "6844"}, {"sentence": "c?2012 Association for Computational Linguistics Learning Preferences for REG: Effects of Domain, Language and Algorithm Ruud Koolen Tilburg University P.O. Box 90135 5000 LE Tilburg The Netherlands r.m.f.koolen@uvt.nl Emiel Krahmer Tilburg University P.O. Box 90135 5000 LE Tilburg The Netherlands e.j.krahmer@uvt.nl Marie?t Theune University of Twente P.O. Box 217 7500 AE Enschede The Netherlands m.theune@utwente.nl Abstract One important sub", "acronym": "REG", "label": "Referring Expression Generation", "ID": "6845"}, {"sentence": "REG We dis- tinguish three types of referring expressions: common names, familiar names and descriptions.", "acronym": "REG", "label": "Referring Expression Generation", "ID": "6846"}, {"sentence": "N Table 3: Syllable Organization for the       Logistic REG Model 4.1 Logistic REG Model for Combining Syllables The model to combine syllables is built upon Binary Logistic REG whose answers are either combine or not combine.", "acronym": "REG", "label": "Regression", "ID": "6847"}, {"sentence": "1095   Combining Prediction by Partial Matching and Logistic REG                for Thai Word Segmentation Ohm Sornil Department of Computer Science National Institute of Development Administration, Bangkok, Thailand osornil@as.nida.ac.th Paweena Chaiwanarom National Statistical Office Bangkok, Thailand paweena@nso.go.th Abstract Word segmentation is an important part of many applications, including information retrieval, information filtering, docum", "acronym": "REG", "label": "Regression", "ID": "6848"}, {"sentence": "10 0 10 2000.05 0.10.15 0.20.25 0.30.35 Density Amplitude REG    Empirical DistributionSemiparametric FitGamma Fit Figure 1: Empirical distributions of saccade amplitudes in training data for first individual, with fitted Gamma distribu- tions and semiparametric distribution fits.", "acronym": "REG", "label": "Regression", "ID": "6849"}, {"sentence": "REG modeling.", "acronym": "REG", "label": "Regression", "ID": "6850"}, {"sentence": "CNs are well analyzed as  noun bigram.", "acronym": "CNs", "label": "Compound nouns", "ID": "6851"}, {"sentence": "CNs evaluated:  We prepared two kinds of data: compound nouns that  included correct homophones (correct homophone data  sets) and compound nouns that included wrong  homophones (wrong homophone data sets).", "acronym": "CNs", "label": "Compound nouns", "ID": "6852"}, {"sentence": "2 Background 2.1 Compound Noun Interpretation CNs were seminally and thoroughly analysed by Levi (1978), who hand?constructs a nine?way set of semantic relations that she identi- fies as broadly defining the observed relationships between the compound head and modifier.", "acronym": "CNs", "label": "Compound nouns", "ID": "6853"}, {"sentence": "1 Introduction CNs are a class of multiword expres- sion (MWE) that have been of interest in recent computational linguistic work, as any task with a lexical semantic dimension (like machine transla- tion or information extraction) must take into ac- count their semantic markedness.", "acronym": "CNs", "label": "Compound nouns", "ID": "6854"}, {"sentence": "CNs were reduced to their base nouns, so that ?", "acronym": "CNs", "label": "Compound nouns", "ID": "6855"}, {"sentence": "The semantic restriction dictionary:  CNs including all homophones in table 1,  were collected from newspaper a ticles over a 90 day  period, and the semantic restriction dictionary was made  based on the semantic restrictions between the  homophones and the adjoining words in compound  nouns.", "acronym": "CNs", "label": "Compound nouns", "ID": "6856"}, {"sentence": "We will have, in particular, the categories  ELECN of regular language CNs,  RREI of regular elation instructions,  EDE$ of regular definition sentences.", "acronym": "CNs", "label": "common ouns", "ID": "6857"}, {"sentence": "For instance, the tag fam-  ily \"MONEY\" contains CNs, proper  nouns, adjectives, and adverbs, the semantic  component of whose tags within the ATR Gen-  eral English Tagset, is \"money\": 500-stock, De-  posit, TOLL-FREE, inexpensively, etc.", "acronym": "CNs", "label": "common ouns", "ID": "6858"}, {"sentence": "We tried  a total of 5 x 5 x 3 x 3 = 225 group settings for the  four variables (front, rear, during weights and  linking length settings) for each of the three  (CNs, proper nouns and pronoun forms)  term types.", "acronym": "CNs", "label": "common ouns", "ID": "6859"}, {"sentence": "Investigation was  restricted to the translation of content words:  CNs, verbs, adjectives and adverbs.", "acronym": "CNs", "label": "common ouns", "ID": "6860"}, {"sentence": "We find that Bayes Point Machines have a good trade-off between perfor- mance and training speed, justifying our repeated usage of BPM in the GA for feature selection.", "acronym": "GA", "label": "genetic algorithm", "ID": "6861"}, {"sentence": "As the train- ing time for BPM is better than CRF, our choice of BPM helped us to run the GA re- peatedly as well.", "acronym": "GA", "label": "genetic algorithm", "ID": "6862"}, {"sentence": "4.2 Combinations of Base Features In order to discover combinations of base features, we implemented a GA (Goldberg, 1989).", "acronym": "GA", "label": "genetic algorithm", "ID": "6863"}, {"sentence": "Since these fea- tures seem unintuitive to the authors, it is likely that they would not have been found without the GA we employed.", "acronym": "GA", "label": "genetic algorithm", "ID": "6864"}, {"sentence": "Moreover, the resolution is implemented with a GA on  its feature selection.", "acronym": "GA", "label": "genetic algorithm", "ID": "6865"}, {"sentence": "Surprisingly, our GA removed features F 10 and F 11, the last two/three let- ters in a token.", "acronym": "GA", "label": "genetic algorithm", "ID": "6866"}, {"sentence": "Exact answer Star, large celestial body composed of GA contained hot gases emitting electromagnetic radiation, especially light, as a result of nuclear reactions inside the star.", "acronym": "GA", "label": "gravitationally", "ID": "6867"}, {"sentence": "73chs)  (iv) ~ (in fact), tl l~l~ ~ ~ (authors) ~ r U k'b ~\" (it) ~E.9\"X: (using), ~ll)'J~ff~l~J~r~: (GA interacting) 3F.~.  \"j\" ~5 (governing)J ~{tgto (astronomical) ~-gw'~ (about the motion), ?", "acronym": "GA", "label": "gravitationally", "ID": "6868"}, {"sentence": "2 The Rule-based Component 2.1 Formal MNS Taken strictly formally, the rule-based component has the form of a restarting automaton with dele- tion (Pla?tek et al, 1995), that is, each rule can be thought of as a finite-state automaton starting from the beginning of the sentence and passing to the right until it finds an input configuration on which it can operate by deletion of some parts of the input.", "acronym": "MNS", "label": "Means", "ID": "6869"}, {"sentence": "They compared K-MNS clustering  with Spectral Clustering.", "acronym": "MNS", "label": "Means", "ID": "6870"}, {"sentence": "Random K-MNS local  global   CSPA  global   centroid  GS #1 0,387 0,586 0,737 0,741 0,741  GS #2 0,415 0,613 0,765 0,777 0,777  GS #3 0,385 0,609 0,794 0,805 0,809  GS #4 0,399 0,606 0,768 0,776 0,776  Avg.", "acronym": "MNS", "label": "Means", "ID": "6871"}, {"sentence": "Assumption 1 (Linear, Rank m, MNS) E[z i |pix(zi),x] = A (z i |z pix(z i ) ,x)pix(zi) ?", "acronym": "MNS", "label": "Means", "ID": "6872"}, {"sentence": "This will be of interest not only in deter- mining expected agreement, but also in terms of   -5 0 5 10 15 20 25 30 0 20 40 60 80 100 120 140 160 180 MNS of Annotated Durations N um be r o f A nn ot at ed  D ur at io ns   Figure 2: Distribution of MNS of Annotated  Durations.", "acronym": "MNS", "label": "Means", "ID": "6873"}, {"sentence": "These relations specify the role of a concept with  respect o an action (John (AGNT) eats), to a function  (building for (MNS) residence) or to an event (a  delay for (CAUSE) a traffic jam).", "acronym": "MNS", "label": "MEANS", "ID": "6874"}, {"sentence": "\\ [ I IUMAN\\]  (o I~J!- --lTVO P \\]  253  b.  e.  (MNS)-- > \\[brain\\]  (PURPOSE)-- > \\[AIM'\\]  while for book would be:  (MNS)<--\\[ACT OF COMMUNICATION\\]  (OBJ) < --\\[MOVE_POSITION\\]  Complement.", "acronym": "MNS", "label": "MEANS", "ID": "6875"}, {"sentence": "MNS (MNS) Profits increase investments  3.", "acronym": "MNS", "label": "MEANS", "ID": "6876"}, {"sentence": "--(MNS) < --\\[RESIDENCE\\]  were I~IIII.I)ING represents the species, or  supertype, and (MNS)<--\\[RESIDENCE\\] the  differentia.", "acronym": "MNS", "label": "MEANS", "ID": "6877"}, {"sentence": "7 Conclusions We introduced models of MTUs for phrasal systems, and showed that they make a substantial and statistically significant improvement on three distinct language-pairs.", "acronym": "MTUs", "label": "Minimal Translation Units", "ID": "6878"}, {"sentence": "Model With MTUs, But Decode With Phrases.", "acronym": "MTUs", "label": "Minimal Translation Units", "ID": "6879"}, {"sentence": "c?2013 Association for Computational Linguistics Model With MTUs, But Decode With Phrases Nadir Durrani?", "acronym": "MTUs", "label": "Minimal Translation Units", "ID": "6880"}, {"sentence": "Can Markov Models Over MTUs Help Phrase- Based SMT?", "acronym": "MTUs", "label": "Minimal Translation Units", "ID": "6881"}, {"sentence": "Can Markov Models Over MTUs Help Phrase-Based SMT?", "acronym": "MTUs", "label": "Minimal Translation Units", "ID": "6882"}, {"sentence": "Task-Specific Alignment Evaluation In this section we evaluate the alignments resulting from using the proposed constraints in two different tasks: Statistical machine translation where alignments are used to restrict the number of possible MTUs; and syntax transfer, where alignments are used to decide how to transfer dependency links.", "acronym": "MTUs", "label": "minimal translation units", "ID": "6883"}, {"sentence": "Word alignments are used primarily for extracting MTUs for machine translation (MT) (e.g., phrases [Koehn, Och, and Marcu 2003] and rules [Galley et al 2004; Chiang et al 2005]) as well as for ?", "acronym": "MTUs", "label": "minimal translation units", "ID": "6884"}, {"sentence": "Model with MTUs, but de- code with phrases.", "acronym": "MTUs", "label": "minimal translation units", "ID": "6885"}, {"sentence": "Therefore, training an n-gram model over MTUs turns out to be a simple and clean choice: the resulting segmen- tation is unique, and the distribution is smooth.", "acronym": "MTUs", "label": "minimal translation units", "ID": "6886"}, {"sentence": "2013) also present a Markov model based on MPs (they call MTUs) and fur- ther define operation sequence over MPs which are taken as the events in the Markov model.", "acronym": "MTUs", "label": "minimal translation units", "ID": "6887"}, {"sentence": "The 982 N Matrix types 891 ordinary 390 pos disjunctions 591 Wambaya-specific types 911 Phrase structure rules 83 Lexical rules 161 Lexical entries 1528 Table 2: Size of Wambaya grammar Matrix core types w/ POS types Directly used 132 34% 136 15% Indirectly used 98 25% 584 66% TL types used 230 59% 720 81% Types unused 160 41% 171 19% Types modified 16 4% 16 2% TL 390 100% 891 100% Table 3: Matrix core types used in Wambaya grammar Wambaya grammar includes 891 types defined in the Matrix core type hierarchy.", "acronym": "TL", "label": "Total", "ID": "6888"}, {"sentence": "Physics EE TL Stud Tut Stud Tut in addition to 0 0 0 3 3 besides 1 3 0 2 6 another 56 124 10 38 228 especially 0 1 0 0 1 except 0 17 1 1 19 other 107 484 36 59 686 in particular 0 6 0 0 6 such 2 18 3 10 33 unlike 0 0 0 1 1 TL 166 653 50 114 983 Alternative Phrases per Dialog Dialogs 203 203 66 66 269 AP/dialog 0.82 3.22 0.76 1.73 3.65 Alternative Phrases in Queries per Dialog Query APs 51 261 16 86 414 per dialog 0.25 1.29 0.24 1.3 1.54 Table 1: Frequency of Alternative Phrases in Di- alog 5 Evaluation 5.1 Frequency of Alternative Phrases First, it is useful to determine how many queries contain alternative phras", "acronym": "TL", "label": "Total", "ID": "6889"}, {"sentence": "5 TL number of pages indexed The total num- ber of the web pages indexed by a search engine varies across time and the numbers provided are somewhat unreliable.", "acronym": "TL", "label": "Total", "ID": "6890"}, {"sentence": "This amounts to a per- 47 Task 1 Task 2 R P F R P F Loc 37.9 88.0 53.0 32.8 76.0 45.8 Bind 23.1 48.2 31.2 22.4 47.0 30.3 Expr 63.0 75.1 68.5 63.0 75.1 68.5 Trans 16.8 29.9 21.5 16.8 29.9 21.5 Cata 64.3 81.8 72.0 64.3 81.8 72.0 Phos 78.5 77.4 77.9 69.1 70.1 69.6 TL 48.3 68.9 56.8 46.8 67.0 55.1 Reg 23.7 40.8 30.0 22.3 38.5 28.2 Pos 26.8 42.8 32.9 26.7 42.3 32.7 Neg 27.2 40.2 32.4 26.1 38.6 31.2 TL 26.3 41.8 32.3 25.8 40.8 31.6 TL 36.9 55.6 44.4 35.9 54.1 43.1 Table 3: (R)ecall, (P)recision, and (F)-Score for task 1 and 2 in terms of event types.", "acronym": "TL", "label": "Total", "ID": "6891"}, {"sentence": "Physics EE TL Stud Tut Stud Tut in addition to 0 0 0 3 3 besides 1 3 0 2 6 another 56 124 10 38 228 especially 0 1 0 0 1 except 0 17 1 1 19 other 107 484 36 59 686 in particular 0 6 0 0 6 such 2 18 3 10 33 unlike 0 0 0 1 1 TL 166 653 50 114 983 Alternative Phrases per Dialog Dialogs 203 203 66 66 269 AP/dialog 0.82 3.22 0.76 1.73 3.65 Alternative Phrases in Queries per Dialog Query APs 51 261 16 86 414", "acronym": "TL", "label": "Total", "ID": "6892"}, {"sentence": "Surface features include TL in words (Tsur et al.,", "acronym": "TL", "label": "tweet length", "ID": "6893"}, {"sentence": "2013) for details of the RepLab series 73 These studies combine Twitter-specific and textual features such as retweet counts, TLs and hashtag frequency, together with sentence-length, character n-grams and punctuation counts.", "acronym": "TL", "label": "tweet length", "ID": "6894"}, {"sentence": "Context Similarity: f2(mi, ei) = coocurence number TL (3) where: coccurence number is the the number of the words that occur in both the tweet containing mi and the Wikipedia page of ei; TL denotes the number of tokens of the tweet containing mention mi.", "acronym": "TL", "label": "tweet length", "ID": "6895"}, {"sentence": "The regression models use both binary presence-of feature classes (quotation; past, present tense; 16 types of discourse relations; 10 NE types; 3 hashtag positions) as well as normalized numeric features (TL, hashtag count, sentence sim- ilarity, 3 sentiment polarity strengths).", "acronym": "TL", "label": "tweet length", "ID": "6896"}, {"sentence": "They found similar language usage trends for both genders, with increasing word and TL with age, and an increasing tendency to write more grammatically correct, standardized 844 text.", "acronym": "TL", "label": "tweet length", "ID": "6897"}, {"sentence": "2008) used the sentence-pair confidence scores estimated with source and TL mod- els to weight phrase translation pairs.", "acronym": "TL", "label": "target language", "ID": "6898"}, {"sentence": "alignment, the correspondence between words in source and TLs.", "acronym": "TL", "label": "target language", "ID": "6899"}, {"sentence": "Background A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a TL (Brown et al 1993b).", "acronym": "TL", "label": "target language", "ID": "6900"}, {"sentence": "Each observation corresponds to a word in the TL xi.", "acronym": "TL", "label": "target language", "ID": "6901"}, {"sentence": "This system uses a word-aligned corpus and a parser for a resource-rich language (source language) in order to create a parser for a resource-poor language (TL).", "acronym": "TL", "label": "target language", "ID": "6902"}, {"sentence": "A rare word in the source language links to many words in the TL that we would ideally like to see unaligned, or aligned to other words in the sentence.", "acronym": "TL", "label": "target language", "ID": "6903"}, {"sentence": "For each such edge, if both end points are aligned to words in the TL, then the edge is transferred.", "acronym": "TL", "label": "target language", "ID": "6904"}, {"sentence": "using the proposed method is available at http://mednlp.jp/influ/.    Figure 8: The TL of Influenza Epidemics in Fukushima.", "acronym": "TL", "label": "Timeline", "ID": "6905"}, {"sentence": "TL visualization in XOpin    The comparison view (Figure 4) allows the user  to compare side by side different product features  in a collection of texts.", "acronym": "TL", "label": "Timeline", "ID": "6906"}, {"sentence": "Detections, Bounds, and TLs: UMass and TDT-3.", "acronym": "TL", "label": "Timeline", "ID": "6907"}, {"sentence": "TL: A dynamic hierarchical Dirichlet process model for re- covering birth/death and evolution of topics in text stream.", "acronym": "TL", "label": "Timeline", "ID": "6908"}, {"sentence": "12) TLss We estimate timeliness using the time- based language models ?", "acronym": "TL", "label": "Timeline", "ID": "6909"}, {"sentence": "c?2012 Association for Computational Linguistics Extracting Narrative TLs as Temporal Dependency Structures Oleksandr Kolomiyets KU Leuven Celestijnenlaan 200A B-3001 Heverlee, Belgium Oleksandr.", "acronym": "TL", "label": "Timeline", "ID": "6910"}, {"sentence": "A post-edlted system  res igns  i t se l f  f rom the start  to inadequacy,   bu i ld ing  in the requ i rement  for (more or less)  radical human revis ion of its output, so that it  might  bet ter  be ca l led  pro - t rans la t ion  than  t rans la t ion  proper;  wh i le  many cur rent  pro-  ed i t ing  sys tems,  a l though o f fe r ing  fu l l y   automat ic  p roduct ion  of TL text  f rom source language text, requ i re  a human  contribution in the pre-input stage, control l ing  and res t r i c t ing  that  source  text,  wh ich   qua l i ta t ive ly  far exceeds the demands of on- l ine  interaction (fn 3).", "acronym": "TL", "label": "target  language", "ID": "6911"}, {"sentence": "But progress  has  been  slower  on  translation models  that  are  able  to  learn  the  rela- tionship  between  the  grammars  of  both the source and TL.", "acronym": "TL", "label": "target  language", "ID": "6912"}, {"sentence": "We used only the target side of the bilingual corpus for the TL  model,  rather  than  the  larger supplied  language  model.", "acronym": "TL", "label": "target  language", "ID": "6913"}, {"sentence": "Malay has  very simple and regular question structure, the  question words appear at the front of question sen- tences (in the same way as the TL) and  do not take any other function in the language (un- like the English ?", "acronym": "TL", "label": "target  language", "ID": "6914"}, {"sentence": "With source-language LUs replaced by unique  mul t i i i ngua l -d ic t ionary  addresses, th is  canonical   representat ion  is the In ter l ingua  which is passed  for  synthesis  in to  the TL(s~.  C. Synthesis  Assuming the analys is  has been cor rec t ly   per formed,  synthesis  is a relatively straight-  forward determin is t i c  process.", "acronym": "TL", "label": "target  language", "ID": "6915"}, {"sentence": "In this work we treat the process of translit- eration as a process of direct  transduction from  sequences of tokens in the source language to  sequences of tokens in the target language with  no modeling of the phonetics of either source or  TL (Knight and Graehl, 1997).", "acronym": "TL", "label": "target  language", "ID": "6916"}, {"sentence": "In the example from Table 1, conTL is captured by the row  marginals, e.g. the conTL for Context1 is  3, which means that", "acronym": "TL", "label": "text length", "ID": "6917"}, {"sentence": "Thus, a conTL could be very small compared to the size of the feature set.", "acronym": "TL", "label": "text length", "ID": "6918"}, {"sentence": "To simu- late the structure of the observed data, the fol- lowing features are to be emulated: (a) ConTL is the number of features  that can occur in a context.", "acronym": "TL", "label": "text length", "ID": "6919"}, {"sentence": "nTL is the number of features  that can occur in a context.", "acronym": "TL", "label": "text length", "ID": "6920"}, {"sentence": "In the example from Table 1, conTL is captured by the row  marginals, e.g. the conTL for Context1 is  3, which means that overall there are only three features for that context.", "acronym": "TL", "label": "text length", "ID": "6921"}, {"sentence": "(b) Sparsity is a consequence of relatively small conTL.", "acronym": "TL", "label": "text length", "ID": "6922"}, {"sentence": "Addi- tionally, conTL is influenced by the fea- ture selection method ?", "acronym": "TL", "label": "text length", "ID": "6923"}, {"sentence": "LSH provides a way of dealing with this problem: instead of retaining the explicit sparse high-dimensional ~ci, we use a ran- dom projection h(?)", "acronym": "LSH", "label": "Locality sensitive hashing", "ID": "6924"}, {"sentence": "LSH: A comparison of hash function types and querying mechanisms.", "acronym": "LSH", "label": "Locality sensitive hashing", "ID": "6925"}, {"sentence": "c?2011 Association for Computational Linguistics Efficient Online LSH via Reservoir Counting Benjamin Van Durme HLTCOE Johns Hopkins University Ashwin Lall Mathematics and Computer Science Denison University Abstract We describe a novel mechanism called Reser- voir Counting for application in online Local- ity Sensitive Hashing.", "acronym": "LSH", "label": "Locality Sensitive Hashing", "ID": "6926"}, {"sentence": "s. 51 LSH, at similar levels of accu- racy, by replacing explicit 32-bit counting variables with approximate counters of smaller size.", "acronym": "LSH", "label": "Locality Sensitive Hashing", "ID": "6927"}, {"sentence": "With regard to context representation, it is also intriguing to explore other dimensionality reduc- tion methods (such as LSH or Random Indexing) and to compare them to the SVD-based model.", "acronym": "LSH", "label": "Locality Sensitive Hashing", "ID": "6928"}, {"sentence": "3 Randomized Model Now situated within a streaming context we exact space savings through approximation, extending the approach of Van Durme and Lall (2011), there con- cerned with online LSH, here initially concerned with taking averages.", "acronym": "LSH", "label": "Locality Sensitive Hashing", "ID": "6929"}, {"sentence": "3.3 LSH As described in Section 3.2, we find paraphrases of a phrase pi by finding its nearest neighbors based on cosine similarity between the feature vector of pi and other phrases.", "acronym": "LSH", "label": "Locality Sensitive Hashing", "ID": "6930"}, {"sentence": "8 add d to inverted index 9 end 2.2 LSH The problem of finding the nearest neighbor to a given query has been intensively studied, but as the dimensionality of the data increases none of the cur- rent solutions provide much improvement over a sim- ple linear search (Datar et al, 2004).", "acronym": "LSH", "label": "Locality Sensitive Hashing", "ID": "6931"}, {"sentence": "Thus  P(sp, dola, M) = P(sp, d.la)  Then we combine sp and de into one vari-  able dIt, Hobbs distance, since the Hobbs  algorithm takes both the SYN and dis-  tance into account.", "acronym": "SYN", "label": "syntax", "ID": "6932"}, {"sentence": "We now turn to briefly present the SYN of adjectives in Catalan and discuss the parameters in more detail.", "acronym": "SYN", "label": "syntax", "ID": "6933"}, {"sentence": "However, any SYN-only pronoun resolution  strategy will be wrong some of the time - these  methods know nothing about discourse bound-  aries, intentions, or real-world knowledge.", "acronym": "SYN", "label": "syntax", "ID": "6934"}, {"sentence": "Shared story reading with parents or teachers helps children to learn about vocabulary, SYN and phonology, and to develop narrative comprehension and awareness of the concepts of print, all of which are linked to developing reading and writing skills (National Early Literacy Panel 2008).", "acronym": "SYN", "label": "syntax", "ID": "6935"}, {"sentence": "We also note that the very simple model that  ignores SYN and takes the last mentioned  noun-phrase as the referent performs quite a  bit worse, about 43% correct.", "acronym": "SYN", "label": "syntax", "ID": "6936"}, {"sentence": "This indicates  that SYN does play a very important role in  anaphora resolution.", "acronym": "SYN", "label": "syntax", "ID": "6937"}, {"sentence": "Yu et al (2002) showed that the Introduction defines the majority of SYN, while Schuemie et al (2004) and Shah et al. (", "acronym": "SYN", "label": "synonyms", "ID": "6938"}, {"sentence": "The specific ontological in- formation extracted is the type hierarchy and sets of SYN (AKA, or ?", "acronym": "SYN", "label": "synonyms", "ID": "6939"}, {"sentence": "Further, it identi- fies a range of NLP tools required, including: identifying SYN, and resolving coref- erence and negated expressions.", "acronym": "SYN", "label": "synonyms", "ID": "6940"}, {"sentence": "We believe it would be quite difficult  to achieve the same accuracy by compiled  knowledge, such as a dictionary of verbs, anto- nyms, SYN, and relation words, and a the- saurus.", "acronym": "SYN", "label": "synonyms", "ID": "6941"}, {"sentence": "The questions about SYN ask relations  of priority/inferiority between words and choos- ing the word in a different group.", "acronym": "SYN", "label": "synonyms", "ID": "6942"}, {"sentence": "5.1 Synonym Facts The frequent use of SYN, abbreviations and acronyms in biomedical text is a common source of ambiguity that is often hard to resolve (Sehgal et al, 2004).", "acronym": "SYN", "label": "synonyms", "ID": "6943"}, {"sentence": "We assume this occurs be- cause Spanish is one of the six official United Na- tions languages, and the GT engine is using the United Nations parallel corpus to train their translation engine, therefore implying that a better quality translation is achieved as compared to the one available for Romanian.", "acronym": "GT", "label": "Google translation", "ID": "6944"}, {"sentence": "For  the NEs not covered by our dictionary, we  use GT service as a back-up.", "acronym": "GT", "label": "Google translation", "ID": "6945"}, {"sentence": "In our  experiments we identify SL (Chinese) NEs                                                    2 We also try online GT service, and the  performance was roughly the same.", "acronym": "GT", "label": "Google translation", "ID": "6946"}, {"sentence": "The number of languages currently covered by the GT system (41 language) is smaller than the number of languages in which there exist Wikipedia articles (265 languages).", "acronym": "GT", "label": "Google translation", "ID": "6947"}, {"sentence": "However, we believe that using for cross-lingual analysis de- scriptions only in those languages that can be han- dled by the GT system does not af- fect the generality of our conclusions.", "acronym": "GT", "label": "Google translation", "ID": "6948"}, {"sentence": "However, we searched for 20 of the 57 English words for which the workers agreed upon a manually entered Russian translation in Google translate, and we found that the Russian translation was the top GT for only 11 of the 20 English words.", "acronym": "GT", "label": "Google translation", "ID": "6949"}, {"sentence": "2 We also try online GT service, and the  performance was roughly the same.", "acronym": "GT", "label": "Google translation", "ID": "6950"}, {"sentence": "All the sentences were automatically  translated into Chinese sentences by using the  GT service.", "acronym": "GT", "label": "Google Translate", "ID": "6951"}, {"sentence": "Finally, the sentences in the English summary  are translated into the corresponding Chinese  sentences by using GT, and the  Chinese summary is formed.", "acronym": "GT", "label": "Google Translate", "ID": "6952"}, {"sentence": "GT  is one of the state-of-the-art commercial machine  translation systems used today.", "acronym": "GT", "label": "Google Translate", "ID": "6953"}, {"sentence": "MT features are  not used because GT is used as a black box.", "acronym": "GT", "label": "Google Translate", "ID": "6954"}, {"sentence": "by using GT is ?????", "acronym": "GT", "label": "Google Translate", "ID": "6955"}, {"sentence": "In this study, we adopt GT1 for  English-to-Chinese translation.", "acronym": "GT", "label": "Google Translate", "ID": "6956"}, {"sentence": "We can easily set  up o as an allophone Of u before k. Only the case of  GT stop needs to be considered.", "acronym": "GT", "label": "glottal", "ID": "6957"}, {"sentence": "The lack of articulatory control also leads to various involuntary non-speech sounds including velopharyngeal or GT noise (Rosen and Yampolsky, 2000).", "acronym": "GT", "label": "glottal", "ID": "6958"}, {"sentence": "So we revise the  form, replacing $V with just the vowels in question,  and replacing the $C df the coda with apostrophe (for  GT stop).", "acronym": "GT", "label": "glottal", "ID": "6959"}, {"sentence": "For example, there are many  possible places of articulation, which form a near-  continuum ranging from \\[labial\\] to \\[GT\\].", "acronym": "GT", "label": "glottal", "ID": "6960"}, {"sentence": "3 Tree Search vs. Dynamic  P rogramming  Once an appropriate function for measuring simi-  larity between pairs of segments has been designed,  290  Feature Phonological Numerical  name term value  Place  Manner  High  Back  \\[bilabial\\]  \\[labiodental\\]  \\[dental\\]  \\[alveolar\\]  \\[retroflex\\]  \\[palato-alveolar\\]  \\[palatal\\]  \\[velar\\]  \\[uvular\\]  \\[pharyngeal\\]  \\[GT\\]  \\[stop\\]  \\[affricate\\]  \\[fricative\\]  \\[approximant\\]  \\[high vowel\\]  \\[mid vowel\\]  \\[low vowel\\]  \\[high\\]  \\[mid\\]  \\[low\\]  \\[front\\]  \\[central\\]  \\[back\\]  1.0  0.95  0.9  0.85  0.8  0.75  0.7  0.6  0.5  0.3  0.1  1.0  0.9  0.8  0.6  0.4  0.2  0.0  1.0  0.5  0.0  1.0  0.5  0.0  Table 3: Multivalued features and their values.", "acronym": "GT", "label": "glottal", "ID": "6961"}, {"sentence": "Suppose we wish to identify the minimal pairs for o /u   discussed above, but without having to specify GT  stop in the query, as shown in Figure 3.", "acronym": "GT", "label": "glottal", "ID": "6962"}, {"sentence": "The 4Substitution connects the root node of one elementary tree in an empty slot in another elementary tree, similarly to a GT of Chomsky (1955/75) or Chomsky (1995), Ch.3.", "acronym": "GT", "label": "Generalized Transformation", "ID": "6963"}, {"sentence": "Some of the cases can be explained by differences in where an attribute is marked: For example, for definiteness the performance is 1% from English to BG, as BG marks definiteness on nouns and adjectives rather than on determiners.", "acronym": "BG", "label": "Bulgarian", "ID": "6964"}, {"sentence": "Figure 10 shows our results for transferring from English to BG (En?Bg) and from English to Spanish (En?Es).", "acronym": "BG", "label": "Bulgarian", "ID": "6965"}, {"sentence": "However, there are also some cases of unrelated source languages performing best: Using Danish as source language gives the highest performing models for both BG and Czech.", "acronym": "BG", "label": "Bulgarian", "ID": "6966"}, {"sentence": "In our experiments we learn taggers for a set of 11 European lan- guages that have both UD training data with mor- phological features, and parallel data in Europarl: BG, Czech, Danish, Dutch, Finnish, Ital- ian, Polish, Portuguese, Slovene, Spanish and Swedish.", "acronym": "BG", "label": "Bulgarian", "ID": "6967"}, {"sentence": "We generated supervised parses using the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005) trained on the Penn Treebank for English and the CoNLL X parses for BG and Spanish.", "acronym": "BG", "label": "Bulgarian", "ID": "6968"}, {"sentence": "One admirable standardization effort in the field of Slavic part of speech (POS) tagging has been the Multext-East project (Erjavec, 2001), one of whose aims was to construct mutually compati- ble tagsets for 8 European languages, including 4 Slavic languages (originally BG, Czech and Slovene, later extended to Croatian); additionally, a Multext-East-style tagset for Russian was con- structed at the University of T?bingen (http: //www.sfb441.uni-tuebingen.de/c1/ tagset.html).", "acronym": "BG", "label": "Bulgarian", "ID": "6969"}, {"sentence": "To bet- ter analyze the influence of the background in- formation, all automatic summarization methods are based on the up-to-date LSA method previ- ously described: one taking as input only the news story to be summarized (Simple) and used as base- line; other taking as input only the selected back- ground information (BG only); and, the last one, using both the news story and the back- ground information (BG + News).", "acronym": "BG", "label": "Background", "ID": "6970"}, {"sentence": "BG  Semantic Extraction has become a strong  research focus in the last few years.", "acronym": "BG", "label": "Background", "ID": "6971"}, {"sentence": "2 BG  Two broad approaches have dominated the lit- erature on constructing paraphrase corpora.", "acronym": "BG", "label": "Background", "ID": "6972"}, {"sentence": "the influence of the background in- formation, all automatic summarization methods are based on the up-to-date LSA method previ- ously described: one taking as input only the news story to be summarized (Simple) and used as base- line; other taking as input only the selected back- ground information (BG only); and, the last one, using both the news story and the back- ground information (BG + News).", "acronym": "BG", "label": "Background", "ID": "6973"}, {"sentence": "2 BG 2.1 The LinGO Grammar Matrix The LinGO Grammar Matrix is situated theoreti- cally within Head-Driven Phrase Structure Gram- mar (HPSG; Pollard and Sag, 1994), a lexicalist, constraint-based framework.", "acronym": "BG", "label": "Background", "ID": "6974"}, {"sentence": "BG A word alignment for a parallel sentence pair represents the correspondence between words in a source language and their translations in a target language (Brown et al 1993b).", "acronym": "BG", "label": "Background", "ID": "6975"}, {"sentence": "Instead of using the traditional output of the ASR module, we use the phonetic transliteration of the output and compare it to the phonetic transliteration of solid BG infor- mation.", "acronym": "BG", "label": "background", "ID": "6976"}, {"sentence": "We use broadcast news as a case study and news stories from online newspapers provide the BG information.", "acronym": "BG", "label": "background", "ID": "6977"}, {"sentence": "In this work, we explore the possibilities offered by pho- netic information to select the BG information and conduct a perceptual evaluation to better assess the relevance of the inclusion of that infor", "acronym": "BG", "label": "background", "ID": "6978"}, {"sentence": "2 provides BG on the Grammar Ma- trix and Wambaya, and situates the project with re- spect to related work. ?", "acronym": "BG", "label": "background", "ID": "6979"}, {"sentence": "We propose the inclusion of related, solid BG information to cope with the difficulties of summarizing spoken language and the use of multi-document summarization techniques in single document speech- to-text summarization.", "acronym": "BG", "label": "background", "ID": "6980"}, {"sentence": "In this work, we explore the possibilities offered by pho- netic information to select the BG information and conduct a perceptual evaluation to better assess the relevance of the inclusion of that information.", "acronym": "BG", "label": "background", "ID": "6981"}, {"sentence": "Furthermore, we build on the conjecture that this BG infor- mation is often used by humans to overcome per- ception difficulties.", "acronym": "BG", "label": "background", "ID": "6982"}, {"sentence": "Aspects  of metaphor constitution, of the role of visual  language, as well as of the understanding of feedback mechanisms, and the role of back  channels were pushed into the BG.", "acronym": "BG", "label": "background", "ID": "6983"}, {"sentence": "On Re- verse Feature Engineering of STKs.", "acronym": "STK", "label": "Syntactic Tree Kernel", "ID": "6984"}, {"sentence": "2.2 STK Syntactic tree kernels were first introduced by Collins and Duffy (2001) and were also used by 401 SNP NP DT A NN cat PP IN with NP DT a JJ red NN collar VP AUX was VP VBD chased ADVP NP CD two NNS days RB ago PP IN by NP DT a JJ fat NN dog Figure 3: Syntactic parse tree of the sentence shown in Figure 1 (b).", "acronym": "STK", "label": "Syntactic Tree Kernel", "ID": "6985"}, {"sentence": "c?2010 Association for Computational Linguistics On Reverse Feature Engineering of STKs Daniele Pighin FBK-irst, DISI, University of Trento Via di Sommarive, 14 I-38123 Povo (TN) Italy daniele.pighin@gmail.com Alessandro Moschitti DISI, University of Trento Via di Sommarive, 14 I-38123 Povo (TN) Italy moschitti@disi.unitn.it Abstract In this paper, we provide a theoretical framework for feature selection in tree ker- nel spaces based on gradient-vector com- pon", "acronym": "STK", "label": "Syntactic Tree Kernel", "ID": "6986"}, {"sentence": "STK (K1), also known as a subset tree kernel (Collins and Duffy, 2002), maps ob- jects in the space of all possible tree fragments constrained by the rule that the sibling nodes cannot be separated from their parents.", "acronym": "STK", "label": "Syntactic Tree Kernel", "ID": "6987"}, {"sentence": "The most general kind of kernels used in NLP are string kernels, e.g. (Shawe-Taylor and Cristianini, 2004), the STKs (Collins and Duffy, 2002) and the Partial Tree Kernels (Moschitti, 2006a).", "acronym": "STK", "label": "Syntactic Tree Kernel", "ID": "6988"}, {"sentence": "We consider three systems which are reported to perform efficiently and effectively on processing syntactic trees using three proposed ap- proaches STK (Moschitti, 2006), Syntactic Generalization (Galitsky, 2013) and Dis- tributed Tree Kernel (Zanzotto and Dell?Arciprete, 2012).", "acronym": "STK", "label": "Syntactic Tree Kernel", "ID": "6989"}, {"sentence": "On Re- verse Feature Engineering of STK.", "acronym": "STK", "label": "Syntactic Tree Kernels", "ID": "6990"}, {"sentence": "2 Semantic STK In kernel-based methods, both learning and classi- fication only depend on the inner product between instances.", "acronym": "STK", "label": "Syntactic Tree Kernels", "ID": "6991"}, {"sentence": "The most general kind of kernels used in NLP are string kernels, e.g. (Shawe-Taylor and Cristianini, 2004), the STK (Collins and Duffy, 2002) and the Partial Tree Kernels (Moschitti, 2006a).", "acronym": "STK", "label": "Syntactic Tree Kernels", "ID": "6992"}, {"sentence": "c?2010 Association for Computational Linguistics On Reverse Feature Engineering of STK Daniele Pighin FBK-irst, DISI, University of Trento Via di Sommarive, 14 I-38123 Povo (TN) Italy daniele.pighin@gmail.com Alessandro Moschitti DISI, University of Trento Via di Sommarive, 14 I-38123 Povo (TN) Italy moschitti@disi.unitn.it Abstract In this paper, we provide a theoretical framework for feature selection in tree ker- nel spaces based on gradient-vector com- pon", "acronym": "STK", "label": "Syntactic Tree Kernels", "ID": "6993"}, {"sentence": "SWNt demonstrates quite an unsatisfactory performance, while SentiStrength, being very preci", "acronym": "SWN", "label": "SentiWordNe", "ID": "6994"}, {"sentence": "SWNt (Esuli and Sebastiani, 2006), SO- CAL (Taboada et al2010) and SentiStrength (Thelwall et al2012).", "acronym": "SWN", "label": "SentiWordNe", "ID": "6995"}, {"sentence": "SWNt demonstrates quite an unsatisfactory performance, while SentiStrength, being very precise, has an insufficient scope and, therefore, finds no sentiment in a substantial number of documents.", "acronym": "SWN", "label": "SentiWordNe", "ID": "6996"}, {"sentence": "Since SWNt entries are associated with word senses and because we don?t perform word sense disambiguation, the SWNt po- larity of the most dominant word sense is used for words in the comment section.", "acronym": "SWN", "label": "SentiWordNe", "ID": "6997"}, {"sentence": "First, we evaluate two methods to automatically determine the comment polarity: SWNt (Baccianella and Sebastiani, 2010) a general purpose resource that assigns sen- timent scores to entries in WordNet, and an auto- 12 mated corpus-specific technique based on pointwise mutual information.", "acronym": "SWN", "label": "SentiWordNe", "ID": "6998"}, {"sentence": "It incorporates a rich fea- ture set, relying on the usage of SWNt (Esuli et al, 2010) and further orthological, morphological and syntactic features.", "acronym": "SWN", "label": "SentiWordNe", "ID": "6999"}, {"sentence": "3.1 SWNt In the first stage of the study, we use SentiWord- Net (Baccianella and Sebastiani, 2010) which as- sociates a large number of words in WordNet with a positive, negative and objective score (summing up to 1).", "acronym": "SWN", "label": "SentiWordNe", "ID": "7000"}, {"sentence": "SWN demonstrates quite an unsatisfactory performance, while SentiStrength, being very preci", "acronym": "SWN", "label": "SentiWordNet", "ID": "7001"}, {"sentence": "SWN (Esuli and Sebastiani, 2006), SO- CAL (Taboada et al2010) and SentiStrength (Thelwall et al2012).", "acronym": "SWN", "label": "SentiWordNet", "ID": "7002"}, {"sentence": "SWN demonstrates quite an unsatisfactory performance, while SentiStrength, being very precise, has an insufficient scope and, therefore, finds no sentiment in a substantial number of documents.", "acronym": "SWN", "label": "SentiWordNet", "ID": "7003"}, {"sentence": "Since SWN entries are associated with word senses and because we don?t perform word sense disambiguation, the SWN po- larity of the most dominant word sense is used for words in the comment section.", "acronym": "SWN", "label": "SentiWordNet", "ID": "7004"}, {"sentence": "First, we evaluate two methods to automatically determine the comment polarity: SWN (Baccianella and Sebastiani, 2010) a general purpose resource that assigns sen- timent scores to entries in WordNet, and an auto- 12 mated corpus-specific technique based on pointwise mutual information.", "acronym": "SWN", "label": "SentiWordNet", "ID": "7005"}, {"sentence": "It incorporates a rich fea- ture set, relying on the usage of SWN (Esuli et al, 2010) and further orthological, morphological and syntactic features.", "acronym": "SWN", "label": "SentiWordNet", "ID": "7006"}, {"sentence": "3.1 SWN In the first stage of the study, we use SentiWord- Net (Baccianella and Sebastiani, 2010) which as- sociates a large number of words in WordNet with a positive, negative and objective score (summing up to 1).", "acronym": "SWN", "label": "SentiWordNet", "ID": "7007"}, {"sentence": "Thus, in many cases two Ns are given, with the larger number the numbe r of templates scored and the smaller number the number of individual template scores used in estimating the variance, calculatin g the standard error and CI, and performing statistical tests .", "acronym": "CI", "label": "confidence intervals", "ID": "7008"}, {"sentence": "The probabilities are accompanied by 95% CI, which are computed from the standard deviation of the binomial distribution.", "acronym": "CI", "label": "confidence intervals", "ID": "7009"}, {"sentence": "la ck lef left jus just wit with goin going doin doing kno know 60 62 64 66 68 70 72 74 %  w hi te lef left jus just wit with goin going doin doing kno know 14 16 18 20 22 24 %  h isp an ic lef left jus just wit with goin going doin doing kno know 2000 4000 6000 8000 10000 12000 14000 16000 po p.  d en sit y Figure 1: Average demographics of the counties in which users of each term live, with 95% CI 16 sonant cluster reduction examples are indeed pre- ferred by authors from densely-populated (urban) counties with more African Americans, although these counties tend to prefer all of the non-standard variants, including the control pair kno/know.", "acronym": "CI", "label": "confidence intervals", "ID": "7010"}, {"sentence": ".038 Table 1: Classification results from a 10-fold cross-validation experiment on ETP-gold with 95% CI.", "acronym": "CI", "label": "confidence intervals", "ID": "7011"}, {"sentence": "The two systems were also re- ported to have overlapping CI in the shared task.", "acronym": "CI", "label": "confidence intervals", "ID": "7012"}, {"sentence": "coefficients with their 95% CI.", "acronym": "CI", "label": "confidence intervals", "ID": "7013"}, {"sentence": "Our good advice went unheeded for a long time but in recent work by Christopher Potts (2004) we see an attempt to build the sort of two-dimensional semantics Stanley and I sketched out that separates CI from truth-conditional aspects of meaning.", "acronym": "CI", "label": "conventional implicatures", "ID": "7014"}, {"sentence": "In contrast, CI arise from the meaning of the uttered sentence and the maxims of communication, without any influence from the interactional situation.", "acronym": "CI", "label": "conventional implicatures", "ID": "7015"}, {"sentence": "By adding the extra utterance to the initial  theory (7), uttered(went(ail(boys),theatre)), one  would obtain one optimistic model schema in which  the CI have been cancelled  (see figure 5).", "acronym": "CI", "label": "conventional implicatures", "ID": "7016"}, {"sentence": "4 An alternative might be to retreat from the notion  of meaning postulates per se, and view them instead  as some form of CI which are  \"usually\" or \"often\" true.", "acronym": "CI", "label": "conventional implicatures", "ID": "7017"}, {"sentence": "2.3 Conversational Implicatures5 Authors can be held responsible for more than just assertions and CI.", "acronym": "CI", "label": "conventional implicatures", "ID": "7018"}, {"sentence": "Conventional impli- catures are a rich source of information for IE tasks because the material presented in them is supposed 4For more on CI, see e.g. Karttunen and Peters (1979) and Potts (2005) to be non-controversial.", "acronym": "CI", "label": "conventional implicatures", "ID": "7019"}, {"sentence": "Second, we use two graph-based summarization approaches, Generalized CWS and Page- Rank, to extract sentences as summaries.", "acronym": "CWS", "label": "ClueWordSummarizer", "ID": "7020"}, {"sentence": "We apply two summarization algorithms, Generalized CWS and Page-Rank to rank nodes in the sentence quotation graph and to select the corresponding most highly ranked sen- tences as the summary.", "acronym": "CWS", "label": "ClueWordSummarizer", "ID": "7021"}, {"sentence": "tagging approach has been widely used in CWS recently (Xue and Shen, 2003; Peng and McCal- lum, 2004; Tseng et al, 2005).", "acronym": "CWS", "label": "Chinese word segmentation", "ID": "7022"}, {"sentence": "The EM-like method for learning dependency  relations described in Section 3.3 has also been  applied to other tasks such as hidden Markov model  training (Rabiner, 1989), syntactic relation learning  (Yuret, 1998), and CWS  (Gao et al, 2002a).", "acronym": "CWS", "label": "Chinese word segmentation", "ID": "7023"}, {"sentence": "Section 5 describes current state- of-the-art methods for CWS.", "acronym": "CWS", "label": "Chinese word segmentation", "ID": "7024"}, {"sentence": "Integrated CWS in statistical machine translation.", "acronym": "CWS", "label": "Chinese word segmentation", "ID": "7025"}, {"sentence": "Word lattice reranking for CWS and part-of-speech tagging.", "acronym": "CWS", "label": "Chinese word segmentation", "ID": "7026"}, {"sentence": "Do we need CWS for statistical machine translation?", "acronym": "CWS", "label": "Chinese word segmentation", "ID": "7027"}, {"sentence": "INESC-ID / Instituto Superior T?enico, Lisboa, Portugal nlp2ct.samuel@gmail.com, {lidiasc, derekfw}@umac.mo, isabel.trancoso@inesc-id.pt,tianliang0123@gmail.com Abstract This study investigates on building a better CWS mod- el for statistical machine translation.", "acronym": "CWS", "label": "Chinese word segmentation", "ID": "7028"}, {"sentence": "= argmax c?C p(c|x) The maxent classifiers are implemented with the toolkit of Zhang Le (2004), and the parameters of the model are estimated using Generalized IIS (Darroch and Ratcli, 1972).", "acronym": "IIS", "label": "Iterative Scaling", "ID": "7029"}, {"sentence": "Generalised IIS (GIS) is used to esti- mate the values of the weights and we use a Gaus- sian prior over the weights (Chen and Rosenfeld, 1999) which allows many rare, but informative, features to be used without overfitting.", "acronym": "IIS", "label": "Iterative Scaling", "ID": "7030"}, {"sentence": "Generalized  IIS for Log-linear Models.", "acronym": "IIS", "label": "Iterative Scaling", "ID": "7031"}, {"sentence": "We have also experimented with Non-negative Matrix Factorization (NMF) (Lee and Seung 1999), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann 1999), Kernel Principal Com- ponents Analysis (KPCA) (Scholkopf, Smola, and Muller 1997), and IIS (IS) (Ando 2000).", "acronym": "IIS", "label": "Iterative Scaling", "ID": "7032"}, {"sentence": "The classifier.maxent module defines the maximum entropy model for text classification, and implements two algorithms for training the model: Generalized IIS and Improved IIS.", "acronym": "IIS", "label": "Iterative Scaling", "ID": "7033"}, {"sentence": "Maximum-likelihood parameter values can be estimated using Generalized IIS (Ratnaparkhi 96), or gradient descent methods.", "acronym": "IIS", "label": "Iterative Scaling", "ID": "7034"}, {"sentence": "The IIS algorithm: A gentle introduction.", "acronym": "IIS", "label": "improved iterative scaling", "ID": "7035"}, {"sentence": "for example, generalized or IIS (Berger, Della Pietra, and 4 It might seem to be a restriction to have the hyperplane passing through the origin of the space.", "acronym": "IIS", "label": "improved iterative scaling", "ID": "7036"}, {"sentence": "Once the set of features functions are selected, algorithm such as IIS (Berger et al, 1996) or sequential conditional generalized iterative scal- ing (Goodman, 2002) can be used to find the optimal parameter values of {?", "acronym": "IIS", "label": "improved iterative scaling", "ID": "7037"}, {"sentence": ", An are Lagrange multipliers  that impose the constraints corresponding to the  chosen features f l ,  ..-,fn- The term Z(x) normal-  izes the probabilities by summing over all possible  outcomes y. Berger et al (1996) demonstrate that  the optimal values for the Ai's can be obtained by  maximizing the likelihood of the training data with  respect o the model, which can be performed using  their IIS algorithm.", "acronym": "IIS", "label": "improved iterative scaling", "ID": "7038"}, {"sentence": "cessing, two aspects of  CRFs have been investigated sufficiently: one is to  apply it to new tasks, such as named entity recog- nition (McCallum and Li, 2003; Li and McCallum,  2003; Settles, 2004), part-of-speech tagging (Laf- ferty et al, 2001), shallow parsing (Sha and Perei- ra, 2003), and language modeling (Roark et al,  2004); the other is to exploit new training methods  for CRFs, such as IIS (Laf- ferty et al, 2001), L-BFGS (McCallum, 2003) and  gradient tree boosting (Dietterich et al, 2004).", "acronym": "IIS", "label": "improved iterative scaling", "ID": "7039"}, {"sentence": "The IIS technique  (Della Pietra et al, 1997) was used to train the  parameters in the ME model.", "acronym": "IIS", "label": "improved iterative scaling", "ID": "7040"}, {"sentence": "ACM CSUR, 34(1):1?47.", "acronym": "CSUR", "label": "computing surveys", "ID": "7041"}, {"sentence": "ACM CSUR, 24(4).", "acronym": "CSUR", "label": "computing surveys", "ID": "7042"}, {"sentence": "Neural CSUR, 3:157?195.", "acronym": "CSUR", "label": "computing surveys", "ID": "7043"}, {"sentence": "ACM CSUR, 24(4):377?439.", "acronym": "CSUR", "label": "Computing Surveys", "ID": "7044"}, {"sentence": "ACM  CSUR, 12, 213-253.", "acronym": "CSUR", "label": "Computing Surveys", "ID": "7045"}, {"sentence": "ACM CSUR, 41:10:1?10:69, February.", "acronym": "CSUR", "label": "Computing Surveys", "ID": "7046"}, {"sentence": "Kukich, Karen, 1992 Techniques for automatically cor- recting words in text, CSUR, 24:4, pp.", "acronym": "CSUR", "label": "Computing Surveys", "ID": "7047"}, {"sentence": "CSUR , Vol, 4 , no.", "acronym": "CSUR", "label": "Computing Surveys", "ID": "7048"}, {"sentence": "ACM CSUR, 41(2):1?69.", "acronym": "CSUR", "label": "Computing Surveys", "ID": "7049"}, {"sentence": "2.3 LB As a simple baseline, we also evaluated a method that labels words as shell if they appear frequently in persuasive writing?specifically, in the set of 100,000 unannotated essays described in ?", "acronym": "LB", "label": "Lexical Baseline", "ID": "7050"}, {"sentence": "2.1 Raw LB The raw lexical baseline is a simple system that only relies on polarity lexicons and takes the aver- age valence of all the words.", "acronym": "LB", "label": "Lexical Baseline", "ID": "7051"}, {"sentence": "Majority Label LB Table 2 Features Reduced Training Training Set CV 54.6 59.7 77.1  Unseen Answers 51.1 56.1 75.5  Unseen Questions 58.4 63.4 61.7 66.5 Unseen Modules 53.4 62.9 61.4 68.8 Table 3.", "acronym": "LB", "label": "Lexical Baseline", "ID": "7052"}, {"sentence": "This provides indirect empirical support for our ear- lier hypothesis that the med class can benefit from 1073 Original Nissim Baseline Baseline+LB+Both R P F R P F R P F R P F old 91.5 94.1 92.8 91.2 85.8 88.5 88.7 91.7 90.2 93.0 95.2 94.1 med 87.6 68.1 76.6 84.7 62.7 72.1 92.5 63.2 75.1 89.1 70.9 79.0 new 22.3 56.3 32.0 30.2 66.4 41.5 32.1 68.3 43.7 34.4 71.5 46.5 Accuracy 79.5 74.1 76.3 82.2 Table 3: Per-class performance of four information-status classifiers.", "acronym": "LB", "label": "Lexical Baseline", "ID": "7053"}, {"sentence": "The state space S of LB", "acronym": "LB", "label": "ListenerBot", "ID": "7054"}, {"sentence": "However, LB does not produce utterances.", "acronym": "LB", "label": "ListenerBot", "ID": "7055"}, {"sentence": "This allows LB to track its beliefs about the location of the card and to incor- porate linguistic advice.", "acronym": "LB", "label": "ListenerBot", "ID": "7056"}, {"sentence": "The state space S of LB consists of the location of the player p and the location of the card c. As discussed above in Section 3.3, we cl", "acronym": "LB", "label": "ListenerBot", "ID": "7057"}, {"sentence": "region r}| 4 LB We first introduce LB, an agent that does not take into account the actions or beliefs of its partner.", "acronym": "LB", "label": "ListenerBot", "ID": "7058"}, {"sentence": "LB decides what actions to take using a Partially Observable Markov Decision Pro- cess (POMDP).", "acronym": "LB", "label": "ListenerBot", "ID": "7059"}, {"sentence": "Us- ing this task and a model of the meaning of spatial language, we next discuss two agents that play the game: LB (Section 4) makes decisions us- ing a single-agent POMDP that does not take into account the beliefs or actions of its partner, whereas DialogBot (Section 5) maintains a model of its part- ner?s beliefs.", "acronym": "LB", "label": "ListenerBot", "ID": "7060"}, {"sentence": "Regarding the M L algorithms tested, the  contribution of this work consist of empiri-  cally demonstrating that the LBing al-  gorithm outperforms other three state-of-the-  art supervised ML methods for WSD.", "acronym": "LB", "label": "LazyBoost", "ID": "7061"}, {"sentence": "177  Naive Bayes  Exemplar Based  Snow  LBing  58  56 1  54  ~?52  o  50  44  4O  58  56  Af~  52  ~o  ~ 48  46   58  56 '  54  o 52  50  46  62  60 '  58   ~o  48  46   4.4  Test on B corpus  (la)  . . . . .", "acronym": "LB", "label": "LazyBoost", "ID": "7062"}, {"sentence": "Extensively evaluate LBing on the  WSD task.", "acronym": "LB", "label": "LazyBoost", "ID": "7063"}, {"sentence": "LBing (Escudero et al, 2000a), is a  simple modification of the AdaBoost.", "acronym": "LB", "label": "LazyBoost", "ID": "7064"}, {"sentence": "This observation  allows both to identify the noisy exam-  ples and use LBing as a way to  improve data quality.", "acronym": "LB", "label": "LazyBoost", "ID": "7065"}, {"sentence": "8.64?1.04  64.26?2.07  69.33?2.92  66.20?2.12  48.61?0.96  48.22?3 06  48.46?1.21  61.38?2.08  64.32?3.27  62.50?1.47  48.87?1 68  48.22?1.90  48.62?1.09  63.19?1.65  68.51?2.45  65.22?1.50  B-B A-B B-A  48.61?0.96 48.99 48.99  48.22?3.06 48.22 48.22  48.46?1.21 48.70 48.70  60.65?1.01 53.45 55.27  63.49?2.27 60.44 62.55  61.74?1.18 56.12 58.05  Table 3: Accuracy results (5= standard eviation) of LBing on the sense-balanced corpora  Furthermore, these results are in contradic-  tion with the idea of \"robust broad-coverage  WSD\" introduced by (Ng, 1997b), in which a  supervised system trained on a large enough  corpora (say a thousand examples per word)  ~hould provide accurate disambiguation on  any corpora (or, at least significantly better  than MFS).", "acronym": "LB", "label": "LazyBoost", "ID": "7066"}, {"sentence": "ng the M L algorithms tested, the  contribution of this work consist of empiri-  cally demonstrating that the LBing al-  gorithm outperforms other three state-of-the-  art supervised ML methods for WSD.", "acronym": "LB", "label": "LazyBoost", "ID": "7067"}, {"sentence": "For each disease, CE classifies drugs into one of six categories: beneficial, LB, trade- offs (i.e., may have adverse side effects), un- known, unLB, and harmful.", "acronym": "LB", "label": "likely beneficial", "ID": "7068"}, {"sentence": "In the first set of judgments, the asses- sor determined which of the six categories (ben- eficial, LB, tradeoffs, unknown, un- LB, harmful) the system answer be- longed to, based on the CE recommendations.", "acronym": "LB", "label": "likely beneficial", "ID": "7069"}, {"sentence": "AFP, English Service   ?", "acronym": "AFP", "label": "Agence France-Presse", "ID": "7070"}, {"sentence": "We formalize this as a supervised clas- 1 https://code.google.com/p/word2vec 2 LDC2012T21, AFP 2010 3 https://code.google.com/p/cistern sification task and apply SVMs (Chang and Lin, 2011).", "acronym": "AFP", "label": "Agence France-Presse", "ID": "7071"}, {"sentence": "The human summaries are not threaded; they are flat, roughly daily news summaries published by AFP and found in the Gigaword corpus, distinguished by their ?", "acronym": "AFP", "label": "Agence France-Presse", "ID": "7072"}, {"sentence": "4.3.1 Automatically-Parsed Corpus The text corpus we use consists of 125 mil- lion words from the L?Est Republicain newspa- per5, 125 million words of dispatches from the AFP, and 225 million words from a French Wikipedia backup dump6.", "acronym": "AFP", "label": "Agence France-Presse", "ID": "7073"}, {"sentence": "Good examples are the multilingual news feeds produced by news agencies such as AFP, Xinhua News, Reuters, CNN, BBC, etc.", "acronym": "AFP", "label": "Agence France Presse", "ID": "7074"}, {"sentence": "These datasets are 2004-2007 newswire feeds col- lected from different news agencies and news pa- pers, such as AFP, Xinhua, Al- Hayat, Al-Asharq Al-Awsat, Al-Quds Al-Arabi, An-Nahar, Al-Ahram and As-Sabah.", "acronym": "AFP", "label": "Agence France Presse", "ID": "7075"}, {"sentence": "The final  round of Spanish retrieval took place in TREC-5,  again with 25 new topics and also with additional  text (1994 newswire from AFP, in-  cluding 308 megabytes or 173,950 documents).", "acronym": "AFP", "label": "Agence France Presse", "ID": "7076"}, {"sentence": "Chinese  English  French  Japanese  Portuguese  Spanish  Xinhua  Wall Street Journal  Le Monde  Kyodo  Radiobras  AFP  China  USA  France  Japan  Brazil  France  Language  Chinese  English  French  Japanese  Portuguese  Spanish  NE TIM NUM ENA  4454 17.2 0 1.8 0 80.9 0  2242 10.7% 9.5% 79.8%  2321 18.6% 3.0% 78.4%  2146 26.4% 4.0% 69.6%  3839 17.7% 12.1% 70.3%  3579 24.6% 3.0% 72.5%  Table 1: Corpora sources.", "acronym": "AFP", "label": "Agence France Presse", "ID": "7077"}, {"sentence": "They have been published by the English and French editors of AFP, and report on the same event, an epidemic of cholera in Pyongyang.", "acronym": "AFP", "label": "Agence France Presse", "ID": "7078"}, {"sentence": "LDC2003T06, roughly 166K words of  written Modern Standard Arabic newswire from  the AFP corpus; and (2) Arabic  Treebank: Part 2 v 2.0, LDC Catalog No.", "acronym": "AFP", "label": "Agence France Presse", "ID": "7079"}, {"sentence": "442  Proceedings of the 7th Workshop on Syntax, Semantics and Structure in SSST, pages 19?28, Atlanta, Georgia, 13 June 2013.", "acronym": "SSST", "label": "Statistical Translation", "ID": "7080"}, {"sentence": "A Syntax- Based SSST Model.", "acronym": "SSST", "label": "Statistical Translation", "ID": "7081"}, {"sentence": "Information Retrieval as SSST.", "acronym": "SSST", "label": "Statistical Translation", "ID": "7082"}, {"sentence": "Headline Generation Based on SSST.", "acronym": "SSST", "label": "Statistical Translation", "ID": "7083"}, {"sentence": "of the Third Workshop on Syntax and Structure in SSST, pages 51?59.", "acronym": "SSST", "label": "Statistical Translation", "ID": "7084"}, {"sentence": "HMM-based  Word Alignment In SSST.", "acronym": "SSST", "label": "Statistical Translation", "ID": "7085"}, {"sentence": "In HLT-NAACL Workshop on SSST, pages 1-8.", "acronym": "SSST", "label": "Syntax and Structure in Statistical Translation", "ID": "7086"}, {"sentence": "In Proceedings of the 4th Workshop on  SSST,  pages 43?51.", "acronym": "SSST", "label": "Syntax and Structure in Statistical Translation", "ID": "7087"}, {"sentence": "In Proceedings of the ACL-HLT Second Workshop on SSST, pages 10?18.", "acronym": "SSST", "label": "Syntax and Structure in Statistical Translation", "ID": "7088"}, {"sentence": "In Proceedings of the NAACL-HLT 2007/AMTA Workshop on SSST, pages 96?102.", "acronym": "SSST", "label": "Syntax and Structure in Statistical Translation", "ID": "7089"}, {"sentence": "In Proceedings of the HLT-NAACL Workshop on SSST, pages 51-59.", "acronym": "SSST", "label": "Syntax and Structure in Statistical Translation", "ID": "7090"}, {"sentence": "In NAACL-HLT/AMTA Workshop on SSST, pages 103?", "acronym": "SSST", "label": "Syntax and Structure in Statistical Translation", "ID": "7091"}, {"sentence": "The learning algorithnl fol-  lows the essence of the MDL principle to search for  the optimal segmentation f an utterance that has  the maximal DLG (and there-  fore approaches the minimum description length  of the utterance).", "acronym": "DLG", "label": "description length gain", "ID": "7092"}, {"sentence": "Exper- iments show that DLG outperforms other measures because of its strength for identifying short words.", "acronym": "DLG", "label": "description length gain", "ID": "7093"}, {"sentence": "Unsupervised learning of word boundary with DLG.", "acronym": "DLG", "label": "description length gain", "ID": "7094"}, {"sentence": "Furthermore, DLG offers a unified statistical account of an MWE as a linguistically motivated structure that can com- press relevant corpus data.", "acronym": "DLG", "label": "description length gain", "ID": "7095"}, {"sentence": "Conc lus ions  and  Future  Work   We have presented an unsupervised learning algo-  rithm for lexical acquisition based on the goodness  measure DLG formulated follow-  ing information theory.", "acronym": "DLG", "label": "description length gain", "ID": "7096"}, {"sentence": "275   Unsupervised Learning of Word Boundary with  DLG  Chunyu K i t  t$  Dept.", "acronym": "DLG", "label": "Description Length Gain", "ID": "7097"}, {"sentence": "occupation; NP1 = n, NC t] Parenthesis6 (e1 pare) NP1 (NP2) [NP1 = t, NC n] Also-known-as7 (e1/2 aka) NP1, (also) known as NP2 [NP1 = t ?", "acronym": "NC", "label": "NP2 =", "ID": "7098"}, {"sentence": "us used for the TREC QA Track, available from the Linguistic Data Consortium Name Pattern Bindings Copular1 (e1 is) NP1 be NP2 [NP1 = t, NC n] Become2 (e1 beca) NP1 become NP2 [NP1 = t, NC n] Verb3 (e1 verb): NP1 v NP2 [where v ?", "acronym": "NC", "label": "NP2 =", "ID": "7099"}, {"sentence": "n] Also-called8 (e2 also) NP1, (also) called NP2 [NP1 = n, NC t] Or9 (e1 or) NP1, or NP2 [NP1 = t, NC n] Like10 (e2 like) NP1 (such as|like) NP2 [NP1 = n, NC t] Relative cl", "acronym": "NC", "label": "NP2 =", "ID": "7100"}, {"sentence": "n] Also-called8 (e2 also) NP1, (also) called NP2 [NP1 = n, NC t] Or9 (e1 or) NP1, or NP2 [NP1 = t, NC n] Like10 (e2 like) NP1 (such as|like) NP2 [NP1 = n, NC t] Relative clause11 (e1 wdt) NP (which|that) VP [NP = t, VP = n] 1In order to filter out spuriou", "acronym": "NC", "label": "NP2 =", "ID": "7101"}, {"sentence": "n] Also-called8 (e2 also) NP1, (also) called NP2 [NP1 = n, NC t] Or9 (e1 or) NP1, or NP2 [NP1 = t, NC n] Like10 (e2 lik", "acronym": "NC", "label": "NP2 =", "ID": "7102"}, {"sentence": "n, NC t ?", "acronym": "NC", "label": "NP2 =", "ID": "7103"}, {"sentence": "nguistic Data Consortium Name Pattern Bindings Copular1 (e1 is) NP1 be NP2 [NP1 = t, NC n] Become2 (e1 beca) NP1 become NP2 [NP1 = t, NC n] Verb3 (e1 verb): NP1 v NP2 [where v ?", "acronym": "NC", "label": "NP2 =", "ID": "7104"}, {"sentence": "However, there were several instances where the target term was not correctly extracted from 1official corpus used for the TREC QA Track, available from the Linguistic Data Consortium Name Pattern Bindings Copular1 (e1 is) NP1 be NP2 [NP1 = t, NC n] Become2 (e1 beca) NP1 become NP2 [NP1 = t, NC n] Verb3 (e1 verb): NP1 v NP2 [where v ?", "acronym": "NC", "label": "NP2 =", "ID": "7105"}, {"sentence": "n] Also-called8 (e2 also) NP1, (also) called NP2 [NP1 = n, NC t] Or9 (e1 or) NP1, or NP2 [NP1 = t, NC n] Like10 (e2 like) NP1 (such as|like) NP2 [NP1 = n, NC t] Relative clause11 (e1 wdt) NP (which|that) VP [NP = t, VP = n] 1In order to filter out spurious nuggets (e.g., progressive tense), our system disc", "acronym": "NC", "label": "NP2 =", "ID": "7106"}, {"sentence": "ral instances where the target term was not correctly extracted from 1official corpus used for the TREC QA Track, available from the Linguistic Data Consortium Name Pattern Bindings Copular1 (e1 is) NP1 be NP2 [NP1 = t, NC n] Become2 (e1 beca) NP1 become NP2 [NP1 = t, NC n] Verb3 (e1 verb): NP1 v NP2 [where v ?", "acronym": "NC", "label": "NP2 =", "ID": "7107"}, {"sentence": "biography-verb; NP1 = t, NC n] Appositive4 (e1/2 appo) NP1, NP2 [NP1 = t ?", "acronym": "NC", "label": "NP2 =", "ID": "7108"}, {"sentence": "Lexical atoms may be found among proper  names, idioms, and many noun-NCs.", "acronym": "NC", "label": "noun compound", "ID": "7109"}, {"sentence": "roll) and NCs (dry ice).", "acronym": "NC", "label": "noun compound", "ID": "7110"}, {"sentence": "ng measure 0.16 student loan entity 0.16 theater orchestra entity 0.17 sunday restrictions abstraction 0.20 yesterday afternoon measure 0.20 relations agency abstraction 0.21 crime novelist entity 0.21 office buildings structure 0.21 Table 5: Best and worst scoring NCs with their Least Common Subsumer and Spear- man ?", "acronym": "NC", "label": "noun compound", "ID": "7111"}, {"sentence": "Our approach to this task is to de- velop a machine learning classifier which determines for each verb pair describing a NC which verb should be ranked higher.", "acronym": "NC", "label": "noun compound", "ID": "7112"}, {"sentence": "correlation good results on the task of ranking verbs para- phrasing NCs.", "acronym": "NC", "label": "noun compound", "ID": "7113"}, {"sentence": "Split- ting NCs via monolingual and bilingual paraphrasing: A study on Japanese Katakana words.", "acronym": "NC", "label": "noun compound", "ID": "7114"}, {"sentence": "Corpus statistics meet the NC: Some empirical results.", "acronym": "NC", "label": "noun compound", "ID": "7115"}, {"sentence": "In Proceedings of the IEEE Work- shop on SLT, pages 79?84.", "acronym": "SLT", "label": "Spoken Language Technology", "ID": "7116"}, {"sentence": "Proceedings of the IEEE/ACL 2006 Workshop on SLT, 134?137.", "acronym": "SLT", "label": "Spoken Language Technology", "ID": "7117"}, {"sentence": "Proceedings of the IEEE/ACL 2006 Workshop on SLT.", "acronym": "SLT", "label": "Spoken Language Technology", "ID": "7118"}, {"sentence": "In Proceedings of the IEEE / ACL 2006 Workshop on SLT.", "acronym": "SLT", "label": "Spoken Language Technology", "ID": "7119"}, {"sentence": "of the 2008 SLT Workshop, Goa.", "acronym": "SLT", "label": "Spoken Language Technology", "ID": "7120"}, {"sentence": "SLT re-  quires (1) an accurate translation and (2) a real-  time response.", "acronym": "SLT", "label": "Spoken language translation", "ID": "7121"}, {"sentence": "1 Introduction SLT technologies attempt to bridge the language barriers between people with different native languages who each want to engage in conversation by using their mother- tongue.", "acronym": "SLT", "label": "Spoken language translation", "ID": "7122"}, {"sentence": "2 Dialect Translation SLT technologies attempt to bridge the language barriers between people with different native languages who each want to engage in conversation by using their mother-tongue.", "acronym": "SLT", "label": "Spoken language translation", "ID": "7123"}, {"sentence": "For instance, the Latin-derived Noun  ADJPe \"Bacillus subtilis\" has a  structure inverse to the canonical English noun  phrase (Adjective Noun).", "acronym": "ADJP", "label": "Adjective phras", "ID": "7124"}, {"sentence": "The phrase recognition rules are to be applied  in the following order:  (VPH) Verb phrases  (APH)  Adverb  phrases  (JPH) ADJPes  (NPH)  Noun phrases   (PPH) Prepositional phrases  29--  The typical features of this system are: taking  tone units as the basis of grammat ica l  analysis,  choosing a general-purpose dictionary for word   class tagging, mak ing  extensive use of phrase  structure rules which are applied in a certain  order and cyclically, and partly adopting an  interactive mode of analysis.", "acronym": "ADJP", "label": "Adjective phras", "ID": "7125"}, {"sentence": "4.1 AP  Segmentat ion   ADJPes are marked by a replacement  transducer which inserts the \\[AP and AP\\]  bound-  aries around any word sequence that matches the  regular expression (RE):  \\[ (ADVP) ADJ ( COMMA \\[ (ADVP) ADJ  COMMA \\]+ ) ( COORD (ADVP) ADJ ) \\]  ADVP stands for adverb phrase and is defined as:  \\[ ADV+ \\[\\[COORD\\[COMMA\\] DV?\\]* \\]  4.2 NP  Segmentat ion   Unlike APs, NPs are marked in two steps", "acronym": "ADJP", "label": "Adjective phras", "ID": "7126"}, {"sentence": "k et al1985) and (Semmelmeyer and Bolander 1992)): (1) Compound Nominals consisting of two consecutive nouns (eg night club - a TEMPORAL relation - indicat- ing that club functions at night), (2) Adjective Noun con- structions where the adjectival modifier is derived from a noun (eg musical clock - a MAKE/PRODUCE relation), (3) Genitives (eg the door of the car - a PART-WHOLE rela- tion), and (4) ADJPes (cf. (", "acronym": "ADJP", "label": "Adjective phras", "ID": "7127"}, {"sentence": "ADJPes indicating measurement (as in \"a 10 ft pole\" or \"a pole 10  f t  long\") are converted to modifiers where the measured quantity is made explicit, e.g.,  (LENGTH 10 FT).", "acronym": "ADJP", "label": "Adjective phras", "ID": "7128"}, {"sentence": "By  and  large, English phrase structure typically has  the head to the right, as in  Verb phrases: will be DOING  Noun phrases: the nice little DOG  ADJPes: stunningly BEAUTIFUL   Assuming  that a good number  of the tone units  consist of, at least, g rammat ica l  phrases, the  nucleus will occur within the phrase and, more   often than not, within the head of the phrase.", "acronym": "ADJP", "label": "Adjective phras", "ID": "7129"}, {"sentence": "SO(a)= -SO(a) Where C(a) denotes the category of DSAAs; C(n) denotes the sentiment expectation of nouns; SO(a) is the SO of DSAAs in a give noun- ADJP.", "acronym": "ADJP", "label": "adjective phrase", "ID": "7130"}, {"sentence": "|price is low Table 1: The SO of DSAAs in noun-ADJPs In previous research, the SO of nouns is classified into three categories: positive, negative and neutral.", "acronym": "ADJP", "label": "adjective phrase", "ID": "7131"}, {"sentence": "Inflection We introduced tag suffixes for inflec- tion as clues to identify the attachment position of the verb and ADJPs, because Japanese verbs and adjectives have inflections, which depends 110 (no label) base form cont continuative form attr attributive form neg negative form hyp hypothetical form imp imperative form stem stem Table 2: Inflection tag suffixes on their modifying words and phrases (e.g. noun and verb phrases).", "acronym": "ADJP", "label": "adjective phrase", "ID": "7132"}, {"sentence": "qing|light} 3.2 Sentiment Expectation of Noun The SO of most DSAAs can be determined by target nouns in noun-ADJPs, as shown in Table 1.", "acronym": "ADJP", "label": "adjective phrase", "ID": "7133"}, {"sentence": "(a) The phrase which is an ADJP and  modifies \"each\", appositive  to the preceding \"statements\",  (b) The phrase which is a past participle phrase  and modifies \"names\".", "acronym": "ADJP", "label": "adjective phrase", "ID": "7134"}, {"sentence": "For example, a noun phrase (the string) such as [adjective+noun] can be represented as NP(AP,noun) where AP refers to a chart of general ADJPs, possibly containing adverbs as in ?", "acronym": "ADJP", "label": "adjective phrase", "ID": "7135"}, {"sentence": "Wilensky, R., Mayfield, J., Chin, D., Luria, M., Martin,  J. and Wu, D. The Berkeley UC Project.", "acronym": "UC", "label": "UNIX Consultant", "ID": "7136"}, {"sentence": "Wilensky, R., Mayfield, L, Chin, D., Lm'ia, M., Martin,  L and Wu, D. The Berkeley UC Project.", "acronym": "UC", "label": "UNIX Consultant", "ID": "7137"}, {"sentence": "Higher-level plan-  ning in the UC, for modularity, is  performed by a separable planning component.", "acronym": "UC", "label": "UNIX Consultant", "ID": "7138"}, {"sentence": "WILENSKY ,R. (1984): Talking To UNIX In English:  An Overview Of An Online UC.", "acronym": "UC", "label": "UNIX Consultant", "ID": "7139"}, {"sentence": "Experience with the UC has  suggested that the interaction of specialized and general  linguistic knowledge is important for a natural anguage  interface.", "acronym": "UC", "label": "UNIX Consultant", "ID": "7140"}, {"sentence": "0362-613X/88/010035-84503.00  Computational Linguistics, Volume 14, Number 4, December 1988 35  Robert Wilensky, David N. Chin, Marc Luria, ,lames Martin, James Mayfield, and Dekai Wu The Berkeley UC Project  progress on fundamental issues that comprise the cen-  tral goals of AI researchers.", "acronym": "UC", "label": "UNIX Consultant", "ID": "7141"}, {"sentence": "In  Donald A. Norman and Stephen W. Draper, edi-  tors, UC System Design: new Perspec-  tives on Human-Computer Interaction, chapter 5,  pages 87-124.", "acronym": "UC", "label": "User Centered", "ID": "7142"}, {"sentence": "The involvement ofusers since  the very beginning of the system design (i.e. the  adoption of a UC approach) can greatly  enhance the effectiveness of a LEADS: user needs  can pervade the design and implementation f all  the basic functionalities of the tool.", "acronym": "UC", "label": "User Centered", "ID": "7143"}, {"sentence": "UC System Design: new Perspectives  on Human-Computer Interaction.", "acronym": "UC", "label": "User Centered", "ID": "7144"}, {"sentence": "In: Norman, D.A,, and Drap-  er, S.W. (eds): UC System Design: New Per-  spectives on Human-Computer Interaction.", "acronym": "UC", "label": "User Centered", "ID": "7145"}, {"sentence": "UC System Design; New Perspectives on Human-Computer Interaction.", "acronym": "UC", "label": "User Centered", "ID": "7146"}, {"sentence": "PPf ) 1 2 (3) 656 Table 1: Size of parallel corpora English Chinese English Chinese In-domain parallel corpus 40 K 40 K 320 K 301 K BTEC Out-of-domain parallel corpus 2.5 M 2.5 M 62 M 54 M LDC corpus (LDC 2002T01, LDC2003T17, LDC2004T07, LDC2004T08, LDC2005T06 and LDC2005T10) # of sentences # of words Explanation 4.", "acronym": "BTEC", "label": "Basic Travel Expressions Corpus", "ID": "7147"}, {"sentence": "The IWSLT09 data set is comprised of short sentences (with an average of 9.5 words per sentence) from a particular domain (the C-STAR project?s BTECs).", "acronym": "BTEC", "label": "Basic Travel Expression Corpu", "ID": "7148"}, {"sentence": "554 Training Test Japanese Korean Japanese Korean # of sentences 162,320 10,150 # of total morphemes 1,153,954 1,179,753 74,366 76,540 # of bunsetsu/eojeol 448,438 587,503 28,882 38,386 vocabulary size 15,682 15,726 5,144 4,594 Table 2: Statistics of BTECs PER mWER BLEU NIST WBIBM 0.3415 / 0.3318 0.3668 / 0.3591 0.5747 / 0.5837 6.9075 / 7.1110 WBLMC 0.2667 / 0.2666 0.2998 / 0.2994 0.5681 / 0.5690 9.0149 / 9.0360 CBIBM 0.2677 / 0.2383 0.2992 / 0.2700 0.6347 / 0.6741 8.0900 / 8.6981 CBLMC 0.1954 / 0.1896 0.2176 / 0.2129 0.7060 / 0.7166 9.9167 / 10.027 Table 3: Evaluation Results of Translation Systems: without BiVN/with", "acronym": "BTEC", "label": "Basic Travel Expression Corpu", "ID": "7149"}, {"sentence": "During the decoding process, when a pair of chunks appeared in the first stage, the score is boosted by using this formula in the log domain, log Ptm(J|E) + log Plm(E) Table 1: BTECs Japanese English # of sentences 171,894 # of words 1,181,188 1,009,065 vocabulary size 20472 16232 # of singletons 82,06 5,854 3-gram perplexity 23.7 35.8 + weight ?", "acronym": "BTEC", "label": "Basic Travel Expression Corpu", "ID": "7150"}, {"sentence": "Statistics of BTECs Chinese English Japanese Korean # of sentences 167,163 # of words(morph) 1,006,838 1,128,151 1,226,774 1,313,407 Vocabulary size(S) 17,472 11,737 19,485 17,600 Vocabulary size(B) 17,472 9172 15,939 15,410 Vocabulary size(SB) 17,472 13,385 20,197 18,259 Vocabulary size(SP) 18,505 13,467 20,118 20,249 Vocabulary size(SBP(L)) 18,505 14,408 20,444 20,369(26,668) # of sin", "acronym": "BTEC", "label": "Basic Travel Expression Corpu", "ID": "7151"}, {"sentence": "Three types of attribution are possible in our sys- tem: 1) ESA mention of speakers, e.g., ?", "acronym": "ESA", "label": "explicit", "ID": "7152"}, {"sentence": "In these chains of utterances, the speaker is not ESAly mentioned because the author relies on the shared understanding with the reader that adja- cent pieces of quoted speech are not independent (Zhang et al.,", "acronym": "ESA", "label": "explicit", "ID": "7153"}, {"sentence": "Several syntac- tic patterns were applied to associate quotes with ESA mention of speakers in their vicinity to characters from the pruned list of story charac- ters.", "acronym": "ESA", "label": "explicit", "ID": "7154"}, {"sentence": "Hatzivassiloglou and McKeown, 1997)  for another application in which no ESA  indicators are available in the stream).", "acronym": "ESA", "label": "explicit", "ID": "7155"}, {"sentence": "They are not ESA  about conlparisc.n of lists of marks of tlncqual englh except m the binary  casK, hi that case, lheir delinitiolls \\]lave the Sllllle consequences ;i those  described here.", "acronym": "ESA", "label": "explicit", "ID": "7156"}, {"sentence": "In the first type of attri- bution, the speaker is ESAly mentioned in the vicinity of the quote.", "acronym": "ESA", "label": "explicit", "ID": "7157"}, {"sentence": "S-Space Package Even though no designated text similarity library, the S-Space Package (Jur- gens and Stevens, 2010)8 contains some text sim- ilarity measures such as Latent Semantic Analysis (LSA) and ESA (see Sec- tion 3.2).", "acronym": "ESA", "label": "Explicit Semantic Analysis", "ID": "7158"}, {"sentence": "These measures include simple distances like Levenshtein edit distance, cosine, Named En- tities overlap and more complex distances like ESA, WordNet-based similarity, IR-based similarity, and a similar- ity measure based on syntactic dependencies.", "acronym": "ESA", "label": "Explicit Semantic Analysis", "ID": "7159"}, {"sentence": "ESA (Gabrilovich and Markovitch, 2007) constructs the vector space on corpora where the documents are assumed to de- scribe natural concepts such as cat or dog.", "acronym": "ESA", "label": "Explicit Semantic Analysis", "ID": "7160"}, {"sentence": "One of the most successful systems in *SEM 2012 STS, (Ba?r et al 2012), managed to grade pairs of sentences accurately by combining focused mea- sures, either simple ones based on surface features (ie n-grams), more elaborate ones based on lexical semantics, or measures requiring external corpora such as ESA, into a robust measure by using a log-linear regression model.", "acronym": "ESA", "label": "Explicit Semantic Analysis", "ID": "7161"}, {"sentence": "Com- puting Semantic Relatedness using Wikipedia-based ESA.", "acronym": "ESA", "label": "Explicit Semantic Analysis", "ID": "7162"}, {"sentence": "Computing Semantic Relatedness using Wikipedia- based ESA.", "acronym": "ESA", "label": "Explicit Semantic Analysis", "ID": "7163"}, {"sentence": "MEAD is significantly different from previous work  on multi-document summarization \\[Radev &  McKeown, 1998; Carbonell and Goldstein, 1998;  Mani and Bloedorn, 1999; MeKeown et aI., 1999\\],  21  which use techniques such as graph matching,  MMR, or language generation.", "acronym": "MMR", "label": "maximal marginal relevance", "ID": "7164"}, {"sentence": "773 2.1 Meeting Summarization Among early work on meeting summarization, Waibel et al (1998) implemented a modified version of the MMR algorithm (Car- bonell and Goldstein, 1998) applied to speech tran- scripts, presenting the user with the n best sentences in a meeting browser interface.", "acronym": "MMR", "label": "Maximal Marginal Relevance", "ID": "7165"}, {"sentence": "A future improvement will be to use a reorder- ing approach like MMR A r t i c l e G o l d M e a d ?", "acronym": "MMR", "label": "Maximal Marginal Relevance", "ID": "7166"}, {"sentence": "Sentence extractive methods comprehend, es- sentially, methods like LSA (Gong and Liu, 2001), MMR (Carbonell and Goldstein, 1998), and feature-based meth- ods (Edmundson, 1969).", "acronym": "MMR", "label": "Maximal Marginal Relevance", "ID": "7167"}, {"sentence": "c?2010 Association for Computational Linguistics Putting the User in the Loop: Interactive MMR for Query-Focused Summarization Jimmy Lin, Nitin Madnani, and Bonnie J. Dorr University of Maryland College Park, MD 20742, USA jimmylin@umd.edu, {nmadnani,bonnie}@umiacs.umd.edu Abstract This work represents an initial attempt to move beyond ?", "acronym": "MMR", "label": "Maximal Marginal Relevance", "ID": "7168"}, {"sentence": "Our experiments with the NTCIR ACLIA ques- tion answering test collections show that our method achieves a pyramid F3-score of up to 0.313, a 36% improvement over a baseline us- ing MMR.", "acronym": "MMR", "label": "Maximal Marginal Relevance", "ID": "7169"}, {"sentence": "In 2004, Conroy (Conroy, 2004) tested MMR (Goldstein et al.,", "acronym": "MMR", "label": "Maximal Marginal Relevance", "ID": "7170"}, {"sentence": "Initial trends in enrolment and completion of MOOCs.", "acronym": "MOOCs", "label": "massive open online courses", "ID": "7171"}, {"sentence": "Linguistic reflections of student engagement in MOOCs.", "acronym": "MOOCs", "label": "massive open online courses", "ID": "7172"}, {"sentence": "turn on, tune in, drop out: Anticipating student dropouts in MOOCs.", "acronym": "MOOCs", "label": "massive open online courses", "ID": "7173"}, {"sentence": "Together we stand, Together we fall, Together we win: Dynamic team formation in MOOCs?", "acronym": "MOOCs", "label": "massive open online courses", "ID": "7174"}, {"sentence": "Predicting student reten- tion in MOOCs using hidden markov models.", "acronym": "MOOCs", "label": "massive open online courses", "ID": "7175"}, {"sentence": "In Proceedings of the 1st Workshop on MOOCs at the 16th Annual Conference on Artificial Intelligence in Education, Memphis, TN.", "acronym": "MOOCs", "label": "Massive Open Online Courses", "ID": "7176"}, {"sentence": "Turn on, Tune in, Drop out: Anticipating  student dropouts in MOOCs,  in NIPS Data-Driven Education Workshop.", "acronym": "MOOCs", "label": "Massive Open Online Courses", "ID": "7177"}, {"sentence": "Turn on, Tune in, Drop out: Anticipating student dropouts in MOOCs?", "acronym": "MOOCs", "label": "Massive Open Online Courses", "ID": "7178"}, {"sentence": "Linguistic Reflections of Student Engagement in  MOOCs.", "acronym": "MOOCs", "label": "Massive Open Online Courses", "ID": "7179"}, {"sentence": "Stu- dents are taking MOOCs as well as online tutorials and paid online courses.", "acronym": "MOOCs", "label": "Massive Open Online Courses", "ID": "7180"}, {"sentence": "TT The port scanner is a utility to scan a system to get the status of the TCP.", "acronym": "TT", "label": "THEME-TOOL", "ID": "7181"}, {"sentence": "This can be explained as the effect of their specifications; the three best-ranked relations are well-defined by human standards, while the TT relation is more ambiguous.", "acronym": "TT", "label": "THEME-TOOL", "ID": "7182"}, {"sentence": "From this table it can be observed that the PRODUCT-PRODUCER, INSTRUMENT-AGENCY, and CAUSE-EFFECT rela- tions were detected with a relatively very high per- formance score, whereas the TT relation classification yielded a relatively small score.", "acronym": "TT", "label": "THEME-TOOL", "ID": "7183"}, {"sentence": "In particular, the models pre- sented here perform relatively badly on the ORIGIN-ENTITY and TT relations, while scoring better than all SemEval entrants on INSTRUMENT-AGENCY and PRODUCT- PRODUCER.", "acronym": "TT", "label": "THEME-TOOL", "ID": "7184"}, {"sentence": "R1 R2 R3 R4 R5 R6 R7 before 682 1200 913 898 861 849 677 after 13 19 10 15 15 8 16 Table 4: The number of features before and af- ter Weka selection, for each semantic relation dataset: R1 CAUSE-EFFECT, R2 INSTRUMENT- AGENCY, R3 PRODUCT-PRODUCER, R4 ORIGIN- ENTITY, R5 TT, R6 PART-WHOLE, and R7 CONTENT-CONTAINER.", "acronym": "TT", "label": "THEME-TOOL", "ID": "7185"}, {"sentence": "In this paper, we  approach the temporal correspondence prob- lem in which, given an input term (e.g., iPod)  and the TT (e.g. 1980s), the task is to  find the counterpart of the query that existed  in the TT.", "acronym": "TT", "label": "target time", "ID": "7186"}, {"sentence": "Figure 3 shows a sample of the interaction be- tween reference times and TTs.", "acronym": "TT", "label": "target time", "ID": "7187"}, {"sentence": "Here the second and the  fourth patches make corresponding temporal  expressions be treated as non-TTs that  need not be processed.", "acronym": "TT", "label": "target time", "ID": "7188"}, {"sentence": "In particular, for an input pair of a  term (e.g., iPod) and the TT (e.g. 1980s),  we find the corresponding term that existed in the  TT (walkman).", "acronym": "TT", "label": "target time", "ID": "7189"}, {"sentence": "Interaction between reference times  and TTs  In Figure 3, we notice that different classes of  time dynamically and automatically choose ref- erences based on their respective classes rather  than do it using the fixed value or the inconside- rate rule under the static mechanism.", "acronym": "TT", "label": "target time", "ID": "7190"}, {"sentence": "4 Extended-LHS TTs (xR) Section 1 informally described the root-to-frontier trans- ducer class R. We saw that R allows, by use of states, finite lookahead and arbitrary rearrangement of non- sibling input subtrees removed by a finite distance.", "acronym": "TT", "label": "Tree Transducer", "ID": "7191"}, {"sentence": "c?2010 Association for Computational Linguistics A TT Model for Synchronous Tree-Adjoining Grammars Andreas Maletti Universitat Rovira i Virgili Avinguda de Catalunya 25, 43002 Tarragona, Spain.", "acronym": "TT", "label": "Tree Transducer", "ID": "7192"}, {"sentence": "6  Training TTs Jonathan Graehl Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 graehl@isi.edu Kevin Knight Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 knight@isi.edu Abstract Many probabilistic models for natural language are now written in terms of hierarchical tree stru", "acronym": "TT", "label": "Tree Transducer", "ID": "7193"}, {"sentence": "c?2010 Association for Computational Linguistics Parsing and Translation Algorithms Based on Weighted Extended TTs Andreas Maletti?", "acronym": "TT", "label": "Tree Transducer", "ID": "7194"}, {"sentence": "An Overview of Probabilistic  TTs for Natural Language Processing.", "acronym": "TT", "label": "Tree Transducer", "ID": "7195"}, {"sentence": "c?2010 Association for Computational Linguistics Efficient Inference Through Cascades of Weighted TTs Jonathan May and Kevin Knight Information Sciences Institute University of Southern California Marina del Rey, CA 90292 {jonmay,knight}@isi.edu Heiko Vogler Technische Universita?t Dresden Institut fu?r Theoretische Informatik 01062 Dresden, Germany heiko.vogler@tu-dresden.de Abstract Weighted tree transducers have been pro- posed as useful formal models for rep- resenting syntact", "acronym": "TT", "label": "Tree Transducer", "ID": "7196"}, {"sentence": "TT: A quantitative ap-  proach to discourse segmentation.", "acronym": "TT", "label": "TextTiling", "ID": "7197"}, {"sentence": "54% 45% 52% 53%  H94(~,a/ 0.67s 0.52s 0.66s 0.88s  H94(c,~) 0.68s 0.52s 0.67s 0.92s  H94(j,~) 3.77s 2.21s 3.69s 5.07s  Table 3: The error rate and speed performance of  TT.", "acronym": "TT", "label": "TextTiling", "ID": "7198"}, {"sentence": "We implemented a variant of Hearst?s [1997] TT algorithm.)", "acronym": "TT", "label": "TextTiling", "ID": "7199"}, {"sentence": "4.3 Experiment 2 - TextTil ing  We compare three versions of the TT algo-  rithm (Hearst, 1994).", "acronym": "TT", "label": "TextTiling", "ID": "7200"}, {"sentence": "TT: Segmenting text into multi-paragraph subtopic passages.", "acronym": "TT", "label": "TextTiling", "ID": "7201"}, {"sentence": "TT: segmenting text into multi-paragraph subtopic passages.", "acronym": "TT", "label": "TextTiling", "ID": "7202"}, {"sentence": "It illustrates that the event TT ?", "acronym": "TT", "label": "trigger term", "ID": "7203"}, {"sentence": "Note that all event TTs are placeholders for alternatives (see text): ?", "acronym": "TT", "label": "trigger term", "ID": "7204"}, {"sentence": "Events are extracted using a newly developed query lan- guage with traverses the BioLG linkages be- tween TTs, arguments, and events.", "acronym": "TT", "label": "trigger term", "ID": "7205"}, {"sentence": "Essentially, we automatically extract all shortest link paths that connect event TTs to themes, themes to sites, themes to locations, and so on.", "acronym": "TT", "label": "trigger term", "ID": "7206"}, {"sentence": "All in all, we extracted 1845 different link paths from the training data (2197 from training plus devel- opment) that connect two constituents each (event TT to protein, or protein to site, for in- stance), corresponding to as many PTQL queries.", "acronym": "TT", "label": "trigger term", "ID": "7207"}, {"sentence": "Per link path type, the increase rate ranged from only 9% (localization: theme to at- loc) over 11-15% for basic events (gene-expression or transcription TT to theme) to almost 27% (regulation: theme to site).", "acronym": "TT", "label": "trigger term", "ID": "7208"}, {"sentence": "A Prototype  System that uses a Mobile Phone to Sup- port Personal NARRe for Children  with Complex Communication   Rolf Black1, Annalu Waller1, Ehud Reiter2, Nava  Tintarev2, Joseph Reddington2   (1University of Dundee, 2University of Aberdeen)  We will show a sensor based mobile phone proto- type that supports personal narrative for children  with complex communication needs.", "acronym": "NARR", "label": "Narrativ", "ID": "7209"}, {"sentence": "NARRe order in the  generation of indirect replies is an area we are cur-  rently investigating also; for related research, see  section 3.)", "acronym": "NARR", "label": "Narrativ", "ID": "7210"}, {"sentence": "NARRe/story  structure, pausing and American Sign Language.", "acronym": "NARR", "label": "Narrativ", "ID": "7211"}, {"sentence": "Topics were  structured using the standard TREC format of Title,  Description and NARRe fields.", "acronym": "NARR", "label": "Narrativ", "ID": "7212"}, {"sentence": "We obtained the following values:  c=0.75 for queries using the Title only, c=1 for queries  using the Title and Description fields, and c=1 for queries  using the Title, Description, and NARRe fields.", "acronym": "NARR", "label": "Narrativ", "ID": "7213"}, {"sentence": "Dyer, M. 1983 In-Depth Understanding: A Computer Model of  NARRe Comprehension, MIT Press, Cambridge, MA.", "acronym": "NARR", "label": "Narrativ", "ID": "7214"}, {"sentence": "A Prototype  System that uses a Mobile Phone to Sup- port Personal NARR for Children  with Complex Communication   Rolf Black1, Annalu Waller1, Ehud Reiter2, Nava  Tintarev2, Joseph Reddington2   (1University of Dundee, 2University of Aberdeen)  We will show a sensor based mobile phone proto- type that supports personal narrative for children  with complex communication needs.", "acronym": "NARR", "label": "Narrative", "ID": "7215"}, {"sentence": "NARR order in the  generation of indirect replies is an area we are cur-  rently investigating also; for related research, see  section 3.)", "acronym": "NARR", "label": "Narrative", "ID": "7216"}, {"sentence": "NARR/story  structure, pausing and American Sign Language.", "acronym": "NARR", "label": "Narrative", "ID": "7217"}, {"sentence": "Topics were  structured using the standard TREC format of Title,  Description and NARR fields.", "acronym": "NARR", "label": "Narrative", "ID": "7218"}, {"sentence": "We obtained the following values:  c=0.75 for queries using the Title only, c=1 for queries  using the Title and Description fields, and c=1 for queries  using the Title, Description, and NARR fields.", "acronym": "NARR", "label": "Narrative", "ID": "7219"}, {"sentence": "Dyer, M. 1983 In-Depth Understanding: A Computer Model of  NARR Comprehension, MIT Press, Cambridge, MA.", "acronym": "NARR", "label": "Narrative", "ID": "7220"}, {"sentence": "Acknowledgments This work has been funded by the DFG within the CRC 673 and the CITEC Excellence Center.", "acronym": "CITEC", "label": "Cognitive Interaction Technology", "ID": "7221"}, {"sentence": "Semantic Computing Group CITEC ?", "acronym": "CITEC", "label": "Cognitive Interaction Technology", "ID": "7222"}, {"sentence": "c?2013 Association for Computational Linguistics Bidirectional Inter-dependencies of Subjective Expressions and Targets and their Value for a Joint Model Roman Klinger and Philipp Cimiano Semantic Computing Group CITEC ?", "acronym": "CITEC", "label": "Cognitive Interaction Technology", "ID": "7223"}, {"sentence": "c?2014 Association for Computational Linguistics An Impact Analysis of Features in a Classification Approach to Irony Detection in Product Reviews Konstantin Buschmeier, Philipp Cimiano and Roman Klinger Semantic Computing Group CITEC ?", "acronym": "CITEC", "label": "Cognitive Interaction Technology", "ID": "7224"}, {"sentence": "Acknowledgments This research was supported by the Human Language Technology CITEC, by the DARPA GALE program under Contract No.", "acronym": "CITEC", "label": "Center of Excellence", "ID": "7225"}, {"sentence": "c?2009 ACL and AFNLP Using Word-Sense Disambiguation Methods to Classify Web Queries by Intent Emily Pitler Computer and Information Science University of Pennsylvania Philadelphia, PA 19104, USA epitler@seas.upenn.edu Ken Church Johns Hopkins University Human Language Technology CITEC Baltimore, MD 21211 Kenneth.Church@jhu.edu Abstract Three methods are proposed to classify queries by intent (CQI), e.g., navigational, informational, commercial, etc.", "acronym": "CITEC", "label": "Center of Excellence", "ID": "7226"}, {"sentence": "Acknowledgments This research was supported by the Human Lan- guage Technology CITEC, by gifts from Google and Microsoft, and by the DARPA GALE program under Contract No.", "acronym": "CITEC", "label": "Center of Excellence", "ID": "7227"}, {"sentence": ", Benjamin Van Durme Human Language Technology CITEC Johns Hopkins University, Baltimore, MD USA ?", "acronym": "CITEC", "label": "Center of Excellence", "ID": "7228"}, {"sentence": "Human Language Technology CITEC Johns Hopkins University Baltimore, MD 21211 USA bloodgood@jhu.edu K. Vijay-Shanker Computer and Information Sciences Department University of Delaware Newark, DE 19716 USA vijay@cis.udel.edu Abstract Actively sampled data can have very different characteristics than passively sampled data.", "acronym": "CITEC", "label": "Center of Excellence", "ID": "7229"}, {"sentence": "European CITEC, CZ.1.05/1.1.00/02.0090.", "acronym": "CITEC", "label": "Center of Excellence", "ID": "7230"}, {"sentence": "Table 1: Visual DG defines eight re- lations between pairs of annotated regions.", "acronym": "DG", "label": "Dependency Grammar", "ID": "7231"}, {"sentence": "cent Advances in DG.", "acronym": "DG", "label": "Dependency Grammar", "ID": "7232"}, {"sentence": "Weighted Constraint DG (Schro?der, 2002) models syntax structure as la- belled dependency trees as shown in the exam- ple.", "acronym": "DG", "label": "Dependency Grammar", "ID": "7233"}, {"sentence": "The process of removing and pruning is based on the knowledge base and the  four axioms of DG (Robinson, J .", "acronym": "DG", "label": "Dependency Grammar", "ID": "7234"}, {"sentence": "They can be a very basic  filtering process as proposed by Constraint  Grammars (see [Karlsson90]) or can be part to  an actual theory as with HPSG (see [Sag03]), the  Optimality Theory (see [Prince03]) or Constraint  DGs (cf. [", "acronym": "DG", "label": "Dependency Grammar", "ID": "7235"}, {"sentence": "\\[2\\] van Zuljlcn, Job M.(1989) : \"The Application of Simulated Annealing in  DG Pars ing' .", "acronym": "DG", "label": "Dependency Grammar", "ID": "7236"}, {"sentence": "DG induction via bitext pro- jection constraints.", "acronym": "DG", "label": "Dependency grammar", "ID": "7237"}, {"sentence": "2.1 Latent Dependency Structure DG is a lexically-oriented syn- tactic formalism in which syntactic relationships are expressed as dependencies between individual words.", "acronym": "DG", "label": "Dependency grammar", "ID": "7238"}, {"sentence": "DG induction via bitext projection constraints.", "acronym": "DG", "label": "Dependency grammar", "ID": "7239"}, {"sentence": "2  DG is equivalent to an X-bar theory  with only one phrasal bar level (Figure 3)--the dependents  of a word are the heads of its sisters.", "acronym": "DG", "label": "Dependency grammar", "ID": "7240"}, {"sentence": "DG and dependency parsing.", "acronym": "DG", "label": "Dependency grammar", "ID": "7241"}, {"sentence": "of the Workshop on DG, pages 88?", "acronym": "DG", "label": "Dependency Grammars", "ID": "7242"}, {"sentence": "In Workshop on Recent Advances in DG (COLING), pages 90?97.", "acronym": "DG", "label": "Dependency Grammars", "ID": "7243"}, {"sentence": "They can be a very basic  filtering process as proposed by Constraint  Grammars (see [Karlsson90]) or can be part to  an actual theory as with HPSG (see [Sag03]), the  Optimality Theory (see [Prince03]) or Constraint  DG (cf. [", "acronym": "DG", "label": "Dependency Grammars", "ID": "7244"}, {"sentence": "TAL (Special Issue Grammaires de D?pendance /  DG), 41 (1): 47?66.", "acronym": "DG", "label": "Dependency Grammars", "ID": "7245"}, {"sentence": "Rambow O., Joshi A., A Formal Look at  DG and Phrase-Structure  Grammars, with Special Consideration of Word-  Order Phenomena, Proc.", "acronym": "DG", "label": "Dependency Grammars", "ID": "7246"}, {"sentence": "LTAG is a lexicalized mildly-context sensitive tree  rewriting system \\[Joshi et al, 1975; Schabes, 1990\\]  that is closely related to DG and  Categorial Grammars.", "acronym": "DG", "label": "Dependency Grammars", "ID": "7247"}, {"sentence": "As most DGrs, the PD-grammars are analyzing.", "acronym": "DG", "label": "dependency gramma", "ID": "7248"}, {"sentence": "One of such properties, projectivity, re- quires that any word occurring between a word \t and a word   dependent on \t be dominated by \t\f\u000b In first DGrs (Gaifman, 1961) and in some more recent proposals: link gram- mars (Sleator and Temperly, 1993), projective DGrs (Lombardo and Lesmo, 1996) the projectivity is implied by definition.", "acronym": "DG", "label": "dependency gramma", "ID": "7249"}, {"sentence": "3 Polarized DGrs Polarized DGrs determine DV- structures in the bottom-up manner in the course of reduction of phrases to their types, just as the categorial grammars do.", "acronym": "DG", "label": "dependency gramma", "ID": "7250"}, {"sentence": "Towards a generic multilingual DGr for text generation.", "acronym": "DG", "label": "dependency gramma", "ID": "7251"}, {"sentence": "VERONIS,  J., IDE, N., M. (1990) Word Sense  Disambiguation with Very Large Neural Networks  Extracted frmn MRD.", "acronym": "MRD", "label": "Machine Readable Dictionaries", "ID": "7252"}, {"sentence": "We intend to derive the entire core lexicon for the system from MRD and then to tun e it against appropriate corpora .", "acronym": "MRD", "label": "Machine Readable Dictionaries", "ID": "7253"}, {"sentence": "Automatic Sense Disambigua- tion Using MRD: How to Tell a Pine Cone from an Ice Cream Cone.", "acronym": "MRD", "label": "Machine Readable Dictionaries", "ID": "7254"}, {"sentence": "Development of a Computational  Methodology for Deriving Natural Language  Semantic Structures via Analysis of  MRD.", "acronym": "MRD", "label": "Machine Readable Dictionaries", "ID": "7255"}, {"sentence": "Word Sense Dis-  ambiguation with Very Large Neural Networks  Extracted from MRD.", "acronym": "MRD", "label": "Machine Readable Dictionaries", "ID": "7256"}, {"sentence": "Veronis, Jean and Nancy Ide, \"Word Sense Disam-  biguation with Very Large Neural Networks Extracted  from MRD,\" in Proceedings,  COLING-90, pp 389-394, 1990.", "acronym": "MRD", "label": "Machine Readable Dictionaries", "ID": "7257"}, {"sentence": "have also translated synset definitions (e.g. Span- ish and Korean), so we can hope to create a multi- lingual corpus here as well and (v) the definitions can be used as a MRD, and various information extracted from there (Barn- brook, 2002; Nichols et al, 2006) 4.3 Kyoto Text Corpus The Kyoto Text Corpus consists of newspaper text from the Mainichi Newspaper (1995), seg- mented and annotated with Japanese POS tags and dependency trees (Kurohashi and Nagao, 2003).", "acronym": "MRD", "label": "machine readable dictionary", "ID": "7258"}, {"sentence": "9 Conclusion  In this paper we have argued that semantic tagging  can be carried out only relative to the senses in some  lexicon and that a MRD pro-  vides an appropriate set of senses.", "acronym": "MRD", "label": "machine readable dictionary", "ID": "7259"}, {"sentence": "A more knowledge-free system would have used a MRD or a large nat- ural language sample to retrieve its synonyms (see, for example, Lin (1998)), but our system falls short of this, relying on Roget?s New Millennium The- saurus1 (henceforth RT) as a source of synonyms.", "acronym": "MRD", "label": "machine readable dictionary", "ID": "7260"}, {"sentence": "Metrics can either employ a MRD, i.e. textual defi- nitions of words therein as an underlying knowledge base [1,11], or operate on the structure of a conceptual network, whereby textual definitions themselves are not available [9,7].", "acronym": "MRD", "label": "machine readable dictionary", "ID": "7261"}, {"sentence": "2 MiniWordnet Ideally the lexicon we would like to extend is a broad coverage MRD like Wordnet (Miller et al, 1990; Fellbaum, 1998).", "acronym": "MRD", "label": "machine readable dictionary", "ID": "7262"}, {"sentence": "Segmentation phase: Employing maximal  matching algorithm to segment a sentences  into some words, and setting a word?s POS  set in MRD as its  POS tagging.", "acronym": "MRD", "label": "machine readable dictionary", "ID": "7263"}, {"sentence": "Variety of MNr of kinds of token-level (0-5) alignments Table 2: Features used for Machine Learning Features 1-7 reflect relative numbers of matches (rel- ative to length of either the target or learner re- sponse).", "acronym": "MN", "label": "Match Numbe", "ID": "7264"}, {"sentence": "Number MNr is determined as fol- lows: Phrases starting with the words a, an, or this are singular; those, these, or some indicate plural.", "acronym": "MN", "label": "Match Numbe", "ID": "7265"}, {"sentence": "Variety of MNr of kinds of (0-5) token-level alignments Table 2: Features used in CoMiC?s classification phase Current versions of CoMiC use the WEKA toolkit (Hall et al 2009), allowing us to experiment with different machine learning strategies.", "acronym": "MN", "label": "Match Numbe", "ID": "7266"}, {"sentence": "Variety of MNr of kinds of (0-5) token-level alignments Table 3: Features used in the CoMiC-EN system dependency graph alignment in connection with two different machine learning approaches.", "acronym": "MN", "label": "Match Numbe", "ID": "7267"}, {"sentence": "Variety of MNr of kinds of token-level (0-5) alignments Table 3: Features used for the memory-based classifier 3 4 Content Assessment Experiment 4.1 Setup We ran our content assessment experiment using the two data sets introduced in section 2, one from Kansas University and the other from The Ohio State University.", "acronym": "MN", "label": "Match Numbe", "ID": "7268"}, {"sentence": "Also, MN helped to capture long patterns like ?", "acronym": "MN", "label": "mixed ngrams", "ID": "7269"}, {"sentence": "We use another fea- ture set LEX to capture word ngrams, POS (part of speech) ngrams and MN.", "acronym": "MN", "label": "mixed ngrams", "ID": "7270"}, {"sentence": "In addition to the features described in Sec- tion 4.2, the power prediction system presented in (Prabhakaran and Rambow, 2014) uses a lexi- cal feature set (LEX) that captures word ngrams, POS (part of speech) ngrams and MN, since lexical features have been established to be very useful for power prediction.", "acronym": "MN", "label": "mixed ngrams", "ID": "7271"}, {"sentence": "Bickel et al (Bickel et al, 2007) discriminatively learns a scaling factor for STrain, so as to adapt the source domain data distribution to resemble the target domain data distribution, under the [S+T-] setting.", "acronym": "STrain", "label": "source domain training data", "ID": "7272"}, {"sentence": "6 Analysis Using only the STrain, a coreference resolution system achieves an F- measure of 39.8% on the GENIA test set (the col- umn of ?", "acronym": "STrain", "label": "source domain training data", "ID": "7273"}, {"sentence": "In the domain adaptation, we do semantic association inference on the STrain using LaSA model at first, then the original source domain NER model is tuned on the STrain set by incorporating these generated semantic association features.", "acronym": "STrain", "label": "source domain training data", "ID": "7274"}, {"sentence": "In LaSA-based domain adaptation, the semantic association features of each unit in the observation window {-2,2} are generated by LaSA model at first, then the basic source domain NER model is tuned on the original STrain set by incor- porating the semantic association features.", "acronym": "STrain", "label": "source domain training data", "ID": "7275"}, {"sentence": "5.5 Domain Adaptation with Active Learning In the experiments on domain adaptation with ac- tive learning for coreference resolution, we as- sume that the STrain are an- notated.", "acronym": "STrain", "label": "source domain training data", "ID": "7276"}, {"sentence": "the set of STrain instances D t ?", "acronym": "STrain", "label": "source domain training", "ID": "7277"}, {"sentence": "To calibrate the conditional probability P (y|x) from the source domain to the target domain, ide- ally each STrain instance (x i , y i ) should be given a weight Pt(y s i |x s i ) P s (y s i |x s i ) .", "acronym": "STrain", "label": "source domain training", "ID": "7278"}, {"sentence": "Bickel et al (Bickel et al, 2007) discriminatively learns a scaling factor for STrain data, so as to adapt the source domain data distribution to resemble the target domain data distribution, under the [S+T-] setting.", "acronym": "STrain", "label": "source domain training", "ID": "7279"}, {"sentence": "We first train an NER model on a large STrain corpus, and then learn the correlation between the source and tar- get NE types.", "acronym": "STrain", "label": "source domain training", "ID": "7280"}, {"sentence": "In domain adaptation, there are typi- cally many more STrain instances than target domain training instances.", "acronym": "STrain", "label": "source domain training", "ID": "7281"}, {"sentence": "few features in target domain test instances appear in STrain instances.", "acronym": "STrain", "label": "source domain training", "ID": "7282"}, {"sentence": "Each HITs (HIT) consists of a pair of top 20 ranked aspect lists for an entity, with one list from UDB-m and the other chosen from one of the baseline algorithms.", "acronym": "HITs", "label": "Human Intelligence Tasks", "ID": "7283"}, {"sentence": "To collect human answers on the test ques- tions, we delivered them to human beings through Amazon Mechanical Turk (AMT), a crowd-sourcing Internet marketplace that allows people to partici- pate in HITs.", "acronym": "HITs", "label": "Human Intelligence Tasks", "ID": "7284"}, {"sentence": "These include inference rule discovery for question-answering and information retrieval (Lin and Pantel, 2001), idiom or multiword ex- 2HITs 3often referred to as turkers 4http://crowdflower.com 205 pression acquisition (Fellbaum et al 2006) and identification (Boukobza and Rappoport, 2009), machine translation evaluation (Snover et al 2009), textual entailment recognition, and many more.", "acronym": "HITs", "label": "Human Intelligence Tasks", "ID": "7285"}, {"sentence": "Based on this, we loaded each MTurk HITs (HIT) with five tweets, and paid workers five cents per HIT.", "acronym": "HITs", "label": "Human Intelligence Tasks", "ID": "7286"}, {"sentence": "However, a less apparent advantage is the need for researchers to provide succinct and comprehensible descriptions of HITs, and the need to break complex annotation tasks down to simpler basic units of work for annotators.", "acronym": "HITs", "label": "Human Intelligence Tasks", "ID": "7287"}, {"sentence": "Regardless of the cause, given these results, we re- stricted the availability of all following experiments to Turkers in the US.Ideally we would include other English-speaking countries, but there is no straight- 2HITs ?", "acronym": "HITs", "label": "Human Intelligence Tasks", "ID": "7288"}, {"sentence": "purpose crowdsourcing marketplace, the Univer- sal Human Relevance System (UHRS).3 The mar- ketplace connects HITs with a large population of workers across the globe.", "acronym": "HITs", "label": "human intelligence tasks", "ID": "7289"}, {"sentence": "In the sec- ond blog, RWNs, which is a conservative blog, we see a different picture.", "acronym": "RWN", "label": "Right Wing New", "ID": "7290"}, {"sentence": "We selected three blog sites from this dataset: the RWNs (right-ideology) ; the Carpetbagger, and Daily Kos as representatives 1144 palest inian is raeli peace year  polit ical  proces s   state  end  rig ht   g overnment  need  conflict way s ecurit y palest inian is raeli Peace polit ical  occupation  proces s end  s ecurit y   conflict   way  g overnment   people t ime year force  neg ot iation bush US pres ident  american sharon admi", "acronym": "RWN", "label": "Right Wing New", "ID": "7291"}, {"sentence": "In the sec- ond blog, RWN, which is a conservative blog, we see a different picture.", "acronym": "RWN", "label": "Right Wing News", "ID": "7292"}, {"sentence": "We selected three blog sites from this dataset: the RWN (right-ideology) ; the Carpetbagger, and Daily Kos as representatives 1144 palest inian is raeli peace year  polit ical  proces s   state  end  rig ht   g overnment  need  conflict way s ecurit y palest inian is raeli Peace polit ical  occupation  proces s end  s ecurit y   conflict   way  g overnment   people t ime year force  neg ot iation bush US pres ident  american sharon admi", "acronym": "RWN", "label": "Right Wing News", "ID": "7293"}, {"sentence": "We also estimated the lower bound  of this evaluation, that is, we also conducted the same  trials using the BGH.", "acronym": "BGH", "label": "Bunruigoihyo thesaurus", "ID": "7294"}, {"sentence": "Since resolution  of equations is time-consuming, we tentatively general-  ized 23,223 nouns into 303 semantic lasses (represented  by the first 4 digits of the semantic ode given in the  BGH), reducing the total number of  equations to 45,753.", "acronym": "BGH", "label": "Bunruigoihyo thesaurus", "ID": "7295"}, {"sentence": "Further, tuples with  nouns appearing in the BGH were se-  lected.", "acronym": "BGH", "label": "Bunruigoihyo thesaurus", "ID": "7296"}, {"sentence": "When the noun comprised a compound noun,  it was transformed into the maximal eftmost substring  contained in the BGH.", "acronym": "BGH", "label": "Bunruigoihyo thesaurus", "ID": "7297"}, {"sentence": "We tentatively use the  BGH, inwhich each word corresponds  to a leaf in the tree structure.", "acronym": "BGH", "label": "Bunruigoihyo thesaurus", "ID": "7298"}, {"sentence": "We use the BGH, which contains 96,000 Japanese words (The National In- stitute for Japanese Language, 2004).", "acronym": "BGH", "label": "Bunruigoihyo thesaurus", "ID": "7299"}, {"sentence": "\\[Utsuro et al, 1993\\] categorise  words using the \"BGH\"  (Japanese) thesaurus.", "acronym": "BGH", "label": "Bunrui Goi Hyou", "ID": "7300"}, {"sentence": "The National Language Research Institute: BGH,  (in Japanese), Shuuei Publishing, 1964.", "acronym": "BGH", "label": "Bunrui Goi Hyou", "ID": "7301"}, {"sentence": "BGH.", "acronym": "BGH", "label": "Bunrui Goi Hyou", "ID": "7302"}, {"sentence": "In this case, we use  the thesaurus dictionary \"BGH\" (NLRI,  1964) to learn the meanings of nouns.", "acronym": "BGH", "label": "Bunrui Goi Hyou", "ID": "7303"}, {"sentence": "A Japanese thesaurus, the BGH (NLRI, 1964), was used to determine the category number of each word.", "acronym": "BGH", "label": "Bunrui Goi Hyou", "ID": "7304"}, {"sentence": "The value S is the semantic  similarity between a possible antecedent and Noun  X of \"Noun X no Noun Y.\" Semantic similarity is  shown by level in BGH (NLRI, 1964).", "acronym": "BGH", "label": "Bunrui Goi Hyou", "ID": "7305"}, {"sentence": "Learning structure using ABL.", "acronym": "ABL", "label": "Alignment Based Learning", "ID": "7306"}, {"sentence": "Nouns are inflected based  on number (singular, plural), article and case [k?- raka] (nominative, accusative, instrumental, dative,  ABL, genitive, locative and vocative).", "acronym": "ABL", "label": "ablative", "ID": "7307"}, {"sentence": "Although the pure DG for-  malism proved to be particularly practical for integration of idioms and exceptions,  its lack of constituent symbols, i.e., non-terminals, would have lead to a grammar  of enormous ize and made it difficult to integrate special Latin constructs uch as  accusative cum infinitive or ABL absolute.", "acronym": "ABL", "label": "ablative", "ID": "7308"}, {"sentence": "At most one dative (locative, ABL, instrumental) adjunct can link to a verb.", "acronym": "ABL", "label": "ablative", "ID": "7309"}, {"sentence": "I then gave a short (over- simplified) tutorial on Latin and Japanese gram- mar, suggesting a connection between Latin cas- es (e.g., nominative, accusative, ABL, etc.)", "acronym": "ABL", "label": "ablative", "ID": "7310"}, {"sentence": "return + ABL case  (e) Vowel elimination ????", "acronym": "ABL", "label": "ablative", "ID": "7311"}, {"sentence": "work + ABL case   Figure 1: Inflection types of content words in  Mongolian phrases.", "acronym": "ABL", "label": "ablative", "ID": "7312"}, {"sentence": "health, care, insurance, public, pri- vate Kent Conrad, Paul Hsieh, PK, Ezra Klein, Jacob Hacker ?", "acronym": "PK", "label": "Paul Krugman", "ID": "7313"}, {"sentence": "User Input Wikipedia?s Suggestion Correct Spelling Suchifun Houkingu Suchin Housing Stephen Hawking Stefan Hoking Stefan Ho king Stephen Hawking Pol Crugman Poll Krugman PK Paal Kragaman PK PK Suburaamaniya Ba- haarachi Subramaniya Baracchi Subrahmaniya Bharati search in Wikipedia.", "acronym": "PK", "label": "Paul Krugman", "ID": "7314"}, {"sentence": "Consider the following sentences from a New York Times (NYT) column written by PK: ?", "acronym": "PK", "label": "Paul Krugman", "ID": "7315"}, {"sentence": "For each document 115 Columnist Cluster I Cluster II Dowd 294 4 Krugman 3 328 Table 2: Results when clustering 629 documents written by Maureen Dowd and PK into two clusters.", "acronym": "PK", "label": "Paul Krugman", "ID": "7316"}, {"sentence": "economist (Princeton, economist, Princeton economist PK 7 PK) was awarded the Nobel prize in 2008.", "acronym": "PK", "label": "Paul Krugman", "ID": "7317"}, {"sentence": "Aspect Extraction with Automated PK Learning.", "acronym": "PK", "label": "Prior Knowledge", "ID": "7318"}, {"sentence": "PK Aspect_i Sentiment_i  ?", "acronym": "PK", "label": "Prior Knowledge", "ID": "7319"}, {"sentence": "Leveraging  Multi-Domain PK in Topic Models.", "acronym": "PK", "label": "Prior Knowledge", "ID": "7320"}, {"sentence": "c?2013 Association for Computational Linguistics AMI&ERIC: How to Learn with Naive Bayes and PK: an Application to Sentiment Analysis Mohamed Dermouche1,2, Leila Khouas1, 1AMI Software R&D 1475 av.", "acronym": "PK", "label": "Prior Knowledge", "ID": "7321"}, {"sentence": "Reading both High- coherence and Low-coherence Texts: Effects of Text  Sequence and PK.", "acronym": "PK", "label": "Prior Knowledge", "ID": "7322"}, {"sentence": "and Anderson, R.C. 1986 What They Don't Know Will  Hurt Them: The Role of PK in Comprehension.", "acronym": "PK", "label": "Prior Knowledge", "ID": "7323"}, {"sentence": "Specification for Corpus Proc- essing at PK:Word Segmentation,  POS Tagging and Phonetic Notation.", "acronym": "PK", "label": "Peking University", "ID": "7324"}, {"sentence": "4 Discussion and Future Work In our survey, only 33% of nouns and 44% of verbal nouns created by kanji/hanzi conversion method exist in the PK dictionary.", "acronym": "PK", "label": "Peking University", "ID": "7325"}, {"sentence": "PK: (PK Dictionary) http://www.icl.pku.edu.cn/.  Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 122?125, Sydney, July 2006.", "acronym": "PK", "label": "Peking University", "ID": "7326"}, {"sentence": "Then, we consult these words using a Chinese dictionary provided by PK [14].", "acronym": "PK", "label": "Peking University", "ID": "7327"}, {"sentence": "Knowledge Sharing via Social Login: Exploiting Microblogging Service for Warming up Social Question Answering Websites Yang Xiao 1 , Wayne Xin Zhao 2 , Kun Wang 1 and Zhen Xiao 1 1 School of Electronics Engineering and Computer Science, PK, China 2 School of Information, Renmin University of China, China {xiaoyangpku, batmanfly}@gmail.com {wangkun, xiaozhen}@net.pku.edu.cn Abstract Community Question Answering (CQA) websites such as Quora are widely used for users to get high quality answers.", "acronym": "PK", "label": "Peking University", "ID": "7328"}, {"sentence": "Contextual Corrector employs lan- guage modelling to rank a list of potential can- didates in the scope of whole sentence whereas WC chooses the best candidate for each syllable that has the highest weights.", "acronym": "WC", "label": "Weighting-based Corrector", "ID": "7329"}, {"sentence": "3.4 Corrector In VOSE, we propose two possible correctors: WC Given a ranked top-K list of potential can- didates from Non-syllable Detector and Real- syllable Detector, WC simply chooses the best candidates based on their weights (Equation 5) to produce the final output.", "acronym": "WC", "label": "Weighting-based Corrector", "ID": "7330"}, {"sentence": "Statistical analyses provide  evidence that machine WC scores  correlate well with scores provided by teachers  and expert scorers, with all (Pearson?s  correlation coefficient) r?s > 0.98 at the  individual response level, and all r?s > 0.99 at  the ?", "acronym": "WC", "label": "Words Correct", "ID": "7331"}, {"sentence": "i pi log pi Window Width 2 4 6 8 10 Nu mb er  of  R ule s 0 1000 2000 3000 4000 5000 6000 LTS Rule Count vs Window Width Legend English 40k Dutch 40k Afrikaans 37k Italian 40k Legend Chars Correct WC WC Number of Rules 0 10 20 30 40 50 60 Pe rce nt  C or re ct 0 20 40 60 80 100 Spanish LTS Ruleset Performance W=3W=2W=1 235 Beyond a window width of 7, rule growth tapers off  considerably.", "acronym": "WC", "label": "Words Correct", "ID": "7332"}, {"sentence": "Answering the  first part of the question involves comparing  machine WC scores to human scores  when teachers make ratings in the classroom  environment as the student reads into the phone.", "acronym": "WC", "label": "Words Correct", "ID": "7333"}, {"sentence": "or median  WC value, from expert scorers.", "acronym": "WC", "label": "Words Correct", "ID": "7334"}, {"sentence": "Answering the second  part of the question involves comparing machine  WC scores to a ?", "acronym": "WC", "label": "Words Correct", "ID": "7335"}, {"sentence": "Dynamic help generation by estimating user?s mental model in SDSs.", "acronym": "SDSs", "label": "spoken dialogue systems", "ID": "7336"}, {"sentence": "1 Introduction Studies on SDSs have recently proceeded from in-laboratory systems to ones de- ployed to the open public (Raux et al, 2006; Ko- matani et al, 2007; Nisimura et al, 2005).", "acronym": "SDSs", "label": "spoken dialogue systems", "ID": "7337"}, {"sentence": "2 Related Work Early discriminative approaches to text generation were introduced in SDSs, and usually tackled content selection and surface re- alization separately.", "acronym": "SDSs", "label": "spoken dialogue systems", "ID": "7338"}, {"sentence": "2 Related Work Various studies have been done on generating help messages in SDSs.", "acronym": "SDSs", "label": "spoken dialogue systems", "ID": "7339"}, {"sentence": "Targeted help for SDSs: intelligent feed- back improves naive users?", "acronym": "SDSs", "label": "spoken dialogue systems", "ID": "7340"}, {"sentence": "Adding intelligent help to mixed-initiative SDSs.", "acronym": "SDSs", "label": "spoken dialogue systems", "ID": "7341"}, {"sentence": "of the AAAI Workshop on Statistical and Empirical Methods in SDSs.", "acronym": "SDSs", "label": "Spoken Dialogue Systems", "ID": "7342"}, {"sentence": "A Reinforce-ment Learning Approach to Evaluating State Rep-resentations in SDSs.", "acronym": "SDSs", "label": "Spoken Dialogue Systems", "ID": "7343"}, {"sentence": "c?2009 Association for Computational Linguistics Ranking Help Message Candidates Based on Robust Grammar Verification Results and Utterance History in SDSs Kazunori Komatani Satoshi Ikeda Yuichiro Fukubayashi Tetsuya Ogata Hiroshi G. Okuno Graduate School of Informatics Kyoto University Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan {komatani,sikeda,fukubaya,ogata,okuno}@kuis.kyoto-u.ac.jp Abstract We address an issue of out-of-grammar (OOG) utterances in spoken dialogue sys- tems by generating help messages for novice users.", "acronym": "SDSs", "label": "Spoken Dialogue Systems", "ID": "7344"}, {"sentence": "SemEval-2014 Task 2: Grammar Induction for SDSs Ioannis Klasinas 1 , Elias Iosif 2,4 , Katerina Louka 3 , Alexandros Potamianos 2,4 1 School of ECE, Technical University of Crete, Chania 73100, Greece 2 School of ECE, National Technical University of Athens, Zografou 15780, Greece 3 Voiceweb S.A., Athens 15124, Greece 4 Athena Research Center, Marousi 15125, Greece iklasinas@isc.tuc.gr,{iosife,potam}@telecom.tuc.gr,klouk", "acronym": "SDSs", "label": "Spoken Dialogue Systems", "ID": "7345"}, {"sentence": "c?2009 Association for Computational Linguistics Natural Language Generation as Planning Under Uncertainty for SDSs Verena Rieser School of Informatics University of Edinburgh vrieser@inf.ed.ac.uk Oliver Lemon School of Informatics University of Edinburgh olemon@inf.ed.ac.uk Abstract We present and evaluate a new model for Natural Language Generation (NLG) in SDSs, based on statis- tical planning, given noisy feedback from the current generation context (e.g. a user and a surface realiser).", "acronym": "SDSs", "label": "Spoken Dialogue Systems", "ID": "7346"}, {"sentence": "In this paper we present and evaluate a new model for NLG in SDSs as planning under uncertainty.", "acronym": "SDSs", "label": "Spoken Dialogue Systems", "ID": "7347"}, {"sentence": "Deep CNNs for sentiment analysis of short texts.", "acronym": "CNN", "label": "convolutional neural network", "ID": "7348"}, {"sentence": "Recent ad- 1161 vances in CNNs.", "acronym": "CNN", "label": "convolutional neural network", "ID": "7349"}, {"sentence": "With the help of an advanced entity linking system and a deep CNN model that matches questions and predicate se- quences, our system outperforms previous meth- ods substantially on the WEBQUESTIONS dataset.", "acronym": "CNN", "label": "convolutional neural network", "ID": "7350"}, {"sentence": "Here we present a non-linear method based on a deep CNN.", "acronym": "CNN", "label": "convolutional neural network", "ID": "7351"}, {"sentence": "Learning semantic representations using CNNs for web search.", "acronym": "CNN", "label": "convolutional neural network", "ID": "7352"}, {"sentence": "A CNN for mod- elling sentences.", "acronym": "CNN", "label": "convolutional neural network", "ID": "7353"}, {"sentence": "Recur- rent CNNs for Discourse Compositionality.", "acronym": "CNN", "label": "Convolutional Neural Network", "ID": "7354"}, {"sentence": "c?2014 Association for Computational Linguistics Learning Image Embeddings using CNNs for Improved Multi-Modal Semantics Douwe Kiela ?", "acronym": "CNN", "label": "Convolutional Neural Network", "ID": "7355"}, {"sentence": "2014 Association for Computational Linguistics A CNN for Modelling Sentences Nal Kalchbrenner Edward Grefenstette {nal.kalchbrenner, edward.grefenstette, phil.blunsom}@cs.ox.ac.uk Department of Computer Science University of Oxford Phil Blunsom Abstract The ability to accurately represent sen- tences is central to language understand- ing.", "acronym": "CNN", "label": "Convolutional Neural Network", "ID": "7356"}, {"sentence": "2015 Association for Computational Linguistics Classifying Relations by Ranking with CNNs C??cero Nogueira dos Santos IBM Research 138/146 Av.", "acronym": "CNN", "label": "Convolutional Neural Network", "ID": "7357"}, {"sentence": "c?2015 Association for Computational Linguistics Non-Linear Text Regression with a Deep CNN Zsolt Bitvai University of Sheffield, UK z.bitvai@shef.ac.uk Trevor Cohn University of Melbourne, Australia t.cohn@unimelb.edu.au Abstract Text regression has traditionally been tackled using linear models.", "acronym": "CNN", "label": "Convolutional Neural Network", "ID": "7358"}, {"sentence": "2016) use a CNN for implicit discourse relation classification.", "acronym": "CNN", "label": "Convolutional Neural Network", "ID": "7359"}, {"sentence": "Deep CNN for sentiment analysis of short texts.", "acronym": "CNN", "label": "convolutional neural networks", "ID": "7360"}, {"sentence": "Kiela and Bottou (2014) showed that transferring representations from deep CNN (ConvNets) yield much better performance than bag-of-visual-words in multi-modal semantics.", "acronym": "CNN", "label": "convolutional neural networks", "ID": "7361"}, {"sentence": "Recent ad- 1161 vances in CNN.", "acronym": "CNN", "label": "convolutional neural networks", "ID": "7362"}, {"sentence": "Learning semantic representations using CNN for web search.", "acronym": "CNN", "label": "convolutional neural networks", "ID": "7363"}, {"sentence": "Our approach does not de- pend on the availability of large amounts of in-domain parallel data, but only re- lies on available large datasets of monolin- gually captioned images, and on state-of- the-art CNN to compute image similarities.", "acronym": "CNN", "label": "convolutional neural networks", "ID": "7364"}, {"sentence": "Learning image embeddings using CNN for improved multi-modal semantics.", "acronym": "CNN", "label": "convolutional neural networks", "ID": "7365"}, {"sentence": "This often  has the BNF effect of separating inflectional  and derivational ffixes from the roots.", "acronym": "BNF", "label": "beneficial", "ID": "7366"}, {"sentence": "It is possible to use an alternative, heuristic search based on Viterbi n  best (we will not go into the PCFG-reduction technique presented in Goodman (1998) since that heuristic only works for Tree- DOP and is BNF only if all subtrees are taken into account and if the so-called \"labeled recall parse\" is computed).", "acronym": "BNF", "label": "beneficial", "ID": "7367"}, {"sentence": "It would be BNF to compact such a network using FSRTs, and to inspect the time versus space tradeoff on such a comprehensive network.", "acronym": "BNF", "label": "beneficial", "ID": "7368"}, {"sentence": "syntactic information would be BNF for the  machine to learn.", "acronym": "BNF", "label": "beneficial", "ID": "7369"}, {"sentence": "Furthermore, the use of predicates can be BNF for describing natural language reduplication where the reduplication is not as bounded as the example we deal with in this work.", "acronym": "BNF", "label": "beneficial", "ID": "7370"}, {"sentence": "For in- tersection, we believe that the use of predicates (van Noord and Gerdemann 2001b) can be BNF.", "acronym": "BNF", "label": "beneficial", "ID": "7371"}, {"sentence": "The d e f i n i t i o n s  are w r i t t e n  i n  BNF, A n  example of  a s t r i n g  d e f i n i t i o n  is:  <ASSERTION> ::= <SA><SUBJECT><SA><TENSE><SA><VERB><SA><OBJECT><RV><SA>.", "acronym": "BNF", "label": "Backus Normal Form", "ID": "7372"}, {"sentence": "Mathematical Model of Serbo- Croatian Morphology (Nominal INFL).", "acronym": "INFL", "label": "Inflection", "ID": "7373"}, {"sentence": "INFLs can change the vowel of the first  o-syllable of the stem.", "acronym": "INFL", "label": "Inflection", "ID": "7374"}, {"sentence": "3  Table 1: O-syllable Form Examples  3 Morphological Impact of INFLs  Like English, the inflections in Bengali work as a  suffix to the stem.", "acronym": "INFL", "label": "Inflection", "ID": "7375"}, {"sentence": "INFL We introduced tag suffixes for inflec- tion as clues to identify the attachment position of the verb and adjective phrases, because Japanese verbs and adjectives have inflections, which depends 110 (no label) base form cont continuative form attr attributive form neg negative form hyp hypothetical form imp imperative form stem stem Table 2: INFL tag suffixes on their modifying wor", "acronym": "INFL", "label": "Inflection", "ID": "7376"}, {"sentence": "INFL We introduced tag suffixes for inflec- tion as clues to identify the attachment position of the verb and adjective phrases, because Japanese verbs and adjectives have inflections, which depends 110 (no label) base form cont continuative form attr attributive form neg negative form hyp hypothetical form imp imperative form stem stem Table 2: INFL tag suffixes on their modifying words and phrases (e.g. noun and verb phrases).", "acronym": "INFL", "label": "Inflection", "ID": "7377"}, {"sentence": "INFLs can act as simple suffix and do not  make any change in the verb stem.", "acronym": "INFL", "label": "Inflection", "ID": "7378"}, {"sentence": "The binary tree?s structure has the follow- NP Noun phrase PP Postposition phrase VP Verb phrase ADJP Adjective phrase ADVP Adverbial phrase CONJP Conjunction phrase S Sentence (=root) IP INFLal phrase IP-MAT Matrix clause IP-ADV Adverb clause IP-REL Gapping relative clause IP-ADN Non-gapping adnominal clause CP Complementizer phrase CP-THT Sentential complement Function tags semantic role for mandatory argument (gap notation) -ARG0 ( arg0) -ARG1 ( arg1) -ARG2 ( arg2) grammatical role for mandatory argument (gap notation) -SBJ ( sbj) Subjective case -OBJ ( obj) Objective case", "acronym": "INFL", "label": "Inflection", "ID": "7379"}, {"sentence": "These labels, that are called phonological and seman- tic features INFL tradition, are computed from the proofs and consist of two parts that can be superimposed: a phonological label, denoted by ffi\"!$#&%\u0006'(ffi , and a semantic label2 de- noted by )*!", "acronym": "INFL", "label": "in the transformational", "ID": "7380"}, {"sentence": "Surface structure is  identical to S-structure, except for the fact that the  association between moved phrases and their traces  is not present; chain indices that reveal history of  movement INFL ccount are not  present.", "acronym": "INFL", "label": "in the transformational", "ID": "7381"}, {"sentence": "TCMPDBLK exemplifies the interaction of syntactic and semantic  information INFL component of the REQUEST Sys-  tem, in that it filters out a variety of otherwise acceptable structures in  which a noun phrase or prepositional phrase with head noun marked  (+ TIME) is adjacent o a noun phrase with head noun marked (+  PERIODIC), but where the former is not analyzed as a modifier of  the latter.", "acronym": "INFL", "label": "in the transformational", "ID": "7382"}, {"sentence": "This modification opera- tion allows the FST to encode information about the history of states INFL FST as part of the model structure.", "acronym": "INFL", "label": "in the transformational", "ID": "7383"}, {"sentence": "predicate -raising \"  INFL grammar).", "acronym": "INFL", "label": "in the transformational", "ID": "7384"}, {"sentence": "We have tried to  show, however, that even the empir ical  approach felt the  need for mult id imensional  structuring, and certain rules  set up on empir ical  considerat ions - e.g. those treating  verb with mult ip le meaning - turn out to be mappings of  phrase markers into phrase markers INFL   sense.- . . . . . .", "acronym": "INFL", "label": "in the transformational", "ID": "7385"}, {"sentence": "In our VJ-WIMP application, we use Deg ree  of C ons trict ion Front Central Back High Mid Low TB Position [iy ] [ix ] [uw ] [ey] [ax ] [ow ] [ae ] [a] [aa ] Figure 1: Vowel configurations as a function of their dominant articulatory configurations.", "acronym": "TB", "label": "Tongue Body", "ID": "7386"}, {"sentence": "For the exact match scheme, the obtained performance is higher7 than TBline (random guess) that equals to 0.250.", "acronym": "TB", "label": "the base", "ID": "7387"}, {"sentence": "Since the  Hobbs' algorithm serves as TB of our  scheme, we expect the accuracy to be much  higher with more accurately transformed trees.", "acronym": "TB", "label": "the base", "ID": "7388"}, {"sentence": "We now list TB lexical features that were considered for this experiment.", "acronym": "TB", "label": "the base", "ID": "7389"}, {"sentence": "This concludes TB features we considered.", "acronym": "TB", "label": "the base", "ID": "7390"}, {"sentence": "Using random guesses, TBline precision is 0.010 and 0.333 for quote-to-speaker attribution and gender estimation, respectively.", "acronym": "TB", "label": "the base", "ID": "7391"}, {"sentence": "Using ran- dom guesses, TBline accuracy is 0.33.", "acronym": "TB", "label": "the base", "ID": "7392"}, {"sentence": "Semeval-2013 task 8: CLTE for content syn- chronization.", "acronym": "CLTE", "label": "Cross-lingual textual entailment", "ID": "7393"}, {"sentence": "Fbk: CLTE with-out translation.", "acronym": "CLTE", "label": "Cross-lingual textual entailment", "ID": "7394"}, {"sentence": "semeval-2012 task 8: CLTE for con- tent synchronization.", "acronym": "CLTE", "label": "Cross-lingual textual entailment", "ID": "7395"}, {"sentence": "Semeval-2012 task 8: CLTE for content synchronization.", "acronym": "CLTE", "label": "Cross-lingual textual entailment", "ID": "7396"}, {"sentence": "semeval- 2012 task 8: CLTE for content syn- chronization.", "acronym": "CLTE", "label": "Cross-lingual textual entailment", "ID": "7397"}, {"sentence": "We got some improvement from training two separate CLTE anaphora and antecedent mentions.", "acronym": "CLTE", "label": "classifiers for detecting", "ID": "7398"}, {"sentence": "We report significant absolute im- provements in performance in multi-class prediction, as well as significant improve- ment of binary CLTE the presence of implicit Temporal, Compari- son and Contingency relations.", "acronym": "CLTE", "label": "classifiers for detecting", "ID": "7399"}, {"sentence": "We trained separate CLTE anaphor and antecedent mentions, and experimented with several clustering techniques to discover the most suitable algorithm for producing coreference chains in this domain.", "acronym": "CLTE", "label": "classifiers for detecting", "ID": "7400"}, {"sentence": "We develop a series of supervised CLTE the expression of views on the legitimacy of income inequality.", "acronym": "CLTE", "label": "classifiers for detecting", "ID": "7401"}, {"sentence": "We apply error generation methods and train CLTE and correcting arti- cle errors in essays written by non-native En- glish speakers; we show that training on data that contain errors produces higher accuracy when compared to a system that is trained on clean native data.", "acronym": "CLTE", "label": "classifiers for detecting", "ID": "7402"}, {"sentence": "Han and Baldwin, 2011) de- veloped CLTE the ill-formed words and generated corrections based on the morpho- phonemic similarity. (", "acronym": "CLTE", "label": "classifiers for detecting", "ID": "7403"}, {"sentence": "The MADCOW collection and evaluation procedures  have provided effective tools for assessing the current ca-  pabilities of interactive SLS.", "acronym": "SLS", "label": "spoken language systems", "ID": "7404"}, {"sentence": "effective tools for assessing the current ca-  pabilities of interactive SLS.", "acronym": "SLS", "label": "spoken language systems", "ID": "7405"}, {"sentence": "These reference answers  were used by the system developers and by NIST to eval-  uate the responses of the MADCOW natural language  and SLS.", "acronym": "SLS", "label": "spoken language systems", "ID": "7406"}, {"sentence": "As our SLS evolve,  data collection and evaluation methods mu", "acronym": "SLS", "label": "spoken language systems", "ID": "7407"}, {"sentence": "Our shared goal is  to build interactive SLS; however,  our evaluation methods rely on a canned corpus and  evaluate a system's recognition performance under static  conditions that are not representative of the interactive  environments in which these systems will eventually be  used.", "acronym": "SLS", "label": "spoken language systems", "ID": "7408"}, {"sentence": "As our SLS evolve,  data collection and evaluation methods must evolve with  them.", "acronym": "SLS", "label": "spoken language systems", "ID": "7409"}, {"sentence": "Conc lus ions   This paper has described several approaches to training  and evaluation of SLS.", "acronym": "SLS", "label": "spoken language systems", "ID": "7410"}, {"sentence": "The ATIS SLS pilot  corpus.", "acronym": "SLS", "label": "spoken language systems", "ID": "7411"}, {"sentence": "In Proceedings  of the ARPA SLSs Tech-  nology Workshop.", "acronym": "SLS", "label": "Spoken Language System", "ID": "7412"}, {"sentence": "INESC-ID Lisboa, SLSs Lab, R. Alves Redol 9, 1000-029 LISBOA, Portugal.", "acronym": "SLS", "label": "Spoken Language System", "ID": "7413"}, {"sentence": "Gauvain, J.L., et al, \"LIMSI Nov92 Evaluation\", Oral  Presentation at the SLSs Technology  Workshop, January 20-22, 1993, Cambridge, MA.", "acronym": "SLS", "label": "Spoken Language System", "ID": "7414"}, {"sentence": "Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 33?40 Manchester, August 2008 Mixed-Source Multi-Document Speech-to-Text Summarization Ricardo Ribeiro INESC ID Lisboa/ISCTE/IST SLSs Lab Rua Alves Redol, 9 1000-029 Lisboa, Portugal rdmr@l2f.inesc-id.pt David Martins de Matos INESC ID Lisboa/IST SLSs Lab Rua Alves Redol, 9 1000-029 Lisboa, Portugal david@l2f.inesc-id.pt Abstract Speech-to-text summarization systems usually take as input the output of an automatic speech recognition (ASR) system that is affected by issues like speech recognition errors, disfluencies, or difficulties in the accurate identification of sentence boundaries.", "acronym": "SLS", "label": "Spoken Language System", "ID": "7415"}, {"sentence": "Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 33?40 Manchester, August 2008 Mixed-Source Multi-Document Speech-to-Text Summarization Ricardo Ribeiro INESC ID Lisboa/ISCTE/IST SLSs Lab Rua Alves Redol, 9 1000-029 Lisboa, Portugal rdmr@l2f.inesc-id.pt David Martins de Matos INESC ID Lisboa/IST SLSs Lab Rua Alves Redol, 9 1000-029 Lisboa, Portugal david@l2f.inesc-id.pt Abstract Speech-to-text summarization systems usually take as input the output of an automatic speech recognition (ASR) system that is affected by issues like speech re", "acronym": "SLS", "label": "Spoken Language System", "ID": "7416"}, {"sentence": "Thanks to Matthew John- son, Ramesh Sridharan, Finale Doshi, S.R.K. Brana- van, the MIT SLSs group and the anonymous reviewers for helpful comments.", "acronym": "SLS", "label": "Spoken Language System", "ID": "7417"}, {"sentence": "In these chains of utterances, the speaker is not explicitly mentioned because the author relies on the shared understanding with the reader that adja- cent pieces of quoted speech are not inDD (Zhang et al.,", "acronym": "DD", "label": "dependent", "ID": "7418"}, {"sentence": "salience(re/)  = -2  log  Making the unrealistic simplifying assumption  that references of one gender class are com-  pletely inDD of references for another  classes 1, the likelihood function in this case is  just the product over all classes of the probabil-  ities of each class of reference to the power of  the number of observations of this class.", "acronym": "DD", "label": "dependent", "ID": "7419"}, {"sentence": "Given a particular choice of the antecedent  candidates, the distance is inDD of  distances of candidates other than the an-  tecedent (and the distance to non-referents  can be ignored):  P(so, d~a, 2~) o?", "acronym": "DD", "label": "dependent", "ID": "7420"}, {"sentence": "It is reason-  able to assume that the antecedents in W are  inDD of each other; in other words,  P(wo+llwo, h,t , l ,a) = P(wo+llh, t,l,a}.", "acronym": "DD", "label": "dependent", "ID": "7421"}, {"sentence": "We assume the selection of the  pronoun is inDD of the candidates  other than the antecedent.", "acronym": "DD", "label": "dependent", "ID": "7422"}, {"sentence": "The syntnctic structure st, and the distance  from the pronoun da are inDD of the  number of times the referent is mentioned.", "acronym": "DD", "label": "dependent", "ID": "7423"}, {"sentence": "The technique of DD has recently been shown to yield state-of-the-art perfor- mance in dependency parsing (Koo et al, 2010).", "acronym": "DD", "label": "dual decomposition", "ID": "7424"}, {"sentence": "3.2 Dual Problem Formulation To describe a DD inference proce- dure for our model, we first restate the inference problem under our graphical model in terms of the two overlapping subgraphs that admit tractable in- ference.", "acronym": "DD", "label": "dual decomposition", "ID": "7425"}, {"sentence": "Inference can be per- formed via DD, which reuses the efficient inference algorithms of the direc- tional models.", "acronym": "DD", "label": "dual decomposition", "ID": "7426"}, {"sentence": "The DD inference approach al- lows us to exploit this sub-graph structure (Rush et al.,", "acronym": "DD", "label": "dual decomposition", "ID": "7427"}, {"sentence": "On DD and linear programming relaxations for natural language processing.", "acronym": "DD", "label": "dual decomposition", "ID": "7428"}, {"sentence": "A comparison of loopy belief propagation and DD for integrated CCG supertagging and parsing.", "acronym": "DD", "label": "dual decomposition", "ID": "7429"}, {"sentence": "The technique of DD has recently been shown to yield state-of-the-art perfor- mance in dependency parsing (Koo et al, 20", "acronym": "DD", "label": "dual decomposition", "ID": "7430"}, {"sentence": "Figure 3 also includes three DIM (the 310 is _n ot (n n) is _n ot (n p) do es _n ot (n n) do es _n ot (n p) n o t(n n) n o t(n p) do _n ot (n n) do _n ot (n p) n o (n n) n o (n p) 0.15 0.20 0.25 0.30 Figure 4: The behavior of individual negators in negated negative (nn) and negated positive (np) context.", "acronym": "DIM", "label": "diminishers", "ID": "7431"}, {"sentence": "no reason at all to believe is irrealis, for example); word sense (e.g., Environmental Trust versus He has won the peo- ple?s trust); the syntactic role of a word in the sen- tence (e.g., polluters are versus they are polluters); and DIM such as little (e.g., little truth, lit- tle threat). (", "acronym": "DIM", "label": "diminishers", "ID": "7432"}, {"sentence": "DIM (the 310 is _n ot (n n) is _n ot (n p) do es _n ot (n n) do es _n ot (n p) n o t(n n) n o t(n p) do _n ot (n n) do _n ot (n p) n o (n n) n o (n p) 0.15 0.20 0.25 0.30 Figure 4: The behavior of individual negators in negated negative (nn) and negated positive (np) context.", "acronym": "DIM", "label": "diminishers", "ID": "7433"}, {"sentence": "This shows that the boundary between negators and DIM can by fuzzy.", "acronym": "DIM", "label": "diminishers", "ID": "7434"}, {"sentence": "By following (Kennedy and Inkpen, 2006), we ex- tracted 319 DIM (also called understate- ment or downtoners) from General Inquirer3.", "acronym": "DIM", "label": "diminishers", "ID": "7435"}, {"sentence": "Based on a shallow error analysis, we believe that including additional classification features may also be promising: modifiers other than nega- tion cues (DIM, increasers, modal verbs, etc.)", "acronym": "DIM", "label": "diminishers", "ID": "7436"}, {"sentence": "Bi- grams are used especially to spot the influence of modifiers (negations, intensifiers, DIM) on the polarity of the sentiment-bearing words.", "acronym": "DIM", "label": "diminishers", "ID": "7437"}, {"sentence": "It can also be found in Hebrew as a DIM formation of nouns and adjectives: keleb klablab $apan $panpan zaqan zqanqan $axor $xarxar dog puppy rabbit bunny beard goatee black dark qatan qtantan little tiny Let ?", "acronym": "DIM", "label": "diminutive", "ID": "7438"}, {"sentence": "The computational  grammar writer reads the description and tries to  implement it, but a question arises: is the  DIM qualifier used in all the environments  that the three allomorphs of the non-DIM  qualifier are used, or only one of those  environments?", "acronym": "DIM", "label": "diminutive", "ID": "7439"}, {"sentence": "In agreement with this pattern, several polite verb forms (-masu, -mashi) and a polite honorific (o-) are among the k-top female words, as is a DIM honorific often used to refer to women (-chan).", "acronym": "DIM", "label": "diminutive", "ID": "7440"}, {"sentence": "The language expert finds examples  showing the DIM in all environments,  enabling the computational grammar writer to  proceed.", "acronym": "DIM", "label": "diminutive", "ID": "7441"}, {"sentence": "This list allows the system to consider the most frequent synonyms and DIMs.", "acronym": "DIM", "label": "diminutive", "ID": "7442"}, {"sentence": "The instances are taken from a corpus of PDN.", "acronym": "PDN", "label": "People?s Daily News", "ID": "7443"}, {"sentence": "It was obtained from the  PDNpaper from 01/1991 to  12/1993 and from the Xinhua News Agency for  04/1994 to 09/1995 from the Linguistic Data  Consortium (http://www.ldc.upenn.edu).", "acronym": "PDN", "label": "People?s Daily News", "ID": "7444"}, {"sentence": "Based on the 20 words, we extracted 28,000  sentences from the 60 MB PDN  with segmentation information as our train- ing/test set which is then manually sense-tagged.", "acronym": "PDN", "label": "People?s Daily News", "ID": "7445"}, {"sentence": "The collocation list is constructed from a  combination of a digital collocation dictionary, a  return result from a collocation automatic ex- traction system [21], and a hand collection from  the PDN.", "acronym": "PDN", "label": "People?s Daily News", "ID": "7446"}, {"sentence": "These features are  extracted form the 60MB human sense-tagged  PDN with segmentation infor- mation.", "acronym": "PDN", "label": "People?s Daily News", "ID": "7447"}, {"sentence": "A useful result  from this work based on (about one million  words) the tagged PDN shows  that adding more features from richer levels of  linguistic information such as PoS tagging  yielded no significant improvement (less than  1%) over using only the bi-gram co-occurrences  information.", "acronym": "PDN", "label": "People?s Daily News", "ID": "7448"}, {"sentence": "At the core of the FERRET?s predictive dialogue module is the PDN (PQN), a large database of QUABs that were either generated off-line by human annotators or created on-line by FERRET (either during the current dialogue or dur- ing some previous dialogue)1.", "acronym": "PDN", "label": "Predictive Dialogue Network", "ID": "7449"}, {"sentence": "In Proceedings of 10th Workshop on ALRs, at 24th International Conference on Computational Linguis- tics (COLING 2012).", "acronym": "ALR", "label": "Asian Language Resource", "ID": "7450"}, {"sentence": "300  Proceedings of the 7th Workshop on ALRs, ACL-IJCNLP 2009, pages 17?23, Suntec, Singapore, 6-7 August 2009.", "acronym": "ALR", "label": "Asian Language Resource", "ID": "7451"}, {"sentence": "In Proceedings of the 3rd Work- shop on ALRs and International Standardization, COLING 19, Taipei, Taiwan.", "acronym": "ALR", "label": "Asian Language Resource", "ID": "7452"}, {"sentence": "352  Proceedings of the 8th Workshop on ALRs, pages 169?177, Beijing, China, 21-22 August 2010.", "acronym": "ALR", "label": "Asian Language Resource", "ID": "7453"}, {"sentence": "In In Proceedings of the Fourth Workshop on ALRs, Sanya, China, pp.", "acronym": "ALR", "label": "Asian Language Resource", "ID": "7454"}, {"sentence": "In Proceedings of the 3rd Workshop on ALRs and In- ternational Standardization at the 19th International Conference on Computational Linguistics, Vol.", "acronym": "ALR", "label": "Asian Language Resource", "ID": "7455"}, {"sentence": "In Proceedings of 10th Workshop on ALR, at 24th International Conference on Computational Linguis- tics (COLING 2012).", "acronym": "ALR", "label": "Asian Language Resources", "ID": "7456"}, {"sentence": "300  Proceedings of the 7th Workshop on ALR, ACL-IJCNLP 2009, pages 17?23, Suntec, Singapore, 6-7 August 2009.", "acronym": "ALR", "label": "Asian Language Resources", "ID": "7457"}, {"sentence": "In Proceedings of the 3rd Work- shop on ALR and International Standardization, COLING 19, Taipei, Taiwan.", "acronym": "ALR", "label": "Asian Language Resources", "ID": "7458"}, {"sentence": "352  Proceedings of the 8th Workshop on ALR, pages 169?177, Beijing, China, 21-22 August 2010.", "acronym": "ALR", "label": "Asian Language Resources", "ID": "7459"}, {"sentence": "In In Proceedings of the Fourth Workshop on ALR, Sanya, China, pp.", "acronym": "ALR", "label": "Asian Language Resources", "ID": "7460"}, {"sentence": "In Proceedings of the 3rd Workshop on ALR and In- ternational Standardization at the 19th International Conference on Computational Linguistics, Vol.", "acronym": "ALR", "label": "Asian Language Resources", "ID": "7461"}, {"sentence": "This document is organized as follows: sec- tion 2 briefly introduces the related work; section 3 presents a characterization of the STT summarization problem and how we propose to address it; section 4 explicits our use of phonetic domain information, given the previously defined context; the next section describes the case study, including the experimental set up and results; con- clusions close the document.", "acronym": "STT", "label": "speech-to-text", "ID": "7462"}, {"sentence": "5.4 Experimental Results Our main objective was to understand if it is pos- sible to select relevant information from back- ground information that could improve the quality of STT summaries.", "acronym": "STT", "label": "speech-to-text", "ID": "7463"}, {"sentence": "The main idea is the inclusion of related, solid background information to cope with the difficulties of summarizing spoken lan- guage and the use of multi-document summariza- tion techniques in single document STT summarization.", "acronym": "STT", "label": "speech-to-text", "ID": "7464"}, {"sentence": "The fisher corpus: a resource for the next generations of STT.", "acronym": "STT", "label": "speech-to-text", "ID": "7465"}, {"sentence": "To support this argument, we de- veloped a new approach to STT summa- rization that combines information from multiple information sources to produce a summary driven by the spoken language document to be summa- rized.", "acronym": "STT", "label": "speech-to-text", "ID": "7466"}, {"sentence": "For the ma- jority of these events, physicians and nurses generate free text data either by typing the information them- selves or by using a local or remote STT engine.", "acronym": "STT", "label": "speech-to-text", "ID": "7467"}, {"sentence": "Indeed standard Language Models (LMs) ap- plied to OOD utterances are likely to generate erro- neous STT and more gener- ally highly noisy word lattices from which it might not be relevant and probably harmful to apply SLU modules.", "acronym": "STT", "label": "speech recognition outputs", "ID": "7468"}, {"sentence": "ation for Computational Linguistics Punctuation Prediction with Transition-based Parsing  Dongdong Zhang1, Shuangzhi Wu2, Nan Yang3, Mu Li1    1Microsoft Research Asia, Beijing, China  2Harbin Institute of Technology, Harbin, China  3University of Science and Technology of China, Hefei, China  {dozhang,v-shuawu,v-nayang,muli}@microsoft.com    Abstract  Punctuations are not available in automatic  STT, which could cre- ate barriers to many subsequent text pro- cessing tasks.", "acronym": "STT", "label": "speech recognition outputs", "ID": "7469"}, {"sentence": "To test this explanation, we looked at the word error rates for the STT for the different systems.", "acronym": "STT", "label": "speech recognition outputs", "ID": "7470"}, {"sentence": "The alignment of STT is fairly straightforward due to the strict constraint in word order.", "acronym": "STT", "label": "speech recognition outputs", "ID": "7471"}, {"sentence": "The method (Izumi et al, 2003) aims to de- tect omission-type and replacement-type errors and transformation-based leaning is employed in (Shi and Zhou, 2005) to learn rules to detect errors for STT.", "acronym": "STT", "label": "speech recognition outputs", "ID": "7472"}, {"sentence": "If this is so then it is possible that a relatively ac-  cepting natural anguage system might work wen with worse  STT (because ven a relatively accept-  ing natural language system can reject very had inputs), but  with better speech recognizer output one might get good per-  formance with a stricter natural language system.", "acronym": "STT", "label": "speech recognition outputs", "ID": "7473"}, {"sentence": "Proceedings of the ACM International Conference on  WSDM.", "acronym": "WSDM", "label": "Web Search and Web Data Mining", "ID": "7474"}, {"sentence": "In Proceedings of the International Conference on  WSDM, pages 231-240.", "acronym": "WSDM", "label": "Web Search and Web Data Mining", "ID": "7475"}, {"sentence": "In Proceedings of the international conference on WSDM, pages 171?182, Palo Alto, CA, USA.", "acronym": "WSDM", "label": "Web Search and Web Data Mining", "ID": "7476"}, {"sentence": "Proceedings of the ACM International Conference  on WSDM.", "acronym": "WSDM", "label": "Web Search and Web Data Mining", "ID": "7477"}, {"sentence": "In Pro- ceedings of the Second ACM International Conference on WSDM, pages 54?63.", "acronym": "WSDM", "label": "Web Search and Data Mining", "ID": "7478"}, {"sentence": "In Pro- ceedings of the Second ACM International Conference on WSDM, pages 94?103.", "acronym": "WSDM", "label": "Web Search and Data Mining", "ID": "7479"}, {"sentence": "In Proceed- ings of the Second ACM International Conference  on WSDM, Barcelona, Spain,  ACM.", "acronym": "WSDM", "label": "Web Search and Data Mining", "ID": "7480"}, {"sentence": "In Proceed- ings of WSDM.", "acronym": "WSDM", "label": "Web Search and Data Mining", "ID": "7481"}, {"sentence": "In Proceedings of the Second ACM International Con- ference on WSDM, Barcelona, Spain, February.", "acronym": "WSDM", "label": "Web Search and Data Mining", "ID": "7482"}, {"sentence": "In Proceedings of First ACM International Conference on WSDM.", "acronym": "WSDM", "label": "Web Search and Data Mining", "ID": "7483"}, {"sentence": "Implicit argument identification for nominal predicates is complementary to VPE r", "acronym": "VPE", "label": "verb phrase ellipsis", "ID": "7484"}, {"sentence": "Implicit argument identification for nominal predicates is complementary to VPE resolution: Both work to make implicit information explicit.", "acronym": "VPE", "label": "verb phrase ellipsis", "ID": "7485"}, {"sentence": "Dynamic interpretation of VPE.", "acronym": "VPE", "label": "verb phrase ellipsis", "ID": "7486"}, {"sentence": "We also implement simple heuristics that allow us to capture simple cases of control and VPE in many cases.", "acronym": "VPE", "label": "verb phrase ellipsis", "ID": "7487"}, {"sentence": "Webber considers three types of antecedents  those for definite pronouns, those for  one-anaphora, 13 and those for VPE.", "acronym": "VPE", "label": "verb phrase ellipsis", "ID": "7488"}, {"sentence": "Exploring the steps of VPE.", "acronym": "VPE", "label": "verb phrase ellipsis", "ID": "7489"}, {"sentence": "If one resolved the VPE, then the implicit agent (Bill) would be recovered.4 Nielsen (2004) created a system able to detect the presence of ellipses, producing the bracketing in Example (17).", "acronym": "VPE", "label": "verb phrase ellipsis", "ID": "7490"}, {"sentence": "A corpus-based study of VPE Identification and Resolution.", "acronym": "VPE", "label": "Verb Phrase Ellipsis", "ID": "7491"}, {"sentence": "887  Subdeletion in VPE  Paul G. Donecker  Villanova University  800 Lancaster Avenue  Villanova, PA 19085  donecker@monet.vill.edu  Abstract  This paper stems from an ongoing research  project ~on verb phrase llipsis.", "acronym": "VPE", "label": "Verb Phrase Ellipsis", "ID": "7492"}, {"sentence": "VPE:  Form, Meaning, and Processing.", "acronym": "VPE", "label": "Verb Phrase Ellipsis", "ID": "7493"}, {"sentence": "\\[Stainton-Ellis 1988\\] Stainton-Ellis, C.S.:1988, A  processing perspective on VPE,  MPhil dissertation, University of Edinburgh.", "acronym": "VPE", "label": "Verb Phrase Ellipsis", "ID": "7494"}, {"sentence": "These include verbs of becoming or seeming (e.g., trans- form, appear), light verbs, AUX, and aspec- tual verbs.", "acronym": "AUX", "label": "auxiliaries", "ID": "7495"}, {"sentence": "The inlransitive verbs.are  claksified accor .ding to semantic criteria (verbs of motion~ state) or  by their syntactic usa~ (like predicativi~ AUX, urgpers6nal  verbs with dative ).W'e should-notice that he same verb maybe  transitiv 9 or intransitive, accor \"ding to its m e.", "acronym": "AUX", "label": "auxiliaries", "ID": "7496"}, {"sentence": "The general left-branching structure of the tree is a result of the analysis of the second-position clitic cluster: The clitic clusters are treated as argument-composition AUX, which combine with a lexical verb and ?", "acronym": "AUX", "label": "auxiliaries", "ID": "7497"}, {"sentence": "For the gold input, apart from LEMMA that pro- vides around 0.7 points, the most useful feature is 4NORK2, NOR1 and NMG are AUX case markers.", "acronym": "AUX", "label": "auxiliaries", "ID": "7498"}, {"sentence": "The AUX first pick up all dependents to the right, and then combine with exactly one constituent to the left.", "acronym": "AUX", "label": "auxiliaries", "ID": "7499"}, {"sentence": "The  light verbs themselves do not diminish in form  over time in a manner similar to AUX (Butt,  2004), although the complements of common  LVCs can change over time such that it is no  longer clear that the complement is a predicating  element.", "acronym": "AUX", "label": "auxiliaries", "ID": "7500"}, {"sentence": "Other constraints require an AUX verb to be modified by a full verb, or prescribe morphosyntactical agreement between a determiner and its regent (the word modified by the determiner).", "acronym": "AUX", "label": "auxiliary", "ID": "7501"}, {"sentence": "There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, AUX verbs used in one lan- guage but not the other (e.g., English He walked and French Il est alle?),", "acronym": "AUX", "label": "auxiliary", "ID": "7502"}, {"sentence": "Perhaps the most striking feature of Wambaya is its word order: it is a radically non-configurational language with a second position AUX/clitic clus- ter.", "acronym": "AUX", "label": "auxiliary", "ID": "7503"}, {"sentence": "in the surface structure, as follows: The AUX ngiya is subject to the constraints in (2), meaning that it combines with a verb as its first complement and then the verb?s complements as its remaining complements.9 The AUX can combine with its complements in any order, thanks to a series of head- complement rules which realize the nth element of 6The grammar in fact finds 42 parses for this example.", "acronym": "AUX", "label": "auxiliary", "ID": "7504"}, {"sentence": "For in- stance, the finite and the full verb must com- bine to form an AUX phrase, because this is the only way of accounting for all words while satisfying valence and category con- straints.", "acronym": "AUX", "label": "auxiliary", "ID": "7505"}, {"sentence": "Technical Report RS-87-190, USC/ISI, 1987.", "acronym": "ISI", "label": "Information Sciences Institute", "ID": "7506"}, {"sentence": "c?2006 Association for Computational Linguistics A Better -Best List: Practical Determinization of Weighted Finite Tree Automata Jonathan May ISI University of Southern California Marina del Rey, CA 90292 jonmay@isi.edu Kevin Knight ISI University of Southern California Marina del Rey, CA 90292 knight@isi.edu Abstract Ranked lists of output trees from syn- tactic statistical NLP applications fre- quently contain multiple repeated entries.", "acronym": "ISI", "label": "Information Sciences Institute", "ID": "7507"}, {"sentence": "c?2010 Association for Computational Linguistics Efficient Incremental Decoding for Tree-to-String Translation Liang Huang 1 1ISI University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292, USA {lhuang,haitaomi}@isi.edu Haitao Mi 2,1 2Key Lab.", "acronym": "ISI", "label": "Information Sciences Institute", "ID": "7508"}, {"sentence": "Available from  USC/ISI, Marina del  Rey, CA.", "acronym": "ISI", "label": "Information Sciences Institute", "ID": "7509"}, {"sentence": "In Defense  of  Syntax:   In fo rmat iona l ,  In tent iona l ,  and Rhetor i ca l  S t ruc tures   in D iscourse   Eduard H. Hovy  ISI  of the University of Southern California  4676 Admiralty Way  Marina del Rey, CA 90292-6695  U.S.A.  tel: 310-822-1511  fax: 310-823-6714  email: hovy@isi.edu  Introduction: The Point of this Paper  Much has been written on the nature and use of so-called rhetorical relations to govern the structure anti  coherence of discourse, and much has been written on the need", "acronym": "ISI", "label": "Information Sciences Institute", "ID": "7510"}, {"sentence": "Bul- letin of the ISI:92?94,  1969  G. Savova, T. Pedersen, A. Kulkarni and A. Puran- dare.", "acronym": "ISI", "label": "International Statistical Institute", "ID": "7511"}, {"sentence": "We map 51 different relations to the corpus and result in about 50,000 entity tuples, 134,000 sentences for training and 30,000 entity tuples, 53,000 STe.", "acronym": "STe", "label": "sentences for testing", "ID": "7512"}, {"sentence": "Out of which, 14500 sentences were taken as training set, 500 for development set and remaining 1000 STe set.", "acronym": "STe", "label": "sentences for testing", "ID": "7513"}, {"sentence": "It means we used 1,800 and 7,200 STe and training the discriminative sequence learning models, respectively.", "acronym": "STe", "label": "sentences for testing", "ID": "7514"}, {"sentence": "For Syntag, we split the treebank into 3,524 sen- tences for training and 1,763 STe.", "acronym": "STe", "label": "sentences for testing", "ID": "7515"}, {"sentence": "The first is 700 STe and the  other is for training.", "acronym": "STe", "label": "sentences for testing", "ID": "7516"}, {"sentence": "Using five-fold validation (i.e., chose different 100  STe each time and repeating the ex-  periment five times), The program achieved an aver-  age success rate of 81.3%.", "acronym": "STe", "label": "sentences for testing", "ID": "7517"}, {"sentence": "The data used for empirical evaluation was taken from (Roth and Yih, 2004) and consists of 1436 sen- tences, which is split into a 1149 (80%) sentence training set and a 287 (20%) STe set such that all have at least one active relation.", "acronym": "STe", "label": "sentence testing", "ID": "7518"}, {"sentence": "c?2016 Association for Computational Linguistics Generalizing and Hybridizing Count-based and NLMs Graham Neubig?", "acronym": "NLM", "label": "Neural Language Model", "ID": "7519"}, {"sentence": "Decoding with Large- scale NLMs improves Transla- tion.", "acronym": "NLM", "label": "Neural Language Model", "ID": "7520"}, {"sentence": "c?2006 Association for Computational Linguistics Factored NLMs Andrei Alexandrescu Department of Comp.", "acronym": "NLM", "label": "Neural Language Model", "ID": "7521"}, {"sentence": "2.1 Softmax NLM Our feed-forward neural network implements an n-gram language model, i.e., it is a parametric function estimating the probability of the next word w t given n ?", "acronym": "NLM", "label": "Neural Language Model", "ID": "7522"}, {"sentence": "Decoding with Large-scale NLMs improves Translation.", "acronym": "NLM", "label": "Neural Language Model", "ID": "7523"}, {"sentence": "c?2016 Association for Computational Linguistics Strategies for Training Large Vocabulary NLMs Wenlin Chen David Grangier Michael Auli Facebook, Menlo Park, CA Abstract Training neural network language mod- els over large vocabularies is computa- tionally costly compared to count-based models such as Kneser-Ney.", "acronym": "NLM", "label": "Neural Language Model", "ID": "7524"}, {"sentence": "Recent work (Lebret and Lebret, 2013) has shown that the Hellinger distance is an especially effective mea- sure in learning distributional embeddings, with Hellinger PCA being much more computationally inexpensive than NLMing ap- proaches, while performing much better than stan- dard PCA, and competitive with the state-of-the- art in downstream evaluations.", "acronym": "NLM", "label": "neural language model", "ID": "7525"}, {"sentence": "5 Experiments To evaluate PLRE, we compared its performance on English and Russian corpora with several vari- 2 for derivation see proof of Lemma 4 in the supplemen- tary material 1493 ants of KN smoothing, class-based models, and the log-bilinear NLM (Mnih and Hinton, 2007).", "acronym": "NLM", "label": "neural language model", "ID": "7526"}, {"sentence": "2.1 Matrix factorization view of SkipGram SkipGram can be categorized as one of the simplest NLMs (Mnih and Kavukcuoglu, 2013).", "acronym": "NLM", "label": "neural language model", "ID": "7527"}, {"sentence": "Decoding with large- scale NLMs improves translation.", "acronym": "NLM", "label": "neural language model", "ID": "7528"}, {"sentence": "In computing these probabilities, the state ht?1 represents the tar- get history, and h0 is typically set to be some func- tion of x. The complete model (including encoder) is trained, analogously to a NLM, to minimize the cross-entropy loss at each time-step while conditioning on the gold history in the train- ing data.", "acronym": "NLM", "label": "neural language model", "ID": "7529"}, {"sentence": "Unifying visual-semantic embeddings with multimodal NLMs.", "acronym": "NLM", "label": "neural language model", "ID": "7530"}, {"sentence": "c?2016 Association for Computational Linguistics CPM in ASL Syntactic Facial Expression Synthesis Hernisa Kacorri Carnegie Mellon University Human-Computer Interaction Institute 5000 Forbes Avenue Pittsburgh, PA 15213, USA hkacorri@andrew.cmu.edu Matt Huenerfauth Rochester Institute of Technology B. Thomas Golisano College of Computing and Information Sciences 152 Lomb Memorial Drive Rochester, NY 14623, USA matt.huenerfaut", "acronym": "CPM", "label": "Continuous Profile Models", "ID": "7531"}, {"sentence": "c?2016 Association for Computational Linguistics CPMs in ASL Syntactic Facial Expression Synthesis Hernisa Kacorri Carnegie Mellon University Human-Computer Interaction Institute 5000 Forbes Avenue Pittsburgh, PA 15213, USA hkacorri@andrew.cmu.edu Matt Huenerfauth Rochester Institute of Technology B. Thomas Golisano College of Computing and Information Sciences 152 Lomb Memorial Drive Rochester, NY 14623, USA matt.huenerfaut", "acronym": "CPM", "label": "Continuous Profile Model", "ID": "7532"}]