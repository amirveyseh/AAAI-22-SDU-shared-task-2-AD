[
  {
    "sentence": "An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System: a RL Approach.",
    "acronym": "RL",
    "label": "Reinforcement Learning"
  },
  {
    "sentence": "RL: An Introduction.",
    "acronym": "RL",
    "label": "Reinforcement Learning"
  },
  {
    "sentence": "RL.",
    "acronym": "RL",
    "label": "Reinforcement Learning"
  },
  {
    "sentence": "2016 Association for Computational Linguistics Deep RL with a Natural Language Action Space Ji He ?",
    "acronym": "RL",
    "label": "Reinforcement Learning"
  },
  {
    "sentence": "RL for Dialog Management Using Least-Squares Policy Iteration and Fast Fea-ture Selection.",
    "acronym": "RL",
    "label": "Reinforcement Learning"
  },
  {
    "sentence": "Optimizing Dialogue Management with RL: Experiments with the NJFun System.",
    "acronym": "RL",
    "label": "Reinforcement Learning"
  },
  {
    "sentence": "prep of(S, object word) relations, recovering RL word ?",
    "acronym": "RL",
    "label": "rel loc"
  },
  {
    "sentence": "252 J?rgensen and L?nning Minimal Recursion Semantic Analysis of Locatives prep relation mode RL rel id rel . . .",
    "acronym": "RL",
    "label": "rel loc"
  },
  {
    "sentence": "We pre-define a dictionary of attribute-values (color word, size word, abs location word, RLation word) for each of the attributes based on the observed data using a combination of POS-tagging and manual labeling.",
    "acronym": "RL",
    "label": "rel loc"
  },
  {
    "sentence": "Re- cently, inspired by advances in deep learning (Le- Cun et al, 2015; Hinton et al, 2012; Krizhevsky et al, 2012; Dahl et al, 2012), significant progress has been made by combining deep learning with RL.",
    "acronym": "RL",
    "label": "reinforcement learning"
  },
  {
    "sentence": "In some applications, it is possible to manually design fea- tures for state-action pairs, which are then used in RL to learn a near-optimal policy (Li et al, 2009).",
    "acronym": "RL",
    "label": "reinforcement learning"
  },
  {
    "sentence": "Because a player?s action changes the environment, RL (Sutton and Barto, 1998) is appropriate for modeling long- term dependency in text games.",
    "acronym": "RL",
    "label": "reinforcement learning"
  },
  {
    "sentence": "In language processing, RL has been ap- plied to a dialogue management system that con- verses with a human user by taking actions that generate natural language (Scheffler and Young, 2002; Young et al, 2013).",
    "acronym": "RL",
    "label": "reinforcement learning"
  },
  {
    "sentence": "4 Related Work There has been increasing interest in applying deep RL to a variety problems, but only a few studies address problems with nat- ural language state or action spaces.",
    "acronym": "RL",
    "label": "reinforcement learning"
  },
  {
    "sentence": "Microsoft Research, Redmond, WA 98052, USA {jianshuc, xiaohe, jfgao, lihongli, deng}@microsoft.com Abstract This paper introduces a novel architec- ture for RL with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games.",
    "acronym": "RL",
    "label": "reinforcement learning"
  },
  {
    "sentence": "Thus the Dative DOt construc-  tion is stated as a construction i the grammar whose combination with  specific lexieal items comes up only in use.",
    "acronym": "DO",
    "label": "Direct Objec"
  },
  {
    "sentence": "So, in relation to the verb afelippen (to cut off) we find -  among others- DOt and Means.",
    "acronym": "DO",
    "label": "Direct Objec"
  },
  {
    "sentence": "Clause  Head: Send  Indire~-I Object: Noun Phrase  Head: Him  DOt Noun Phrase  Head: Message  Article: The  Relative: Clause  Head: Arrive  Subject: That  Time: Yesterday  Figure 1: Syntactic Analysis of \"Send him the message that  arrived yesterday.\".",
    "acronym": "DO",
    "label": "Direct Objec"
  },
  {
    "sentence": "The semantic onstraints on when a Dative  DOt can be used can not be expressed in the lexicon - the  difference between (12) and (13) depends on a constmal of the entire  utterance conceptualization.",
    "acronym": "DO",
    "label": "Direct Objec"
  },
  {
    "sentence": "DOt NPs and Indirect Object NPs are all untagged? (",
    "acronym": "DO",
    "label": "Direct Objec"
  },
  {
    "sentence": "In the above example, the preposition met (with) is an  indicator for Means and it is the absence of a preposition  which points to the DOt.",
    "acronym": "DO",
    "label": "Direct Objec"
  },
  {
    "sentence": "SYNTACTIC FORM:  - Realize the Goal as the DOt  - Realize the Theme as the Second Object.",
    "acronym": "DO",
    "label": "Direct Objec"
  },
  {
    "sentence": "Thus the Dative DO construc-  tion is stated as a construction i the grammar whose combination with  specific lexieal items comes up only in use.",
    "acronym": "DO",
    "label": "Direct Object"
  },
  {
    "sentence": "So, in relation to the verb afelippen (to cut off) we find -  among others- DO and Means.",
    "acronym": "DO",
    "label": "Direct Object"
  },
  {
    "sentence": "Clause  Head: Send  Indire~-I Object: Noun Phrase  Head: Him  DO Noun Phrase  Head: Message  Article: The  Relative: Clause  Head: Arrive  Subject: That  Time: Yesterday  Figure 1: Syntactic Analysis of \"Send him the message that  arrived yesterday.\".",
    "acronym": "DO",
    "label": "Direct Object"
  },
  {
    "sentence": "The semantic onstraints on when a Dative  DO can be used can not be expressed in the lexicon - the  difference between (12) and (13) depends on a constmal of the entire  utterance conceptualization.",
    "acronym": "DO",
    "label": "Direct Object"
  },
  {
    "sentence": "DO NPs and Indirect Object NPs are all untagged? (",
    "acronym": "DO",
    "label": "Direct Object"
  },
  {
    "sentence": "In the above example, the preposition met (with) is an  indicator for Means and it is the absence of a preposition  which points to the DO.",
    "acronym": "DO",
    "label": "Direct Object"
  },
  {
    "sentence": "SYNTACTIC FORM:  - Realize the Goal as the DO  - Realize the Theme as the Second Object.",
    "acronym": "DO",
    "label": "Direct Object"
  },
  {
    "sentence": "SVMs have been used for text classification (Tong and Koller, 2002), using properties of the support vector ma- chine algorithm for determining what unlabelled data to select for classification.",
    "acronym": "SVMs",
    "label": "Support vector machines"
  },
  {
    "sentence": "SVMs (SVM, Cortes and Vapnik (1995)) have gained a lot of popularity in the past decade and very often are state-of-the-art approach for text mining challenges.",
    "acronym": "SVMs",
    "label": "Support vector machines"
  },
  {
    "sentence": "SVMs are powerful statisti- cal classifiers, as employed in the ?",
    "acronym": "SVMs",
    "label": "Support vector machines"
  },
  {
    "sentence": "2.6 Support Vector Machine SVMs (SVM) are statistical classifiers which use labelled training data to pre- dict the class of unseen inputs.",
    "acronym": "SVMs",
    "label": "Support vector machines"
  },
  {
    "sentence": "At  this  point  the  focus switches over to the tool itself, which learns  regular  patterns  using  SVMs  and then uses the information gathered to tag any  possible list of words  (Figure 1, Line 5).",
    "acronym": "SVMs",
    "label": "Support  Vector Machines"
  },
  {
    "sentence": "Decisions were  made  by  an  annotator  with  a  well-grounded  knowledge of SVMs and their  behaviour,  which  turned  out  to  be  quite  useful  when deciding which output should be classified as  ?",
    "acronym": "SVMs",
    "label": "Support  Vector Machines"
  },
  {
    "sentence": "The fragments  also consisted of parsed ones: terse question,  TR and definitions, and nonparsed ones:  false starts and phatics.",
    "acronym": "TR",
    "label": "terse reply"
  },
  {
    "sentence": "Of particular interest in the  analysis is the notion of a terse question, which is a  type of telegraphic ellipsis, and a TR, which is  a type of contextual ellipsis.",
    "acronym": "TR",
    "label": "terse reply"
  },
  {
    "sentence": "To illustrate concretely, suppose our search state is: (some felines have tails, valid) 538 Transition TR WordNet hypernym v WordNet hyponym w WordNet antonym ?",
    "acronym": "TR",
    "label": "Template Relation"
  },
  {
    "sentence": "2 Related Work  The relation extraction task was first introduced as  part of the Template Element task in MUC6 and then  formulated as the TR task in MUC7.",
    "acronym": "TR",
    "label": "Template Relation"
  },
  {
    "sentence": "The reports are ordered by task (Template Element, TR, Scenario Template, Named Entity, and Coreference) and secondarily by site and language (in alphabetical order).",
    "acronym": "TR",
    "label": "Template Relation"
  },
  {
    "sentence": "While the  MUC TR and Scenario Template  tasks targeted relations and events plus their attrib- utes, the focus of these tasks was domain specific.",
    "acronym": "TR",
    "label": "Template Relation"
  },
  {
    "sentence": "4.5 Base Phrase Chunking  It is well known that chunking plays a critical role  in the TR task of the 7th Message  Understanding Conference (MUC-7 1998).",
    "acronym": "TR",
    "label": "Template Relation"
  },
  {
    "sentence": "2 Related Work  The task of relation extraction was introduced as a  part of the Template Element task in MUC6 and  formulated as the TR task in MUC7  (MUC, 1987-1998).",
    "acronym": "TR",
    "label": "Template Relation"
  },
  {
    "sentence": "1 Introduction TR aims to search for the most probable translation candidate from a set of target- language strings for a given source-language string.",
    "acronym": "TR",
    "label": "Translation retrieval"
  },
  {
    "sentence": "Institute for Interdisciplinary Information Sciences Tsinghua University, Beijing, China chengyong3001@gmail.com, xu@tsinghua.edu.cn \u0005 Toshiba Corporation Corporate Research & Development Center tatsuya.izuha@toshiba.co.jp # Toshiba (China) R&D Center haojie@toshiba.com.cn Abstract TR aims to find the most likely translation among a set of target-language strings for a given source-language string.",
    "acronym": "TR",
    "label": "Translation retrieval"
  },
  {
    "sentence": "To illustrate concretely, suppose our search state is: (some felines have tails, valid) 538 Transition TRn WordNet hypernym v WordNet hyponym w WordNet antonym ?",
    "acronym": "TR",
    "label": "Template Relatio"
  },
  {
    "sentence": "2 Related Work  The relation extraction task was first introduced as  part of the Template Element task in MUC6 and then  formulated as the TRn task in MUC7.",
    "acronym": "TR",
    "label": "Template Relatio"
  },
  {
    "sentence": "The reports are ordered by task (Template Element, TRn, Scenario Template, Named Entity, and Coreference) and secondarily by site and language (in alphabetical order).",
    "acronym": "TR",
    "label": "Template Relatio"
  },
  {
    "sentence": "While the  MUC TRn and Scenario Template  tasks targeted relations and events plus their attrib- utes, the focus of these tasks was domain specific.",
    "acronym": "TR",
    "label": "Template Relatio"
  },
  {
    "sentence": "4.5 Base Phrase Chunking  It is well known that chunking plays a critical role  in the TRn task of the 7th Message  Understanding Conference (MUC-7 1998).",
    "acronym": "TR",
    "label": "Template Relatio"
  },
  {
    "sentence": "2 Related Work  The task of relation extraction was introduced as a  part of the Template Element task in MUC6 and  formulated as the TRn task in MUC7  (MUC, 1987-1998).",
    "acronym": "TR",
    "label": "Template Relatio"
  },
  {
    "sentence": "In this paper, we use Chinese  Propbank 1.0 provided by Linguistic Data Consor- tium (LDC), which is based on CTB.",
    "acronym": "CTB",
    "label": "Chinese Treebank"
  },
  {
    "sentence": "This was effective  because of the properties of Chinese: First, there is  no multi-root in CTB.",
    "acronym": "CTB",
    "label": "Chinese Treebank"
  },
  {
    "sentence": "5 Experiment and discussion  This section will describe the experiment on the  SRL in CTB, compare TSVM with  regular SVM, and evaluate the effect of the pro- posed argument-specific heuristics.",
    "acronym": "CTB",
    "label": "Chinese Treebank"
  },
  {
    "sentence": "It consists of 37,183 propositions indexed to the                                                    1 F1 measure computes the harmonic mean of precision  and recall of SRL systems in CoNLL-2005  first 250k words in CTB 5.1, includ- ing 4,865 verb types and 5,298 framesets.",
    "acronym": "CTB",
    "label": "Chinese Treebank"
  },
  {
    "sentence": "We incorporated Nivre?s method with these  preprocessing methods for Chinese dependency  analysis with Penn CTB and Sinica  Treebank (Chen   et al, 2003).",
    "acronym": "CTB",
    "label": "Chinese Treebank"
  },
  {
    "sentence": "Adding seman- tic roles to the CTB.",
    "acronym": "CTB",
    "label": "Chinese Treebank"
  },
  {
    "sentence": "The Penn CTB: Phrase structure annotation of a large corpus.",
    "acronym": "CTB",
    "label": "Chinese TreeBank"
  },
  {
    "sentence": "The Penn CTB: phrase structure an- notation of a large corpus.",
    "acronym": "CTB",
    "label": "Chinese TreeBank"
  },
  {
    "sentence": "The Penn CTB: Phrase Structure Annotation of a Large Corpus.",
    "acronym": "CTB",
    "label": "Chinese TreeBank"
  },
  {
    "sentence": "k+njk (7) where the second product is over pairs (kl) where k is a parent of l on the path from the root to x. The HDT model we pro- posed has a large number of parameters and hy- perparameters (even after integrating out the ?",
    "acronym": "HDT",
    "label": "hierarchical Dirichlet tree"
  },
  {
    "sentence": "6 Conclusion and Future Work We presented a HDT model for information retrieval which can inject (semantical or syntactical) word relationships as the domain knowl- edge into a probabilistic model for information re- trieval.",
    "acronym": "HDT",
    "label": "hierarchical Dirichlet tree"
  },
  {
    "sentence": "We give encour- aging experimental evidence of the superiority of the HDT compared to standard baselines.",
    "acronym": "HDT",
    "label": "hierarchical Dirichlet tree"
  },
  {
    "sentence": "We generalize the model of (Cowans, 2004) by re- placing the Dirichlet distributions with Dirichlet tree distributions (Minka, 2003), thus we call our model the HDT.",
    "acronym": "HDT",
    "label": "hierarchical Dirichlet tree"
  },
  {
    "sentence": "b) The global tree and local trees in HDT docu- ment model.",
    "acronym": "HDT",
    "label": "hierarchical Dirichlet tree"
  },
  {
    "sentence": "c?2011 Association for Computational Linguistics Empty Categories in HDT: Analysis and Recovery Chaitanya GSK Intl Institute of Info.",
    "acronym": "HDT",
    "label": "Hindi Dependency Treebank"
  },
  {
    "sentence": "Unlike PropBanks in most other languages, the Hind PropBank is annotated on top of dependency structure, the HDT.",
    "acronym": "HDT",
    "label": "Hindi Dependency Treebank"
  },
  {
    "sentence": "3 HDT (HTB) A multi layered and multi representational tree- bank for Hindi is developed by annotating with morpho-syntactic (morphological analyses, POS tags, chunk) and syntacto-semantic (dependency re- lations labeled in the computational paninian frame- work) information.",
    "acronym": "HDT",
    "label": "Hindi Dependency Treebank"
  },
  {
    "sentence": "(2012) describe a graphical tool that was used in the annotation of the HDT.",
    "acronym": "HDT",
    "label": "Hindi Dependency Treebank"
  },
  {
    "sentence": "94 2 The HDT  Hindi is a free word order language with SOV as  the default order.",
    "acronym": "HDT",
    "label": "Hindi Dependency Treebank"
  },
  {
    "sentence": "A GUI to Detect and Correct Errors in HDT.",
    "acronym": "HDT",
    "label": "Hindi Dependency Treebank"
  },
  {
    "sentence": "If bx corresponds to empirical expectations and p?(z|x) is uniform, then Equation (10) would be a log-likelihood and Equation (14) (fol- lowing) would be a maxiMaxEnt problem.",
    "acronym": "MaxEnt",
    "label": "mum entropy"
  },
  {
    "sentence": "A max- iMaxEnt approach to combining word align- ments.",
    "acronym": "MaxEnt",
    "label": "mum entropy"
  },
  {
    "sentence": "A maxiMaxEnt word aligner for arabic-english machine translation.",
    "acronym": "MaxEnt",
    "label": "mum entropy"
  },
  {
    "sentence": "Note that this primal?dual relationship is very similar to the one between maximum likelihood and maxiMaxEnt.",
    "acronym": "MaxEnt",
    "label": "mum entropy"
  },
  {
    "sentence": "Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi- MaxEnt models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998).",
    "acronym": "MaxEnt",
    "label": "mum entropy"
  },
  {
    "sentence": "Adaptation of maxiMaxEnt capitalizer: Little data can help a lot.",
    "acronym": "MaxEnt",
    "label": "mum entropy"
  },
  {
    "sentence": "Ratnaparkhi et al, 1994) 228 extended the problem instances to quadruples by also considering the kernel noun of the PP, and used maxiMaxEnt models to estimate the preferences.",
    "acronym": "MaxEnt",
    "label": "mum entropy"
  },
  {
    "sentence": "As with maxiMaxEnt, gradient computation involves computing an expectation under q(z | x), which can be performed efficiently if the features f(x, z) factor in the same way as the model p?(x, z), and t",
    "acronym": "MaxEnt",
    "label": "mum entropy"
  },
  {
    "sentence": "MaxEnt based restoration of arabic diacritics.",
    "acronym": "MaxEnt",
    "label": "Maximum entropy"
  },
  {
    "sentence": "Learned MaxEnt parameters x ct First-person pronouns features ?",
    "acronym": "MaxEnt",
    "label": "Maximum entropy"
  },
  {
    "sentence": "MaxEnt model for punctuation annotation from speech.",
    "acronym": "MaxEnt",
    "label": "Maximum entropy"
  },
  {
    "sentence": "the same experiment applied to TB2 dataset (TB2- 278 Description Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al, 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al, 1994) 93.2 RRR Maximum Likelihood-based (Hindle and Rooth, 1993) 79.7 AP MaxEnt, words (Ratnaparkhi et al, 1994) 77.7 RRR MaxEnt, words & classes (Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and",
    "acronym": "MaxEnt",
    "label": "Maximum entropy"
  },
  {
    "sentence": "ription Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al, 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al, 1994) 93.2 RRR Maximum Likelihood-based (Hindle and Rooth, 1993) 79.7 AP MaxEnt, words (Ratnaparkhi et al, 1994) 77.7 RRR MaxEnt, words & classes (Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and Nagao, 1997) 88.1 RRR WordNet Memory-based Learning (Zavre",
    "acronym": "MaxEnt",
    "label": "Maximum entropy"
  },
  {
    "sentence": "(Ratnaparkhi et al, 1994) 81.6 RRR Decision trees (Ratnaparkhi et al, 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and Nagao, 1997) 88.1 RRR WordNet Memory-based Learning (Zavrel et al, 1997) 84.4 RRR LexSpace MaxEnt, unsupervised (Ratnaparkhi, 1998) 81.9 MaxEnt, supervised (Ratnaparkhi, 1998) 83.7 RRR Neural Nets (Alegre et al, 1999) 86.0 RRR WordNet Boosting (Abney et al, 1999) 84.4 RRR Semi-probabilistic (Pantel and Lin, 2000) 84.31 RRR MaxEnt, ensemble (McLauchlan, 2001) 85.5 RRR LSA SVM (Vanschoenwinkel and Manderick, 2003) 84.8 RRR Nearest-neighbor (Zhao and Lin, 2004) 8",
    "acronym": "MaxEnt",
    "label": "Maximum entropy"
  },
  {
    "sentence": "2.2 MaxEnt The second step of our paraphrasing system consists of a supervised maximum entropy classification ap- proach.",
    "acronym": "MaxEnt",
    "label": "Maximum entropy classification"
  },
  {
    "sentence": "Girju et al (2005) apply both classic (SVM and decision trees) and novel supervised models (seman- tic scattering and ISS), using WordNet, word sense disambiguation, and a set of linguistic features.",
    "acronym": "ISS",
    "label": "iterative semantic specialization"
  },
  {
    "sentence": "In 2003, Girju, Badulescu, and Moldovan (Girju, Badulescu, and Moldovan, 2003) detected the PART- WHOLE relations for some of the most frequent patterns (including the genitives) using the Itera- tive ISS, a learning model that searches for constraints in the WordNet noun hierar- chies.",
    "acronym": "ISS",
    "label": "Semantic Specialization"
  },
  {
    "sentence": "c?2009 ACL and AFNLP Graphemic Approximation of Phonological Context  for English-Chinese Transliteration      Oi Yee Kwong  Department of Chinese, Translation and Linguistics  City University of Hong Kong  Tat Chee Avenue, Kowloon, Hong Kong  Olivia.Kwong@cityu.edu.hk         Abstract  Although DOM has  been shown to outperform phoneme-based  methods in English-to-Chinese (E2C) translit- eration, it is observed that phonological con- text plays an important role in resolving gra- phemic ambiguity.",
    "acronym": "DOM",
    "label": "direct orthographic mapping"
  },
  {
    "sentence": "Although DOM has  been shown to be an effective method, it is nev- ertheless observed that phonological context sig- nificantly contributes to the resolution of some  graphemic ambiguity.",
    "acronym": "DOM",
    "label": "direct orthographic mapping"
  },
  {
    "sentence": "Although the DOM approach advo- cates a direct transfer of grapheme at run-time, we still need to establish the grapheme correspon- dence at the model training stage, when phoneme level alignment can help.",
    "acronym": "DOM",
    "label": "direct orthographic mapping"
  },
  {
    "sentence": "The core of our systems is based on Li et al?s  (2004) Joint Source-Channel Model under the  DOM framework, which  skips the middle phonemic representation in  conventional phoneme-based methods and mod- els the segmentation and alignment preferences  by means of contextual n-grams of the translit- eration segment pairs (or token pairs in their ter- minology).",
    "acronym": "DOM",
    "label": "direct orthographic mapping"
  },
  {
    "sentence": "Grapheme- based method (Li et al, 2004) treats translitera- tion as a DOM and only uses orthography-related features while phoneme- based method (Knight and Graehl, 1998) makes use of phonetic correspondence to generate the transliteration.",
    "acronym": "DOM",
    "label": "direct orthographic mapping"
  },
  {
    "sentence": "5 DOM.",
    "acronym": "DOM",
    "label": "Document Object Model"
  },
  {
    "sentence": "Gupta et al (2003) parsed HTML documents  to a DOM tree and to extract the  main content of a web page by removing the link  lists and empty tables.",
    "acronym": "DOM",
    "label": "Document Object Model"
  },
  {
    "sentence": "If you intend to automatically process only a few selected fora, you will probably use XPath queries on the HTML DOM.",
    "acronym": "DOM",
    "label": "Document Object Model"
  },
  {
    "sentence": "DOM.",
    "acronym": "DOM",
    "label": "Document Object Model"
  },
  {
    "sentence": "The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Romanian), which enables comparative evaluations of different scenarios (see Section 4).",
    "acronym": "SC",
    "label": "SemCor"
  },
  {
    "sentence": "The automatic translation of the MPQA and of the SC corpus was performed using Language Weaver,1 a commercial statistical machine transla- tion software.",
    "acronym": "SC",
    "label": "SemCor"
  },
  {
    "sentence": "P R F high-precision 86.7 32.6 47.4 high-coverage 79.4 70.6 74.7 Table 1: Precision (P), Recall (R) and F-measure (F) for the two OpinionFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.",
    "acronym": "SC",
    "label": "SemCor"
  },
  {
    "sentence": "In (Mihalcea et al, 2007), we used the manual translation of the SC corpus into Romanian to form an English-Romanian par- allel data set.",
    "acronym": "SC",
    "label": "SemCor"
  },
  {
    "sentence": "These sentences represent the manual translation into Romanian of a small subset of the SC corpus, which was removed from the training corpora used in experi- ments two and three.",
    "acronym": "SC",
    "label": "SemCor"
  },
  {
    "sentence": "Figure 3: Experiment three: machine translation of raw training data from target language into source language As before, we use the high-coverage classifier available in OpinionFinder, and the SC corpus.",
    "acronym": "SC",
    "label": "SemCor"
  },
  {
    "sentence": "nFinder classifiers, as measured on the MPQA corpus As a raw corpus, we use a subset of the SC corpus (Miller et al, 1993), consisting of 107 docu- ments with roughly 11,000 sentences.",
    "acronym": "SC",
    "label": "SemCor"
  },
  {
    "sentence": "We compare our results with those obtained by a previously proposed method that was based on the manual translation of the SC subjectivity- annotated corpus.",
    "acronym": "SC",
    "label": "SemCor"
  },
  {
    "sentence": "The reason for working with this collection is the fact that we also have a manual translation of the SC docu- ments from English into one of the target languages used in the experiments (Ro",
    "acronym": "SC",
    "label": "SemCor"
  },
  {
    "sentence": "The previous n para- graphs1 were searched and the pronoun under in- vestigation was mapped to the closest (in terms of textual proximity) story character that had the same gender as the pronoun (see SC 3.4.1 re- garding gender estimation).",
    "acronym": "SC",
    "label": "Section"
  },
  {
    "sentence": "The paper is structured as follows: SC 2 in- troduces the classification we are aiming at and the hypotheses that led to the experiments; SC 3 fo- cuses on the methodology used to produce the clas- sification; in SC 4 we discuss the results ob- tained so far; finally, SC 5 contains some con- clusions and proposals for further work.",
    "acronym": "SC",
    "label": "Section"
  },
  {
    "sentence": "This is applied to char- acters (rather than only speakers) because the gen- der information is exploited during the attribution of quotes (see SC 3.3).",
    "acronym": "SC",
    "label": "Section"
  },
  {
    "sentence": "Thus our SC method of finding the  pronoun/noun co-occurrences is simply to parse  the text and then assume that the noun-phrase  at Hobbs distance one is the antecedent.",
    "acronym": "SC",
    "label": "second"
  },
  {
    "sentence": "The SC problem is resolving utterance chains with implicit speakers.",
    "acronym": "SC",
    "label": "second"
  },
  {
    "sentence": "The SC half of the paper describes a  method for using (portions of) t~e aforemen-  tioned program to learn automatically the typi-  cal gender of English words, information that is  itself used in the pronoun resolution program.",
    "acronym": "SC",
    "label": "second"
  },
  {
    "sentence": "3.2 Identification of Story Characters The SC step is identifying candidate charac- ters (i.e., entities) that appear in the stories under analysis.",
    "acronym": "SC",
    "label": "second"
  },
  {
    "sentence": "Even if Kim's gender  was unknown before seeing the first sentence,  after the SC sentence, it is known.",
    "acronym": "SC",
    "label": "second"
  },
  {
    "sentence": "The SC  experiment investigates a method for unsuper-  vised learning of gender/number/animaticity  information.",
    "acronym": "SC",
    "label": "second"
  },
  {
    "sentence": "5.1 Comparison with SC We compared the RAkEL algorithm with single- label (SL) classification.",
    "acronym": "SC",
    "label": "Simple Classification"
  },
  {
    "sentence": "Last but not least, we have to work on the definition of polysemy within our task, so that we can achieve significant agreement SCs among judges and in- tegrate this parameter in the experiment.",
    "acronym": "SC",
    "label": "score"
  },
  {
    "sentence": "We cannot perform any analysis on the clustering results with respect to polysemy until reliable SCs are obtained.",
    "acronym": "SC",
    "label": "score"
  },
  {
    "sentence": "However, the agreement SCs for polysemy judgments were not significant at all.",
    "acronym": "SC",
    "label": "score"
  },
  {
    "sentence": "The precision SC computed over all phrases  containing any of the target honorifics are 66.0%  l In effect, this is the same as admi t t ing  that  a ref-  erent  can be in different gender  classes across different  observations.",
    "acronym": "SC",
    "label": "score"
  },
  {
    "sentence": "The low- est agreement SCs are those of J2, the only judge who had not done research on adjectives.",
    "acronym": "SC",
    "label": "score"
  },
  {
    "sentence": "The clustering procedure achieves a comparable agreement SC for one of the pa- rameters, and a little lower for the other.",
    "acronym": "SC",
    "label": "score"
  },
  {
    "sentence": "2 SC Algorithm The median string of a set is defined as the string that minimises the sum of distances to the strings in the set.",
    "acronym": "SC",
    "label": "System Combination"
  },
  {
    "sentence": "1.3 SC Combination of (manual) rule-writing and statis- tical learning has been studied before.",
    "acronym": "SC",
    "label": "System Combination"
  },
  {
    "sentence": "3.3 SC Results Our framework to compute consensus translations allows multiple combinations varying the median string algorithm or the set of weight values used in the weighted sum of distances.",
    "acronym": "SC",
    "label": "System Combination"
  },
  {
    "sentence": "2.3 SC System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines.",
    "acronym": "SC",
    "label": "System Combination"
  },
  {
    "sentence": "5.3 Comparison with SC We re-implemented a state-of-the-art system com- bination method (Rosti et al, 2007).",
    "acronym": "SC",
    "label": "System Combination"
  },
  {
    "sentence": "c?2010 Association for Computational Linguistics An Augmented Three-Pass SC Framework: DCU Combination System for WMT 2010 Jinhua Du, Pavel Pecina, Andy Way CNGL, School of Computing Dublin City University Dublin 9, Ireland {jdu,ppecina,away}@computing.dcu.ie Abstract This paper describes the augmented three- pass system combination framework of the Dublin City University (DCU) MT group for the WMT 2010 system combi- nation task.",
    "acronym": "SC",
    "label": "System Combination"
  },
  {
    "sentence": "To test this hypothesis, we devise the following classification problem: can we discriminate between 78 Baseline Dialog length (turns) Mean, standard deviation, min and max acts per turn Presence of special machine acts (flight offer and confirm) Presence of user acts (provide a dest city and arrival city) Proportion of acts which were provides SC Did the user provide inconsistent information about dest city?",
    "acronym": "SC",
    "label": "String Consistency"
  },
  {
    "sentence": "he documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.",
    "acronym": "SC",
    "label": "simplified Chinese"
  },
  {
    "sentence": "The traditional Chinese sentences are transferred  into SC.",
    "acronym": "SC",
    "label": "simplified Chinese"
  },
  {
    "sentence": "The documents adopted in MET-2 are selected from newspapers in China, thus we have to transform SC characters in GB coding set to traditional Chinese characters in Big-5 coding set before testing.",
    "acronym": "SC",
    "label": "simplified Chinese"
  },
  {
    "sentence": "The experiment results demonstrate that adopting our two strategies generally benefi- cial to NWI and CWS on both traditional and SC datasets.",
    "acronym": "SC",
    "label": "simplified Chinese"
  },
  {
    "sentence": "It was converted to SC by using Wikipedia?s traditional-simplified conversion table http://svn.",
    "acronym": "SC",
    "label": "simplified Chinese"
  },
  {
    "sentence": "Traditional  Chinese edition of a SC  edition published in 1984.)",
    "acronym": "SC",
    "label": "simplified Chinese"
  },
  {
    "sentence": "However, \"D\" is used in SC characters and it is also a legal traditional Chinese character that denotes",
    "acronym": "SC",
    "label": "simplified Chinese"
  },
  {
    "sentence": "However, \"D\" is used in SC characters and it is also a legal traditional Chinese character that denotes another meaning.",
    "acronym": "SC",
    "label": "simplified Chinese"
  },
  {
    "sentence": "and F-SC (LF ?",
    "acronym": "SC",
    "label": "Score"
  },
  {
    "sentence": "All classifiers obtain a relatively high accuracy but vary in the precision, recall and F-SC values.",
    "acronym": "SC",
    "label": "Score"
  },
  {
    "sentence": "Furthermore, by using rules to identify the metaphors in riddles, we get an improvement of 10.1% at Acc@1, which proves the validity of the 852 SC Criterion 5 Elegant metaphors, totally coherent 4 Correct metaphors, mostly coherent 3 Acceptable metaphors, more of less coherent 2 Tolerable metaphors, little coherent 1 Wrong metaphors, incoherent Table 7: The criterion of riddle evaluation rule we define.",
    "acronym": "SC",
    "label": "Score"
  },
  {
    "sentence": "The ranking score is calculated as SC(c) = m?",
    "acronym": "SC",
    "label": "Score"
  },
  {
    "sentence": "haracters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.",
    "acronym": "SC",
    "label": "Score"
  },
  {
    "sentence": "that the character decompose Avg Freq Character average number of frequencies of characters in riddle Max Freq Radical maximized number of frequencies of characters in riddle Number Alignment number of alignments used for generating the candidate Length Alignment length of words from alignments Number Rule number of rules used for generating the candidate Length Rule length of words from rules LM SC R score of language model trained by Chinese riddles, poems and couplets LM SC G score of language model trained by web documents Table 4: Features for riddle generation and count the unigram and bigram word frequency of the rest words.",
    "acronym": "SC",
    "label": "Score"
  },
  {
    "sentence": "with 0 referring to case-insensitivity, 1 being replacement of hyphens with spaces, 2 being removal of punc- tuation, 3 being removal of PMs, and 6 being removal of spaces; grouped digits indi- cate simultaneous invocation of each specified rule in the group.",
    "acronym": "PM",
    "label": "parenthesized material"
  },
  {
    "sentence": "Removal of PMs.",
    "acronym": "PM",
    "label": "parenthesized material"
  },
  {
    "sentence": "Even strict pattern matches and forms that  vary only with respect to inflectional  morphology (i.e., the plurals) yield a nontrivial  percentage of false positives?a percentage  which is actually higher than two of our  heuristics (optionality of hyphenation and  optionality of PM).",
    "acronym": "PM",
    "label": "parenthesized material"
  },
  {
    "sentence": "Optionality of PM: for any  regular expression representing a gene name,  substitute the regular expression formed by  making any paired parentheses and the material  they enclose (and surrounding whitespace, as  appropriate) optional.",
    "acronym": "PM",
    "label": "parenthesized material"
  },
  {
    "sentence": "For example in pr tence 36 patients with inflammatory bowel disease  (11 with ulcerative colitis and 25 with Crohn?s disease),  the PM caused SemRep to incor- rectly  returned ?",
    "acronym": "PM",
    "label": "parenthesized material"
  },
  {
    "sentence": "These consisted of mapping vowel  sequences to a constant string (the purpose of  this being to look at American vs. British  dialectal differences in gene names);  replacement of hyphens with spaces; removal of  PM; and normalization of  case.",
    "acronym": "PM",
    "label": "parenthesized material"
  },
  {
    "sentence": "Punctuation annotation using statisti- cal PMs.",
    "acronym": "PM",
    "label": "prosody model"
  },
  {
    "sentence": "Recent PM im- provements include the use of bagging techniques in deci- sion tree training to reduce the variability due to a single tree (Liu et al, 2003).",
    "acronym": "PM",
    "label": "prosody model"
  },
  {
    "sentence": "Recent PM im- provements include the use of bagging techniques in deci- sion tree training to reduce the",
    "acronym": "PM",
    "label": "prosody model"
  },
  {
    "sentence": "Punctua- tion annotation using statistical PMs.",
    "acronym": "PM",
    "label": "prosody model"
  },
  {
    "sentence": "Identifying the function words is  important for applications such as information  retrieval, PMing in speech synthesis,  semantic role labeling, and dependency parsing.",
    "acronym": "PM",
    "label": "prosody model"
  },
  {
    "sentence": "Punctu- ation annotation using statistical PMs.",
    "acronym": "PM",
    "label": "prosody model"
  },
  {
    "sentence": "The PM is a de- cision tree classifier that generates the posterior probabil- ity of an SU boundary at each interword boundary given the prosodic features.",
    "acronym": "PM",
    "label": "prosody model"
  },
  {
    "sentence": "To  overcome this problem, we combine BANNER  with two other predictors, a Sematic Model and a  PM.",
    "acronym": "PM",
    "label": "Priority Model"
  },
  {
    "sentence": "Second, these two models learn  name patterns in different ways, i.e., semantic rela- tionships for the Semantic Model and positional  and lexical information for the PM.",
    "acronym": "PM",
    "label": "Priority Model"
  },
  {
    "sentence": "First, the Semantic Model and the  PM do not use previous gold-standard  sets for training.",
    "acronym": "PM",
    "label": "Priority Model"
  },
  {
    "sentence": "These phrases are then evaluated using the  PM.",
    "acronym": "PM",
    "label": "Priority Model"
  },
  {
    "sentence": "2.4 PM  The Semantic Model detects four different catego- ries for a single word.",
    "acronym": "PM",
    "label": "Priority Model"
  },
  {
    "sentence": "The PM is a statistical language  model for named entity recognition (Tanabe and  Wilbur, 2006).",
    "acronym": "PM",
    "label": "Priority Model"
  },
  {
    "sentence": "However, the PM  captures gene name patterns by analyzing the order  of words and the character strings making up  words.",
    "acronym": "PM",
    "label": "Priority Model"
  },
  {
    "sentence": "In our experiments we trained DSSMs using  mini-batch SGD.",
    "acronym": "SGD",
    "label": "Stochastic Gradient Descent"
  },
  {
    "sentence": "k?3?i?k {f 1 , f 2 , f 3 } Bernoulli-Bernoulli RBM was applied to pre- train DNNs and SGD with cross-entropy criterion to fine-tune DNNs.",
    "acronym": "SGD",
    "label": "Stochastic Gradient Descent"
  },
  {
    "sentence": "SGD Training for L1-regularized Log-linear Models with Cumulative Penalty.",
    "acronym": "SGD",
    "label": "Stochastic Gradient Descent"
  },
  {
    "sentence": "GU-MLT-LT:  Sentiment Analysis of Short Messages using Lin- guistic Features and SGD.",
    "acronym": "SGD",
    "label": "Stochastic Gradient Descent"
  },
  {
    "sentence": "3 SGD For M3N optimization, Taskar et al (2003) has proposed a reparametrization of the dual variables to take advantage of the network structure of the labeling sequence problem.",
    "acronym": "SGD",
    "label": "Stochastic Gradient Descent"
  },
  {
    "sentence": "The opti- mum has been found using the online method (SGD).",
    "acronym": "SGD",
    "label": "Stochastic Gradient Descent"
  },
  {
    "sentence": "In particular, we developed an efficient parallelized implementation of our SGD algorithm using the message-passing interface (MPI).",
    "acronym": "SGD",
    "label": "stochastic gradient descent"
  },
  {
    "sentence": "The SGD is adopted to optimize the parameters.",
    "acronym": "SGD",
    "label": "stochastic gradient descent"
  },
  {
    "sentence": "Large-scale machine learning with SGD.",
    "acronym": "SGD",
    "label": "stochastic gradient descent"
  },
  {
    "sentence": "Training is performed with SGD by per- forming a gradient step against the violating tag.",
    "acronym": "SGD",
    "label": "stochastic gradient descent"
  },
  {
    "sentence": "2010) and SGD code from deeplearning.net/tutorial (Bengio, 2009).",
    "acronym": "SGD",
    "label": "stochastic gradient descent"
  },
  {
    "sentence": "i is updated using SGD.",
    "acronym": "SGD",
    "label": "stochastic gradient descent"
  },
  {
    "sentence": "Parsing goes basically bottom-up with top-down  confirmation, improving the so called LC  technique.",
    "acronym": "LC",
    "label": "Left Corner"
  },
  {
    "sentence": "LC Transforms and Fi-  nite State Approximations.",
    "acronym": "LC",
    "label": "Left Corner"
  },
  {
    "sentence": "One dimension in this space is represented by the parsing algorithm used: For example, within the framework of Generalized LC Pars- ing (Demers, 1977), algorithms can be char- acterized in terms of the point at which a context-free rule is recognized, in relation to the recognition-point of the symbols on its right- hand side.",
    "acronym": "LC",
    "label": "Left Corner"
  },
  {
    "sentence": "Rosenkrantz, D. J / P. M. Lewis (1970) l)e-  terministic LC Parser, IEEE Con\\[er-  ence Record of the l l th Annual Symposium on  Switching and Automata Theory, 139-152.",
    "acronym": "LC",
    "label": "Left Corner"
  },
  {
    "sentence": "IS_A_LEFT_CORNER(  \\[Real_LC Cat,Structure\\],  \\[Real GoalCat,Structurel,   Input Str ing,RestStr ing  /* reflexive closure of the relation \"being a left  corne r\" * /  is_a_leftcorner(   \\ [Rea iLe f tCornerCategory ,   Rea lLe f tCornerCategory_St ructure \\ ] ,   \\[Real GoalCategory,RealGoal_Category_Structure\\] ,   String,String )  uni fy(RealLeftCorner_Category,   Real_Goal_Category ) .",
    "acronym": "LC",
    "label": "Left Corner"
  },
  {
    "sentence": "These include well stud- ied grammars such as Hierarchical Phrase Structure Grammars and Combinatory Categorial Grammars, and transforms that rearrange the tree such as the LC Transform used in Roark and Johnson (1999).",
    "acronym": "LC",
    "label": "Left Corner"
  },
  {
    "sentence": "c?2007 Association for Computational Linguistics UofL: Word Sense Disambiguation Using LC  Yllias Chali           Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  chali@cs.uleth.ca  Shafiq R. Joty  Department of Computer Science  University of Lethbridge   Lethbridge, Alberta, Canada, T1K 3M4  jotys@cs.uleth.ca      Abstract  One of the main challenges in the applica- tions (i.e.: text summarization, question an- swering,",
    "acronym": "LC",
    "label": "Lexical Cohesion"
  },
  {
    "sentence": "Morris J. and Hirst G. (1991) \"LC  Computed by Thesaural Relations as an  Indicator of the Structure of Text.\"",
    "acronym": "LC",
    "label": "Lexical Cohesion"
  },
  {
    "sentence": "LC Com-  puted by Thesaural Relations as an Indicator of the  Structure of Text.",
    "acronym": "LC",
    "label": "Lexical Cohesion"
  },
  {
    "sentence": "G. 1991, LC  Computed by Thesaural Relations as an Indica- tor of the Structure of Text .Computational Lin- guistics, 17(1):21-48.",
    "acronym": "LC",
    "label": "Lexical Cohesion"
  },
  {
    "sentence": "Complexity .983* 1.404*  LC -.266* -.440*  Interactive/Conv.",
    "acronym": "LC",
    "label": "Lexical Cohesion"
  },
  {
    "sentence": "c?2013 Association for Computational Linguistics Bilingual LC Trigger Model for Document-Level Machine Translation Guosheng Ben?",
    "acronym": "LC",
    "label": "Lexical Cohesion"
  },
  {
    "sentence": "To  relate the language resources among the LC languages has brought us to the following  open questions:    1.",
    "acronym": "LC",
    "label": "less computerized"
  },
  {
    "sentence": "t for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.",
    "acronym": "LC",
    "label": "less computerized"
  },
  {
    "sentence": "The problem is magnified when we need to deal with the LC lan- guages.",
    "acronym": "LC",
    "label": "less computerized"
  },
  {
    "sentence": "In this paper, we will present a method of providing language and encod- ing support for LC languages.",
    "acronym": "LC",
    "label": "less computerized"
  },
  {
    "sentence": "The major concern in the  LC languages is how to leverage the technology for those languages which will result in  scaling up the number of online language populations.",
    "acronym": "LC",
    "label": "less computerized"
  },
  {
    "sentence": "A related problem is that of support for encodings, as many of these LC languages do not have one stan- dard encoding that is used by all.",
    "acronym": "LC",
    "label": "less computerized"
  },
  {
    "sentence": "We then describe an ed- itor called Sanchay Editor, which uses this method and also has many other facilities useful for those using LC lan- guages for simple text editing or for Nat- ural Language Processing purposes, espe- cially for annotation.",
    "acronym": "LC",
    "label": "less computerized"
  },
  {
    "sentence": "r between words works as an indi-  cator of the LC.",
    "acronym": "LC",
    "label": "lexical cohesion"
  },
  {
    "sentence": "This is because c(X) is based only on the  LC of the words in X.  6 Discussion  The structure of Paradigme represents the knowl-  edge system of English, and an activated state pro-  duced on it represents word meaning.",
    "acronym": "LC",
    "label": "lexical cohesion"
  },
  {
    "sentence": "The similarity rep-  resents the strength of LC or  semantic relation, and also provides valu-  able information about similarity and co-  herence of texts.",
    "acronym": "LC",
    "label": "lexical cohesion"
  },
  {
    "sentence": "Recogniz-  ing the structure of text is an essential task in text  understanding.\\[Grosz andSidner, 1986\\]  One of the valuable indicators of the structure of  text is LC.\\[Halliday nd Hasan, 1976\\]  Lexical cohesion is the relationship between words,  classified as follows:  1.",
    "acronym": "LC",
    "label": "lexical cohesion"
  },
  {
    "sentence": "We consider LC as semantic similarity  between words.",
    "acronym": "LC",
    "label": "lexical cohesion"
  },
  {
    "sentence": "2009 ACL and AFNLP A Cohesion Graph Based Approach for Unsupervised Recognition of Literal and Non-literal Use of Multiword Expressions Linlin Li and Caroline Sporleder Saarland University Postfach 15 11 50 66041 Saarbr?ucken Germany {linlin,csporled}@coli.uni-saarland.de Abstract We present a graph-based model for rep- resenting the LC of a dis- course.",
    "acronym": "LC",
    "label": "lexical cohesion"
  },
  {
    "sentence": "Since r tends to be quite small and can be bounded by a LC, this already gives polynomial time complexity.",
    "acronym": "LC",
    "label": "low constant"
  },
  {
    "sentence": "Since that the size of the right-hand-side and the number of outgoing d-edges per node are practically bounded by LCs, applying k rules on a tree T yields a linear increase in the size of the forest.",
    "acronym": "LC",
    "label": "low constant"
  },
  {
    "sentence": "We  use hash tables to alLC time access to the  language model data.",
    "acronym": "LC",
    "label": "low constant"
  },
  {
    "sentence": "Furthermore, they are very different 1Logarithmic values are used in the actual implementation which are floored to a LC in case of zero ?",
    "acronym": "LC",
    "label": "low constant"
  },
  {
    "sentence": "LC Focus Right Context Combined Class - - - - - a a n b i d ?",
    "acronym": "LC",
    "label": "Left Context"
  },
  {
    "sentence": "Context Accuracy (%) LC Only 91.31 Right Context Only 88.26 Both Contexts 92.54 Table 3: The effect of using both left and right context.",
    "acronym": "LC",
    "label": "Left Context"
  },
  {
    "sentence": "\b\u000b\t\u000b\u000b  \u0001 shi ,1Lo \u0002 zhang \u0003 ma \u0004 ying \u0005 jiu \u0006 biao \u0007 shi ,2Lo ,1Co ,2Co ,3Co ,1Ro ,2Ro \b\u000b\t\u000b  \b\u000b\t\u000b  LC Right Context Candidate to be parsed), it will be rejected immediately.",
    "acronym": "LC",
    "label": "Left Context"
  },
  {
    "sentence": "LC (-lc): nodes in this partition  do not cover any phrase word and they are  all in the left of the left phrase.",
    "acronym": "LC",
    "label": "Left Context"
  },
  {
    "sentence": "Description Feature Trigram + Context x1x2x3x4x5 Trigram x2x3x4 LC x1x2 Right Context x4x5 Center Word x3 Trigram - Center Word x2x4 Left Word + Right Context x2x4x5 Right Word + LC x1x2x3 Type of Trigram: number, punctuation, alphabetic letter and other t(x2)t(x3)t(x4) Table 2: Features employed to measure the sim- ilarity between two vertices, in a given tex- t ?",
    "acronym": "LC",
    "label": "Left Context"
  },
  {
    "sentence": "|  201/206 Pre-processing Configurations & MeasuresSpearman?s Rank Correlation 00,1 0,20,3 0,40,5 0,6 N, V, A Nouns Keywords EBEB+SYNEB+HypoLINESA-WordESA-Text 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  202/206 ESA-Text tf.idf with Different Lexical-Semantic Resources Nouns,Verbs,Adjectives Nouns Keywords0 0.050.1 0.150.2 0.250.3 0.350.4 0.450.5 0.550.6 0.65 MAP WikipediaGermaNet HyperGermaNet RadialWiktionary 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  203/206 ?",
    "acronym": "MAP",
    "label": "Mean Average Precision"
  },
  {
    "sentence": "Collection PL2 TFIDF FV LSI CLEF?03 35.7 16.4 23.7 9.2 TREC-1&2 22.6 12.4 10.8 6.5 ROBUST 24.8 12.6 10.5 4.5 Table 4: MAP(%) for the PL2 and TFIDF model on the three IR Collections compared to Fisher Vector and LSI These results are not surprising as it has been shown experimentally in many studies that latent- based approaches such as LSI are generally out- performed by state-of-the-art IR models in Ad- Hoc tasks.",
    "acronym": "MAP",
    "label": "Mean Average Precision"
  },
  {
    "sentence": "The NIST-B2 system with a higher WER (46.6%) has an improvement in MAP of 6.5%.",
    "acronym": "MAP",
    "label": "Mean Average Precision"
  },
  {
    "sentence": "Below, we summarize the three measures we used: MAP, TOP1, and Aver- age Rank of Last Synonym.",
    "acronym": "MAP",
    "label": "Mean Average Precision"
  },
  {
    "sentence": "MAP (APR) Perf implements a definition of average preci- sion sometimes called ?",
    "acronym": "MAP",
    "label": "Mean Average Precision"
  },
  {
    "sentence": "5.1 Data The data set we use is from the TAC-KBP2013 Slot Filling Validation (SFV) task, which consists of the merged responses returned by 52 runs (regarded as systems in MTM) from 18 teams submitted to the Slot 1573 Methods Precision Recall F-measure Accuracy MAP 1.Random 28.64% 50.48% 36.54% 50.54% 34% 2.Voting 42.16% 70.18% 52.68% 62.54% 62% 3.Linguistic Indicators 50.24% 70.69% 58.73% 72.29% 60% 4.SVM (3 + System + Source) 56.59% 48.72% 52.36% 75.86% 56% 5.MTM (3 + System + Source) 53.94% 72.11% 61.72% 81.57% 70% Table 2: Overall Performance Comparison.",
    "acronym": "MAP",
    "label": "Mean Average Precision"
  },
  {
    "sentence": "7.2 Comparison to Dirichlet MAP Solutions The transformation T(?,?)",
    "acronym": "MAP",
    "label": "Maximum A Posteriori"
  },
  {
    "sentence": "Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a MAP combination (Bacchiani et al 2006).",
    "acronym": "MAP",
    "label": "maximum a posteriori"
  },
  {
    "sentence": "Future work might explore sensitivity to these choices, or empirical Bayesian or MAP inference for their values (Johnson and Goldwater, 2009).",
    "acronym": "MAP",
    "label": "maximum a posteriori"
  },
  {
    "sentence": "In this study, we use the MAP es- timation with Gaussian priors for parameter estima- tion.",
    "acronym": "MAP",
    "label": "maximum a posteriori"
  },
  {
    "sentence": "2.2.3 Regularization Schemes One can convert a maximum likelihood problem into MAP using Bayes?",
    "acronym": "MAP",
    "label": "maximum a posteriori"
  },
  {
    "sentence": "4 Evaluation We compared the Gibbs sampling compressor (GS) against a version of MAP EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM).",
    "acronym": "MAP",
    "label": "maximum a posteriori"
  },
  {
    "sentence": "While they train the parameters using a MAP estima- tor, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account.",
    "acronym": "MAP",
    "label": "maximum a posteriori"
  },
  {
    "sentence": "4 Bayesian Learning for word-dependent  transition models   4.1 MAP training   Using ML training, we can obtain the estimation  formula for word dependent transition probabilities  { }( | , , )p i i e I?",
    "acronym": "MAP",
    "label": "Maximum a posteriori"
  },
  {
    "sentence": "The MAP for a run consisting of  multiple topics is the mean of the average precision  scores of each of the individual topics in the run.",
    "acronym": "MAP",
    "label": "mean average precision"
  },
  {
    "sentence": "Each line in the graph con-  nects the MAP scores produced by  each version of the system for a single test.",
    "acronym": "MAP",
    "label": "mean average precision"
  },
  {
    "sentence": "The last  row shows the MAPs from a 10-fold  cross validation to learn how to distinguish each class  from the union of the other three.",
    "acronym": "MAP",
    "label": "mean average precision"
  },
  {
    "sentence": "For instance, the weighted MAP of the previous best ap- proach in (Riedel et al.,",
    "acronym": "MAP",
    "label": "mean average precision"
  },
  {
    "sentence": "For instance, this strategy im- proves the weighted MAP of the best approach in (Riedel et al.,",
    "acronym": "MAP",
    "label": "mean average precision"
  },
  {
    "sentence": "In addition, when ap- plied to a relation extraction task, our ap- proach alone is comparable to several ex- isting systems, and improves the weighted MAP of a state-of-the- art method by 10 points when used as a subcomponent.",
    "acronym": "MAP",
    "label": "mean average precision"
  },
  {
    "sentence": "We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing HD informa-  tion and noun phrase repetition.",
    "acronym": "HD",
    "label": "head"
  },
  {
    "sentence": "In the conditioning events, h is the  HD constituent above p, l~ r is the list of candi-  date antecedents o be considered, t is the type  of phrase of the proposed antecedent (always  a noun-phrase in this study), I is the type of  the HD constituent, sp describes the syntactic  structure in which p appears, dspecifies the dis-  tance of each antecedent from p and M\" is the  number of times the referent is mentioned.",
    "acronym": "HD",
    "label": "head"
  },
  {
    "sentence": "It is also useful to note the interaction be-  tween the HD constituent of the pronoun p  and the antecedent.",
    "acronym": "HD",
    "label": "head"
  },
  {
    "sentence": "Also, some HD  verbs are too general to restrict he selection of  any NP.",
    "acronym": "HD",
    "label": "head"
  },
  {
    "sentence": "To avoid the sparse-data prob-  lem, the HDs h are clustered according to how  they behave in P(w~lh, t, l).",
    "acronym": "HD",
    "label": "head"
  },
  {
    "sentence": "We introduce a HD clas- sification method for text-based geotagging.",
    "acronym": "HD",
    "label": "hierarchical discriminative"
  },
  {
    "sentence": "For each test segment, the judges were presented with its text, and 3 alternative titles consisting of the reference and the titles produced by the HD model, and the best performing baseline.",
    "acronym": "HD",
    "label": "hierarchical discriminative"
  },
  {
    "sentence": "2.2 Factor Graph Model To construct the entity factoid hierarchy, we make use of a HD factor graph model.",
    "acronym": "HD",
    "label": "hierarchical discriminative"
  },
  {
    "sentence": "This paper presents a HD approach for table-of-contents generation.",
    "acronym": "HD",
    "label": "hierarchical discriminative"
  },
  {
    "sentence": "The HD method consis- tently outperforms the four baselines according to all ROUGE metrics.",
    "acronym": "HD",
    "label": "hierarchical discriminative"
  },
  {
    "sentence": "HD Phrase  Structure Grammar.",
    "acronym": "HD",
    "label": "Head-Driven"
  },
  {
    "sentence": "In Ger-  man in HD Phrase Structure Grammar.",
    "acronym": "HD",
    "label": "Head-Driven"
  },
  {
    "sentence": "2 Background 2.1 The LinGO Grammar Matrix The LinGO Grammar Matrix is situated theoreti- cally within HD Phrase Structure Gram- mar (HPSG; Pollard and Sag, 1994), a lexicalist, constraint-based framework.",
    "acronym": "HD",
    "label": "Head-Driven"
  },
  {
    "sentence": "HD Statistical Models for Natural Language Parsing.",
    "acronym": "HD",
    "label": "Head-Driven"
  },
  {
    "sentence": "HD Phrase Structure Grammar.",
    "acronym": "HD",
    "label": "Head-Driven"
  },
  {
    "sentence": "ency annotations we can also determine how many instances need addi- 175 Location Main Fact Subfact Synonym Extra Title 3.3 ( 0.2) 1.9 ( 0.7) 0.0 ( 0.0) 0.8 ( 0.8) Abstract 19.1 (10.1) 9.3 ( 5.1) 36.2 (21.7) 25.8 (14.8) Introduction 11.3 ( 5.2) 8.3 ( 3.4) 30.4 (17.4) 17.2 ( 7.8) Results 31.0 (13.8) 37.6 (16.1) 20.3 (15.9) 32.0 (12.5) Discussion 21.8 ( 7.3) 19.5 ( 6.6) 2.9 ( 1.4) 9.4 ( 3.1) Figure HDing 5.0 ( 0.6) 10.7 ( 3.8) 1.4 ( 1.4) 2.3 ( 0.0) Figure Legend 3.1 ( 1.3) 4.8 ( 2.0) 0.0 ( 0.0) 7.0 ( 4.7) Table Data 0.0 ( 0.0) 0.2 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Methods 0.2 ( 0.0) 0.1 ( 0.1) 0.0 ( 0.0) 4.7 ( 0.8) Conclusion 0.6 ( 0.4) 0.1 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Footnotes 0.0 ( 0.0) 0.0 ( 0.0) 5.8 ( 2.9) 0.0 ( 0.0) HDings 4.8 ( 0.6) 7.5 ( 2.7) 2.9 ( 1.4) 0.8 ( 0.8) Full-text 100.0 (39.",
    "acronym": "HD",
    "label": "Head"
  },
  {
    "sentence": "0 (12.5) Discussion 21.8 ( 7.3) 19.5 ( 6.6) 2.9 ( 1.4) 9.4 ( 3.1) Figure HDing 5.0 ( 0.6) 10.7 ( 3.8) 1.4 ( 1.4) 2.3 ( 0.0) Figure Legend 3.1 ( 1.3) 4.8 ( 2.0) 0.0 ( 0.0) 7.0 ( 4.7) Table Data 0.0 ( 0.0) 0.2 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Methods 0.2 ( 0.0) 0.1 ( 0.1) 0.0 ( 0.0) 4.7 ( 0.8) Conclusion 0.6 ( 0.4) 0.1 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0) Footnotes 0.0 ( 0.0) 0.0 ( 0.0) 5.8 ( 2.9) 0.0 ( 0.0) HDings 4.8 ( 0.6) 7.5 ( 2.7) 2.9 ( 1.4) 0.8 ( 0.8) Full-text 100.0 (39.4) 100.0 (40.6) 100.0 (62.3) 100.0 (45.3) Table 6: Instances found excluding (including) all dependencies Fact Type # Created # Found # Instances Main Fact 170 156 523 Subfact 251 251 1196 Synonym 155 62 69 Extra 152 87 128 Total 728 556 1916 Table 7: Distribution of fact types in corpus tional knowledge outside of the curren",
    "acronym": "HD",
    "label": "Head"
  },
  {
    "sentence": "But the model has also been applied to several other grammatical frameworks, e.g. Tree-Insertion Grammar (Hoogweg 2000), Tree-Adjoining Grammar (Neumann 1998), Lexical-Functional Grammar (Bod & Kaplan 1998; Cormons 1999), HD-driven Phrase Structure Grammar (Neumann & Flickinger 1999), and Montague Grammar (Bonnema et al 1997; Bod 1999).",
    "acronym": "HD",
    "label": "Head"
  },
  {
    "sentence": "2 Background 2.1 The LinGO Grammar Matrix The LinGO Grammar Matrix is situated theoreti- cally within HD-Driven Phrase Structure Gram- mar (HPSG; Pollard and Sag, 1994), a lexicalist, constraint-based framework.",
    "acronym": "HD",
    "label": "Head"
  },
  {
    "sentence": "HD-Driven Statistical Models for Natural Language Parsing.",
    "acronym": "HD",
    "label": "Head"
  },
  {
    "sentence": "HD-Driven Phrase Structure Grammar.",
    "acronym": "HD",
    "label": "Head"
  },
  {
    "sentence": "1089 predicting grades as compared to a pure ML approach and to using CG only?",
    "acronym": "CG",
    "label": "Crowd Grades"
  },
  {
    "sentence": "0 Table 1: Regression Results Technique Model Code Feature Type Train r Validation r RR-1 RS/LR 0.51 0.47 Ridge Regression RR-2 Pure ML 0.54 0.47 RR-3 CG 0.63 0.57 RR-4 ML-CS 0.55 0.60 RR-5 All 0.76 0.76 SVM-1 RS/LR 0.50 0.46 SVM SVM-2 Pure ML 0.53 0.46 SVM-3 CG 0.62 0.57 SVM-4 ML-CS 0.60 0.61 SVM-5 All 0.75 0.74 NN-1 RS/LR 0.56 0.51 Neural Networks NN-2 Pure ML 0.60 0.44 NN-3 CG 0.63 0.57 NN-4 ML-CS 0.66 0.57 NN-5 All 0.80 0.76 between 1 and 1000, was selected based on the the least RMS error in cross-validation.",
    "acronym": "CG",
    "label": "Crowd Grades"
  },
  {
    "sentence": "Do CG add additional value in predicting grades over and above the features derived from the crowdsourced transcription?",
    "acronym": "CG",
    "label": "Crowd Grades"
  },
  {
    "sentence": "CG: The crowd transcribes the speech in addition to providing scores on each of the following?",
    "acronym": "CG",
    "label": "Crowd Grades"
  },
  {
    "sentence": "1090 Table 1: Regression Results Technique Model Code Feature Type Train r Validation r RR-1 RS/LR 0.51 0.47 Ridge Regression RR-2 Pure ML 0.54 0.47 RR-3 CG 0.63 0.57 RR-4 ML-CS 0.55 0.60 RR-5 All 0.76 0.76 SVM-1 RS/LR 0.50 0.46 SVM SVM-2 Pure ML 0.53 0.46 SVM-3 CG 0.62 0.57 SVM-4 ML-CS 0.60 0.61 SVM-5 All 0.75 0.74 NN-1 RS/LR 0.56 0.51 Neural Networks NN-2 Pure ML 0.60 0.44 NN-3 CG 0.63 0.57 NN-4 ML-CS 0.66 0.57 NN-5 All 0.80 0.76 between 1 and 1000, was selected based on the the least RMS error in cross-validation.",
    "acronym": "CG",
    "label": "Crowd Grades"
  },
  {
    "sentence": "1University of Engineering and Technology, VNU, Hanoi, Vietnam  2National Institute of Informatics, Tokyo, Japan  3European Bioinformatics Institute, Cambridge, UK  {vutm,lhquynh,thuypv,binhpt}@vnu.edu.vn, collier@ebi.ac.uk     Abstract  We describe a high precision system for ex- tracting events of biomedical significance that  was developed during the BioNLP shared task  2013 and tested on the CG data  set.",
    "acronym": "CG",
    "label": "Cancer Genetics"
  },
  {
    "sentence": "S. V. Ramanan RelAgent Private Ltd. 56, Venkatratnam Nagar Adyar, Chennai 600020 ramanan@npjoint.com P. Senthil Nathan RelAgent Private Ltd. 56, Venkatratnam Nagar Adyar, Chennai 600020 senthil@npjoint.com Abstract We tested a linguistically motivated rule- based system in the CG task  of the BioNLP13 shared task challenge.",
    "acronym": "CG",
    "label": "Cancer Genetics"
  },
  {
    "sentence": "I pre- pared a total of 17 such lexicons, which include 7 that were entered by hand (Greek letters, amino acids, chemical elements, known viruses, plus abbreviations of all these), and 4 corre- sponding to genes, chromosome locations, pro- teins, and cell lines, drawn from online public databases (CGWeb,2 BBID,3 Swis- sProt,4 and the Cell Line Database5).",
    "acronym": "CG",
    "label": "Cancer Genetics"
  },
  {
    "sentence": "c?2013 Association for Computational Linguistics Generalizing an Approximate Subgraph Matching-based System to Extract Events in Molecular Biology and CG Haibin Liu haibin.liu@nih.gov NCBI, Bethesda, MD, USA Karin Verspoor karin.verspoor@nicta.com.au NICTA, Melbourne, VIC, Australia Donald C. Comeau comeau@ncbi.nlm.nih.gov NCBI, Bethesda, MD, USA Andrew MacKinlay andrew.mackinlay@nicta.com.au NICTA, Melbourne, VIC, Australia W. John Wilbur wilbur@ncbi.nlm.nih.gov NCBI, Bethesda, MD, USA Abstract We participated in the BioNLP 2013 sh",
    "acronym": "CG",
    "label": "Cancer Genetics"
  },
  {
    "sentence": "1987  CG, Unification Grammar and Parsing.",
    "acronym": "CG",
    "label": "Categorial Grammar"
  },
  {
    "sentence": "The framework is based on  Combinatory CGs and it  uses the morpheme as the basic building  block of the categorial lexicon.",
    "acronym": "CG",
    "label": "Categorial Grammar"
  },
  {
    "sentence": "CG.",
    "acronym": "CG",
    "label": "Categorial Grammar"
  },
  {
    "sentence": "1987 CGs and Natural Language Structures, D.  Reidel, Dordrecht, Holland.",
    "acronym": "CG",
    "label": "Categorial Grammar"
  },
  {
    "sentence": "In order to perform morphological nd syntactic  compositions in a unified framework, the slash oper-  ators of CG must be enriched with  the knowledge about the type of process and the  type of morpheme.",
    "acronym": "CG",
    "label": "Categorial Grammar"
  },
  {
    "sentence": "\\[Calder, 89\\] J. Calder, M. Reape, and H. Zeevat,  \"An Algorithm for Generation i  Unification  CG\", Proceedings of the 4th  Conference of the European Chapter of the  ACL, pp.",
    "acronym": "CG",
    "label": "Categorial Grammar"
  },
  {
    "sentence": "Proceedings of the 3rd Workshop on Computational LNguistics for Literature (CLfL) @ EACL 2014, pages 40?49, Gothenburg, Sweden, April 27, 2014.",
    "acronym": "LN",
    "label": "Lin"
  },
  {
    "sentence": "Associa-  tion of Computational LNguistics.",
    "acronym": "LN",
    "label": "Lin"
  },
  {
    "sentence": "c?2014 Association for Computational LNguistics From Speaker Identification to Affective Analysis: A Multi-Step System for Analyzing Children?s Stories Elias Iosif?",
    "acronym": "LN",
    "label": "Lin"
  },
  {
    "sentence": "Com-  putational LNguistics, 19(1), March.",
    "acronym": "LN",
    "label": "Lin"
  },
  {
    "sentence": "Computational LNguistics,  19:313-330.",
    "acronym": "LN",
    "label": "Lin"
  },
  {
    "sentence": "Computational LNguistics, pages 535-  \"561.",
    "acronym": "LN",
    "label": "Lin"
  },
  {
    "sentence": "We filter most such cases by allowing only capital- ized LNs.",
    "acronym": "LN",
    "label": "location name"
  },
  {
    "sentence": "which allows LNs and connections to be captured starting from the given seed location set.",
    "acronym": "LN",
    "label": "location name"
  },
  {
    "sentence": "We acquire search engine snip- pets and extract contexts where LNs co- appear.",
    "acronym": "LN",
    "label": "location name"
  },
  {
    "sentence": "refers to at least 5 LNs including farms in Africa and Australia and a locality in Ireland.",
    "acronym": "LN",
    "label": "location name"
  },
  {
    "sentence": "In this paper we present a framework that given a few seed locations as a specification of a region, discovers additional locations (including alternate LNs) and map-like travel paths through this region labeled by transport type labels.",
    "acronym": "LN",
    "label": "location name"
  },
  {
    "sentence": "Given a LN L we start the search with patterns ?",
    "acronym": "LN",
    "label": "location name"
  },
  {
    "sentence": "3 The Algorithm As input to our algorithm we are given a seed of a few LNs specifiying some geograph- ical region.",
    "acronym": "LN",
    "label": "location name"
  },
  {
    "sentence": "LN.",
    "acronym": "LN",
    "label": "Logical Necessity"
  },
  {
    "sentence": "= 2.3 Recognition of Chinese LNs  Based on SVM  The identification process of location names is  the same as that of person names except for the  features extraction.",
    "acronym": "LN",
    "label": "Location Name"
  },
  {
    "sentence": "LC: uni-gram characters in LN or Geopo- litical entity.",
    "acronym": "LN",
    "label": "Location Name"
  },
  {
    "sentence": "= n i if if f FP FP FP                                  (12)  where ,  is the  number of F )2)((log)( 2 += iif FCFP )( iFC i as the i-th middle character of loca- tion names in the Chinese LNs Re- cord.",
    "acronym": "LN",
    "label": "Location Name"
  },
  {
    "sentence": "i in the Chinese LNs  Record.",
    "acronym": "LN",
    "label": "Location Name"
  },
  {
    "sentence": "=                                          (13)  where ,  is the  number of  S as the last character of location  names in the Chinese LNs Record.",
    "acronym": "LN",
    "label": "Location Name"
  },
  {
    "sentence": "=                                          (13)  where ,  is the  number of  S as the last character of location  names in the Chinese LNs",
    "acronym": "LN",
    "label": "Location Name"
  },
  {
    "sentence": "0 in the Chinese LNs  Record.",
    "acronym": "LN",
    "label": "Location Name"
  },
  {
    "sentence": "=                                      (11)  where ,  is the  number of F )2)((log)( 0200 += FCFPh )( 0FC 0 as the first character of location  names in the Chinese LNs Record.",
    "acronym": "LN",
    "label": "Location Name"
  },
  {
    "sentence": "5.2 Extracting Chinese LNs  We use 1.5M characters corpus of year 1998  from the People?s Daily as the training corpus  and extract sentences of year 2000 from the Peo- ple?s Daily (containing 2919 Chinese location  names) as testing corpus to conduct an open test  experiment.",
    "acronym": "LN",
    "label": "Location Name"
  },
  {
    "sentence": "Then the GE score is  computed as:    ????",
    "acronym": "GE",
    "label": "global entropy"
  },
  {
    "sentence": "The GE combines the local entropies for all antecedent candidates of a given anaphor.",
    "acronym": "GE",
    "label": "global entropy"
  },
  {
    "sentence": "GE?",
    "acronym": "GE",
    "label": "global entropy"
  },
  {
    "sentence": "The higher the GE, the higher the uncertainty of the model about the antecedent for an anaphor.",
    "acronym": "GE",
    "label": "global entropy"
  },
  {
    "sentence": "The GE aims to combine the uncertainty information from all antecedent candi- dates for a given anaphor (instead of considering only a single candidate-anaphor pair as for LE).",
    "acronym": "GE",
    "label": "global entropy"
  },
  {
    "sentence": "The GE c",
    "acronym": "GE",
    "label": "global entropy"
  },
  {
    "sentence": "We scaled the rows of the matrix using GE weights and used L = 0.9A and U = 1.1A. 4.4 Term Weighting and Dimension Choice for Multi-Document Summarization A natural term weighting can be obtained by com- puting the row sums of the dimension-reduced approximation to the term-sentence matrix.",
    "acronym": "GE",
    "label": "global entropy"
  },
  {
    "sentence": "c?2008 Association for Computational Linguistics GE Criteria for Semi-Supervised Learning of Conditional Random Fields Gideon S. Mann Google Inc. 76 Ninth Avenue New York, NY 10011 Andrew McCallum Department of Computer Science University of Massachusetts 140 Governors Drive Amherst, MA 01003 Abstract This paper presents a semi-supervised train- ing method for linear-chain conditional ran- dom fields that makes use of labele",
    "acronym": "GE",
    "label": "Generalized Expectation"
  },
  {
    "sentence": "GE Criteria for Semi-Supervised Learning of Conditional Random Fields.",
    "acronym": "GE",
    "label": "Generalized Expectation"
  },
  {
    "sentence": "Additionally, the alternat- ing projection method is computationally more effi- cient than GE (Mann and Mc- Callum, 2008) and can be applied in an online fash- ion using stochastic gradient.",
    "acronym": "GE",
    "label": "Generalized Expectation"
  },
  {
    "sentence": "Druck et al, 2008) constrain the predictions of a multinomial logistic regression model on unla- beled instances in a GE for- mulation for learning from labeled features.",
    "acronym": "GE",
    "label": "Generalized Expectation"
  },
  {
    "sentence": "ME with GE The objec- tive function and the constraints on expectations in (1) can be generalized to: max ?",
    "acronym": "GE",
    "label": "Generalized Expectation"
  },
  {
    "sentence": "2009 ACL and AFNLP GE Criteria for Bootstrapping Extractors using Record-Text Alignment Kedar Bellare Dept.",
    "acronym": "GE",
    "label": "Generalized Expectation"
  },
  {
    "sentence": "The GE Event and Protein Coreference tasks of the BioNLP Shared Task 2011.",
    "acronym": "GE",
    "label": "Genia"
  },
  {
    "sentence": "We further used some simple fea- tures from syntactic phrases (OpenNLP4 parser) and dependency parse trees (McDonald et al, 2005), ex- tracted using parsers trained on GE corpora.",
    "acronym": "GE",
    "label": "Genia"
  },
  {
    "sentence": "n from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.",
    "acronym": "GE",
    "label": "Genia"
  },
  {
    "sentence": "3 Trigger Word Tagging The training and the development abstracts were first tokenized and split into sentences using maxi- mum entropy models trained on the GE3 corpora.",
    "acronym": "GE",
    "label": "Genia"
  },
  {
    "sentence": "dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpus with precision p. dep(i,j,d,parser) i is head of token j with dependency d according to parser parser.",
    "acronym": "GE",
    "label": "Genia"
  },
  {
    "sentence": "dict(i,d) i appears in dictionary d. genia(i,p) i is event clue in the GE corpu",
    "acronym": "GE",
    "label": "Genia"
  },
  {
    "sentence": "As dictionaries we use a collection of cellular lo- cation terms taken from the GE event cor- pus (Kim et al, 2008), a small handpicked set of event triggers and a list of English stop words.",
    "acronym": "GE",
    "label": "Genia"
  },
  {
    "sentence": "Overview of the GE Event task in BioNLP Shared Task 2011.",
    "acronym": "GE",
    "label": "Genia"
  },
  {
    "sentence": "Overview of the GE task in BioNLP Shared Task 2011.",
    "acronym": "GE",
    "label": "Genia Event"
  },
  {
    "sentence": "The GE corpus (Kim, Ohta, and Tsujii 2008) contains 9,372 sentences where biological events are annotated with negation and uncertainty.",
    "acronym": "GE",
    "label": "Genia Event"
  },
  {
    "sentence": "Because the GE and BioScope corpus share 958 abstracts, it is possible to compare their annotations, as it is done by Vincze et al (2010).",
    "acronym": "GE",
    "label": "Genia Event"
  },
  {
    "sentence": "The GE and Protein Coreference tasks of the BioNLP Shared Task 2011.",
    "acronym": "GE",
    "label": "Genia Event"
  },
  {
    "sentence": "Lever- aging existing resources using GE criteria.",
    "acronym": "GE",
    "label": "generalized expectation"
  },
  {
    "sentence": "Equation (5) is an instance of GE criteria (Mann and Mc- Callum, 2008) that penalizes the divergence of a specific model expectation from a given target value.",
    "acronym": "GE",
    "label": "generalized expectation"
  },
  {
    "sentence": "The use of GE criteria allows for a dramatic reduction in annotation time by shifting from traditional instance-labeling to feature-labeling, and the methods presented outperform traditional CRF training and other semi-supervised methods when limited human effort is available.",
    "acronym": "GE",
    "label": "generalized expectation"
  },
  {
    "sentence": "The use of GE criteria allow",
    "acronym": "GE",
    "label": "generalized expectation"
  },
  {
    "sentence": "This is accom- plished by using GE cri- teria to express a preference for parameter set- tings in which the model?s distribution on un- labeled data matches a target distribution.",
    "acronym": "GE",
    "label": "generalized expectation"
  },
  {
    "sentence": "6 Conclusion We have presented GE criteria for linear-chain conditional random fields, a new semi-supervised training method that makes use of labeled features rather than labeled instances.",
    "acronym": "GE",
    "label": "generalized expectation"
  },
  {
    "sentence": "Semi-supervised learning of dependency parsers using GE criteria.",
    "acronym": "GE",
    "label": "generalized expectation"
  },
  {
    "sentence": "is a MOD of ngabulu milk.",
    "acronym": "MOD",
    "label": "modifier"
  },
  {
    "sentence": "It has a rich system of case marking, and ad- nominal MODs agree with the heads they modify in case, number, and four genders.",
    "acronym": "MOD",
    "label": "modifier"
  },
  {
    "sentence": "Furthermore, head nouns are gener- ally not required: argument positions can be instan- tiated by MODs only, or, if the referent is clear from the context, by no nominal constituent of any kind.",
    "acronym": "MOD",
    "label": "modifier"
  },
  {
    "sentence": "marking, and ad- nominal MODs agree with the heads they modify in case, number, and four genders.",
    "acronym": "MOD",
    "label": "modifier"
  },
  {
    "sentence": "That is, aside from the constraint that verbal clauses require a clitic cluster (marking subject and object agreement and tense, aspect and mood) in second position, the word order is otherwise free, to the point that noun phrases can be non-contiguous, with head nouns and their MODs separated by un- related words.",
    "acronym": "MOD",
    "label": "modifier"
  },
  {
    "sentence": "rder is otherwise free, to the point that noun phrases can be non-contiguous, with head nouns and their MODs separated by un- related words.",
    "acronym": "MOD",
    "label": "modifier"
  },
  {
    "sentence": "the constraint that verbal clauses require a clitic cluster (marking subject and object agreement and tense, aspect and mood) in second position, the word order is otherwise free, to the point that noun phrases can be non-contiguous, with head nouns and their MODs separated by un- related words.",
    "acronym": "MOD",
    "label": "modifier"
  },
  {
    "sentence": "are predicated of the same entity re- quires a departure from the ordinary way that heads are combined with arguments and MODs com- bined with heads in HPSG in general and in the Matrix in particular.3 In the Grammar Matrix, as in most work in HPSG, lexical heads record the de- pendents they require in valence lists (SUBJ, COMPS, SPR).",
    "acronym": "MOD",
    "label": "modifier"
  },
  {
    "sentence": "For the same reason, they will appear in postnominal posi- tion when acting as MODs.",
    "acronym": "MOD",
    "label": "modifier"
  },
  {
    "sentence": "However~ it has been argued convincingly by  Ross  (1967a) and others that MOD  should be analyzed as main  ver\"bs  o?",
    "acronym": "MOD",
    "label": "modals"
  },
  {
    "sentence": "Verb moods are composed with the aid of MOD, while tenses and expressions are built with the aid of auxiliary verbs.",
    "acronym": "MOD",
    "label": "modals"
  },
  {
    "sentence": "We also assign the main clause to past, present, or future tense by using the fine-grained POS tag and a set of heuristics (for example, to check for MOD).4 We assign a tense agreement score to each verb v as follows.",
    "acronym": "MOD",
    "label": "modals"
  },
  {
    "sentence": "Many lexical and grammatical devices aid hedging (expressions such as epistemics verbs, MOD, adjectives, etc.",
    "acronym": "MOD",
    "label": "modals"
  },
  {
    "sentence": "The present pr`oblem~ is  in  fact~ another  point in favor` of  the v iew that  MOD  o r ig inate  in a  higher\" sentence~ because i t  enables us to acknowledge the s imi la r i ty   of  the anomaly  in (10) and ( \\ ]1 ) .",
    "acronym": "MOD",
    "label": "modals"
  },
  {
    "sentence": "Secondly, since many stop words (e.g. determiners, MOD) typi- cally demonstrate little variation in the dependencies they engage in, we ignore type equivalences for stop words and implement only exact matching of depen- dencies. (",
    "acronym": "MOD",
    "label": "modals"
  },
  {
    "sentence": "CPs in Urdu.",
    "acronym": "CP",
    "label": "Complex Predicate"
  },
  {
    "sentence": "CPs.",
    "acronym": "CP",
    "label": "Complex Predicate"
  },
  {
    "sentence": "CPs:Structure and  Theory.",
    "acronym": "CP",
    "label": "Complex Predicate"
  },
  {
    "sentence": "Words by Default:  Inheritance and the Persian CP  Construction.?",
    "acronym": "CP",
    "label": "Complex Predicate"
  },
  {
    "sentence": "CPs in In- dian Language Wordnets, Lexical Resources and  Evaluation Journal, 40 (3-4).",
    "acronym": "CP",
    "label": "Complex Predicate"
  },
  {
    "sentence": "Detecting CPs in Hindi using  POS Projection across Parallel Corpora, Proceed- ings of the Workshop on Multiword Expressions:  Identifying and Exploiting Underlying Properties,  Sydney, 11?18,  Alex Alsina.",
    "acronym": "CP",
    "label": "Complex Predicate"
  },
  {
    "sentence": "Multidimensionality of  Representation: NV CPs in Hindi.",
    "acronym": "CP",
    "label": "Complex Predicate"
  },
  {
    "sentence": "On the task of identifying cognates from over 21,000 words in 218 different lan- guages from the Oceanic language family, our model achieves a CP score over 91%, while maintaining pairwise recall over 62%.",
    "acronym": "CP",
    "label": "cluster purity"
  },
  {
    "sentence": "On the larger Oceanic data, our model can achieve CP scores of 91.8%, while maintaining pairwise recall of 62.1%.",
    "acronym": "CP",
    "label": "cluster purity"
  },
  {
    "sentence": "The weighted average of  CP (i.e. the number of predominant tags  divided by cluster size) was measured at 88.8%,  which exceeds significantly the precision of 53%  on word type as reported by Sch?tze (1995) on a  related task.",
    "acronym": "CP",
    "label": "cluster purity"
  },
  {
    "sentence": "We report CP, accuracy, precision, recall, and F1 for our latent variable logistic classifier (LogLV) and a baseline that assigns arguments to clusters accord- ing to their syntactic function (SyntFunc).",
    "acronym": "CP",
    "label": "cluster purity"
  },
  {
    "sentence": "Our model achieves a CP score of 90.3% on this dataset com- pared to 89.7% reported in Grenager and Manning.",
    "acronym": "CP",
    "label": "cluster purity"
  },
  {
    "sentence": "First, we report CP, which is a kind of pre- cision measure for clusterings.",
    "acronym": "CP",
    "label": "cluster purity"
  },
  {
    "sentence": "215  Chart Parsing and CP  Frank Morawietz  Seminar flit Sprachwissenschaft  Universifiit Tfibingen  Wilhehnstr.",
    "acronym": "CP",
    "label": "Constraint Programming"
  },
  {
    "sentence": "This solver has been developed in the CP paradigm using the constraint satisfaction approach of (Duchier and Niehren, 2000).",
    "acronym": "CP",
    "label": "Constraint Programming"
  },
  {
    "sentence": "Concurrent  CP Languages.",
    "acronym": "CP",
    "label": "Constraint Programming"
  },
  {
    "sentence": "Po-  sition paper for the First Workshop on Princi-  ples and Practice of CP  (Rhode Island, April 1993).",
    "acronym": "CP",
    "label": "Constraint Programming"
  },
  {
    "sentence": "Principles of CP.",
    "acronym": "CP",
    "label": "Constraint Programming"
  },
  {
    "sentence": "In P. van Hentenryck and  V. Saraswat, editors, Principles and Practice of  CP, chapter 2, pages 27-  48.",
    "acronym": "CP",
    "label": "Constraint Programming"
  },
  {
    "sentence": "Figure 2 shows the convergence plot of the composite pairwise coreference function based on RF8.",
    "acronym": "RF",
    "label": "Random Forest"
  },
  {
    "sentence": "In a 10-fold cross-validation experiment, we tested a RF classifier (Breiman, 2001) and an SVM (Platt, 1998) with polynomial ker- nel.",
    "acronym": "RF",
    "label": "Random Forest"
  },
  {
    "sentence": "190 To understand the mistakes of the classifier, we manually assessed error patterns within the model of the RF classifier.",
    "acronym": "RF",
    "label": "Random Forest"
  },
  {
    "sentence": "With an overall macro-averaged F1 of .79, RF yielded the best results, both with respect to precision as well as recall.",
    "acronym": "RF",
    "label": "Random Forest"
  },
  {
    "sentence": "RFs.",
    "acronym": "RF",
    "label": "Random Forest"
  },
  {
    "sentence": "2 ranker improved the results for both RF as well as the SVM, so we limited our feature set to the 100 best features.",
    "acronym": "RF",
    "label": "Random Forest"
  },
  {
    "sentence": "The results suggest that knowledge of the student?s confusion-related facial expressions can signifi-cantly enhance dialogue act classification for two types of dialogue acts, GROUNDING and RF.",
    "acronym": "RF",
    "label": "REQUEST FOR FEEDBACK"
  },
  {
    "sentence": "This paper has reported on a first step toward using knowledge of user facial expres-sions to improve a dialogue act classification mod-el for tutorial dialogue, and the results demonstrate that facial expressions hold great promise for dis-tinguishing the pedagogically relevant dialogue act RF, and the conversational moves of GROUNDING.",
    "acronym": "RF",
    "label": "REQUEST FOR FEEDBACK"
  },
  {
    "sentence": "For example, a student?s RF requires the tutor to assess the condition of the task, rather than to query the in-domain factual knowledge base.",
    "acronym": "RF",
    "label": "REQUEST FOR FEEDBACK"
  },
  {
    "sentence": "For RF, the predictive features were presence or absence of AU4 within ten seconds of the longest available history (three turns in the past), as well as the presence of AU4 within five seconds of the current utterance (the utterance whose dialogue act is being classified).",
    "acronym": "RF",
    "label": "REQUEST FOR FEEDBACK"
  },
  {
    "sentence": "NICTA Victoria mhlui@unimelb.edu.au, ned@nedletcher.net, oadams@student.unimelb.edu.au, lduong@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.net Abstract The Discriminating between Similar Languages (DSL) shared task at VarDial challenged partici- pants to build an automatic LI system to discriminate between 13 languages in 6 groups of highly-similar languages (or national varieties of the same language).",
    "acronym": "LI",
    "label": "language identification"
  },
  {
    "sentence": "Cross-domain feature selection for LI.",
    "acronym": "LI",
    "label": "language identification"
  },
  {
    "sentence": "We examined a number of text representations for the per-group language identifiers, including a standard representation for LI based on language-indicative byte sequences, as well as with de-lexicalized text representations.",
    "acronym": "LI",
    "label": "language identification"
  },
  {
    "sentence": "4 Conclusion Discriminating between similar languages is an interesting sub-problem in LI, and the DSL shared task at VarDial has given us an opportunity to examine possible solutions in greater detail.",
    "acronym": "LI",
    "label": "language identification"
  },
  {
    "sentence": "langid.py: An off-the-shelf LI tool.",
    "acronym": "LI",
    "label": "language identification"
  },
  {
    "sentence": "Linguini: LI for multilingual documents.",
    "acronym": "LI",
    "label": "language identification"
  },
  {
    "sentence": "\u0002 a bucketed LI smoothing for both models.",
    "acronym": "LI",
    "label": "linear interpolation"
  },
  {
    "sentence": "Compensation vectors for new envi-  ronments are obtained by LI of several of the  MFCDCN compensation vectors:  ?",
    "acronym": "LI",
    "label": "linear interpolation"
  },
  {
    "sentence": "We tested the LI of the in- and general-domain translation models as follows: Given one model which assigns the probability P1(t|s) to the trans- lation of source string s into target string t, and a second model which assigns the probability P2(t|s) to the same event, then the interpolated translation probability is: P (t|s) = ?",
    "acronym": "LI",
    "label": "linear interpolation"
  },
  {
    "sentence": "Table 2 shows the resulting interpolation coefficients for the tag language model using the usual LI smoothing formula ZL}-~IPA?x?)[ffi] ^\u0012_ ]A^fi` \u0014 1 ]A^ffi`ba\u000fc \u0013??",
    "acronym": "LI",
    "label": "linear interpolation"
  },
  {
    "sentence": "Relative frequency can be used directly for es-  timating the word probabilities, and trigram  backoff and LI can be used for  estimating the POS probabilities.",
    "acronym": "LI",
    "label": "linear interpolation"
  },
  {
    "sentence": "Given a derivation d, most existing phrase- based models approximate the derivation probabil- ity through a LI of a finite set of feature functions (?(",
    "acronym": "LI",
    "label": "linear interpolation"
  },
  {
    "sentence": "Proceedings of the 3rd Workshop on Computational LIics for Literature (CLfL) @ EACL 2014, pages 40?49, Gothenburg, Sweden, April 27, 2014.",
    "acronym": "LI",
    "label": "Linguist"
  },
  {
    "sentence": "Associa-  tion of Computational LIics.",
    "acronym": "LI",
    "label": "Linguist"
  },
  {
    "sentence": "c?2014 Association for Computational LIics From Speaker Identification to Affective Analysis: A Multi-Step System for Analyzing Children?s Stories Elias Iosif?",
    "acronym": "LI",
    "label": "Linguist"
  },
  {
    "sentence": "Com-  putational LIics, 19(1), March.",
    "acronym": "LI",
    "label": "Linguist"
  },
  {
    "sentence": "Computational LIics,  19:313-330.",
    "acronym": "LI",
    "label": "Linguist"
  },
  {
    "sentence": "Computational LIics, pages 535-  \"561.",
    "acronym": "LI",
    "label": "Linguist"
  },
  {
    "sentence": "Statistical analysis of  LNRE and related  problems.",
    "acronym": "LNRE",
    "label": "large number of rare events"
  },
  {
    "sentence": "The statistical analysis of LNRE?,",
    "acronym": "LNRE",
    "label": "large number of rare events"
  },
  {
    "sentence": "2 Statistical considerations  2.1 Highly skewed distributions  As first observed e.g. by Zipf (1935, 1949) the  frequency of words and other linguistic units tend  to follow highly skewed distributions in which  there are a LNRE.",
    "acronym": "LNRE",
    "label": "large number of rare events"
  },
  {
    "sentence": "Sta-  tistical Analysis of LNRE  and Related Problems.",
    "acronym": "LNRE",
    "label": "Large Number of Rare Events"
  },
  {
    "sentence": "x)] Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to specify a PT indicating entailment.",
    "acronym": "PT",
    "label": "probability threshold"
  },
  {
    "sentence": "A more  sophisticated one is the PT strategy,  in which all the categories above a user-defined thresh-  old are assigned to a document.",
    "acronym": "PT",
    "label": "probability threshold"
  },
  {
    "sentence": "log-PT - -  the difference between  the log-probability score of the top-most hypothesis  and the bottom-most hypothesis at any given state  of the stack cannot be larger than a given threshold.",
    "acronym": "PT",
    "label": "probability threshold"
  },
  {
    "sentence": "A lower PT of 0.01 was set, so that hun- dreds of tags of equal likelihood were not produced in the case where the tagger was unable to make an informed prediction.",
    "acronym": "PT",
    "label": "probability threshold"
  },
  {
    "sentence": "Ju- rafsky (1996) assumes that processing difficulty is triggered if the correct analysis falls below a certain PT (i.e., is pruned by the parser).",
    "acronym": "PT",
    "label": "probability threshold"
  },
  {
    "sentence": "Given a PT \u000f (0 < \u000f < 1) and a span threshold s, the document is said to support the hypothesis ??",
    "acronym": "PT",
    "label": "probability threshold"
  },
  {
    "sentence": "This algorithm searches the PT in a left-  to-right, breadth-first fashion that obeys the  major reflexive pronoun constraints while giv-  ing a preference to antecedents hat are closer  to the pronoun.",
    "acronym": "PT",
    "label": "parse tree"
  },
  {
    "sentence": "We have im-  plemented a slightly modified version of Hobbs  algorithm for the Tree-bank PTs.",
    "acronym": "PT",
    "label": "parse tree"
  },
  {
    "sentence": "Feature Sets entity type of e1 and e2 words in e1 and e2 word bigrams in e1 and e2 POS of e1 and e2 words between e1 and e2 word bigrams between e1 and e2 POS between e1 and e2 distance between e1 and e2 distance between e1 and e2 in the dependency graph steps in PT to get e1 and e2 in the same phrase various combinations of the above features Table 2: Our feature set for the MIRA classifier that pre- dicts binary relations.",
    "acronym": "PT",
    "label": "parse tree"
  },
  {
    "sentence": "Due to relatively large differences between  Tree:bank PTs and Hobbs' trees, our  Hobbs' implementation does not yield as high  an accuracy as it would have if we had had  perfect Hobbs' tree representations.",
    "acronym": "PT",
    "label": "parse tree"
  },
  {
    "sentence": "In order to evaluate the alignments we computed the fraction of correctly transferred edges as a function of the average number of edges transferred by using supervised PTs on the target side.",
    "acronym": "PT",
    "label": "parse tree"
  },
  {
    "sentence": "We consider a PT on the source language as a set of dependency edges to be transferred.",
    "acronym": "PT",
    "label": "parse tree"
  },
  {
    "sentence": "The full pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and filter long sentences); (2) build language models; (3) create word alignments in each direction; (4) symmetrize directional word alignments; (5) build PT; (6) tune weights for the PT.",
    "acronym": "PT",
    "label": "phrase table"
  },
  {
    "sentence": "Similarly, the PT size is significantly re- duced by 35%, while the gains on the tail docu- ments range from 0.6 to 1.4.",
    "acronym": "PT",
    "label": "phrase table"
  },
  {
    "sentence": "Contrary to conventional wisdom in the MT community, bigger PTs did not always perform better.",
    "acronym": "PT",
    "label": "phrase table"
  },
  {
    "sentence": "In 14 out of 18 cases, the threshold picked was 0.4 (medium size PTs) and the other four times 0.2 was picked (smaller PTs).",
    "acronym": "PT",
    "label": "phrase table"
  },
  {
    "sentence": "We also used the English/ PT (En-Pt), PT/Spanish (Pt-Es), PT/French (Pt-Fr), and Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of 40%/60%.",
    "acronym": "PT",
    "label": "Portuguese"
  },
  {
    "sentence": "articles required in one language but optional in the other (e.g., English Cars use gas and PT Os carros usam gasolina), cases where the content is expressed using multiple words in one language and a single word in the other language (e.g., agglutination such as English weapons of mass destruction and German Massenvernichtungswaffen), and expres- sions translated indirectly.",
    "acronym": "PT",
    "label": "Portuguese"
  },
  {
    "sentence": "483 Computational Linguistics Volume 36, Number 3 Table 1 Test corpora statistics: English?French, English?Spanish, English?PT, PT?Spanish, PT?French, and Spanish?French.",
    "acronym": "PT",
    "label": "Portuguese"
  },
  {
    "sentence": "G. & C. Merriam Co., Springfield, Mass.  158  Appendix A. PT  DECL PRON*  VERB* \"ate\"  DET  NOUN*  ?",
    "acronym": "PT",
    "label": "Parse Trees"
  },
  {
    "sentence": "However, these attachments become  unsatisfactory after the system understands the next  20 Computational Linguistics, Volume 15, Number 1, March 1989  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  sentence.",
    "acronym": "PT",
    "label": "Parse Trees"
  },
  {
    "sentence": "3 Reranking PT Using SRL Information Here we give the general framework for the rerank- ing methods that we present in the next section.",
    "acronym": "PT",
    "label": "Parse Trees"
  },
  {
    "sentence": "0362-613X/89/010019-32503.00  Computational Linguistics, Volume 15, Number 1, March 1989 19  Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous PT  . ,",
    "acronym": "PT",
    "label": "Parse Trees"
  },
  {
    "sentence": "The  node \\[1,8oo,v\\] represents a VP with head word  21  Jungyun See and Robert F. Simmons Syntactic Graphs: ,% Representation for the Union of All Ambiguous PT  GRAMMAR RULES AND CORRESPONDING TRIPLES:  Grammar rules arc-name head modifier  i. SNT--~NP VP snp header  VP head of NP  2.",
    "acronym": "PT",
    "label": "Parse Trees"
  },
  {
    "sentence": "PT Used SRL F1 Gold 77.1 1-best 63.9 Reranked by gold parse F1 68.1 Reranked by gold frame F1 74.2 Simple SRL combination (?",
    "acronym": "PT",
    "label": "Parse Trees"
  },
  {
    "sentence": "This is PTly because selection restrictions are  not clearcut in many cases.",
    "acronym": "PT",
    "label": "part"
  },
  {
    "sentence": "The probability that a referent is in a PTic-  166  ular gender class is just the relative frequency  with which that referent is referred to by a pro-  noun p that is PT of that gender class.",
    "acronym": "PT",
    "label": "part"
  },
  {
    "sentence": "Given a PTicular choice of the antecedent  candidates, the distance is independent of  distances of candidates other than the an-  tecedent (and the distance to non-referents  can be ignored):  P(so, d~a, 2~) o?",
    "acronym": "PT",
    "label": "part"
  },
  {
    "sentence": "In PTicular, the scheme infers the gender of a  referent from the gender of the pronouns that  161  refer to it and selects referents using the pro-  noun anaphora program.",
    "acronym": "PT",
    "label": "part"
  },
  {
    "sentence": "When viewed in this way, a can be regarded as  an index into these vectors that specifies which  value is relevant o the PTicular choice of an-  tecedent.",
    "acronym": "PT",
    "label": "part"
  },
  {
    "sentence": "These questions were machine translated to Spanish, French and Brazilian PT using IBM?s n.",
    "acronym": "PT",
    "label": "Portugese"
  },
  {
    "sentence": "We will initially focus on the case of English/Brazillian-PT but we intend our work to be generalizable to other language pairs.",
    "acronym": "PT",
    "label": "Portugese"
  },
  {
    "sentence": "UAS with out preprocessing: Arabic 65.19 77.74 79.02 76.74  Chinese 84.27 89.46 86.42 90.03  Czech 76.24 83.4 83.52 82.88  Danish 81.72 88.64 86.11 88.45  Dutch 71.77 75.49 75.83 74.97  German 84.11 87.66 90.67 87.53  Japanese 89.91 93.12 92.40 92.99  PT 85.07 90.3 88.00 90.21  Slovene 71.42 81.14 80.96 80.43  Spanish 80.46 85.15 88.90 85.19  Swedish 81.08 88.57 83.99 88.83  Turkish 61.22 74.49 73.91 74.3  AV: 77.7 84.6 84.1 84.38  SD: 8.67 6.15 5.78 6.42  Bulgarian 86.34 91.3 89.27 91.44  Table 1: Results  195  An Alternative Conception of  Tree-Adjoining Derivation  Yves Schabes*  Mitsubishi Electric Research Laboratory  Stuart M. Shie",
    "acronym": "PT",
    "label": "Portugese"
  },
  {
    "sentence": "We enumerate  them below as a key: Arabic (ar), Danish (da),  German (de), English (en), Spanish (es), French  (fr), Indonesian (Malay) (id), Italian (it), Japanese  (ja), Korean (ko), Malaysian (Malay) (ms), Dutch  (nl), PT (pt), Russian (ru), Thai (th), Viet- namese (vi) and Chinese (zh).",
    "acronym": "PT",
    "label": "Portugese"
  },
  {
    "sentence": "Our initial investigations using this set for English, Spanish, French, Brazilian PT and German text, suggest min- imal modification is required to adapt for use in question answering.",
    "acronym": "PT",
    "label": "Portugese"
  },
  {
    "sentence": "workshop on joint evaluation of computational processing of PT at PorTAL 2002.",
    "acronym": "PT",
    "label": "Portugese"
  },
  {
    "sentence": "PT the user to interactively enter sam- ple documents (used primarily for testing, or entering queries).",
    "acronym": "PT",
    "label": "Prompts"
  },
  {
    "sentence": "4.2 Automatic Generation of Corrective PT In this study, not all prompts were modified to match the user?s choice of words.",
    "acronym": "PT",
    "label": "Prompts"
  },
  {
    "sentence": "PT.",
    "acronym": "PT",
    "label": "Prompts"
  },
  {
    "sentence": "Of course, the role of PT and SI is not surprising, although interest- ingly they are significant only in association with certain tutor moves.",
    "acronym": "PT",
    "label": "Prompts"
  },
  {
    "sentence": "4PT are short texts that present an argument or issue and ask test takers to respond to it, either by analyzing the given argument or taking a stance on the given issue.",
    "acronym": "PT",
    "label": "Prompts"
  },
  {
    "sentence": "For example, since Student Initiatives (SI) are not as frequent, we needed to double code more sessions to find a number of SI?s high enough to compute a meaningful Kappa (in our whole corpus, there are 1157 SIs but e.g. 4957 PT).",
    "acronym": "PT",
    "label": "Prompts"
  },
  {
    "sentence": "The only evaluation set where Romanian-English data leads to better performance is the PT set.",
    "acronym": "PT",
    "label": "Probable alignments"
  },
  {
    "sentence": "That is, \u0000\u0002\u0001\u0004\u0003\u0006\u0005 ff \u0007 ff\b\u0007\u0006\t \u0001 \u0005   \u0005 \u0005 \u000b \u0005 \u0001\u0004\u0003\u0006\u0005 \u0003 \u0010fi\u0010 \u0001 \u0005   \u0005 \u0005 \f \u0005 (4) The F?measure is the harmonic mean of precision and recall:  \u000f\u000e\u001a\u0012 \u0003 \u0003 \u0007\u0011\u0010 \u0001\u0004\u0003 \u0001 \u0012  \u0000\u0013\u0001\u0004\u0003\u0014\u0005 ff \u0007 ff\b\u0007\u0006\t  \u0001\u0004\u0003\u0006\u0005 \u0003 \u0010fi\u0010 \u0000\u0002\u0001\u0004\u0003\u0006\u0005 ff \u0007 ff\b\u0007\u0006\t\u0016\u0015 \u0001\u0004\u0003\u0006\u0005 \u0003 \u0010fi\u0010 (5) AER is defined by (Och and Ney, 2000a) and accounts for both Sure and PT in scoring.",
    "acronym": "PT",
    "label": "Probable alignments"
  },
  {
    "sentence": "PT are large blocks of words which the annotator was uncertain of how to align.",
    "acronym": "PT",
    "label": "Probable alignments"
  },
  {
    "sentence": "The intersection of the Sure alignments pro- duced by the two annotators led to the final Sure aligned set, while the reunion of the PT led to the final Probable aligned set.",
    "acronym": "PT",
    "label": "Probable alignments"
  },
  {
    "sentence": "Given an alignment \u0003, and a gold standard alignment \u0004, each such alignment set eventually consisting of two sets \u0003 \u0001 , \u0003 \u0002 , and \u0004 \u0001 , \u0004 \u0002 corresponding to Sure and PT, the following measures are defined (where \u0005 is the alignment type, and can be set to either S or P).",
    "acronym": "PT",
    "label": "Probable alignments"
  },
  {
    "sentence": "While this may seem contradictory, AER factors in both Sure and PT into is scoring while only the English?French data included such alignments in its gold standard.",
    "acronym": "PT",
    "label": "Probable alignments"
  },
  {
    "sentence": "Sen -- sentence branch     full_propernoun -- proper noun phrase propernoun JOHN [Sexed]     neg_copula -- copula verb phrase asCopulaN RETIRE [Fut] comnoun CHAIRMAN [Sing,Per3] Figure 3: PT for \"John will retire as chairman\".",
    "acronym": "PT",
    "label": "Parse tree"
  },
  {
    "sentence": "I I l l  I i I l i   ' t i l t   ADJ'* \"a\"  \"fish\"  PREP \"with\"  DET ADJ*  NOUN* \"fork\"  t ta t '   Tree I. PT for a syntactically ambiguous PP attachment  DECL NP  VERB*  NP  PUNC  ' t i \"  PRON*  ttwant\"  DET  NOUN*  PP  I t  t,   ADJ*  \"book\"  PREP  DET  NOUN*  ~LCL   \"the\"  \"by\"  ADJ'*  \"uncle\"  NP  VERB*  PP  \"my\"  PRON* \"that\"  'tlSt'  PREP \"on\"  DET ADJ*  NOUN* \"shelf\"  \"the\"  Tree 2.",
    "acronym": "PT",
    "label": "Parse tree"
  },
  {
    "sentence": "PT for \"About which very important topic has John talked on Sunday?\"",
    "acronym": "PT",
    "label": "Parse tree"
  },
  {
    "sentence": "PT for \"Bill gives the woman a book which",
    "acronym": "PT",
    "label": "Parse tree"
  },
  {
    "sentence": "The best TB2 feature set is approximately the same as the best FN feature set in spite of the dif- ferences between the datasets (PTs: TB2 ?",
    "acronym": "PT",
    "label": "Parse tree"
  },
  {
    "sentence": "PT for \"Which man's woman will he persuade to date him?\"",
    "acronym": "PT",
    "label": "Parse tree"
  },
  {
    "sentence": "edu/?mgo031000/ppa/ 274 FN TB2 Source FrameNet annotation samples (British National Corpus) Penn Treebank-II (WSJ articles) Instance identifica- tion Semantic-centered (related to Frame Elements) Syntactic-centered (related to the structure of the parse tree) PTs Automatically generated (Charniak) Gold standard Total size 27,421 instances 60,699 instances Distribution statistics 70.28% ambiguous verb attachments 2.36:1 v-attch:n-attch 35.71% ambiguous verb attachments 1:1.8 v-attch:n-attch Training / test sets 90% - 10% ?",
    "acronym": "PT",
    "label": "Parse tree"
  },
  {
    "sentence": "Further, the de-  sign of a graphical user-interface to enter the  queries is planned, allowing to specify queries  by drawing PTs instead of typing in  the expressions in the query language.",
    "acronym": "PT",
    "label": "partial tree"
  },
  {
    "sentence": "Here in incremental dependency parsing we define the loss function between a gold tree y and an incorrect PT z as the number of incorrect edges in z, plus the number of correct edges in y which are already ruled out by z. This MIRA extension results in slightly higher accuracy of 92.36, which we will use as the pure online learn- ing baseline in the comparisons below.",
    "acronym": "PT",
    "label": "partial tree"
  },
  {
    "sentence": "Horeover~ a PT in the whole parse  tree plays a role of adjusting semantic and syntactic  interpretation.",
    "acronym": "PT",
    "label": "partial tree"
  },
  {
    "sentence": "On the basis of these derivation trees, we selected several features for training our disam- biguation models: local trees of depth 1, several lev- els of grandparenting, i.e. inclusion of grandpar- ent node (GP 2), great-grandparent node (GP 3) and great-great-grandparent node (GP 4), PTs of depth 1 (+AE).",
    "acronym": "PT",
    "label": "partial tree"
  },
  {
    "sentence": "The idea is that the supertags might give a more fine-grained definition of struc- ture, using PTs rather than parts of speech.",
    "acronym": "PT",
    "label": "partial tree"
  },
  {
    "sentence": "Bottom-up generation locally applies the translation model to a PT.",
    "acronym": "PT",
    "label": "partial tree"
  },
  {
    "sentence": "using Europarl LM & PT (max phrase length 7) ?",
    "acronym": "PT",
    "label": "Phrase Table"
  },
  {
    "sentence": "using Europarl LM & PT (max phrase length 10) ?",
    "acronym": "PT",
    "label": "Phrase Table"
  },
  {
    "sentence": "2 PT Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large nega- tive impact in the translation quality.",
    "acronym": "PT",
    "label": "Phrase Table"
  },
  {
    "sentence": "A Systematic Comparison of PT Pruning Techniques.",
    "acronym": "PT",
    "label": "Phrase Table"
  },
  {
    "sentence": "3.3 Additional PT of bilingual MWEs Wu et al (2008) proposed a method to construct a phrase table by a manually-made translation dic- tionary.",
    "acronym": "PT",
    "label": "Phrase Table"
  },
  {
    "sentence": "or, How to  Extract Paraphrases from PTs   1Roland Kuhn, 1Boxing Chen, 1George Foster and  2Evan Stratford  1National Research Council of Canada  2University of Waterloo  1First.Last@nrc.gc.ca; 2evan.stratford@gmail.com    Abstract  This paper describes how to cluster to- gether the phrases of a phrase-based sta- tistical machine translation (SMT) sys- tem, using information in the phrase table  itself.",
    "acronym": "PT",
    "label": "Phrase Table"
  },
  {
    "sentence": "As future work we aim at allowing for the comparison at fragment level, where a fragment is considered a part of a function, a GR of functions.",
    "acronym": "GR",
    "label": "group"
  },
  {
    "sentence": "9 Acknowledgements   The authors would like to thank Mark Johnson  and other members of the Brown NLP GR  for many useful ideas and NSF and ONR for  support (NSF grants IRI-9319516 and SBR-  9720368, ONR grant N0014-96-1-0549).",
    "acronym": "GR",
    "label": "group"
  },
  {
    "sentence": "In clustering, objects are GRed together according to their feature value distribution, not to a predefined classification (as is the case when using supervised techniques), so that we achieve a better guarantee that we are learning a structure already present in the data.",
    "acronym": "GR",
    "label": "group"
  },
  {
    "sentence": "3.1 The  gender /an imat ic i ty  stat ist ics   After we have identified the correct antecedents  it is a simple counting procedure to compute  P(p\\[wa) where wa is in the correct antecedent  for the pronoun p (Note the pronouns are  GRed by their gender):  \\[ wain the antecedent for p \\[  P(pl o) =  When there are multiple relevant words in the  antecedent we apply the likelihood test designed  by Dunning (1993) on all the words in the candi-  date NP.",
    "acronym": "GR",
    "label": "group"
  },
  {
    "sentence": "To make sure that the resulting system is practically useful, several user GRs have been identified, who drive the interface development process by providing practical use cases.",
    "acronym": "GR",
    "label": "group"
  },
  {
    "sentence": "Instead of computing the proba-  bUity for each one of them we GR them into  \"buckets\", so that rrt a iS the bucket for the num-  ber of times that a is mentioned.",
    "acronym": "GR",
    "label": "group"
  },
  {
    "sentence": "A Statistical Approach to AnaphGRa Resolution  Niyu  Ge, John  Hale and Eugene Charn iak   Dept.",
    "acronym": "GR",
    "label": "or"
  },
  {
    "sentence": "brown, edu  Abst ract   This paper presents an algGRithm fGR identi-  fying pronominal anaphGRa and two experi-  ments based upon this algGRithm.",
    "acronym": "GR",
    "label": "or"
  },
  {
    "sentence": "We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head infGRma-  tion and noun phrase repetition.",
    "acronym": "GR",
    "label": "or"
  },
  {
    "sentence": "We incGRpo-  rate multiple anaphGRa resolution factGRs into  a statistical framewGRk - -  specifically the dis-  tance be",
    "acronym": "GR",
    "label": "or"
  },
  {
    "sentence": "In the Mental Representation of GRs (Joan Bresnan ed.),",
    "acronym": "GR",
    "label": "Grammatical Relation"
  },
  {
    "sentence": "The Mental Representation of GRs, The MIT Press, Cambridge, Mass. J. Maxwell and R. Kaplan, 1991. \"",
    "acronym": "GR",
    "label": "Grammatical Relation"
  },
  {
    "sentence": "In Joan Bresnan, editor, The Mental Repre- sentation of GRs, pages 173?281.",
    "acronym": "GR",
    "label": "Grammatical Relation"
  },
  {
    "sentence": "The Mental  Computational Linguistics, Volume 15, Number 1, March 1989  Representation fGRs.",
    "acronym": "GR",
    "label": "Grammatical Relation"
  },
  {
    "sentence": "In Joan Bresnan, editor, The Mental Representation of GRs, pages 173?281.",
    "acronym": "GR",
    "label": "Grammatical Relation"
  },
  {
    "sentence": "The Mental Representation fGRs.",
    "acronym": "GR",
    "label": "Grammatical Relation"
  },
  {
    "sentence": "Booch G. 1994, Analyse et conception orientees  objets , Addison-Wesley, Reading Mass.  Bresnan Joan and Ronald Kaplan 1981, Lexical  functional grammars ; a formal system for gram-  105  matical representation, The mental representation  of GR, MIT Press, Cambridge,  Mass.  Delmonte R. 1990, Semantic Parsing with LFG and  Conceptual Representations, Computers and the  Humanities, Kluwer Academic Publishers, 24 , p.  461-488.",
    "acronym": "GR",
    "label": "grammatical relations"
  },
  {
    "sentence": "Pairs of syntactic units connected by GR are extracted from the parse trees.",
    "acronym": "GR",
    "label": "grammatical relations"
  },
  {
    "sentence": "We therefore designed the lexical entries for the case markers so that they specify information about what GR they attach to and what se- mantic information is needed in the clausal analysis.",
    "acronym": "GR",
    "label": "grammatical relations"
  },
  {
    "sentence": "The most important differences are the use of manually defined rules and the inclusion of GR from a parser as critical fea- tures.",
    "acronym": "GR",
    "label": "grammatical relations"
  },
  {
    "sentence": "Discriminative re- ordering with Chinese GR fea- tures.",
    "acronym": "GR",
    "label": "grammatical relations"
  },
  {
    "sentence": "Discriminative re- ordering with Chinese GR features.",
    "acronym": "GR",
    "label": "grammatical relations"
  },
  {
    "sentence": "GRs are in italics (con- junction and nominal-subject).",
    "acronym": "GR",
    "label": "Grammatical relation"
  },
  {
    "sentence": "GRs  and Montague Grammar.",
    "acronym": "GR",
    "label": "Grammatical relation"
  },
  {
    "sentence": "GRs are arranged in a  hierarchy and are usually referred to by  numbers: 1, which is the highest, corresponds  to SUBJECT, 2 to DIRECT OBJEC~I ', 3 to  INDIRECT OBJECT.",
    "acronym": "GR",
    "label": "Grammatical relation"
  },
  {
    "sentence": "To do this, we Figure 8 GRs output for metaphorical expressions.",
    "acronym": "GR",
    "label": "Grammatical relation"
  },
  {
    "sentence": "GRs  This kind of features refers to the grammatical  relation themselves.",
    "acronym": "GR",
    "label": "Grammatical relation"
  },
  {
    "sentence": "DP-HWC(i)c-l GRships ?",
    "acronym": "GR",
    "label": "Grammatical relation"
  },
  {
    "sentence": "72  THIRTEENTH ANNUAL MEETING  THE AS$OCIATtON FOR COMPUTATIONAL UNGUISTICS  Sheraton Boston Hotel  Boston, Massachusef t s   October $0-November 1, 1975  Thursday, October 30, 197.5  S&SSION 1: i,AhrCUAGE C/IVn ERSTAArDlllFG SYSTElhf S  Session Chairman: Yorick Wdks - h r v e r s l t y  of Edtnburgh  990 A.M. GRgs and Irrtroductory Rernarks  9: 15 A.M. PE'DfiGI,OT and Underrl art d i n g  Natural 1,nnguagr Procr t sing  Willtarn Fabens - Rutgers University  9:40 AM A Syrtcm /or Gencral Scmankc Analysis And l t , q  Use  I n  Drawirt g A l  apa from Dircctiotts  Jerry R. t.lobbs - The C ~ t y  College of CUNY  tO:OS A.M An Adaprivo Nutural Lartguago Parser  Ferry t. M~ller - M I T.  1030 AM.",
    "acronym": "GR",
    "label": "Greetin"
  },
  {
    "sentence": "GRgs Contact.",
    "acronym": "GR",
    "label": "Greetin"
  },
  {
    "sentence": "GRg SYS2 Saikin no oishii mono ni tsuite kikasete kudasai Q-Plan Topic-Inducing (Tell me about delicious food that you?ve had recently) USR2 Karei ni hamatterunda! (",
    "acronym": "GR",
    "label": "Greetin"
  },
  {
    "sentence": "930 Utterance (English translation by the authors) DA Gen. Module SYS1 Doumo desu (Hi) GRg Initial prompt USR1 Doumo.",
    "acronym": "GR",
    "label": "Greetin"
  },
  {
    "sentence": "JP) Table 3: Templates for topic types (translated by authors) Dialogue act Example GRgs Hello.",
    "acronym": "GR",
    "label": "Greetin"
  },
  {
    "sentence": "Features extracted from the alignments and used in translation are in common use: target lan- guage model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the GR, source-to-target and target-to-source lexical models, and three rule Figure 1: Spreading neighborhood exploration within a cube, just before and after extraction of the item C. Grey squares represent the fron- tier queue; black squares are candidates already extracted.",
    "acronym": "GR",
    "label": "glue rule"
  },
  {
    "sentence": "Glue(r): exp(1) if rule is a GR ? ?",
    "acronym": "GR",
    "label": "glue rule"
  },
  {
    "sentence": "To pro- duce the result, the joint decoder made use of 8,114 hierarchical phrase pairs learned from train- ing data, 6,800 GRs connecting partial trans- lations monotonically, and 16,554 tree-to-string rules.",
    "acronym": "GR",
    "label": "glue rule"
  },
  {
    "sentence": "Since concatenation is already possible under the general GR, rules with this pattern are redundant.",
    "acronym": "GR",
    "label": "glue rule"
  },
  {
    "sentence": "The Hiero decoder can easily be made to implement MJ1 reordering by allowing only a restricted set of reordering rules in addition to the usual GR, as shown in left-hand column of Table 1, where T is the set of terminals.",
    "acronym": "GR",
    "label": "glue rule"
  },
  {
    "sentence": "T)+ Table 1: Hierarchical grammars (not including GRs).",
    "acronym": "GR",
    "label": "glue rule"
  },
  {
    "sentence": "This class of models includes popular tag- ging models for named entities such as conditional random fields, MEMMs and max-margin Markov networks.",
    "acronym": "MEMM",
    "label": "maximum entropy Markov model"
  },
  {
    "sentence": "We use a MEMM to estimate these probabilities, to pre-annotate in- stances, and to evaluate accuracy.",
    "acronym": "MEMM",
    "label": "maximum entropy Markov model"
  },
  {
    "sentence": "OSCAR (Open-Source Chemistry Analysis Rou- tines) (Corbett and Murray-Rust, 2006; Jessop et al 2011) extracts mentions of a wide range of chemi- cals using a MEMM (Mc- Callum et al 2000).",
    "acronym": "MEMM",
    "label": "maximum entropy Markov model"
  },
  {
    "sentence": "Our features were implemented  in a MEMM.",
    "acronym": "MEMM",
    "label": "maximum entropy Markov model"
  },
  {
    "sentence": "UBC-UPC: Sequential SRL using selectional preferences: an approach with MEMMs.",
    "acronym": "MEMM",
    "label": "maximum entropy Markov model"
  },
  {
    "sentence": "It is based on maxent models, MEMMs and linear-chain CRF and proposes various optimization and regularization methods to improve both the computational complexity and the prediction performance of standard models.",
    "acronym": "MEMM",
    "label": "maximum entropy Markov model"
  },
  {
    "sentence": "An Approach with MEMMs.",
    "acronym": "MEMM",
    "label": "Maximum Entropy Markov Model"
  },
  {
    "sentence": "68  CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 243?247 Manchester, August 2008 The Integration of Dependency Relation Classification and Semantic Role Labeling Using Bilayer MEMMs Weiwei Sun and Hongzhan Li and Zhifang Sui Institute of Computational Linguistics Peking University {weiwsun, lihongzhan.pku}@gmail.com, szf@pku.edu.cn Abstract This paper describes a system to solve the joint learning of syntactic and seman- tic dependencies.",
    "acronym": "MEMM",
    "label": "Maximum Entropy Markov Model"
  },
  {
    "sentence": "Rudnick et al(2013) present a combina- tion of MEMMs and HMM to perform lexical selection in the sense of cross-lingual word sense disam- biguation (i.e. by choice from the set of trans- lation alternatives).",
    "acronym": "MEMM",
    "label": "Maximum Entropy Markov Model"
  },
  {
    "sentence": "An aproach with MEMMs Ben?at Zapirain, Eneko Agirre IXA NLP Group University of the Basque Country Donostia, Basque Country {benat.zapirain,e.agirre}@ehu.es Llu??s Ma`rquez TALP Research Center Technical University of Catalonia Barcelona, Catalonia lluism@lsi.upc.edu Abstract We present a sequential Semantic Role La- beling system that describes the tagging problem as a Maximum Entropy Mar",
    "acronym": "MEMM",
    "label": "Maximum Entropy Markov Model"
  },
  {
    "sentence": "To provide part-of-speech tags to MSTParser, we use the MElt tagger (Denis and Sagot, 2009), a MEMM tagger en- riched with information from a large-scale dictio- nary.5 The tagger was trained on the training set to provide POS tags for the dev and test sets, and we used 10-way jackkni?ng to generate tags for the training set.",
    "acronym": "MEMM",
    "label": "Maximum Entropy Markov Model"
  },
  {
    "sentence": "To estimate the probability distribution of each arc and do inference, we im- plement a MEMM (Mc- Callum et al, 2000).",
    "acronym": "MEMM",
    "label": "Maximum Entropy Markov Model"
  },
  {
    "sentence": "Using AWN to disambiguate par-  allel texts allows us to calculate the intersection  among the synsets accessible from an English text  through the English WoRDNET and the synsets  accessible from the parallel Italian text through  the Italian WORDNET.",
    "acronym": "AWN",
    "label": "aligned wordnets"
  },
  {
    "sentence": "TufiS, D., Ion, R., Ide, N.: Fine-grained word  sense disambiguation based on parallel corpo- ra, word alignment, word clustering and  AWN.",
    "acronym": "AWN",
    "label": "aligned wordnets"
  },
  {
    "sentence": "in (2004) presented a method that exploits word  clustering based on automatic extraction of trans- lation equivalents, being supported by available  AWN.",
    "acronym": "AWN",
    "label": "aligned wordnets"
  },
  {
    "sentence": "lexical resources (e.g., dictionaries,  AWN), or  ?",
    "acronym": "AWN",
    "label": "aligned wordnets"
  },
  {
    "sentence": "In this way the  AWN can be used to help each other and  derive a more compatible and consistent structure.",
    "acronym": "AWN",
    "label": "aligned wordnets"
  },
  {
    "sentence": "There are then two general ways in which the  AWN can be accessed:  ?",
    "acronym": "AWN",
    "label": "aligned wordnets"
  },
  {
    "sentence": "Automatically Extending Named Entities coverage  of AWN using Wikipedia.",
    "acronym": "AWN",
    "label": "Arabic WordNet"
  },
  {
    "sentence": "2010) proposed a system  that uses AWN to enhance Arabic  question/answering.",
    "acronym": "AWN",
    "label": "Arabic WordNet"
  },
  {
    "sentence": "Our corpus will be further extended to include more text, and all lexical annotations (i.e., Lemmas) will be linked with existing Arabic ontology resources such as the AWN (Black et al.,",
    "acronym": "AWN",
    "label": "Arabic WordNet"
  },
  {
    "sentence": "As in (Alkhalifa and Rodrguez,  2008), they proposed an automatic technique for  extending Named Entities of AWN  using Wikipedia.",
    "acronym": "AWN",
    "label": "Arabic WordNet"
  },
  {
    "sentence": "A Proposed Model for Quranic AWN.",
    "acronym": "AWN",
    "label": "Arabic WordNet"
  },
  {
    "sentence": "AWN: current state  and future extensions.",
    "acronym": "AWN",
    "label": "Arabic WordNet"
  },
  {
    "sentence": "Thus, the FM only chooses roles from frames that are the best syntac- tic matches with the extracted argument set.",
    "acronym": "FM",
    "label": "frame matcher"
  },
  {
    "sentence": "The potential argument slots are sub- ject, object, indirect object, and PP-object, where the latter is specialized by the individual preposi- tion.1 Given chunked sentences with our verbs, the FM uses VerbNet both to restrict the list of candidate roles for each slot, and to eliminate some of the PP slots that are likely not arguments.",
    "acronym": "FM",
    "label": "frame matcher"
  },
  {
    "sentence": "3.2 Adjustments to the Role Mapping We further extend the FM, which has ex- tensive knowledge of VerbNet, for the separate task of helping to eliminate some of the inconsistencies that are introduced by our role mapping procedure.",
    "acronym": "FM",
    "label": "frame matcher"
  },
  {
    "sentence": "3.1 Initialization of Candidate Roles The FM construes extracted arguments from the parsed sentence as being in one of the four main types of syntactic positions (or slots) used by VerbNet frames: subject, object, indirect object, and PP-object.3 Additionally, we specialize the lat- ter by the individual preposition, such as ?",
    "acronym": "FM",
    "label": "frame matcher"
  },
  {
    "sentence": "4 The Bootstrapping Algorithm We have described the FM that produces a set of slots with candidate role lists (some unam- biguous), and our backoff probability model.",
    "acronym": "FM",
    "label": "frame matcher"
  },
  {
    "sentence": "The automatic FM aligns arguments extracted from an automatically parsed sentence with the frames in VerbNet for the target verb in the sentence.",
    "acronym": "FM",
    "label": "frame matcher"
  },
  {
    "sentence": "FM for our categories range from .61 (TEXTUAL) and .52 (AIM) to .45 (BACKGROUND), .38 (BASIS), and .26 (CONTRAST).",
    "acronym": "FM",
    "label": "F-measures"
  },
  {
    "sentence": "Tables 3 and 4 show the improvement of C-E and A-E alignment FM with the confidence-based alignment link filtering (ALF).",
    "acronym": "FM",
    "label": "F-measures"
  },
  {
    "sentence": "Performing 5-fold cross validation on the nwire and bnews corpora, (Jiang and Zhai, 2007) and (Chan and Roth, 2010) reported FM of 71.5 and 71.2, respectively.",
    "acronym": "FM",
    "label": "F-measures"
  },
  {
    "sentence": "Macro-F is the mean of the FM of all seven categories.",
    "acronym": "FM",
    "label": "F-measures"
  },
  {
    "sentence": "Table 4 shows the NER results obtained by the meth- ods without considering NER-level rejection (i.e., to = 0), using threshold tw = 0.4 for Conf-A, Proposed, and Conf-Reject, which resulted in the best NER FM (see Table 5).",
    "acronym": "FM",
    "label": "F-measures"
  },
  {
    "sentence": "The scored results show that our CWS can  increase the Bakeoff baseline system with 4.86%  and 5.04% FM for the CKIP and the CityU  word segmentation tasks, respectively.",
    "acronym": "FM",
    "label": "F-measures"
  },
  {
    "sentence": "This system has achieved competitive results with an FM of 82.7 when trained on the seven main types of ACE data with access to wordnet and part-of-speech-tag informa- tion as well as output of other MD and named-entity recognizers (Zitouni and Florian, 2008).",
    "acronym": "FM",
    "label": "F -measure"
  },
  {
    "sentence": "Precision/recall/FM re- sults are shown in Table 2.",
    "acronym": "FM",
    "label": "F -measure"
  },
  {
    "sentence": "This module achieved an FM of 68% (Amaral et al, 2007).",
    "acronym": "FM",
    "label": "F -measure"
  },
  {
    "sentence": "But system combination serves us well: it recovers all but 0.5 FM point of this loss, while also ac- tually performing better on the noisy data sets than the two classifiers specifically targeted toward them, as can be seen in Table 2.",
    "acronym": "FM",
    "label": "F -measure"
  },
  {
    "sentence": "This module achieved an FM of 56% and SER (Slot Error Rate, the measure commonly used to evaluate this kind of task) of 0.74.",
    "acronym": "FM",
    "label": "F -measure"
  },
  {
    "sentence": "However, because the mixed classifier, and moreso the gazetteer classifier, are oriented to noisy data, on clean data they suffer in performance by 2.5 and 5 FM points, respectively.",
    "acronym": "FM",
    "label": "F -measure"
  },
  {
    "sentence": "Performance is presented in terms of Precision (P), Recall (R), and FM (F).",
    "acronym": "FM",
    "label": "F -measure"
  },
  {
    "sentence": "7 Related Work Dawid and Skene (1979) investigated filtering annotations using the EM algorithm, estimating annotator-specific ERR in the context of patient medical records.",
    "acronym": "ERR",
    "label": "error rates"
  },
  {
    "sentence": "This shift did not nec- essarily translate to higher ERR, especially after optimizations.",
    "acronym": "ERR",
    "label": "error rates"
  },
  {
    "sentence": "Percentage denotes the ERR, i.e. the number of err",
    "acronym": "ERR",
    "label": "error rates"
  },
  {
    "sentence": "However, for sensitive applications where ERR must practically be zero, or other situations where speech recognition ERR are too high, we are currently developing a real-time correction interface.",
    "acronym": "ERR",
    "label": "error rates"
  },
  {
    "sentence": "The low ERR are the key reason the error cor- rection task is so difficult: it is quite challenging for a system to improve over a writer that already per- forms at the level of over 90%.",
    "acronym": "ERR",
    "label": "error rates"
  },
  {
    "sentence": "Table 1 shows the number of mistakes1 of each type and the ERR, i.e. the percentage of erroneous words by error type.",
    "acronym": "ERR",
    "label": "error rates"
  },
  {
    "sentence": "Percentage denotes the ERR, i.e. the number of erroneous instances with respect to the to- tal number of relevant instances in the data.",
    "acronym": "ERR",
    "label": "error rates"
  },
  {
    "sentence": "of mistakes1 of each type and the ERR, i.e. the percentage of erroneous words by error type.",
    "acronym": "ERR",
    "label": "error rates"
  },
  {
    "sentence": "In  section 5, we present word alignment results that  show significant alignment ERR  compared to the baseline HMM and IBM model 4.",
    "acronym": "ERR",
    "label": "error rate reductions"
  },
  {
    "sentence": "Even so, we are able to achieve ERR ranging from 6.52% to 63.41% for TWA, and from 3.33% to 34.62% for SEMEVAL.",
    "acronym": "ERR",
    "label": "error rate reductions"
  },
  {
    "sentence": "We present experimen- tal results for heavily pruned backoff n- gram models, and demonstrate perplexity and word ERR when used with various baseline smoothing methods.",
    "acronym": "ERR",
    "label": "error rate reductions"
  },
  {
    "sentence": "We translate the context of an ambiguous word in multiple languages, and show through experiments on standard datasets that by using a multilingual vector space we can obtain ERR of up to 25%, as compared to a monolingual classifier.",
    "acronym": "ERR",
    "label": "error rate reductions"
  },
  {
    "sentence": "We now look at the impacts on system perfor- mance we can achieve with these new models4, and whether the perplexity differences that we ob- serve translate to real ERR.",
    "acronym": "ERR",
    "label": "error rate reductions"
  },
  {
    "sentence": "Using only low-level features capturing when partic- ipants choose to vocalize relative to one another, it attains relative ERR on unseen data of 37%, 67%, and 40% over chance on classifying role, leadership, and seniority, respectively.",
    "acronym": "ERR",
    "label": "error rate reductions"
  },
  {
    "sentence": "Fortunately, work is being done on verb subcategorization frames in HI.5 We plan to incorporate this information into the Urdu grammar verb lexicon.",
    "acronym": "HI",
    "label": "Hindi"
  },
  {
    "sentence": "In addition, a guesser can be added to guess words that the morphology does not yet recognize (Chanod 4A web search on HI dictionary results in several promising sites.",
    "acronym": "HI",
    "label": "Hindi"
  },
  {
    "sentence": "While some morphological analyzers al- ready exist for HI,3 e.g., as part of the tools developed at the Language Technolo- gies Research Centre (LTRC), IIT Hyderabad (http://www.iiit.net/ltrc/index.html), they are not immediately compatible with the XLE grammar development platform, nor is it clear that the morphological analyses they produce conform to the standards and methods developed within the ParGram project.",
    "acronym": "HI",
    "label": "Hindi"
  },
  {
    "sentence": "Even within a linguistic formalism, LFG for Par- Gram, there is often more than one way to ana- 5One significant effort is the HI Verb Project run by Prof. Alice Davison at the University of Iowa; further information is available via their web site.",
    "acronym": "HI",
    "label": "Hindi"
  },
  {
    "sentence": "5 Script One issue that has not been dealt with in the Urdu grammar is the different script systems used for Urdu and HI.",
    "acronym": "HI",
    "label": "Hindi"
  },
  {
    "sentence": "Transliteration by analogical learning has been attempted by Dandapat et al (2010) for an English-to-HI transliteration task.",
    "acronym": "HI",
    "label": "Hindi"
  },
  {
    "sentence": "scores for English dataset derived from the HI project.",
    "acronym": "HI",
    "label": "Harvard Inquirer"
  },
  {
    "sentence": "We then re- placed sentiment words with a sentiment cat- egory identifier using the sentiment lexica of the HI (Stone, 1966) and LIWC (Pennebaker et al, 2007).",
    "acronym": "HI",
    "label": "Harvard Inquirer"
  },
  {
    "sentence": "Of the string-similarity features, we reused the LCS, Longest Common Subsequence (with and without normaliza- tion), and Greedy String Tiling measures.",
    "acronym": "LCS",
    "label": "Longest Common Substring"
  },
  {
    "sentence": "LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  75/206 Example of Tiling for Derived and Non-Derived Text (from Clough 2003) ?",
    "acronym": "LCS",
    "label": "Longest Common Substring"
  },
  {
    "sentence": "LCSsComputed between Two Sentences 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  74/206 ?",
    "acronym": "LCS",
    "label": "Longest Common Substring"
  },
  {
    "sentence": "As  Dowry (1989), Jackendoff (1987, 1990) and oth-  ers have noted, LCS is  needed for correct semantic interpretation. (",
    "acronym": "LCS",
    "label": "lexical conceptual structure"
  },
  {
    "sentence": "This sentence is processed by the system  and the following LCS is  obtained:  [Event GO Loc   ([Thing JOHN],   [Path TO Loc ([Position AT Loc ([Thing JOHN],  [Property HOUSE])])],   [Manner RUNNINGLY])]  which is stored by the tutoring system and later  matched against the student?s answer.",
    "acronym": "LCS",
    "label": "lexical conceptual structure"
  },
  {
    "sentence": "predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).",
    "acronym": "LCS",
    "label": "lexical conceptual structure"
  },
  {
    "sentence": "Generation from LCS.",
    "acronym": "LCS",
    "label": "lexical conceptual structure"
  },
  {
    "sentence": "For while rule (16) would  make correct predictions regarding complex event  nominals uch as those illustrated in (29), it fails  on nominals that do not represent complex events  as in (30)  (29) a. TREE felling  b. COOKIE baking  (30) a. HISTORY teacher  b. BIT guzzler  1 believe the distinction is a semantic one involv-  ing 0-participants in LCS  (Ics).",
    "acronym": "LCS",
    "label": "lexical conceptual structure"
  },
  {
    "sentence": "Every verb and noun (including deverbal  nouns) has a LCS that in-  cludes the entities involved in the events or states  described (see, for example, Dowty (1989),  Fillmore (1968), and Jackendoff (1987, 1990)).",
    "acronym": "LCS",
    "label": "lexical conceptual structure"
  },
  {
    "sentence": "Resnik and Diab [2000], in which similarity is computed according to the structure, rather than content, of LCS representations of verbs; see Jackendoff [1983] and Dorr [1993]).",
    "acronym": "LCS",
    "label": "lexical conceptual structure"
  },
  {
    "sentence": "From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) In order to measure the precision of the ex- tracted patterns, a new corpus is downloaded us- ing the hook Dickens as the only query word, and the system looks for appearances of the patterns in the corpus.",
    "acronym": "LCS",
    "label": "longest common substring"
  },
  {
    "sentence": "We use the Recall-Oriented Un- derstudy for Gisting Evaluation (ROUGE) based on the LCSs (ROUGE-L) as our evaluation metric.",
    "acronym": "LCS",
    "label": "longest common substring"
  },
  {
    "sentence": "From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their LCS to produce the follow- ing patterns: <hook> was born in <target> <hook> ( <target> - 1870 ) The rote extractor needs to estimate automati- cally the precision of the extracted patterns, in or- der to keep the best ones.",
    "acronym": "LCS",
    "label": "longest common substring"
  },
  {
    "sentence": "This may just be the m characters to the left or to the right (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).",
    "acronym": "LCS",
    "label": "longest common substring"
  },
  {
    "sentence": "This may just be the m characters to the left or to the right, (Brin, 1998), the LCS of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suf- fix tree constructor (Ravichandran and Hovy, 2002).",
    "acronym": "LCS",
    "label": "longest common substring"
  },
  {
    "sentence": "6As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and LCS (Allison and Dix, 1986), and word n-gram containment(Lyon et al 2001) with n = 2.",
    "acronym": "LCS",
    "label": "longest common substring"
  },
  {
    "sentence": "lin uses the information content (Resnik, 1995) of the LCS of concepts A and B. Information content (IC) indi- cates the specificity of a concept; the least com- mon subsumer of a concept A and B is the most specific concept from which A and B are inherited.",
    "acronym": "LCS",
    "label": "least common subsumer"
  },
  {
    "sentence": "LCS?",
    "acronym": "LCS",
    "label": "least common subsumer"
  },
  {
    "sentence": "Tracing for the information content measures (res, lin, jcn) includes both the paths between concepts as well as the LCS.",
    "acronym": "LCS",
    "label": "least common subsumer"
  },
  {
    "sentence": "features (covering lexical items before, af- ter and between both mentions (of the event trigger and an argument) as described by Zhou and Zhang (2007)); second, chunking features (concerned with head words of the phrases between two mentions as described by Zhou and Zhang (2007)); third, de- pendency parse features (considering both the se- lected dependency levels of the arguments (parents and LCS) as discussed by Ka- trenko and Adriaans (2006), as well as a shortest de- pendency path structure between the arguments as used by Kim et al (2008b) for walk features).",
    "acronym": "LCS",
    "label": "least common subsumer"
  },
  {
    "sentence": "lin similarity 5 returns the difference between two times of the IC of the LCS of A and B, and the sum of IC of both concepts.",
    "acronym": "LCS",
    "label": "least common subsumer"
  },
  {
    "sentence": "Path search takes place as a depth-limited search of maximum depth of 4 for a LCS.",
    "acronym": "LCS",
    "label": "least common subsumer"
  },
  {
    "sentence": "WUP finds the depth of the LCS of the words, and scales that by the sum of the depths of individual words.",
    "acronym": "LCS",
    "label": "least common subsumer"
  },
  {
    "sentence": "LCSe is computed as follows: lcs(X,Y ) = (length(X) + length(Y )?",
    "acronym": "LCS",
    "label": "Longest Common Subsequenc"
  },
  {
    "sentence": "Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statistics.",
    "acronym": "LCS",
    "label": "Longest Common Subsequenc"
  },
  {
    "sentence": "Auto- matic Evaluation of Machine Translation Quality Us- ing LCSe and Skip-Bigram Statics.",
    "acronym": "LCS",
    "label": "Longest Common Subsequenc"
  },
  {
    "sentence": "Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSe 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.",
    "acronym": "LCS",
    "label": "Longest Common Subsequenc"
  },
  {
    "sentence": "Automatic Evaluation of Machine Translation Quality Us- ing the LCSe and Skip- Bigram Statistics.",
    "acronym": "LCS",
    "label": "Longest Common Subsequenc"
  },
  {
    "sentence": "Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSe and Skip- Bigram Statics.",
    "acronym": "LCS",
    "label": "Longest Common Subsequenc"
  },
  {
    "sentence": "Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statistics.",
    "acronym": "LCS",
    "label": "Longest Common Sub"
  },
  {
    "sentence": "Auto- matic Evaluation of Machine Translation Quality Us- ing LCSsequence and Skip-Bigram Statics.",
    "acronym": "LCS",
    "label": "Longest Common Sub"
  },
  {
    "sentence": "Salience grading for candidate antecedents  Features Score  F1  recency  0, if in two sentences away from anaphor  1, if in one sentence away from anaphor  2, if in same sentence as anaphor 0-2  F2 Subject and Object Preference 1  F3 Grammatical function agreement 1  F4 Number Agreement 1  F5 Semantic LCSsequence 0 to 3  F6 Semantic Type Agreement -1 to +2  F7 Biomedical antecedent preference -2 if not or +2  The first feature F1 is recency which measures the distance between an anaphor  and candidate antecedents in number of sentences.",
    "acronym": "LCS",
    "label": "Longest Common Sub"
  },
  {
    "sentence": "Of the string-similarity features, we reused the LCSstring, LCSsequence (with and without normaliza- tion), and Greedy String Tiling measures.",
    "acronym": "LCS",
    "label": "Longest Common Sub"
  },
  {
    "sentence": "Automatic Evaluation of Machine Translation Quality Us- ing the LCSsequence and Skip- Bigram Statistics.",
    "acronym": "LCS",
    "label": "Longest Common Sub"
  },
  {
    "sentence": "Au- tomatic Evaluation of Machine Translation Qual- ity Using LCSsequence and Skip- Bigram Statics.",
    "acronym": "LCS",
    "label": "Longest Common Sub"
  },
  {
    "sentence": "1) Additionally, we relax the constraint of requiring exact string matches between the two sentences by using the LCS (Allison and Dix, 1986) and greedy string tiling (Wise, 1996) al- gorithms.",
    "acronym": "LCS",
    "label": "longest common subsequence"
  },
  {
    "sentence": "Size of the LCS.",
    "acronym": "LCS",
    "label": "longest common subsequence"
  },
  {
    "sentence": "A LCS algorithm suitable for similar text strings.",
    "acronym": "LCS",
    "label": "longest common subsequence"
  },
  {
    "sentence": "We used the cosine similarity, LCS, and word n-gram similarity measures.",
    "acronym": "LCS",
    "label": "longest common subsequence"
  },
  {
    "sentence": "Size of the LCS, normalized by TGT length.",
    "acronym": "LCS",
    "label": "longest common subsequence"
  },
  {
    "sentence": "Cosine similarity, LCS, and word n-gram similarity were also applied to measure the similarity between the edit comment and the turn text as well as the similarity between the edit comment and the turn topic name.",
    "acronym": "LCS",
    "label": "longest common subsequence"
  },
  {
    "sentence": "With tile description length of a model de-  fined in the above manner, we wish to select a  model having the MDL and  output it as the result of clustering.",
    "acronym": "MDL",
    "label": "minimum description length"
  },
  {
    "sentence": "5 Experimental setup To test whether MDL is a good driver for unsupervised inversion transduc- tion induction, we implemented and executed the method described above.",
    "acronym": "MDL",
    "label": "minimum description length"
  },
  {
    "sentence": "Comparing the MDL principle and boosting in the automatic analysis of discourse.",
    "acronym": "MDL",
    "label": "minimum description length"
  },
  {
    "sentence": "7 Conclusions We have presented a minimalist, unsupervised learning model that induces relatively clean phrasal ITGs by iteratively splitting existing rules into smaller rules using a theoretically well- founded MDL objective.",
    "acronym": "MDL",
    "label": "minimum description length"
  },
  {
    "sentence": "successor count as a means to re- duce the search space of a more powerful al- gorithm based on MDL (Marcken, 1996).",
    "acronym": "MDL",
    "label": "minimum description length"
  },
  {
    "sentence": "Besides specifying the means for forming new ECG constructions, the acquisition model provides an overarching computational framework for converging on an optimal set of constructions, based on a MDL principle ( Rissanen 1978) that favors compactness in describing both the grammar and the statistical properties of the data.",
    "acronym": "MDL",
    "label": "minimum description length"
  },
  {
    "sentence": "Os-  borne, 1999) extended a definite clause grammar  with rules induced by a learner that was based upon  the maxiMDL principle.",
    "acronym": "MDL",
    "label": "mum description length"
  },
  {
    "sentence": "The min- iMDL principle in coding and modeling.",
    "acronym": "MDL",
    "label": "mum description length"
  },
  {
    "sentence": "In Figure 2, we present information about the length of the Wikipedia entity descriptions for English and for the language other than English with the maxiMDL.",
    "acronym": "MDL",
    "label": "mum description length"
  },
  {
    "sentence": "Comparing the miniMDL principle and boosting in the automatic analysis of discourse.",
    "acronym": "MDL",
    "label": "mum description length"
  },
  {
    "sentence": "successor count as a means to re- duce the search space of a more powerful al- gorithm based on miniMDL (Marcken, 1996).",
    "acronym": "MDL",
    "label": "mum description length"
  },
  {
    "sentence": "Besides specifying the means for forming new ECG constructions, the acquisition model provides an overarching computational framework for converging on an optimal set of constructions, based on a miniMDL principle ( Rissanen 1978) that favors compactness in describing both the grammar and the statistical properties of the data.",
    "acronym": "MDL",
    "label": "mum description length"
  },
  {
    "sentence": "Discovering Morphemic Suffixes: A Case Study in MDL Induction.",
    "acronym": "MDL",
    "label": "Minimum Description Length"
  },
  {
    "sentence": "The model of Cartwright & Brent (1997) uses an algorithm which incrementally merges word clusters so that a MDL criterion for a template grammar is optimized.",
    "acronym": "MDL",
    "label": "Minimum Description Length"
  },
  {
    "sentence": "Their idea is closely related to the classic MDL princi- ple for model selection (Barron et al.,",
    "acronym": "MDL",
    "label": "Minimum Description Length"
  },
  {
    "sentence": "c?2013 Association for Computational Linguistics Unsupervised Transduction Grammar Induction via MDL Markus Saers and Karteek Addanki and Dekai Wu Human Language Technology Center Dept.",
    "acronym": "MDL",
    "label": "Minimum Description Length"
  },
  {
    "sentence": "The MDL Principle in Cod- ing and Modeling.",
    "acronym": "MDL",
    "label": "Minimum Description Length"
  },
  {
    "sentence": "1.1 Hierarchical versus flat lexicons From the viewpoint of data compression and following the two-part MDL prin- ciple (Rissanen, 1978), Morfessor tries to minimize the number of bits needed to encode both the model parameters and the training data.",
    "acronym": "MDL",
    "label": "Minimum Description Length"
  },
  {
    "sentence": "The alterna-  t:ives here are: OW the respective piece  of information by the correct one and re-gen-  erate the whole morphosyntactic surface  structure; or exchange just a partial structure.",
    "acronym": "OW",
    "label": "Overwrite"
  },
  {
    "sentence": "OW and add conservatively are also highly restricted operations.",
    "acronym": "OW",
    "label": "Overwrite"
  },
  {
    "sentence": "Thus, following Matuschek et al (2014), we induced a clustering of WordNet senses by aligning WordNet to the more coarse-grained OW LSR.",
    "acronym": "OW",
    "label": "OmegaWiki"
  },
  {
    "sentence": "OW OW is a collaborative multilingual dictionary based on a relational database.",
    "acronym": "OW",
    "label": "OmegaWiki"
  },
  {
    "sentence": "It currently contains nine resources in two lan- guages: English WordNet, Wiktionary, Wikipedia, FrameNet and VerbNet, German Wikipedia, Wiktionary and GermaNet, and multilingual OW modeled according to the LMF standard.",
    "acronym": "OW",
    "label": "OmegaWiki"
  },
  {
    "sentence": "We observe that translation 11http://www.statmt.org/moses/ 12OW consists of interlinked language- independent concepts to which lexicalizations in several languages are attached.",
    "acronym": "OW",
    "label": "OmegaWiki"
  },
  {
    "sentence": "To improve alignment coverage, we re- trained the alignment model by supplying GIZA++ with an English-French bilingual dictionary that we assembled using three online dictionary databases: OW, Wiktionary, and Universal Dictionary.",
    "acronym": "OW",
    "label": "OmegaWiki"
  },
  {
    "sentence": "Currently, we are using AT&T Bell  Laboratories' TTS System).",
    "acronym": "TTS",
    "label": "Text To Speech"
  },
  {
    "sentence": "It handles ellipsis and refering expressions and also provides the  final prosodically annotated output to the TTS ('ITS) synthesiser.",
    "acronym": "TTS",
    "label": "Text To Speech"
  },
  {
    "sentence": "The system uses a 14k vocabulary, automatically generated by the AT&T Labs NextGen TTS system.",
    "acronym": "TTS",
    "label": "Text To Speech"
  },
  {
    "sentence": "The remaining predictions are sent to the TTS engine that articulates the top  CDC predictions at the user's request.",
    "acronym": "TTS",
    "label": "Text To Speech"
  },
  {
    "sentence": "server generally implements a key system function including Speech Recognition, Frame Construction (language parsing), Context Tracking, Dialogue Management, Application Interface, Language Generation, and TTS.",
    "acronym": "TTS",
    "label": "Text-to-speech"
  },
  {
    "sentence": "1 System configuration  Interface  User submits anew query  by selecting one or more  specific terms displayed  on the interface (web  page)  User enters an initial query  .I Reformulate the L query \\]~'  q, ,   I  Search over the \\[  I database  ,  I  I Display search result /  Summarize individual  TTS i  conversion :  Fig.",
    "acronym": "TTS",
    "label": "Text-to-speech"
  },
  {
    "sentence": "Each server generally implements a key system function including Speech Recognition, Frame Construction (language parsing), Context Tracking, Dialogue Management, Application Interface, Language Generation, and TTS.",
    "acronym": "TTS",
    "label": "Text-to-speech"
  },
  {
    "sentence": "As a final  note, we suggest hat its fundamental rationale is  arguably also highly pertinent to TTS  systems, which, however, cannot be elaborated here.",
    "acronym": "TTS",
    "label": "Text-to-speech"
  },
  {
    "sentence": "2001 HRL Laboratories, LLC, All rights reserved Speech Recognition Language Generation Dialogue Management Application Back-end Context Tracking Frame Construction Audio Server TTS Conversion Hub Figure 1.",
    "acronym": "TTS",
    "label": "Text-to-speech"
  },
  {
    "sentence": "Build the Microvox TTS  synthesizer.",
    "acronym": "TTS",
    "label": "Text-to-speech"
  },
  {
    "sentence": "Computational Linguistics Volume 16, No.3, pp.155-  170 1990  \\[3\\] Bachenkn, J., and Fitzpatrick, E.  Parsing for Prosody: What a TTS system  needs from syntax  Proceedings of lE~E Artificial Intelligence Systems in  Government (AISIG), 1989  \\[41 Bachenko, J., Fitzpatrick, E., and Wright, C.E.  The contribution of parsing to prosodic phrasing in an  experimental text-to-speech system  Proceedings of the 24th Annual Meeting of the  Association for Computational Linguistics, pp.",
    "acronym": "TTS",
    "label": "Text-to-speech"
  },
  {
    "sentence": "2 Previous Work on Stress Prediction Pronunciation prediction, of which stress pre- diction is a part, is important for many speech applications including automatic speech recog- nition, TTS, and translit- eration for, say, machine translation.",
    "acronym": "TTS",
    "label": "text-to-speech synthesis"
  },
  {
    "sentence": "A large number of text process- ing applications have already employed techniques for automatic subjectivity analysis, including auto- matic expressive TTS (Alm et al.,",
    "acronym": "TTS",
    "label": "text-to-speech synthesis"
  },
  {
    "sentence": "Multilingual TTS.",
    "acronym": "TTS",
    "label": "text-to-speech synthesis"
  },
  {
    "sentence": "Multilingual text analy-  sis for TTS.",
    "acronym": "TTS",
    "label": "text-to-speech synthesis"
  },
  {
    "sentence": "Word stress assignment in a TTS system for British English.",
    "acronym": "TTS",
    "label": "text-to-speech synthesis"
  },
  {
    "sentence": "For these reasons, computational mod-  eling of prosodic phrases is important both for TTS and speech  understanding applications.",
    "acronym": "TTS",
    "label": "text-to-speech synthesis"
  },
  {
    "sentence": "for  ident i fy ing  STA verbs, Many  instances of this pat tern  appear  to be  pass ives  or s tat ive use of normal ly  non-  s tat ive verbs.",
    "acronym": "STA",
    "label": "stat ive"
  },
  {
    "sentence": "Act ion  verbs can appear in a number of  embedded sentences  where STAs cannot  be used.",
    "acronym": "STA",
    "label": "stat ive"
  },
  {
    "sentence": "This paper descr ibes  methods  for f inding taxonomy and set -membersh ip   re lat ionships,  recogniz ing nouns that  ord inar i ly  represent  human beings, and  ident i fy ing act ive and STA verbs and  adject ives.",
    "acronym": "STA",
    "label": "stat ive"
  },
  {
    "sentence": "In testing this  one should  i. always use sentences with a non-stative basic  proposit ion, for i~ the la t te r  is STA the  sentence can never be habitual (of.",
    "acronym": "STA",
    "label": "stat ive"
  },
  {
    "sentence": "l+STAI+durative ..ADJECTIUE, DA, etc.",
    "acronym": "STA",
    "label": "stat ive"
  },
  {
    "sentence": "IF Lexical Aspect=stative  THEN Situation is a state  AND its Time Argument is a period  AND this period is unbounded  As shown here, if the lexical aspect of a predica-  tion is STA,  its grammatical aspect is  irrelevant.",
    "acronym": "STA",
    "label": "stat ive"
  },
  {
    "sentence": "1 In t roduct ion   We present a STAistical method for determin-  ing pronoun anaphora.",
    "acronym": "STA",
    "label": "stat"
  },
  {
    "sentence": "We incorpo-  rate multiple anaphora resolution factors into  a STAistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.",
    "acronym": "STA",
    "label": "stat"
  },
  {
    "sentence": "This equation is decomposed into pieces that  correspond to all the above factors but are more  STAistically manageable.",
    "acronym": "STA",
    "label": "stat"
  },
  {
    "sentence": "From  this data, we collect the three STAistics detailed  ha the following subsections.",
    "acronym": "STA",
    "label": "stat"
  },
  {
    "sentence": "In building a STAistical parser for the Penn  Tree-bank various STALstics have been collected  (Charniak, 1997), two of which are P(w~lh, t, l)  and P(w~lt , l).",
    "acronym": "STA",
    "label": "stat"
  },
  {
    "sentence": "Currently, the target parsing engine is the Stuttgart  FST Tools (http://www.ims.uni-                                                   6 We have resisted the temptation to make our linguistics too  modern, since linguistic theories also have a short half-life.",
    "acronym": "FST",
    "label": "Finite State Transducer"
  },
  {
    "sentence": "This exten-  sion treats phonological features as I/O tapes for  FSTs in a parallel sequential  incrementation (PSI) architecture; phonological  processes (e.g. assimilation) are seen as variants of  an elementary unification operation over feature  tapes (linear unification phonology, LUP).",
    "acronym": "FST",
    "label": "Finite State Transducer"
  },
  {
    "sentence": "c?2012 Association for Computational Linguistics Automated Essay Scoring Based on FST: towards ASR Transcription of Oral English Speech Xingyuan Peng?,",
    "acronym": "FST",
    "label": "Finite State Transducer"
  },
  {
    "sentence": "In contrast, Algorithm/Model Time Acc.(%) Complexity This paper Stack Dependency Analysis (SVMs) \u0002 89.56 Stack Dependency Analysis (linear SVMs) \u0002 87.36 KM02 Cascaded Chunking (SVMs) \u0002\u0002 89.29 KM00 Backward Beam Search (SVMs) \u0002\u0002 89.09 USI99 Backward Beam Search (ME) \u0002\u0002 87.14 Seki00 Deterministic FST \u0002 77.97 Table 2: Comparison to Related Work.",
    "acronym": "FST",
    "label": "Finite State Transducer"
  },
  {
    "sentence": "720  Japanese Dependency Analysis  using a Determinist ic FST  Satosh i  Sek ine   Computer  Science Depar tment   New York University  715 Broadway, 7th floor  New York, NY 10003, USA  Abst ract   A deternfinistic finite state transducer is a fast  device fbr analyzing strings.",
    "acronym": "FST",
    "label": "Finite State Transducer"
  },
  {
    "sentence": "4 Implementing Voting Constraints with FSTs  The approach described above can also be implemented by finite state transducers.",
    "acronym": "FST",
    "label": "Finite State Transducer"
  },
  {
    "sentence": "A weighted FST translation template model for statistical machine translation.",
    "acronym": "FST",
    "label": "finite state transducer"
  },
  {
    "sentence": "The morphological component of the plat- form is a FST that makes use of the  lexicon for traversing the arcs of the diagram  (adopted, with changes, from Kemal Oflazer?s work  on Turkish morphology (Oflazer 1993)) to associate  a character string with the list of meaning contribu- tions of its morphemes.",
    "acronym": "FST",
    "label": "finite state transducer"
  },
  {
    "sentence": "We  adopt a FST as a computing model  which governs the fundamental interpretation control.",
    "acronym": "FST",
    "label": "finite state transducer"
  },
  {
    "sentence": "Head transducer models consist of collections of  weighted FSTs associated with  pairs of lexical items in a bilingual exicon.",
    "acronym": "FST",
    "label": "finite state transducer"
  },
  {
    "sentence": "The Temporal Expression Tagger we have developed is based on a large coverage cascade of FSTs and our Event Tagger on a set of simple heuris- tics applied over local context in a chunked text.",
    "acronym": "FST",
    "label": "finite state transducer"
  },
  {
    "sentence": "He begins with FSTs, which es- sentially implement a general preference for onsets.",
    "acronym": "FST",
    "label": "finite state transducer"
  },
  {
    "sentence": "The European project IST ALERT: Alert sys- tem for SDI (http://www.fb9- ti.uni-duisburg.de/alert) aims to associate state- of-the-art speech recognition with audio and video segmentation and automatic topic index- ing to develop an automatic media monitoring demonstrator and evaluate it in the context of real world applications.",
    "acronym": "SDI",
    "label": "selective dissemination"
  },
  {
    "sentence": "A va- riety of near-term applications are possible such as audio data mining, SDI of information (News-on-Demand), media monitor- ing, content-based audio and video retrieval.",
    "acronym": "SDI",
    "label": "selective dissemination"
  },
  {
    "sentence": "The term can also be a variable that is used in  statist ical c lassif icat ion or clustering processes of documents (\\[BLO  92\\] and \\[STA 95a\\]), or in SDI of information, in  which it is used to bring together a document to be disseminated and  its target \\[STA 93\\].",
    "acronym": "SDI",
    "label": "selective dissemination"
  },
  {
    "sentence": "Why not just include in each  Key Data Item a full list of pointers to the corresponding SDIs.",
    "acronym": "SDI",
    "label": "Sense Data Item"
  },
  {
    "sentence": "We use MERT to tune the best weights for each DL, and dmax = 10 performs the best on our dev set.",
    "acronym": "DL",
    "label": "distortion limit"
  },
  {
    "sentence": "2), unless a small DL (say, d=5) further restricts the possi- ble set of reorderings to those local ones by ruling out any long-distance reorderings that have a ?",
    "acronym": "DL",
    "label": "distortion limit"
  },
  {
    "sentence": "Moses is shown with various DLs (0, 6, 10, +?;",
    "acronym": "DL",
    "label": "distortion limit"
  },
  {
    "sentence": "Consistent with the theoretical anal- ysis in Section 2, Moses with no DL (dmax = +?)",
    "acronym": "DL",
    "label": "distortion limit"
  },
  {
    "sentence": "Our linear-time incremental decoder with the small beam of size b = 10 achieves a BLEU score of 29.54, compara- ble to Moses with the optimal DL of 10 (BLEU score 29.41).",
    "acronym": "DL",
    "label": "distortion limit"
  },
  {
    "sentence": "As a special case, phrase-based decoding with DL dmax is O(nbdmax). *:",
    "acronym": "DL",
    "label": "distortion limit"
  },
  {
    "sentence": "with Moses at various DLs (dmax=0, 6, 10, and +?).",
    "acronym": "DL",
    "label": "distortion limit"
  },
  {
    "sentence": "817 3 Distinguishing Mass and Count Nouns In the proposed method, DLs [13] are used to distinguish mass and count nouns.",
    "acronym": "DL",
    "label": "decision list"
  },
  {
    "sentence": "Section 3.3 explains the method for distinguishing mass and count nouns using the DLs.",
    "acronym": "DL",
    "label": "decision list"
  },
  {
    "sentence": "Exploring au- tomatic word sense disambiguation with DLs and the Web.",
    "acronym": "DL",
    "label": "decision list"
  },
  {
    "sentence": "Detecting Article Errors Based on the Mass Count Distinction 817 3 Distinguishing Mass and Count Nouns In the proposed method, DLs [13] are used to distinguish mass and count nouns.",
    "acronym": "DL",
    "label": "decision list"
  },
  {
    "sentence": "Section 3.2 describes how to learn DLs from the training data.",
    "acronym": "DL",
    "label": "decision list"
  },
  {
    "sentence": "These in- clude a variety of Bayesian classifiers (Golding, 1995; Golding and Schabes, 1996), DLs (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture mod- els (Cucerzan and Yarowsky, 2002).",
    "acronym": "DL",
    "label": "decision list"
  },
  {
    "sentence": "Generally, DLs are learned from a set of manually tagged training data.",
    "acronym": "DL",
    "label": "decision list"
  },
  {
    "sentence": "709  Text Authoring, Knowledge Acquisition and DL Marc Dymetman Xerox Research Centre Europe 6 chemin de Maupertuis 38240 Meylan email: marc.dymetman@xrce.xerox.com Abstract We present a principled approach to the problem of con- necting a controlled document authoring system with a knowledge base.",
    "acronym": "DL",
    "label": "Description Logics"
  },
  {
    "sentence": "(5)In this paper we address the problem of query          answering using views for non-recursive data log            queries embedded in a DL          knowledge base.",
    "acronym": "DL",
    "label": "Description Logics"
  },
  {
    "sentence": "We have presented a formal ap- proach to closed-world authoring that shows a correspon- dence between life-death problems and conjunctive Dat- alog queries, as well as a formal approach to open-world document authoring based on DL.",
    "acronym": "DL",
    "label": "Description Logics"
  },
  {
    "sentence": "In addition, the top level part of the ontology (i.e., the Tbox in the DL terminology) is very 1http://nlp.cs.swarthmore.edu/semeval/ tasks/task10/description.shtml 248 often modified during the ontology engineering life- cycle, for example by introducing new concepts and restructuring the subclass of hierarchy according to the renewed application needs required by the evo- lution of the application domain.",
    "acronym": "DL",
    "label": "Description Logics"
  },
  {
    "sentence": "In DL.",
    "acronym": "DL",
    "label": "Description Logics"
  },
  {
    "sentence": "Optimising Tableaux Decision Procedures for DL.",
    "acronym": "DL",
    "label": "Description Logics"
  },
  {
    "sentence": "The DL Handbook: Theory, Implementation, and Applica- tions.",
    "acronym": "DL",
    "label": "Description Logic"
  },
  {
    "sentence": "Baader F., D. Calvanese, D. L. McGuinness, D. Nardi, P. F. Patel-Schneider (2003) The DL Hand- book : Theory, Implementation, Applications.",
    "acronym": "DL",
    "label": "Description Logic"
  },
  {
    "sentence": "Polarity compositions on the nodes  2.3 Tree DL in IG  Another specification of IG is that syntactic  structures can be underspecified: these structures  are trees descriptions.",
    "acronym": "DL",
    "label": "Description Logic"
  },
  {
    "sentence": "Higher order relations and DL  Although description logics on which OIL and  RACER are based allow only binary relations, we  use OIL and Racer in a way that also allows us to  employ arbitrary n-ary relations and higher-order  relations.",
    "acronym": "DL",
    "label": "Description Logic"
  },
  {
    "sentence": "In addition, the top level part of the ontology (i.e., the Tbox in the DLs terminology) is very 1http://nlp.cs.swarthmore.edu/semeval/ tasks/task10/description.shtml 248 often modified during the ontology engineering life- cycle, for example by introducing new concepts and restructuring the subclass of hierarchy according to the renewed application needs required by the evo- lution of the application domain.",
    "acronym": "DL",
    "label": "Description Logic"
  },
  {
    "sentence": "The two KBs codify, in DL (Baader et al, 2003), assertions and concepts relevant for a given game scenario.",
    "acronym": "DL",
    "label": "Description Logic"
  },
  {
    "sentence": "A derivation in OT consists of an ORIGal candidate  set produced by a \\[unction called GI,;N, and the subsequent  application of constraints o reduce tile candidate set, elimi-  nating non-optimal candidates and I)reserving those with  Ihe greatest harmony.",
    "acronym": "ORIG",
    "label": "origin"
  },
  {
    "sentence": "Similarly, at lexical level, words of foreign ORIG  are used to form multi-word terms (e.g. redun- dantan atribut (Engl.",
    "acronym": "ORIG",
    "label": "origin"
  },
  {
    "sentence": "For  example, at morphological level, foreign suffixes,  mostly ORIGating from Latin and Greek, are of- ten ?",
    "acronym": "ORIG",
    "label": "origin"
  },
  {
    "sentence": "We subsumed the information in the ORIGal mor- phological tags in order to have the minimal number of categories needed for our task, listed in Table 1.2 In order to further reduce the number of features in a linguistically principled way, we took phrase boundaries into account: All words beyond a POS considered to be a phrase boundary marker (see Ta- ble 1) were assigned the tag empty.",
    "acronym": "ORIG",
    "label": "origin"
  },
  {
    "sentence": "This classification was ORIGally devised for sys- tems using an external ontology (so that semantic representations are directly linked to concepts in the ontology), but it is also suitable for broader settings, as we argue in the rest of the Section.",
    "acronym": "ORIG",
    "label": "origin"
  },
  {
    "sentence": "We  compiled a large corpus of text (News stories) and  made a second smaller corpus from the ORIGal one  which contains only sentences which are relevant to  the IE task.",
    "acronym": "ORIG",
    "label": "origin"
  },
  {
    "sentence": "ORIG?",
    "acronym": "ORIG",
    "label": "origin"
  },
  {
    "sentence": "In Annals of the New York Academy  of Sciences: Conferences on the ORIG and Devel- opment of Language and Speech, Volume 280: 20- 32.",
    "acronym": "ORIG",
    "label": "Origin"
  },
  {
    "sentence": "Annals of the New York Academy of Sciences: Conference on the ORIG and Develop- ment of Language and Speech, 280:20?32.",
    "acronym": "ORIG",
    "label": "Origin"
  },
  {
    "sentence": "This should be taken with a grain Baseline ORIGal Query Hybrid Tot Good Top 5 Tot Good Top 5 Tot Good Top 5 Poe 12 6.5 3 10 0.5 0.5 10 5.5 2.5 Romantics 10 0 0 15 0 0 10 3 3 Witch Hunts 10 8 3 14 2 1 10 8 5 US Wars 15 12 2 0 0 0 16 13 4 Sonnets 15 10 5 10 2 0 10 8 4 Presidents 15 2 2 15 0 0 15 2 2 Epics 10 7 4 10 5 3 10 7 4 Dec of Ind 10 2 0 0 0 0 10 5.5 2 Avr.",
    "acronym": "ORIG",
    "label": "Origin"
  },
  {
    "sentence": "In Proceed- ings of the Fourteenth International Conference Baseline ORIGal Query Hybrid FPF FPF/Tot FPF FPF/Tot FPF FPF/Tot Poe 1 .08 1 .1 1 .1 Romantics 8 .8 15 1 2 .2 Witch Hunts 2 .2 1 .07 0 0 US Wars 3 .2 3 .19 Sonnets 5 .33 8 .8 2 .2 Presidents 5 .33 10 .67 2 .13 Epics 0 0 0 0 0 0 Dec of Ind 3 .3 2 .2 28.1% 44% 12.8% Table 4: False Positives Containing Figure on Computational Linguistics, Nantes, France, July.",
    "acronym": "ORIG",
    "label": "Origin"
  },
  {
    "sentence": "Table 2 shows the word error rate (WER) 115 Source of alternates WER ORIGal closed-captions 5.8% Phoneme confusion matrix 4.4% Word confusion matrix 3.1% Combined 2.9% Table 2: Error rate for perfect correction.",
    "acronym": "ORIG",
    "label": "Origin"
  },
  {
    "sentence": "ORIGal form in source:  Where there is a source  for the entity or for some canonical form of the entity,  the original form is given.",
    "acronym": "ORIG",
    "label": "Origin"
  },
  {
    "sentence": "We will be describing a project with  partners including the UK and  the Assistive Technology team at Barnsley District  General Hospital.",
    "acronym": "UK",
    "label": "University of Sheffield"
  },
  {
    "sentence": "8  The Web as a Baseline: Evaluating the Performance of Unsupervised Web-based Models for a Range of NLP Tasks Mirella Lapata Department of Computer Science UK 211 Portobello St., Sheffield S1 4DP mlap@dcs.shef.ac.uk Frank Keller School of Informatics University of Edinburgh 2 Buccleuch Pl.,",
    "acronym": "UK",
    "label": "University of Sheffield"
  },
  {
    "sentence": "c?2013 Association for Computational Linguistics A temporal model of text periodicities using Gaussian Processes Daniel Preot?iuc-Pietro, Trevor Cohn Department of Computer Science UK Regent Court, 211 Portobello Street Sheffield, S1 4DP, United Kingdom {daniel,t.cohn}@dcs.shef.ac.uk Abstract Temporal variations of text are usually ig- nored in NLP applications.",
    "acronym": "UK",
    "label": "University of Sheffield"
  },
  {
    "sentence": "of  Computer Science, UK.",
    "acronym": "UK",
    "label": "University of Sheffield"
  },
  {
    "sentence": "c?2013 Association for Computational Linguistics DALE: A Word Sense Disambiguation System for Biomedical Documents Trained using Automatically Labeled Examples Judita Preiss and Mark Stevenson Department of Computer Science, UK Regent Court, 211 Portobello Sheffield S1 4DP, United Kingdom j.preiss,m.stevenson@dcs.shef.ac.uk Abstract Automatic interpretation of documents is hampered by the fact that language contains terms which have multiple meanings.",
    "acronym": "UK",
    "label": "University of Sheffield"
  },
  {
    "sentence": "UK Introduction The title of this piece refers to Newton?s only known modest remark: ?",
    "acronym": "UK",
    "label": "University of Sheffield"
  },
  {
    "sentence": "Phi)  thesis, UK of I{dinburgh.",
    "acronym": "UK",
    "label": "University"
  },
  {
    "sentence": "School of ECE, Technical UK of Crete, Chania 73100, Greece ?",
    "acronym": "UK",
    "label": "University"
  },
  {
    "sentence": "Ph.D. thesis, UK of Illinois at Urbana-Champaign.",
    "acronym": "UK",
    "label": "University"
  },
  {
    "sentence": ".ambridge UK Press.",
    "acronym": "UK",
    "label": "University"
  },
  {
    "sentence": "of Computer Science,  Brown UK,  \\[nge I j th \\[ ec\\] ~cs.",
    "acronym": "UK",
    "label": "University"
  },
  {
    "sentence": "Tech-  nical Rcl)ort 2, Center \\['or Cognitive Science, Rutgcrs  UK.",
    "acronym": "UK",
    "label": "University"
  },
  {
    "sentence": "16  Advances in domain independent linear text segmentat ion  Freddy Y. Y.  Cho i   Artificial Intell igence Group  Department  of Computer  Science  UK  Manchester,  England  choif@cs.man.ac.uk  Abst rac t   This paper describes a method for linear text seg-  mentation which is twice as accurate and over seven  times as fast as the state-of-the-art (Reynar, 1998).",
    "acronym": "UK",
    "label": "University of Manchester"
  },
  {
    "sentence": "c?2006 Association for Computational Linguistics Extremely Lexicalized Models for Accurate and Fast HPSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics UK Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, UK SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremel",
    "acronym": "UK",
    "label": "University of Manchester"
  },
  {
    "sentence": "c?2005 Association for Computational Linguistics Chunk Parsing Revisited Yoshimasa Tsuruoka \u0000\u0002\u0001 and Jun?ichi Tsujii \u0001\u0004\u0003\u0005\u0000 \u0000 CREST, JST (Japan Science and Technology Corporation) \u0001 Department of Computer Science, University of Tokyo \u0003 School of Informatics, UK \u0006 tsuruoka,tsujii \u0007 @is.s.u-tokyo.ac.jp Abstract Chunk parsing is conceptually appealing but its performance has not been satis- factory for practical use.",
    "acronym": "UK",
    "label": "University of Manchester"
  },
  {
    "sentence": "d Models for Accurate and Fast HPSG Parsing Takashi Ninomiya Information Technology Center University of Tokyo Takuya Matsuzaki Department of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics UK Yusuke Miyao Department of Computer Science University of Tokyo Jun?ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, UK SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract This paper describes an extremely lexi- calized probabilistic model for fast and accurate HPSG parsing.",
    "acronym": "UK",
    "label": "University of Manchester"
  },
  {
    "sentence": "168  The FINITE STRING Newslet ter   S i te  Repor t   Control l ing Complex  Systems of Linguistic  Rules  Rod Johnson  UK Institute of Science and  Technology, U.K.  Steven Krauwer  Rijksuniversiteit, Utrecht, Holland  Mike Rosner  Fondazione dalle Molle, ISSCO, University of Geneva,  Switzerland  Nino Varile  Commission of the European Communities,  Luxembourg  \\[Most of the ideas in this paper evolved during work  on design studies for the EEC Machine Translation  Project Eurotra.",
    "acronym": "UK",
    "label": "University of Manchester"
  },
  {
    "sentence": "Master's Thesis, UK (1981).",
    "acronym": "UK",
    "label": "University of Manchester"
  },
  {
    "sentence": "Practical Considerations in Building a Multi-Lingual Authoring System for  Business Letters  John Tait, Huw Sanderson  School of Computing +Information Systems  UK  Sunderland SR6 ODD, U.K.  {John.",
    "acronym": "UK",
    "label": "University of Sunderland"
  },
  {
    "sentence": "thesis, UK.",
    "acronym": "UK",
    "label": "University of Sunderland"
  },
  {
    "sentence": "Ph.D. thesis, UK.",
    "acronym": "UK",
    "label": "University of Sunderland"
  },
  {
    "sentence": "241  Selforganizing classification on the Reuters news corpus    Stefan Wermter  The Informatics Centre   School of CET  UK  St. Peter?s Way, Sunderland SR6 0DD   United Kingdom  Stefan.wermter@sunderland.ac.uk  Chihli Hung1  The Informatics Centre   School of CET  UK  St. Peter?s Way, Sunderland SR6 0DD   United Kingdom  Chihli.hung@sunderland.ac.uk                                                           1 Hung is a lecturer of De Lin Institute of Technology as well.",
    "acronym": "UK",
    "label": "University of Sunderland"
  },
  {
    "sentence": "12  D-70174 Stuttgart  Germany  Psychology  UK  South Parks Road  Oxford OX1 3UD  England  University of Edinburgh  2 Buccleuch Place  Edinburgh EH8 9LW  Scotland  {vogel,holly,ueh}~cogsci.ed.ac.uk  Abstract  Cross-serial dependencies in Dutdl  and  Swiss-German are the only known extra-  context fi'ee natural language syntactic  phenonmna.",
    "acronym": "UK",
    "label": "University of Oxford"
  },
  {
    "sentence": "c?2014 Association for Computational Linguistics Monads as a Solution for Generalized Opacity Gianluca Giorgolo UK Ash Asudeh UK and Carleton University {gianluca.giorgolo,ash.asudeh}@ling-phil.ox.ac.uk Abstract In this paper we discuss a conservative ex- tension of the simply-typed lambda calcu- lus in order to model a class of expres- sions that generalize the notion of opaque contexts.",
    "acronym": "UK",
    "label": "University of Oxford"
  },
  {
    "sentence": "S. G. Pulman, J. Boye UK sgp@clg.ox.ac.uk M. Cavazza, C. Smith Teesside University m.o.cavazza@tees.ac.uk R. S. de la Ca?mara Telefonica I+D e.rsai@tid.es Abstract We describe a ?",
    "acronym": "UK",
    "label": "University of Oxford"
  },
  {
    "sentence": "c?2016 Association for Computational Linguistics Cross-Lingual Morphological Tagging for Low-Resource Languages Jan Buys Department of Computer Science UK jan.buys@cs.ox.ac.uk Jan A. Botha Google Inc. London jabot@google.com Abstract Morphologically rich languages often lack the annotated linguistic resources required to develop accurate natural language pro- cessing tools.",
    "acronym": "UK",
    "label": "University of Oxford"
  },
  {
    "sentence": "published for the TEI Consortium by Hu- manities Computing Unit, UK.",
    "acronym": "UK",
    "label": "University of Oxford"
  },
  {
    "sentence": "4.3 TAGs The construction that allows to handle the tree adjoining grammars of Joshi (Joshi and Schabes, 1997) may be seen as a generalization of the con- struction that we have described for the context- free grammars.",
    "acronym": "TAGs",
    "label": "Tree adjoining grammars"
  },
  {
    "sentence": "TAGs: How much contextsensitivity is required ro provide reasonable structural descriptions?",
    "acronym": "TAGs",
    "label": "Tree adjoining grammars"
  },
  {
    "sentence": "TAGs.",
    "acronym": "TAGs",
    "label": "Tree adjoining grammars"
  },
  {
    "sentence": "TAGs (Joshi and Levy, 1977) can also be viewed as a special kind of LCFRS with f = 2, since each auxil- iary tree generates two strings, and with r given by the maximum number of adjunction and sub- stitution sites in an elementary tree.",
    "acronym": "TAGs",
    "label": "Tree adjoining grammars"
  },
  {
    "sentence": "TAGs (Joshi and Levy, 1977), or TAG for short, can be viewed as a special kind of LCFRS with f = 2, since each elementary tree generates two strings, and r given by the maximum number of adjunction sites in an elementary tree.",
    "acronym": "TAGs",
    "label": "Tree adjoining grammars"
  },
  {
    "sentence": "How much context sensi- tivity is necessary for characterizing structural de- scriptions: TAGs.",
    "acronym": "TAGs",
    "label": "Tree adjoining grammars"
  },
  {
    "sentence": "In Gardent, C. and A. Sarkar, editors, Proceedings of the 9th In- ternational Workshop on TAGs and Related Formalisms (TAG+?08), pages 141?",
    "acronym": "TAGs",
    "label": "Tree Adjoining Grammars"
  },
  {
    "sentence": "In Proceedings of the 9th International Workshop on TAGs and Related For- malisms.",
    "acronym": "TAGs",
    "label": "Tree Adjoining Grammars"
  },
  {
    "sentence": "Feature Struc- tures Based TAGs.",
    "acronym": "TAGs",
    "label": "Tree Adjoining Grammars"
  },
  {
    "sentence": "rkar, editors, Proceedings of the 9th In- ternational Workshop on TAGs and Related Formalisms (TAG+?08), pages 141?",
    "acronym": "TAGs",
    "label": "Tree Adjoining Grammars"
  },
  {
    "sentence": "In Proceedings of the 6th In- ternational Workshop on TAGs and Related Frameworks (TAG+ 6), pages 19?24.",
    "acronym": "TAGs",
    "label": "Tree Adjoining Grammars"
  },
  {
    "sentence": "The methodology works as follows: given a set of topics (including intruder words), we compute the word association features for each of the top- N topic words of a topic, 3 and combine the fea- tures in a ranking SVR model (SVM rank : Joachims (2006)) to learn the intruder words.",
    "acronym": "SVR",
    "label": "support vector regression"
  },
  {
    "sentence": "We use SVR implemented in the SVMLight toolkit 10 with Radial Basis Function (RBF) kernel to build this baseline.",
    "acronym": "SVR",
    "label": "support vector regression"
  },
  {
    "sentence": "A tutorial on SVR.",
    "acronym": "SVR",
    "label": "support vector regression"
  },
  {
    "sentence": "We  propose to use the SVR  model to achieve this goal.",
    "acronym": "SVR",
    "label": "support vector regression"
  },
  {
    "sentence": "Shrinking the tube: a new SVR algorithm.",
    "acronym": "SVR",
    "label": "support vector regression"
  },
  {
    "sentence": "Extractive Multi- Document Summarization with Integer Linear Pro- gramming and SVR.",
    "acronym": "SVR",
    "label": "Support Vector Regression"
  },
  {
    "sentence": "719 3.4 Other Baselines We train a SVR model (see Section 4), which is one of the most widely used approaches in text scoring.",
    "acronym": "SVR",
    "label": "Support Vector Regression"
  },
  {
    "sentence": "We find that our manifold ranking approach outperforms several state-of-the- art learning baselines on this task, including trans- ductive SVR.",
    "acronym": "SVR",
    "label": "Support Vector Regression"
  },
  {
    "sentence": "Extractive multi-document summarization with ILP and SVR.",
    "acronym": "SVR",
    "label": "Support Vector Regression"
  },
  {
    "sentence": "Accurate Online SVR.",
    "acronym": "SVR",
    "label": "Support Vector Regression"
  },
  {
    "sentence": "A Hierarchic AE  Logic.",
    "acronym": "AE",
    "label": "Autoepistemic"
  },
  {
    "sentence": "sssss???sssss,  Figure 1: A Hierarchic AE Theory  Hearer's beliefs as a consequence of the speech act:  in AI:  ^ (13)  -~L0--\\[h/\\]~b ^ ~Zo\\[hy\\]',\\[Sl\\]?~ ^  \"~Lo\\[hy\\]\"{si}\\[hy\\]~) D \\[h/l~b  The asymmetry between Axioms 12 and 13 is a  consequence of the fact that a speech act has dif-  ferent effects on the speaker's and hearer's mental  states.",
    "acronym": "AE",
    "label": "Autoepistemic"
  },
  {
    "sentence": "AE as Se- quence Tagging with Tree Edit Distance.",
    "acronym": "AE",
    "label": "Answer Extraction"
  },
  {
    "sentence": "3.3 Personalized QA Algorithm The interaction between the UM component and the core QA component modifies the standard QA process at the AE phase, which is modified as follows: 3 The TF ?",
    "acronym": "AE",
    "label": "Answer Extraction"
  },
  {
    "sentence": "Diego Molla, Rolf Schwitter, Michael Hess, Rachel Fournier, 2000, Extrans, an AE System, Traitement Automatique des Langues, Hermes Science Publication, 41-2, 495-522.",
    "acronym": "AE",
    "label": "Answer Extraction"
  },
  {
    "sentence": "Where Q and T are sets of the bag-of-words  for the question relation and the triple relation  respectively, Lin(a,b) is a measure for the seman- tic similarity between a and b based on WordNet  (Lin, 1998), and L(x) is the number of elements  in the set x.  The AE: this component first  filters out the triples mismatching the expected  answer type.",
    "acronym": "AE",
    "label": "Answer Extraction"
  },
  {
    "sentence": "AE as Sequence Tagging with Tree Edit Distance.",
    "acronym": "AE",
    "label": "Answer Extraction"
  },
  {
    "sentence": "AE: 1.",
    "acronym": "AE",
    "label": "Answer Extraction"
  },
  {
    "sentence": "When analysing the corresponding NG, the  case frame for the head noun \"liquid\" is used at first.",
    "acronym": "NG",
    "label": "noun group"
  },
  {
    "sentence": "The semanl;i(:al analysis of a sentence is based on its head  verb, while the analysis of a NG is be based on the  head noun and also on adjective descriptors, genitive deter-  miners and preposit ional phrases.",
    "acronym": "NG",
    "label": "noun group"
  },
  {
    "sentence": "The  constraint of the NG (being a physical object) is full-  filled, thus the analysis proceeds.",
    "acronym": "NG",
    "label": "noun group"
  },
  {
    "sentence": "67 Set Comment Size IN  The inflection set for NG.",
    "acronym": "NG",
    "label": "noun group"
  },
  {
    "sentence": "40  IP The inflection set for proNG.",
    "acronym": "NG",
    "label": "noun group"
  },
  {
    "sentence": "In natural anguages, propositions  can be expressed not only by sentences, but also by other  syntactic structures uch as NGs, infinitive phrases  and embedded sentences.",
    "acronym": "NG",
    "label": "noun group"
  },
  {
    "sentence": "The prepositional phrase of the NG is analysed by  first selecting the case frame for the",
    "acronym": "NG",
    "label": "noun group"
  },
  {
    "sentence": "1 MOTIVAT ION  In recent DARPA speech community-wide r cognition  system evaluations, the recognition systems have been  tested using two grammatical conditions: no grammar  (or NG), and the word-pair grammar.",
    "acronym": "NG",
    "label": "null grammar"
  },
  {
    "sentence": "The word accuracies for various versions of  SPHINX with the word-pair grammar (perplexity 60) and  the NG (perplexity 991) are shown in Table 1.",
    "acronym": "NG",
    "label": "null grammar"
  },
  {
    "sentence": "The NG provides \"only a worst -case  recognit ion test point\", whi le  the word-pa i r  grammar not only  excludes \"many reasonable word sequences\", but the use of the word-  pair  grammar y ie lds such high recognit ion per formance that re l iable  measurement  of system improvements (i.e. s tat is t ica l ly  s igni f icant   inferences of improvements) cannot be obtained wi thout  use of",
    "acronym": "NG",
    "label": "null grammar"
  },
  {
    "sentence": "The NG simply forces the recognition sys-  tem to partition the input speech into whole-word units  without using any knowledge of the language to place  restrictions on the possible sequences of words that are  allowed.",
    "acronym": "NG",
    "label": "null grammar"
  },
  {
    "sentence": "Therefore, even in  a NG there is a great deal of a priori knowledge  being brought to bear.",
    "acronym": "NG",
    "label": "null grammar"
  },
  {
    "sentence": "For example, we could use  a NG in the forward direction and a more com-  plex grammar in the backward search.",
    "acronym": "NG",
    "label": "null grammar"
  },
  {
    "sentence": "Each instance is con- verted into a feature vector where the features are  10 NG (unigrams or bigrams) and each cell is  the frequency of occurrence of a unigram or bi- gram or the log-likelihood of a bigram occurring in that particular instance after applying a feature selection method.",
    "acronym": "NG",
    "label": "ngrams"
  },
  {
    "sentence": "These features can be broken into four categories: NG: Ngrams of order to 1 to 3, found via Hap- pierFunTokenizer, and restricted to those used by at least 5% of users (resulting in 10,450 NG).",
    "acronym": "NG",
    "label": "ngrams"
  },
  {
    "sentence": "4 Differential Language Analysis Figure 4 shows the 100 NG most highly cor- related with depression score across the 21,913 Facebook users in our dataset writing at least 1,000 words.",
    "acronym": "NG",
    "label": "ngrams"
  },
  {
    "sentence": "This provides a continuous value outcome, for which we fit a regression model based on NG, LDA topics, and lexica usage.",
    "acronym": "NG",
    "label": "ngrams"
  },
  {
    "sentence": "It  offers a variety of lexical features (NG, col- locations, etc.)",
    "acronym": "NG",
    "label": "ngrams"
  },
  {
    "sentence": "ence has been identified by the  parser, so that also the underlying dependency relations (valency positions) of  the complementations (to the governing verb) are known, the verb and all the  complementations are first assumed to be NB, i.e., to belong to the focus,  which we denote by f.  (b) If the verb occupies the rightmost position in the sentence and its subject is  (ba) definite (including NG with this, with oneofthe, etc.),",
    "acronym": "NG",
    "label": "noun groups"
  },
  {
    "sentence": "5 The tools can be used to acquire non-clausal patterns as well, e.g. patterns for NG and complex noun phrases, to extend an existing pattern library.",
    "acronym": "NG",
    "label": "noun groups"
  },
  {
    "sentence": "In natural anguages, propositions  can be expressed not only by sentences, but also by other  syntactic structures uch as NG, infinitive phrases  and embedded sentences.",
    "acronym": "NG",
    "label": "noun groups"
  },
  {
    "sentence": "Thus,  the semantic domain for NG is \\[C-~S\\].",
    "acronym": "NG",
    "label": "noun groups"
  },
  {
    "sentence": "Above all, this  concerns the following points in which a more general procedure could be formulated:  (i) The procedure should also take into account deeper embedded sentence parts  (embedded verb clauses, modifiers in NG, etc.).",
    "acronym": "NG",
    "label": "noun groups"
  },
  {
    "sentence": "5.3 Case markers Turkish, being a fairly scrambling language, uses case markers to denote the syntactic functions of nouns and NG.",
    "acronym": "NG",
    "label": "noun groups"
  },
  {
    "sentence": "RM uh, I mean, ? ?? ?",
    "acronym": "RM",
    "label": "Reparandum"
  },
  {
    "sentence": "Words with italic font are RMs.",
    "acronym": "RM",
    "label": "Reparandum"
  },
  {
    "sentence": "RM uh ????",
    "acronym": "RM",
    "label": "Reparandum"
  },
  {
    "sentence": "Second, hu- man processing of spoken language is complex and mixes acoustic and syntactic indicators (Cutler et al, 1997), so an automatic system must employ fea- tures targeting all levels of the perceptual stack to in  the    upper     school                          upper  four   grades  Fluent Fluent Disfluent  RM  Repair   Figure 1: Example of a disfluency where the speaker corrected upper school.",
    "acronym": "RM",
    "label": "Reparandum"
  },
  {
    "sentence": "RM + uh, I mean, ? ?? ?",
    "acronym": "RM",
    "label": "Reparandum"
  },
  {
    "sentence": "RM : what precedes the interruption point.",
    "acronym": "RM",
    "label": "Reparandum"
  },
  {
    "sentence": "Many tests on the RM task were  performed with both context-independent (CI) PLUs (set size = 47) and context-dependent (CD)  PLUs (set sizes range from 638 to 2340).",
    "acronym": "RM",
    "label": "Resource Management"
  },
  {
    "sentence": "Murveit, H., J. Butzberger, and M. Weintraub,\"Speech  Recognition in SRI's RM and ATIS  Systems,\" Proc.",
    "acronym": "RM",
    "label": "Resource Management"
  },
  {
    "sentence": "Lee discussed CMU's present progress,  including the use of semi-continuous hidden Markov models (SCHMMs) applied to the 1000-  word speaker-independent RM continuous peech recognition task.",
    "acronym": "RM",
    "label": "Resource Management"
  },
  {
    "sentence": "Lee discussed CMU's present progress,  including the use of semi-continuous hidden Markov models (SCHMMs) applied to the 1000-  word speaker-independent RM continuous peech recognition ta",
    "acronym": "RM",
    "label": "Resource Management"
  },
  {
    "sentence": "Although not all of these methods have yet been combined  into one system, the error rate on the May 1988 RM test set (using word-pair  grammar) has been halved.",
    "acronym": "RM",
    "label": "Resource Management"
  },
  {
    "sentence": "Encouraging test results were obtained using  RM speech data where \"new\" words were created simply by removing a  subset of in-vocabulary words from the standard system lexicon.",
    "acronym": "RM",
    "label": "Resource Management"
  },
  {
    "sentence": "H. Murveit, J. Butzberger, and M. Weintraub, \"Speech  Recognition in SRI's RM and ATIS  Systems,\" 1991 DARPA Speech and Natural Language  Workshop, pp.",
    "acronym": "RM",
    "label": "Resource Management"
  },
  {
    "sentence": "% correct)  CBL Default i ~ -   Algorithm Strategy Heuristics  w/o  feature  set selection  76.2 74.3 80.5  In the sections below, we describe the recency  bias, the RM bias, and the subject  accessibility bias in turn.",
    "acronym": "RM",
    "label": "restricted memory"
  },
  {
    "sentence": "Our baseline case representation does not nec-  essarily make use of this RM bias,  however.",
    "acronym": "RM",
    "label": "restricted memory"
  },
  {
    "sentence": "In addition, the RM bias  alon",
    "acronym": "RM",
    "label": "restricted memory"
  },
  {
    "sentence": "117  Thus far, we have incorporated three such bi-  ases into the feature set selection algorithm: (1) a  recency bias, (2) a RM bias, and (3)  a subject accessibility bias.",
    "acronym": "RM",
    "label": "restricted memory"
  },
  {
    "sentence": "Unfortunately, incorporat-  ing the RM limitations into the case  representation is problematic.",
    "acronym": "RM",
    "label": "restricted memory"
  },
  {
    "sentence": "To apply the RM bias to the  baseline case representation, we let n represent the  memory limit and, in each of five runs, set n to one  of five, six, seven, eight, or nine.",
    "acronym": "RM",
    "label": "restricted memory"
  },
  {
    "sentence": "ately, incorporat-  ing the RM limitations into the case  representation is problematic.",
    "acronym": "RM",
    "label": "restricted memory"
  },
  {
    "sentence": "In addition, the RM bias  alone does not state which chunks, or features, to  keep and which to discard.",
    "acronym": "RM",
    "label": "restricted memory"
  },
  {
    "sentence": "We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named entity reCOG was used for identifying proper names, e.g., ?",
    "acronym": "COG",
    "label": "cognition"
  },
  {
    "sentence": "Unsupervised personality reCOG for social network sites.",
    "acronym": "COG",
    "label": "cognition"
  },
  {
    "sentence": "3) We employed IE methods (including pattern sets  and Named Entity ReCOG) as initial extraction  steps.",
    "acronym": "COG",
    "label": "cognition"
  },
  {
    "sentence": "This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) named entity reCOG, (vi) dependency parsing, and (vii) co-reference analysis.",
    "acronym": "COG",
    "label": "cognition"
  },
  {
    "sentence": "Two broad approaches for the iden- tification of story characters were followed: (i) named entity reCOG, and (ii) identification of character nominals, e.g., ?",
    "acronym": "COG",
    "label": "cognition"
  },
  {
    "sentence": "The approach incorporates a set of ge- neric morpho-syntactic filters for recogni- tion of term candidates, a method for  conflation of morphological variants and  a module for foreign word reCOG.",
    "acronym": "COG",
    "label": "cognition"
  },
  {
    "sentence": "We used two approaches for identifying story characters motivated by (Elson and McKeown, 2010): 1) named entity reCOGn was used for identifying proper names, e.g., ?",
    "acronym": "COG",
    "label": "cognitio"
  },
  {
    "sentence": "Unsupervised personality reCOGn for social network sites.",
    "acronym": "COG",
    "label": "cognitio"
  },
  {
    "sentence": "3) We employed IE methods (including pattern sets  and Named Entity ReCOGn) as initial extraction  steps.",
    "acronym": "COG",
    "label": "cognitio"
  },
  {
    "sentence": "This includes (i) tokenization, (ii) sen- tence splitting and identification of paragraph boundaries, (iii) part-of-speech (POS) tagging, (iv) lemmatization, (v) named entity reCOGn, (vi) dependency parsing, and (vii) co-reference analysis.",
    "acronym": "COG",
    "label": "cognitio"
  },
  {
    "sentence": "Two broad approaches for the iden- tification of story characters were followed: (i) named entity reCOGn, and (ii) identification of character nominals, e.g., ?",
    "acronym": "COG",
    "label": "cognitio"
  },
  {
    "sentence": "The approach incorporates a set of ge- neric morpho-syntactic filters for recogni- tion of term candidates, a method for  conflation of morphological variants and  a module for foreign word reCOGn.",
    "acronym": "COG",
    "label": "cognitio"
  },
  {
    "sentence": "ADDe.",
    "acronym": "ADD",
    "label": "ATR Dialogue Databas"
  },
  {
    "sentence": "Activation by pRb and c-Myc is not ADD, suggesting that they act upon the same site, thereby perhaps blocking the binding of an unidentified inhibitor.",
    "acronym": "ADD",
    "label": "additive"
  },
  {
    "sentence": "One of the main facts of interaction M4 (Ta- ble 1) is Activation by pRb and c-Myc is not ADD . . .",
    "acronym": "ADD",
    "label": "additive"
  },
  {
    "sentence": "Moreover,  this deviation cannot be remedied later by sum- ming up individual termhoods, since C-value is  not an ADD measure.",
    "acronym": "ADD",
    "label": "additive"
  },
  {
    "sentence": "the positive effects of RB and c-Myc were not ADD. (",
    "acronym": "ADD",
    "label": "additive"
  },
  {
    "sentence": "M4 Subfact: Activation of E-cadherin by pRb and c-Myc is not ADD, suggesting they act on the same site a) However, the precise molecular mechanisms by which RB, Myc, and AP-2 cooperate to effect transcriptional activation of E-cadherin requires further study. . . .",
    "acronym": "ADD",
    "label": "additive"
  },
  {
    "sentence": "An instance supporting part of this fact, the subfact in Table 2 Activation of E-cadherin by pRb and c- Myc is not ADD . . . ,",
    "acronym": "ADD",
    "label": "additive"
  },
  {
    "sentence": "Sparse ADD generative models of text.",
    "acronym": "ADD",
    "label": "additive"
  },
  {
    "sentence": "relevant nodes (m=5 for our reported experiment) and  ADD them to the previous set of WordNet nodes.",
    "acronym": "ADD",
    "label": "added"
  },
  {
    "sentence": "As before, the harmonic vahlations arc ADD during con-  catenation.",
    "acronym": "ADD",
    "label": "added"
  },
  {
    "sentence": "We present some experiments il-  lustrating the accuracy of the method and note  that with this information ADD, our pronoun  resolution method achieves 84.2% accuracy.",
    "acronym": "ADD",
    "label": "added"
  },
  {
    "sentence": "As a measure of the utility of these results, we  also ran our pronoun-anaphora program with  these statistics ADD.",
    "acronym": "ADD",
    "label": "added"
  },
  {
    "sentence": "If ally of  them point to states with undelined Imrtnony values, tile  harlnony of the state being expanded, and o1' tile arc, are  used to calculate the lmtumny value of tile {}tiler state and  it is ADD to the list.",
    "acronym": "ADD",
    "label": "added"
  },
  {
    "sentence": "We see a significant improvement after the  word knowledge is ADD to the program.",
    "acronym": "ADD",
    "label": "added"
  },
  {
    "sentence": "Recent reports by (Nivre, 2007) delineated a class of richly-inflected languages with relatively free word-order (including Greek, Basque, and MSA) for which the parsers performed poorly, regardless of the parsing method used.",
    "acronym": "MSA",
    "label": "Modern Standard Arabic"
  },
  {
    "sentence": "Quantitative ap- proaches to analyzing COME constructions in  MSA.",
    "acronym": "MSA",
    "label": "Modern Standard Arabic"
  },
  {
    "sentence": "c?2014 Association for Computational Linguistics Annotating corpus data for a quantitative, constructional analysis of  motion verbs in MSA     Dana Abdulrahim  University of Bahrain  darahim@uob.edu.bh         Abstract  This article proposes an annotation method of corpus  data for the purposes of providing a constructionist  account of lexical behavior.",
    "acronym": "MSA",
    "label": "Modern Standard Arabic"
  },
  {
    "sentence": "In MSA, the existence of  several verbs denoting the motion events COME  (at?, ???",
    "acronym": "MSA",
    "label": "Modern Standard Arabic"
  },
  {
    "sentence": "A corpus study of basic  motion events in MSA.",
    "acronym": "MSA",
    "label": "Modern Standard Arabic"
  },
  {
    "sentence": "However, a closer examination of the length difference evident through the BLEU brevity penalty and the reference:system-output length ra- tio (columns 4-5 of Table 2), reveals that the dif- ferences are small and inconsistent; on average, the brevity penalty difference accounts for roughly 0.1 absolute BLEU points and 0.2 absolute lemmatized BLEU points of the respective differences.7 Last, MSA is a morphologi- cally rich language: It has many inflected forms for most verbs, and several inflected forms for nouns, adjectives and other parts of speech ?",
    "acronym": "MSA",
    "label": "Modern Standard Arabic"
  },
  {
    "sentence": "PROMT parsers are rule-based multi-level  MSA.",
    "acronym": "MSA",
    "label": "morphosyntactic analyzers"
  },
  {
    "sentence": "PROMT uses  MSA to analyze the source  sentence and transfer rules to translate the sentence  345 into the target language.",
    "acronym": "MSA",
    "label": "morphosyntactic analyzers"
  },
  {
    "sentence": "However, the flexible sys- tem architecture also allows us to experiment with different MSA, such as TextMorfo (Kielikone Oy 1999) and Conexor FDG (Conexor Oy 1997-2000), and we plan to run them in parallel as separate competing agents to test and compare their applicability as well as the Jaspis architecture in the given task.",
    "acronym": "MSA",
    "label": "morphosyntactic analyzers"
  },
  {
    "sentence": "Un- like language analysis, for which different ex- isting Finnish MSA can be used, there are no readily available general- purpose Finnish language generators.",
    "acronym": "MSA",
    "label": "morphosyntactic analyzers"
  },
  {
    "sentence": "QA systems have been evaluated at TREC QA-Track1 in U.S. and QAC (Q&A Challenge)2 in Japan.",
    "acronym": "Q&A",
    "label": "Question & Answering"
  },
  {
    "sentence": "WordNet has been widely criticised for being a sense repository that often offers too fine?grained sense distinctions for higher level applications like Machine Translation or Q&A.",
    "acronym": "Q&A",
    "label": "Question & Answering"
  },
  {
    "sentence": "Therefore, Short Message Service (SMS) is a  better way for giving knowledge service, espe- cially automatic interchange of short text mes- sages, by providing the information from an  automatic Q&A System.",
    "acronym": "Q&A",
    "label": "Question & Answering"
  },
  {
    "sentence": "Issues, Tasks and Program Structures to Roadmap Research in Q&A.",
    "acronym": "Q&A",
    "label": "Question & Answering"
  },
  {
    "sentence": "But, WordNet (WN) has been widely criticized for being a sense repository that often provides too fine?grained sense distinctions for higher level applications like Machine Translation or Q&A.",
    "acronym": "Q&A",
    "label": "Question & Answering"
  },
  {
    "sentence": "The tasks set in these conferences have molded a specific kind of Q&Ag that is easy to evaluate and that focuses on the use of fast and shallow methods that are generally independent of the application domain.",
    "acronym": "Q&A",
    "label": "question answerin"
  },
  {
    "sentence": "A main characteristic of Q&Ag in restricted domains is the integration of domain-specific information that is either developed for Q&Ag or that has been developed for other purposes.",
    "acronym": "Q&A",
    "label": "question answerin"
  },
  {
    "sentence": "A main characteristic of Q&Ag in restricted domains is the integration of domain-specific information that is either developed for Q&Ag or that has been developed for other pur",
    "acronym": "Q&A",
    "label": "question answerin"
  },
  {
    "sentence": "article is on the use of restricted domains for automated Q&Ag.",
    "acronym": "Q&A",
    "label": "question answerin"
  },
  {
    "sentence": "From this perspective, Q&Ag focuses on finding text excerpts that contain the answer within large collections of documents.",
    "acronym": "Q&A",
    "label": "question answerin"
  },
  {
    "sentence": "The focus of this article is on the use of restricted domains for automated Q&Ag.",
    "acronym": "Q&A",
    "label": "question answerin"
  },
  {
    "sentence": "University of Alicante, Spain Automated Q&Ag has been a topic of research and development since the earliest AI applications.",
    "acronym": "Q&A",
    "label": "question answerin"
  },
  {
    "sentence": "The setting is also appropriate for cases that may require making  global decisions that involve multiple components, possibly pre-designed or pre- learned, as in summarization, paraphrasing, textual entailment and Q&Ag.",
    "acronym": "Q&A",
    "label": "question answerin"
  },
  {
    "sentence": "The tasks set in these conferences have molded a specific kind of Q&Aing that is easy to evaluate and that focuses on the use of fast and shallow methods that are generally independent of the application domain.",
    "acronym": "Q&A",
    "label": "question answer"
  },
  {
    "sentence": "A main characteristic of Q&Aing in restricted domains is the integration of domain-specific information that is either developed for Q&Aing or that has been developed for other purposes.",
    "acronym": "Q&A",
    "label": "question answer"
  },
  {
    "sentence": "A main characteristic of Q&Aing in restricted domains is the integration of domain-specific information that is either developed for Q&Aing or that has been developed for other pur",
    "acronym": "Q&A",
    "label": "question answer"
  },
  {
    "sentence": "article is on the use of restricted domains for automated Q&Aing.",
    "acronym": "Q&A",
    "label": "question answer"
  },
  {
    "sentence": "From this perspective, Q&Aing focuses on finding text excerpts that contain the answer within large collections of documents.",
    "acronym": "Q&A",
    "label": "question answer"
  },
  {
    "sentence": "The focus of this article is on the use of restricted domains for automated Q&Aing.",
    "acronym": "Q&A",
    "label": "question answer"
  },
  {
    "sentence": "University of Alicante, Spain Automated Q&Aing has been a topic of research and development since the earliest AI applications.",
    "acronym": "Q&A",
    "label": "question answer"
  },
  {
    "sentence": "The setting is also appropriate for cases that may require making  global decisions that involve multiple components, possibly pre-designed or pre- learned, as in summarization, paraphrasing, textual entailment and Q&Aing.",
    "acronym": "Q&A",
    "label": "question answer"
  },
  {
    "sentence": "\"sCBt\\] SUr Ile~ert, 2)   \\] G_N_.~( ~unphot rca, i t er  ~ n ~ I ,mval;ur ore!mr1 t ~ 2 )  G_N_V(mvvsucwr.entm,I 1~ ub~,;~ i n .2 )   G. N. V{ meac, ur e~r.er, t ,  n ~. ~., ShO..,, ~ )  .~: G_N.H(&Cr.~e?.fc3ounCier',nl t~measur'onmn~;3)  G.H.H(pntltnna, n|  1 ~aea~'~.ent ,  3)  :~' G. N,.",
    "acronym": "CB",
    "label": "Ui"
  },
  {
    "sentence": "Juyup Sung, Jae-Gil Lee, and CBchin Lee.",
    "acronym": "CB",
    "label": "Ui"
  },
  {
    "sentence": "The HITs for the following languages were posted for a week and were never completed: Tigrinya, CBghur, Tibetan, Kyrgyz, and Kazakh.",
    "acronym": "CB",
    "label": "Ui"
  },
  {
    "sentence": "Instead of computing the proba-  bCBty for each one of them we group them into  \"buckets\", so that rrt a iS the bucket for the num-  ber of times that a is mentioned.",
    "acronym": "CB",
    "label": "Ui"
  },
  {
    "sentence": "Each word can thus be represented as a 108 Tigrinya 36 Punjabi 401 Kyrgyz 492 Somali 585 Nepali 1293 Tibetan 1358 CBghur 1814 Maltese 1896 Turkmen 3137 Kazakh 3470 Mongolian 4009 Tatar 4180 Kurdish 5059 Uzbek 5875 Kapampangan 6827 Urdu 7674 Irish 9859 Azeri 12568 Tamil 13470 Albanian 13714 Afrikaans 14315 Hindi 14824 Bangla 16026 Tagalog 17757 Latvian 22737 Bosnian 23144 Welsh 25292 Latin 31195 Basque 38594 Thai 40182 Farsi 58651 Bulgarian 68446 Serbian 71018 Indonesian 73962 Slovak 76421 Korean 84385 Turkish",
    "acronym": "CB",
    "label": "Ui"
  },
  {
    "sentence": "CBth  progress in automatic classification, another possibility  is being explored, that of creating new information rather  than merely gathering, maintaining or distributing the  results of human intellectual activities.",
    "acronym": "CB",
    "label": "Ui"
  },
  {
    "sentence": "We selected three blog sites from this dataset: the Right Wing News (right-ideology) ; the CBr, and Daily Kos as representatives 1144 palest inian is raeli peace year  polit ical  proces s   state  end  rig ht   g overnment  need  conflict way s ecurit y palest inian is raeli Peace polit ical  occupation  proces s end  s ecurit y   conflict   way  g overnment   people t ime year force  neg ot iation bush US pres ident  american sharon administ ration prime  s ett lem ent  pres",
    "acronym": "CB",
    "label": "Carpetbagge"
  },
  {
    "sentence": "We selected three blog sites from this dataset: the Right Wing News (right-ideology) ; the CB, and Daily Kos as representatives 1144 palest inian is raeli peace year  polit ical  proces s   state  end  rig ht   g overnment  need  conflict way s ecurit y palest inian is raeli Peace polit ical  occupation  proces s end  s ecurit y   conflict   way  g overnment   people t ime year force  neg ot iation bush US pres ident  american sharon administ ration prime  s ett lem ent  pres",
    "acronym": "CB",
    "label": "Carpetbagger"
  },
  {
    "sentence": "syntactically inte- grated in the sentence, i.e. located in the MF of a sentence.",
    "acronym": "MF",
    "label": "middle field"
  },
  {
    "sentence": "last cl .... \\] == \\[first: \\[verbal_modifi~r \\] l \\[v~rb  L rest \\[ verb_last_claus~ 11\\]j  V  first: \\[finite verb \\] 1 est : \\[nil \\]  \\[forefield \\] == \\[verbal modifier \\]  \\[MF and rest fields \\] ==  - \\[first: \\[verhe~ modifier \\] _7  L rest : \\[middJe field and rest fields J v  \\[nil \\]  v  f irst:\\[nonfini ..... b \\] 1 est : \\[after field \\]  \\[after field \\] == \\[niJ \\]  V  r first : \\[ verbal modifier 1  est : \\[after field \\]  \\[verbal modifier \\] == \\[complement \\]  V \\[adjunct \\] \"  The f irst def in i t ion  in (i0) descr ibes   the fact that a clau",
    "acronym": "MF",
    "label": "middle field"
  },
  {
    "sentence": "of the MF (middle), and the el-  ement of the initial field cannot be a relative  phrase (o~nore l  in \\[4\\]).",
    "acronym": "MF",
    "label": "middle field"
  },
  {
    "sentence": "of the  MF, with no restrictions for the initial  field (relative clauses and non-relative verb-final  clauses are subordinated to the noun and con-  junction, resp.)",
    "acronym": "MF",
    "label": "middle field"
  },
  {
    "sentence": "Orderings in the MF.",
    "acronym": "MF",
    "label": "middle field"
  },
  {
    "sentence": "5 The German Clause  Traditionally, the German main clause is de-  scribed using three topological fields; the ini-  tial and MFs are separated by the fi-  nite (auxiliary) verb, and the middle and the  5The modality O~ can be viewed as an abbreviation  of o~ O~,  composed of a mapping from a word to its ith  order domain and from that domain to all its elements.",
    "acronym": "MF",
    "label": "middle field"
  },
  {
    "sentence": "A Generalized  MF Algorithm for Variational Inference in  Exponential Families.",
    "acronym": "MF",
    "label": "Mean Field"
  },
  {
    "sentence": "2.1 MF Approximation Methods The simplest example of a variation method is the mean field method, originally introduced in statis- tical mechanics and later applied to unsupervised neural networks in (Hinton et al, 1995).",
    "acronym": "MF",
    "label": "Mean Field"
  },
  {
    "sentence": "3 Spin Model and MF Approximation We give a brief introduction to the spin model and the mean field approximation, which are well- studied subjects both in the statistical mechanics and the machine learning communities (Geman and Geman, 1984; Inoue and Carlucci, 2001; Mackay, 2003).",
    "acronym": "MF",
    "label": "Mean Field"
  },
  {
    "sentence": "Structured MF ?",
    "acronym": "MF",
    "label": "Mean Field"
  },
  {
    "sentence": "3.3 A MF Approximation This section proposes a more accurate way to ap- proximate ISBNs with mean field methods, which we will call the mean field approximation.",
    "acronym": "MF",
    "label": "Mean Field"
  },
  {
    "sentence": "MF ?",
    "acronym": "MF",
    "label": "Mean Field"
  },
  {
    "sentence": "Marginalization of hidden structure is also funda- mental to other work, and featured most prominently in generative Bayesian LVMs (Teh et al2006).",
    "acronym": "LVM",
    "label": "latent variable model"
  },
  {
    "sentence": "A LVM for geographic lexical variation.",
    "acronym": "LVM",
    "label": "latent variable model"
  },
  {
    "sentence": "Posterior regularization for structured LVMs.",
    "acronym": "LVM",
    "label": "latent variable model"
  },
  {
    "sentence": "In recent years, great research has shown the strength of LVMs for natural lan- guage processing (Blunsom et al, 2008).",
    "acronym": "LVM",
    "label": "latent variable model"
  },
  {
    "sentence": "A discrimi- native LVM for statistical machine trans- lation.",
    "acronym": "LVM",
    "label": "latent variable model"
  },
  {
    "sentence": "A LVM for geo- graphic lexical variation.",
    "acronym": "LVM",
    "label": "latent variable model"
  },
  {
    "sentence": "Finally, our focus on these methods is justified by their clear advantages over other classes of models: unlike token-based or LVMs, they are much simpler and require no parameter tuning.",
    "acronym": "LVM",
    "label": "latent variable method"
  },
  {
    "sentence": "However, we have to note that this accuracy of the proposed method was computed using the unlabeled data classified by the LVM.",
    "acronym": "LVM",
    "label": "latent variable method"
  },
  {
    "sentence": "Therefore, we computed the accuracy for 6586 instances using the LVM and obtained 80.76 %.",
    "acronym": "LVM",
    "label": "latent variable method"
  },
  {
    "sentence": "This comparison shows that our method is better than or at least comparable to the LVM.",
    "acronym": "LVM",
    "label": "latent variable method"
  }
]