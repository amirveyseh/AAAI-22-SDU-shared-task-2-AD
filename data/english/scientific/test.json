[{"sentence": "It has been sug- 878 Change test years BOW sLDA FWD SemTreeFWD CS 2008-2010 0.1015 0.0774 0.1079 0.1426 2011-2012 0.1663 0.1203 0.1664 0.1736 5 years 0.1274 0.0945 0.1313 0.1550 Information Technology 2008-2010 0.0580 0.0585 0.0701 0.0846 2011-2012 0.0894 0.0681 0.1076 0.1273 5 years 0.0705 0.0623 0.0851 0.1017 Telecommunication Services 2008-2010 0.1501 0.1615 0.1497 0.2409 2011-2012 0.2256 0.2084 0.2191 0.4009 5 years 0.1803 0.1803 0.1774 0.30", "acronym": "CS", "ID": "1"}, {"sentence": "ples 2008-2010 0.1015 0.0774 0.1079 0.1426 2011-2012 0.1663 0.1203 0.1664 0.1736 5 years 0.1274 0.0945 0.1313 0.1550 Information Technology 2008-2010 0.0580 0.0585 0.0701 0.0846 2011-2012 0.0894 0.0681 0.1076 0.1273 5 years 0.0705 0.0623 0.0851 0.1017 Telecommunication Services 2008-2010 0.1501 0.1615 0.1497 0.2409 2011-2012 0.2256 0.2084 0.2191 0.4009 5 years 0.1803 0.1803 0.1774 0.3049 Polarity CS 2008-2010 0.0359 0.0383 0.0956 0.1054 2011-2012 0.0938 0.0270 0.1131 0.1285 5 years 0.0590 0.0338 0.1026 0.1147 p-value >>0.1000 0.0918 0.0489 Information Technology 2008-2010 0.0551 0.0332 0.0697 0.0763 2011-2012 0.0591 0.0516 0.0764 0.0857 5 years 0.0567 0.0405 0.0723 0.0801 p-value 0.0626 0.0948 0.0103 Telecommunication Services 2008-2010 0.0402 0.0464 0.0821 0.0745 2011-2012 0", "acronym": "CS", "ID": "2"}, {"sentence": "Regarding coordination, the analysis reveals that different parsers show different biases with re- spect to CS.", "acronym": "CS", "ID": "3"}, {"sentence": "under-produces the same structures as the first- order model, and that both models have specific problems in dealing with CS, specifically coordination of NPs containing PPs.", "acronym": "CS", "ID": "4"}, {"sentence": "More generally, experiment 4 suggests that for the notoriously difficult problem of pars- ing CS, a hybrid approach that combines parse selection of n best analyses with pre-bracketed scope in the input results in a con- siderable reduction in error rate compared to each of these methods used in isolation.", "acronym": "CS", "ID": "5"}, {"sentence": "Another motivation for using t-trees is that we believe that local tree contexts in t-trees carry more information relevant for correct lexical choice, compared to linear contexts in the surface sentence shapes, mainly because of long-distance dependencies and CS.", "acronym": "CS", "ID": "6"}, {"sentence": "Yet we show that the second-order model under-produces the same structures as the first- order model, and that both models have specific problems in dealing with CS, specifically coordination of NPs containing PPs.", "acronym": "CS", "ID": "7"}, {"sentence": "From the con- stituent parse we extracted CS into a simplified data structure that captures each conjunction along with its conjuncts.", "acronym": "CS", "ID": "8"}, {"sentence": "c i e n t i s t s  : \"  NATURAL SCIENTISTS AND MTHEblA'CICIANS  181 CSs  1812 Coquter Systems Anal yszs  Applications engineer  Engineering analyst  Programer engineering and scientific  Systcms engineer  Cb@pUier analp t  Cornput ifig-sys tems analyst  Computer-systems planning  Syst ws ancflyst, data processing  Systems andryst, computer sys tems  Systems engineer- 189,739  1819 CSs, Not Elsewhere Classified  Systems engiheen electronic data proc  Systems analyst 6usiness elect~onic d  Computer application,engineer  ~igital-co*hputer programmer  Electronic data programmer  !", "acronym": "CS", "ID": "9"}, {"sentence": "In the International MultiConference of Engineers and CSs, volume 3, pages 2086?", "acronym": "CS", "ID": "10"}, {"sentence": "Ann E. Robinson is a CS in the  Artificial Intelligence Center at SR I  International.", "acronym": "CS", "ID": "11"}, {"sentence": "with AFIPS, OMB s t a t e d  t h a t  profess ional   or2anizgt isns  and o the r  i n t e r e s t ed  p a r t i e s  were provided an opportunity t o   comment on the  c l a s s i f i c a t i o n   me Manual i s e s t ruc tu red  he i r a r ch i ca l ly  and,.for example, proposes t he   fellowing categor ies  f o r  \"computer s c i e n t i s t s  : \"  NATURAL SCIENTISTS AND MTHEblA'CICIANS  181 CSs  1812 Coquter Systems Anal yszs  Applications engineer  Engineering analyst  Programer engineering and scientific  Systcms engineer  Cb@pUier analp t  Cornput ifig-sys tems analyst  Computer-systems planning  Syst ws ancflyst, data processing  Systems andryst, computer sys tems  Systems engineer- 189,739  1819 CSs, Not Elsewhere Classified  Systems engiheen elec", "acronym": "CS", "ID": "12"}, {"sentence": "Second, it is not clear that vari- ous grammatical categories and their values have the same interpretation in each language; for ex- ample, it is rather surprising that only the Roma- nian tagset explicitly mentions strong and weak pronominal forms, it is not clear whether negative pronouns in Romanian, Slovene, CS and Bul- garian are negative in the same sense of participat- ing in Negative Concord, it is not clear why Roma- nian has negative adverbs while, say, CS lacks them, etc.", "acronym": "CS", "ID": "13"}, {"sentence": "values have the same interpretation in each language; for ex- ample, it is rather surprising that only the Roma- nian tagset explicitly mentions strong and weak pronominal forms, it is not clear whether negative pronouns in Romanian, Slovene, CS and Bul- garian are negative in the same sense of participat- ing in Negative Concord, it is not clear why Roma- nian has negative adverbs while, say, CS lacks them, etc.", "acronym": "CS", "ID": "14"}, {"sentence": "Such correspondences are not exceptional, e.g., the at least three masculine gen- ders of Polish (Man?czak, 1956; Saloni, 1976) are mapped into the single masculine gender of many other languages, the dual and the plural numbers of some languages (Slovene, CS) are mapped to plural of other languages, etc.", "acronym": "CS", "ID": "15"}, {"sentence": "6 Future  D i rec t ions   The goal of my current research is to combine the  new alignment algorithm with a cognate identifica-  tion procedure, The alignment of cognates is possi-  8For example, stress regularly falls on the initial syllable  in CS and on the penultimate syllable in Polish, while in  Russian itcan fall anywhere in the word.", "acronym": "CS", "ID": "16"}, {"sentence": "For example, the Multext-East tagset for CS assumes the following parts of speech: noun, verb, adjective, pronoun, adverb, adposi- tion, conjunction, numeral, interjection, resid- ual, abbreviation and particle.", "acronym": "CS", "ID": "17"}, {"sentence": "One admirable standardization effort in the field of Slavic part of speech (POS) tagging has been the Multext-East project (Erjavec, 2001), one of whose aims was to construct mutually compati- ble tagsets for 8 European languages, including 4 Slavic languages (originally Bulgarian, CS and Slovene, later extended to Croatian); additionally, a Multext-East-style tagset for Russian was con- structed at the University of T?bingen (http: //www.sfb441.uni-tuebingen.de/c1/ tagset.html).", "acronym": "CS", "ID": "18"}, {"sentence": "CSs have used LS- COLIN from a comparability point of view, to  analyze the visual modality in LSF: they studied  torso (Segouat, 2006) and facial (Ch?telat-Pel?,", "acronym": "CS", "ID": "19"}, {"sentence": "universal s, the analysis  of  discourse and text, computar technology can bc of help tc the l i n g u i s t ,  and,  in many subfields of computer science automated lnngua~e processing, the  deslgn of human/machme i ~ t e r f ~ c e s ,  the structuring of data bases,  linguistics  has much to offer the ccnnputer scientist, vet up until how, relatively few  such cross contributions have been made CSs have been slow  to discgvei the v d u e  of Ilnguistirs to their w o r ~ ,  the tine has come for  linguists t o  take the initiitivc and to  train themselves (and their students)  to hake use of and contribute to the field of computer science,  Speci?lized traltning in the us& of t he  corrputer with-ln a particular  discipline is not new Students i n  mary soclal sciences nok", "acronym": "CS", "ID": "20"}, {"sentence": "CSs will appreciate the cognate problem of extracting information from the web, and the economic riches associated with state-of-the-art text mining technologies.", "acronym": "CS", "ID": "21"}, {"sentence": "CSs can be taught methods for automatic text processing, leading to projects on text mining and chatbots.", "acronym": "CS", "ID": "22"}, {"sentence": "This convention  somewhat simplifies the statement of the satisfaction  3CSs may have met L KR in another guise.", "acronym": "CS", "ID": "23"}, {"sentence": "Introduction Development of large-scale grammars for natural languages is an active area of research in HLT.", "acronym": "HLT", "ID": "24"}, {"sentence": "1st Internal Conference on  HLT research, 1-5, San Di- ego, CA.", "acronym": "HLT", "ID": "25"}, {"sentence": "Promising directions for future  application of HLT to  language tutors include incorporating continuous  speech recognition in a range of target languages  and continuing the development of dialog and  NLP-dnven animated graphics.", "acronym": "HLT", "ID": "26"}, {"sentence": "Consequently, phenomena in- herent to natural languages may severely hamper the performance of HLT when applied to small collections.", "acronym": "HLT", "ID": "27"}, {"sentence": "This paper describes a system that attempts to cope with semantic variability through the use of state of the art HLT.", "acronym": "HLT", "ID": "28"}, {"sentence": "1156   HLT: The 2010 Annual Conference of the North American Chapter of the ACL, pages 939?947, Los Angeles, California, June 2010.", "acronym": "HLT", "ID": "29"}, {"sentence": "In HLT 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Com- panion Volume, Short Papers, NAACL-Short ?", "acronym": "HLT", "ID": "30"}, {"sentence": "In Pro- ceedings of HLT: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 101?109, Boulder, Colorado, June.", "acronym": "HLT", "ID": "31"}, {"sentence": "In Pro- ceedings of HLT: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 172?180.", "acronym": "HLT", "ID": "32"}, {"sentence": "38   HLT: The 2010 Annual Conference of the North American Chapter of the ACL, pages 813?821, Los Angeles, California, June 2010.", "acronym": "HLT", "ID": "33"}, {"sentence": "In Proceed- ings of HLT: The 2009 Annual Conference of the North American Chap- ter of the Association for Computational Linguistics, NAACL ?", "acronym": "HLT", "ID": "34"}, {"sentence": "In Proceedings of the HLT Conference and Conference on Empirical Methods in Natural Language Processing, pages 779?786, Vancouver.", "acronym": "HLT", "ID": "35"}, {"sentence": "In James Allan, editor, Proceedings of the 1st International Conference on HLT Research.", "acronym": "HLT", "ID": "36"}, {"sentence": "of the HLT / Empirical Meth- ods in Natural Language Processing Conference, pages 411?418.", "acronym": "HLT", "ID": "37"}, {"sentence": "of the Second International Confer- ence of HLT Research, pages 138?145, March.", "acronym": "HLT", "ID": "38"}, {"sentence": "In Proceedings of the HLT Conference of the NAACL, Main Conference, pages 160?167, June.", "acronym": "HLT", "ID": "39"}, {"sentence": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on HLT (NAACL), pages 48?54, Morristown, NJ.", "acronym": "HLT", "ID": "40"}, {"sentence": "For com- puting the counts of positive and negative words (Feature 15 and 16) we used the GI database (Stone et al.,", "acronym": "GI", "ID": "41"}, {"sentence": "An em- pirical evaluation was conducted on 3596 words ex- tracted from GI (Stone et al, 1966).", "acronym": "GI", "ID": "42"}, {"sentence": "The labeled dataset used as a gold standard is GI lexicon (Stone et al, 1966) as in the work by Turney and Littman (2003).", "acronym": "GI", "ID": "43"}, {"sentence": "Largely missing from popular lexical resources such as WordNet and the GI (Stone et al, 1966) is stylistic information; there are, for instance, no resources which provide com- prehensive information about the formality level of words, which relates to the appropriateness of a word in a given context.", "acronym": "GI", "ID": "44"}, {"sentence": "Operative GI  and Radiology reports contain similar proportions  of historical conditions (9% and 6%).", "acronym": "GI", "ID": "45"}, {"sentence": "Echocardio- grams appear to be most similar to Radiology re- ports and Operative GI reports, which  may be supported by the fact that these reports are  used to document findings from tests conducted  during the current visit.", "acronym": "GI", "ID": "46"}, {"sentence": "Examples of report sections include Review of Sys- tems (Emergency Department), Findings (Opera- tive GI and Radiology) and  Discharge Diagnosis (Emergency Department and  Discharge Summary).", "acronym": "GI", "ID": "47"}, {"sentence": "One is that some outcomes do not have any cue word: (5) GI symptoms and headaches have been reported with both montelukast and zafirlukast.", "acronym": "GI", "ID": "48"}, {"sentence": "4 Methods  4.1 Dataset Generation  We randomly selected seven reports from each of  six genres of clinical reports dictated at the Univer- sity of Pittsburgh Medical Center during 2007  These included Discharge Summaries, Surgical  Pathology, Radiology, Echocardiograms, Opera- tive GI, and Emergency Department  reports.", "acronym": "GI", "ID": "49"}, {"sentence": "Operative GI and  Radiology reports showed the lowest prevalence of  both temporal expressions and trigger terms.", "acronym": "GI", "ID": "50"}, {"sentence": "252  4250  A  +  I  B  +  I  C  +  I  D  +  I  3000 3000 3000 3000  Figure 1: The setup o f thematr ix fo r the  first SVD.", "acronym": "SVD", "ID": "51"}, {"sentence": "The mathematical basis for this transformation is SVD5; for the details of the matrix transformations, we refer the reader to the discussion of Turney and Littman (2003).", "acronym": "SVD", "ID": "52"}, {"sentence": "e wonder assume feel say mean bet  angeles francisco sox rouge kong diego zone vegas inning layer  Oil   must  through in at over into with from for by across  we you i he she nobody who it everybody there they  might would could cannot will should can may does helps  500 features  500 features  500 features  500 features  A  B  C  D  22,771 words  Figure 2: The setup of the matrix for the second SVD.", "acronym": "SVD", "ID": "53"}, {"sentence": "252  4250  A  +  I  B  +  I  C  +  I  D  +  I  3000 3000 3000 3000  Figure 1: The setup o f thematr ix fo r the  first SVDn.", "acronym": "SVD", "ID": "54"}, {"sentence": "The mathematical basis for this transformation is SVDn5; for the details of the matrix transformations, we refer the reader to the discussion of Turney and Littman (2003).", "acronym": "SVD", "ID": "55"}, {"sentence": "e wonder assume feel say mean bet  angeles francisco sox rouge kong diego zone vegas inning layer  Oil   must  through in at over into with from for by across  we you i he she nobody who it everybody there they  might would could cannot will should can may does helps  500 features  500 features  500 features  500 features  A  B  C  D  22,771 words  Figure 2: The setup of the matrix for the second SVDn.", "acronym": "SVD", "ID": "56"}, {"sentence": "This result indicates that it is not the words selected for the calculation of the proximities that matter, but the semantic relations in the spaces extracted from the word co-occurrences by the SVD.", "acronym": "SVD", "ID": "57"}, {"sentence": "In or- der to reduce the noise and the data sparsity, we apply the SVD algo- rithm by reducing the original vector space into 300 dimensions.", "acronym": "SVD", "ID": "58"}, {"sentence": "In general, the algo- rithms that incorporate both statistical feature se- lection and SVD lead to the best results, except for the Hungary data when no stoplist is used.", "acronym": "SVD", "ID": "59"}, {"sentence": "The system is based on a com- bination of k-Nearest Neighbor classifiers, with each classifier learning from a distinct set of features: local features (syntactic, col- locations features), topical features (bag-of- words, domain information) and latent fea- tures learned from a reduced space using SVD.", "acronym": "SVD", "ID": "60"}, {"sentence": "This frequency table undergoes a SVD that extracts the most important orthogonal dimensions, and, consequently, discards the small sources of variability in term usage.", "acronym": "SVD", "ID": "61"}, {"sentence": "1990) is a technique for implicitly capturing the semantic properties of texts, based on the use of SVD to produce a rank- reduced approximation of an original matrix of word and document frequencies.", "acronym": "SVD", "ID": "62"}, {"sentence": "J1 J2 J3 %agr \u000b %agr \u000b %agr \u000b J2 0.88 0.59 J3 0.98 0.91 0.90 0.67 GS 0.97 0.89 0.90 0.65 0.98 0.90 Table 2: AG for the unary/binary parameter: inter-judge (J1, J2, J3), and with GS J1 J2 J3 %agr \u000b %agr \u000b %agr \u000b J2 0.83 0.74 J3 0.88 0.80 0.80 0.68 GS 0.93 0.89 0.83 0.74 0.92 0.87 Table 3: AG for the basic/event/object pa- rameter: inter-judge (J1, J2, J3), and with GS As can be seen, the agreement among judges is remarkably high for a lexical semantics task: All but one values of the kappa", "acronym": "AG", "ID": "63"}, {"sentence": "AG: subject and object agreement in per- son and number (and to some extent gender) marked in the clitic cluster, agreement between nouns and adnominal modifiers in case, number and gender ?", "acronym": "AG", "ID": "64"}, {"sentence": "16 Sample Correct Incorrect N/A AG CLIP 519 sets 65.8% 31.7% 2.5% 76.1% CLIP?", "acronym": "AG", "ID": "65"}, {"sentence": "Human AG: After obtaining the human annotation results, we first study human consen- sus on the ordering task.", "acronym": "AG", "ID": "66"}, {"sentence": "J1 J2 J3 %agr \u000b %agr \u000b %agr \u000b J2 0.88 0.59 J3 0.98 0.91 0.90 0.67 GS 0.97 0.89 0.90 0.65 0.98 0.90 Table 2: AG for the unary/binary parameter: inter-judge (J1, J2, J3), and with GS J1 J2 J3 %agr \u000b %agr \u000b %agr \u000b J2 0.83 0.74 J3 0.88 0.80 0.80 0.68 GS 0.93 0.89 0.83 0.74 0.92 0.87 Table 3: AG for the basic/event/object pa- rameter: inter-judge (J1, J2, J3), and with GS As can be seen, the agreement among judges is remarkably high for a lexical semantics task: All but one values of the kappa statistics are above 0.6 (+/-0.13 for a 95% confidence interval).", "acronym": "AG", "ID": "67"}, {"sentence": "Measures: Given the human generated gold stan- dard of partial constraints, we use the follow- ing measures to evaluate the automatically gen- AgreedBy Cluster Constraint Order Constraint 1 37.14% 89.22% 2 46.95% 10.78% 3 15.92% 0.00% Table 7: Human AG on Ordering erated full ordering of aspects: (1) Cluster Pre- cision (prc): for all the aspect pairs placed in the same cluster by human, we calculate the per- centage of them that are also placed together in the system output. (", "acronym": "AG", "ID": "68"}, {"sentence": "c?2009 Association for Computational Linguistics Automatic AG Construction from Human-Human Dialogs  using Clustering Method    Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, Gary Geunbae Lee  Department of Computer Science and Engineering  Pohang University of Science and Technology  Pohang, South Korea  {lcj80,hugman,getta,gblee}@postech.ac.kr         Abstract  Various knowledge sources are used for spo- ken dialog systems such as task model, do-", "acronym": "AG", "ID": "69"}, {"sentence": "3 AG  In this section, we begin with a brief overview of  EBDM framework and agenda graph.", "acronym": "AG", "ID": "70"}, {"sentence": "4 AG In this paper, agenda graph G is simply a way of encoding the domain-specific dialog control to com- plete the task.", "acronym": "AG", "ID": "71"}, {"sentence": "HYPms are only extracted for the events  and their subjects and objects, not for the local  context words.", "acronym": "HYP", "ID": "72"}, {"sentence": "Relation Sample size Validation HYPmy of 419 synsets 44,1% Member of 379 synsets 24,3% Part of 290 synsets 24,8% Table 4: Automatic validation of triples idence on each combination of terms a ?", "acronym": "HYP", "ID": "73"}, {"sentence": "3.3 WordNet HYPms  Events with the same hypernyms may have simi- lar durations.", "acronym": "HYP", "ID": "74"}, {"sentence": "HYPm of Part of Member of Term-based triples 62,591 2,805 5,929 1st Mapped 27,750 1,460 3,962 Same synset 233 5 12 Already present 3,970 40 167 Semi-mapped triples 7,952 262 357 2nd Mapped 88 1 0 Could be inferred 50 0 0 Already present 13 0 0 Synset-based triples 23,572 1,416 3,783 Table 2: Results of triples mapping A small sample of this problem can be observed in Figure 1.", "acronym": "HYP", "ID": "75"}, {"sentence": "Then, for each triple (A R B), the patterns were used to search for ev- 8HYPmy patterns included: [hypo] e?", "acronym": "HYP", "ID": "76"}, {"sentence": "Then, UC HYP the actual information needs of the user by consulting the user model and applying goal analysis.", "acronym": "HYP", "ID": "77"}, {"sentence": "We HYP that coreferencing entities in the same  genre domain can be considered to be harder in terms  of achieving high precision because the consistency of  the contents between documents in the same genre  domain makes it significantly harder to create a unique  model for each entity to aid the task of distinguishing  one entity from another.", "acronym": "HYP", "ID": "78"}, {"sentence": "For each entity mention e in the evaluation set, we  first locate the truth chain TC that contains that mention  (it can be in only one truth chain) and the system?s  HYP chain HC that contains it (again, there can  Chung Heong Gooi and James Allan  Center for Intelligent Information Retrieval  Department of Computer Science  University of Massachusetts  Amherst, MA 01003  {cgooi,allan}@cs.umass.edu  be only one hypothesis chain).", "acronym": "HYP", "ID": "79"}, {"sentence": "Evaluation then  proceeds by comparing the true chains to the system?s  HYP chains.", "acronym": "HYP", "ID": "80"}, {"sentence": "We HYP that  using a highly precise set of patterns along with  precise lexicon should enable a promising IE  performance.", "acronym": "HYP", "ID": "81"}, {"sentence": "For each entity mention e in the evaluation set, we  first locate the truth chain TC that contains that mention  (it can be in only one truth chain) and the system?s  HYP chain HC that contains it (again,", "acronym": "HYP", "ID": "82"}, {"sentence": "True validation of the Matrix qua HYP linguistic universals re- quires many more such case studies, but this first test is promising.", "acronym": "HYP", "ID": "83"}, {"sentence": "To recognize that HYP 1 is entailed by the text, a human reader must recognize that ?", "acronym": "HYP", "ID": "84"}, {"sentence": "2 Classification and HYP As mentioned above, the semantic classification of adjectives is not settled in theoretical linguistics.", "acronym": "HYP", "ID": "85"}, {"sentence": "In the example shown in figure 1, this means recognizing that the Text entails HYP 1, while HYP 2 con- tradicts the Text.", "acronym": "HYP", "ID": "86"}, {"sentence": "Test Set HYP WSJ BROWN Greater Lesser F1 UAS LAS F1 UAS LAS phrase+deps phrase .042 .029 .018 .140 .022 .009 phrase+deps deps ?", "acronym": "HYP", "ID": "87"}, {"sentence": "To recognize that HYP 2 contradicts the Text, similar steps are required, together with the inference that because the stated purchase pr", "acronym": "HYP", "ID": "88"}, {"sentence": "We  would like to know, therefore, whether the pat-  tern of pronoun references that we observe for  a given referent is the result of our supposed  \"HYP about pronoun reference\" - that is,  the pronoun reference strategy we have provi-  sionally adopted in order to gather statistics -  or whether the result of some other unidentified  process.", "acronym": "HYP", "ID": "89"}, {"sentence": "The intuition behind this HYP is that if a certain suffix forms basic adjectives, they will behave like ordinary basic adjectives; similarly, if a derived ad- jective has undergone semantic change and as a re- sult has shifted class, it will also behav", "acronym": "HYP", "ID": "90"}, {"sentence": "One of the purposes of the paper is to test whether this HYP is right.", "acronym": "HYP", "ID": "91"}, {"sentence": "1 Introduction The main HYP underlying the tasks in Lex- ical Acquisition is that it is possible to infer lexi- cal properties from distributional evidence, taken as a generalisation of a word?s linguistic behaviour in corpora.", "acronym": "HYP", "ID": "92"}, {"sentence": "The likelihood ratio is adapted  from Dunning (1993, page 66) and uses the raw  frequencies of each pronoun class in the cor-  pus as the null HYP, Pr(gc0i) as well as  Pr(ref E gci) from equation 9.", "acronym": "HYP", "ID": "93"}, {"sentence": "Our HYP, which will be tested on Sec- tion 4.3, is that syntax is more reliable than mor- phology as a basis for semantic classification.", "acronym": "HYP", "ID": "94"}, {"sentence": "Rather, a more important MOT for a circumscribed domain is the need for clearly defined knowledge sources.", "acronym": "MOT", "ID": "95"}, {"sentence": "The reasons underlying these results are that the newspaper writing is naturally better planned than speech and that speech transcriptions are affected by the several problems described before (and the original MOT for the work), hence the idea of using them as background information.", "acronym": "MOT", "ID": "96"}, {"sentence": "The MOT for generalized gap scores arises  from the fact that in diachronic phonology not only  individual segments but also entire morphemes and  syllables are sometimes deleted.", "acronym": "MOT", "ID": "97"}, {"sentence": "With the MOT of parsing efficiency, much research has been recently devoted to the design of efficient algorithms for rank reduction, in cases in which this can be carried out at no extra increase in the fan-out. (", "acronym": "MOT", "ID": "98"}, {"sentence": "Their primary MOT is to classify  phonological oppositions rather than to reflect the  phonetic haracteristics of sounds.", "acronym": "MOT", "ID": "99"}, {"sentence": "The TD  admits that certain instructions may be incom-  patible with the definition of coreference but no  reason is given for these incompatibilities and  no intuitive MOT for the relation IDENT is  offered.", "acronym": "MOT", "ID": "100"}, {"sentence": "Rather, a more important MOTn for a circumscribed domain is the need for clearly defined knowledge sources.", "acronym": "MOT", "ID": "101"}, {"sentence": "The reasons underlying these results are that the newspaper writing is naturally better planned than speech and that speech transcriptions are affected by the several problems described before (and the original MOTn for the work), hence the idea of using them as background information.", "acronym": "MOT", "ID": "102"}, {"sentence": "The MOTn for generalized gap scores arises  from the fact that in diachronic phonology not only  individual segments but also entire morphemes and  syllables are sometimes deleted.", "acronym": "MOT", "ID": "103"}, {"sentence": "With the MOTn of parsing efficiency, much research has been recently devoted to the design of efficient algorithms for rank reduction, in cases in which this can be carried out at no extra increase in the fan-out. (", "acronym": "MOT", "ID": "104"}, {"sentence": "Their primary MOTn is to classify  phonological oppositions rather than to reflect the  phonetic haracteristics of sounds.", "acronym": "MOT", "ID": "105"}, {"sentence": "The TD  admits that certain instructions may be incom-  patible with the definition of coreference but no  reason is given for these incompatibilities and  no intuitive MOTn for the relation IDENT is  offered.", "acronym": "MOT", "ID": "106"}, {"sentence": "2 MOT PP attachment disambiguation has often been studied as a benchmark test for empirical meth- ods in natural language processing.", "acronym": "MOT", "ID": "107"}, {"sentence": "3.3 Experiments 3.3.1 Experiment 1 MOT and Noise Model For our first exper- iment, we assume that the probability of arriving at some word w?", "acronym": "MOT", "ID": "108"}, {"sentence": "MOT  We will begin by introducing structured prediction with various NLP examples.", "acronym": "MOT", "ID": "109"}, {"sentence": "1 Introduction and MOT Automated verbal irony detection is a challenging problem.", "acronym": "MOT", "ID": "110"}, {"sentence": "2 MOT  A trigram language model predicts the next word  based only on two preceding words, blindly dis- carding any other relevant word that may lie three  or more positions to the left.", "acronym": "MOT", "ID": "111"}, {"sentence": "0.007246 0.009528 3.3.2 Experiment 2 MOT and Noise Model For our second ex- periment, we hypothesize that there is an inverse re- lationship between unobserved word frequency and random walk path probability.", "acronym": "MOT", "ID": "112"}, {"sentence": "Sentence) (per Sentence) NAIST Text Corpus Training 1,751 24,283 664,898 (27.4) 68,602 (2.83) Development 480 4,833 136,585 (28.3) 13,852 (2.87) Test 696 9,284 255,624 (27.5) 26,309 (2.83) Chat Dialog Corpus Training 184 6,960 61,872 (8.9) 7,470 (1.07) Test 101 4,056 38,099 (9.4) 5,333 (1.31) Table 1: Sizes of Corpora Zero- Zero- Exophora Case Corpus # of Arguments Dep Intra Inter exo1 exo2 exog NOM NAIST 68,598 54.5% 17.3% 11.4% 2.0% 0.0% 14.7% Dialogue 7,467 31.8% 7.4% 12.6% 23.9% 5.6% 18.8% Accusative NAIST 27,986 89.2% 6.9% 3.4% 0.0% 0.0% 0.4% Dialogue 1,901 46.6% 12.8% 27.5% 0.8% 0.1% 12.2% Datative NAIST 6,893 84.7% 10.2% 4.3% 0.0% 0.0% 0.8% Dialogue 2,089 37.6% 7.8% 15.0% 2.5% 1.1% 36.1% Table 2: Distribution of Arguments in Training Corpora Table 1 shows the statistics of t", "acronym": "NOM", "ID": "113"}, {"sentence": "Special Noun Phrases Candidate Argumentsin Past  Sentences Candidate Argumentsin Current Sentence Candidate Arguments SelectorNOMModel SelectorAccusativeModel SelectorDativeModel exo1exophoric(first person) zero-anaphoric(inter-sentential) Phrase 2 NULLno argument Figure 2: Structure of Argument Identification and Classification assigned to exo1 and exo2.", "acronym": "NOM", "ID": "114"}, {"sentence": "lace/temporal/modal  SYNTB5 case/preposition case/preposition  SYNTB6 case/preposition case/preposition  SYNTB7 case/preposition case/preposition  SYNTB8 case/preposition case/preposition  SYNTB9 Gender Separable Prefix  SYNTB 10 Alternate Gender Reilexivity  SYNTC1 .. Inlpersomd Subject only  SYNTC2 Usage of Participle  SYNTC3 .o x  SIJItJECT x x  SOURCE x x  FREQ x x  FREQS x x  STYLE x x  XREF NOM Singular lmqnitive without ~-(e)n\",  without separat}lc prefix  XREF (cont'd) Plural Furm lml}erfect, Past Participle  'I'RI{NN x x  I)ATUM x x  141,F2,F3,F4,1;5 . . . .", "acronym": "NOM", "ID": "115"}, {"sentence": "The first set (ANERGaz) pro- posed by (Benajiba and Rosso, 2008), which 80 Feature Feature Values Aspect Verb aspect: Command, Imperfective, Perfective, Not applicable Case Grammatical case: NOM, Accusative, Genitive, Not applicable, Undefined Gender Nominal Gender: Feminine, Masculine, Not applicable Mood Grammatical mood: Indicative, Jussive, Subjunctive, Not applicable, Undefined Number Grammatical number: Singular, Plural, Dual, Not applicable, Undefined Person Person Information: 1st, 2nd, 3rd, Not applicable State Grammatical state: Indefinite, Definite, Construct/Poss/Id", "acronym": "NOM", "ID": "116"}, {"sentence": "each part of speech:  Different for each part of speech:  Different for each part of speech:  Different for each part of speech:  Obligatory Coml}lcments  Subject Area  Source: Userid of who coded; origin of entry  Frequency Count  Source of Frequency Cmmt  Level of Style  Stem for 1st Entry:  Secondary Stems:  11yphenation  Date of last Update  5 additional, not yet used fields  WORT Infinitive NOM  CONT # #  CAT N VERB  MORPII1 Declension code Sets of suffixes  MORPtI2 Alternative declension Syntax dud scope of  stems  SYNTA 1 .. Valency  SYNTA2-5 Preposition Preposition  SYNTB 1-4 place/temporal/modal place/temporal/modal  SYNTB5 case/preposition case/preposition  SYNTB6 case/preposition case/preposition  SYNTB7 case/preposition case/preposition  SYNTB8 case/preposition case/pre", "acronym": "NOM", "ID": "117"}, {"sentence": "For each term candidate, we gener- ated a canonical form (NOM, singular), a  morphologically normalised form (list of normal- ised words comprising the term candidate) and a  list of nested term candidates (see Table 3 for  examples).", "acronym": "NOM", "ID": "118"}, {"sentence": "refers to the co- occurrence pattern between a verb and a noun 33 [Sentence pattern] <word1> ga <word2> wo taberu (eat) [Sense relation] agent object [Case particle] ga (NOM) wo (accusative) [Sense identifier] 30f6b0 (human);30f6bf (animal) 30f6bf(animal);30f6ca(plants); 30f6e5(parts of plants); 3f9639(food and drink); 3f963a(feed) Figure 1: An example of a verb ?", "acronym": "NOM", "ID": "119"}, {"sentence": "Sense id Pattern Synonyms 1 kare(he) ga(NOM) soba(noodles) wo(accusative) kuu (eat) 2 kare (he) ga(NOM) fukugyo(a part-time job) de(accusative) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense Verb Classes Id Sense Verb Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratu", "acronym": "NOM", "ID": "120"}, {"sentence": "Sense id Pattern Synonyms 1 kare(he) ga(NOM) soba(noodles) wo(accusative) kuu (eat) 2 kare (he) ga(NOM) fukugyo(a part-time job) de(accusative) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense Verb Classes Id Sense Verb Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4 ask {negau, tanomu} 14 accept {uketor", "acronym": "NOM", "ID": "121"}, {"sentence": "At the end of the ATR proc- ess, terms are converted into their canonical form  (singular, NOM case), which is not neces- sarily identical to the normalised form (the se- quence of the corresponding singular words in  singular, NOM case).", "acronym": "NOM", "ID": "122"}, {"sentence": "brown, edu  Abst ract   This paper presents an algorithm for identi-  fying proNOM anaphora and two experi-  ments based upon this algorithm.", "acronym": "NOM", "ID": "123"}, {"sentence": "The idea is similar to that used in  the centering approach (Brennan et al, 1987)  where a continued topic is the highest-ranked  candidate for proNOMization.", "acronym": "NOM", "ID": "124"}, {"sentence": "8 Conc lus t ion  and  Future  Research   We have presented a statistical method for  proNOM anaphora that achieves an accuracy  of 84.2%.", "acronym": "NOM", "ID": "125"}, {"sentence": "Two broad approaches for the iden- tification of story characters were followed: (i) named entity recognition, and (ii) identification of character NOMs, e.g., ?", "acronym": "NOM", "ID": "126"}, {"sentence": "The more fre-  quently an entity is repeated, the more likely it  is to be the topic of the story and thus to be  a candidate for proNOMization.", "acronym": "NOM", "ID": "127"}, {"sentence": "The asymmetry of these  PRs over sets should be represented in the  semantic representation, i  order to account for the  difference in truth conditions.", "acronym": "PR", "ID": "128"}, {"sentence": "However, given that Urdu employs produc- tive syntactic complex predicate formation for much of its verbal PR, the verb lexicon for Urdu will be smaller than its English counterpart.", "acronym": "PR", "ID": "129"}, {"sentence": "b. anjum nE dI saddaf kO [ciTTHI likHnE] c. anjum nE [ciTTHI likHnE] saddaf kO dI The manipulation of PRal structures in the lexicon via lexical rules (as is done for the English passive, for example), is therefore inadequate for complex PR.", "acronym": "PR", "ID": "130"}, {"sentence": "Examples are correlative clauses (these are an old Indo-European feature which most modern European languages have lost), extensive use of complex PR, and rampant pro-drop.", "acronym": "PR", "ID": "131"}, {"sentence": "in the sense that the labels of several of the elementary PRs (eps) are not related to any argument position of any other ep.", "acronym": "PR", "ID": "132"}, {"sentence": "Nev-  ertheless it can still give us guidance on which  candidates are more PR than others.", "acronym": "PR", "ID": "133"}, {"sentence": "In  other words, the nearer the end of the story a  pronoun occurs, the more PR it is that  its referent has been mentioned several times.", "acronym": "PR", "ID": "134"}, {"sentence": "2.3 Decoding Alignments are normally predicted using the Viterbi algorithm (which selects the single most PR path through the HMM?s lattice).", "acronym": "PR", "ID": "135"}, {"sentence": "Q(U k ) shows comparison of the actual val- ues of internal or external edges with its re- spective expectation value under the assump- tion of equally PR links and given data sizes.", "acronym": "PR", "ID": "136"}, {"sentence": "This is almost cer- tainly a misleading figure, since those two words do not form a plausible verb phrase; it is much more PR that the very strong, in fact id- iomatic, correlation ?", "acronym": "PR", "ID": "137"}, {"sentence": "It seems PR that the ef- fect of the 10K training corpus can be greatly  augmented by adding sentence pairs that have  been aligned from multiple translations using  the techniques described in, e.g., Barzilay &  McKeown (2001) and Pang et al (2003).", "acronym": "PR", "ID": "138"}, {"sentence": "is reset, and the same PR is repeated for the rest of the story.", "acronym": "PR", "ID": "139"}, {"sentence": "We  would like to know, therefore, whether the pat-  tern of pronoun references that we observe for  a given referent is the result of our supposed  \"hypothesis about pronoun reference\" - that is,  the pronoun reference strategy we have provi-  sionally adopted in order to gather statistics -  or whether the result of some other unidentified  PR.", "acronym": "PR", "ID": "140"}, {"sentence": "In the testing phase, we used a different subset with 80 adjectives as Gold Standard against which we could compare the clustering results (see Section 3.2 for details on the manual annotation PR).", "acronym": "PR", "ID": "141"}, {"sentence": "In order to address this distant association issue, we examined the collapsed-ccPRed-dependencies output besides the basic-dependenciesout- put of the Stanford CoreNLP dependency engine (de Marneffe and Manning, 2012).", "acronym": "PR", "ID": "142"}, {"sentence": "3.1 Linguistic PrePRing The first step is linguistic pre-PRing of the stories.", "acronym": "PR", "ID": "143"}, {"sentence": "In most cases this meant four iterations for normal EM training and two iterations using PR.", "acronym": "PR", "ID": "144"}, {"sentence": "We see in Figure 10 that for both domains, the models trained using PR perform better than the baseline model trained using EM.", "acronym": "PR", "ID": "145"}, {"sentence": "The PR?trained models still performed better, but the differences get smaller after doing the symmetrization.", "acronym": "PR", "ID": "146"}, {"sentence": "4.3 Rare vs. Common Words One of the main benefits of using the PR constraints described is an alleviation of the garbage collector effect (Brown et al 1993a).", "acronym": "PR", "ID": "147"}, {"sentence": "Within Acquilex IP Project, a unification framework  based on typed feature structures \\[4\\] was ddveloped, the  LKB (Lexical Knowledge Base), in order to represent  conceptual units corresponding to lexieal senses, lexical  and PRs, multilingual rclalionships, elc.", "acronym": "PR", "ID": "148"}, {"sentence": "Rule Coverage Marcu et al (2006) showed that many useful PRs cannot be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al, 2006).", "acronym": "PR", "ID": "149"}, {"sentence": "In our experiments with the publicly available SVM system we used all except paraPRs extracted from bilingual corpora (Cohn and Lap- ata, 2008).", "acronym": "PR", "ID": "150"}, {"sentence": "Hierarchical rules were extracted from a subset which has about 35M/41M words5, and the rest of the training data were used to extract PRs as in (Och, 2003; Chiang, 2005).", "acronym": "PR", "ID": "151"}, {"sentence": "The original SHOGUN design integrated several different approaches b y combining different knowledge sources, such as syntax, semantics, PRs, and domain knowledge, a t run-time .", "acronym": "PR", "ID": "152"}, {"sentence": "For the example, after Skolemizing the existential quantifiers, this contains the ground atoms: {man(A), agent(B,A), drive(B)} KB: The knowledge base is a set of lexical and PRs generated from distributional semantics, along with a similarity score for each rule (section 2.6).", "acronym": "PR", "ID": "153"}, {"sentence": "We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the PRonoun and the PRoposed  antecedent, gender/number/animaticity of the  PRoposed antecedent, governing head informa-  tion and noun phrase repetition.", "acronym": "PR", "ID": "154"}, {"sentence": "A Statistical ApPRoach to Anaphora Resolution  Niyu  Ge, John  Hale and Eugene Charn iak   Dept.", "acronym": "PR", "ID": "155"}, {"sentence": "brown, edu  Abst ract   This paper PResents an algorithm for identi-  fying PRonominal anaphora and two experi-  ments based upon this algorithm.", "acronym": "PR", "ID": "156"}, {"sentence": "We combine  them into a single PRobability that enables us  to identify the referent.", "acronym": "PR", "ID": "157"}, {"sentence": "Annotators were tasked with evaluating three types of output from our Q/A system: (1) the ranked list of passages retrieved by our system?s PR module, (2) the list of passages identified as being CE by the scenario, and (3) the set of answers marked as being CE by the scenario (AnsSet3).", "acronym": "PR", "ID": "158"}, {"sentence": "c?2013 Association for Computational Linguistics The Answer is at your Fingertips: Improving PR for Web Question Answering with Search Behavior Data Mikhail Ageev?", "acronym": "PR", "ID": "159"}, {"sentence": "Each document is split by sentences, and for each sentence a QA-SYS PR Score (TextScore) is computed as a linear combination of term frequency score, proximity score, and term coverage score.", "acronym": "PR", "ID": "160"}, {"sentence": "2.1 PR  The first step is to find passages likely to contain the  answer to the query.", "acronym": "PR", "ID": "161"}, {"sentence": "We believe KBA TREC 10 (313) TREC 11 (453) + - + - SA + 185 43 254 58 - 24 61 41 100 Table 2: PR Analysis that this is because of compounding errors that occurred during the multiple combination process.", "acronym": "PR", "ID": "162"}, {"sentence": "Using just the word token, positive preci- sion is slightly higher than for the 10-feature clas- sifier, but PRl is 11.6% lower.", "acronym": "PR", "ID": "163"}, {"sentence": "Figure 1 plots the positive precision of the two methods against their PRl, and figure 2 shows negative precision against negative recall.", "acronym": "PR", "ID": "164"}, {"sentence": "Add the prior polarity, and PRl improves, but at the expense of precision, which is 12.6% lower than for the 10-feature classifier.", "acronym": "PR", "ID": "165"}, {"sentence": "In Figure 1, this leads to a positive precision of 0.72 (and PRl 0.49), which does not improve much by adopting a larger t+, unless one is willing to set t+ at almost 1 at the price of very low posi- tive recall.", "acronym": "PR", "ID": "166"}, {"sentence": "is adopted, where rateTP   is the true positive rate (also called PRl  or sensitivity) and rateTN  is the true negative rate  (also called negative recall or specificity) (Kubat  and Matwin, 1997).", "acronym": "PR", "ID": "167"}, {"sentence": "988 1 Cohesion 5 Relation 9 NodeSpec 2 CohesionTxt 6 RelationTxt 10 NodeSpecTxt 3 CohesionMod 7 RelationMod 11 NodeSpecMod 4 CohesionTxtMod 8 RelationTxtMod 12 NodeSpecTxtMod Table 2: Metric types in Figures 1-3 Figure 1: F Score for Positive Ratings All metrics have a bias towards positive ratings with attendant high PRl values and im- proved f-score for positive polarity assignments.", "acronym": "PR", "ID": "168"}, {"sentence": "Longest matching is the most popular approach to Thai WS (Pooworawan, 1986).", "acronym": "WS", "ID": "169"}, {"sentence": "6 Conclusion This paper proposes a two-step approach to Thai WS.", "acronym": "WS", "ID": "170"}, {"sentence": "Studying the characteristics of Thai language, we find that WS possesses ambiguities at both character and syllable levels.", "acronym": "WS", "ID": "171"}, {"sentence": "A recognized effective approach to WS is Longest Matching, a method based on dictionary.", "acronym": "WS", "ID": "172"}, {"sentence": "The experimental results show the syllable segmentation accuracy of more than 96.65% and the overall WS accuracy of 97%.", "acronym": "WS", "ID": "173"}, {"sentence": "In the 80-sentence corpus under consideration, the  sentence structure is complex and stylized; with an  average of 20 WS.", "acronym": "WS", "ID": "174"}, {"sentence": "5 10 15 10 20 30 40 50 average sentence length in words numbe r of ali gnmen ts per  senten ce Figure 1: Linear relation between the average number of WS and number of alignments per sentence We use the LL matrix as the similarity matrix for languages including all 6, 660 alignments.", "acronym": "WS", "ID": "175"}, {"sentence": "# of web pages 12,938,606 # of unique bloggers 60,658 average # of pages/blogger 213.3 # of pages with comments 6,421,577 # of comments 50,560,024 average # of comment/page 7.873 # of words 5,600,597,095 # of all sentences 354,288,529 # of WS (average) 15 # of characters per sentence (average) 77 taken from Google (response to one simple query: ?", "acronym": "WS", "ID": "176"}, {"sentence": "The average sentence length for Serbian is about 8.5 WS, and for English about 9.5.", "acronym": "WS", "ID": "177"}, {"sentence": "This consists of 518,080 words (ap- proximately 20 WS, on average) of text annotated with a detailed semantic and syntac- tic tagset.", "acronym": "WS", "ID": "178"}, {"sentence": "characters per word 5.22 WS 21.17 words per text 8,476 simple words 75.52% sentences per text 400.34 passive voice 15.11% total sentences 13,091 simplified sentences 16,71% Table 5: Statistics from the balanced text sample Figure 2: Clauses per sentence in the sample 4.2 Simplification analysis We manually analysed and annotated all sentences  in  our  samples.", "acronym": "WS", "ID": "179"}, {"sentence": "Unsupervised Large Vocabulary WS Disambiguation with Graph-based Al- gorithms for Sequence Data Labeling, In Proc.", "acronym": "WS", "ID": "180"}, {"sentence": "2.3 WS Determination The shared task of CoNLL-2008 for word sense disambiguation task is to determine the sense of an output predicate.", "acronym": "WS", "ID": "181"}, {"sentence": "c?2013 Association for Computational Linguistics DALE: A WS Disambiguation System for Biomedical Documents Trained using Automatically Labeled Examples Judita Preiss and Mark Stevenson Department of Computer Science, University of Sheffield Regent Court, 211 Portobello Sheffield S1 4DP, United Kingdom j.preiss,m.stevenson@dcs.shef.ac.uk Abstract Automatic interpretation of documents is hampered by the fact that language contains terms which have", "acronym": "WS", "ID": "182"}, {"sentence": "Finally, the WS Disam- biguation module uses cosine similarity to compare the centroid of each possible CUI of the ambiguous term (retrieved from the Centroid Database) with the ambiguous term?s feature vector (Stevenson et al 2008).", "acronym": "WS", "ID": "183"}, {"sentence": "Unsupervised Graph- based WS Disambiguation Using Measures of Word Semantic Similarity.", "acronym": "WS", "ID": "184"}, {"sentence": "Phrase-level tags Functional tags S Sentence SBJ Subject Q Quotative clause OBJ Object NP Noun phrase CMP Complement VP Verb phrase MOD Modifier VNP Copula phrase AJT Adjunct AP ADVP CNJ Conjunctive DP Adnoun phrase INT Vocative IP Interjection phrasePRN parenthetical Table 2: Phrase tags used in Sejong treebank.", "acronym": "ADVP", "ID": "185"}, {"sentence": "osition- object phrase  Measure phrase Location phrase   Sentence level  BA sentence BEI sentence SHI sentence  YOU sentence Compound sentence  Table 1:  Chinese check-point taxonomy    Word level  Noun Verb (with Tense) Modal verb  Adjective Adverb Pronoun  Preposition Ambiguous word Plurality  Possessive Comparative & Superlative  degree  Phrase level  Noun phrase Verb phrase Adjective  phrase  ADVP Preposition phrase   Sentence level  Attribute clause Adverbial clause Noun clause  Hyperbaton   Table 2: English check-point taxonomy  4 Construction of Check-Point Data- base  Given a bilingual corpus with word alignment,  the construction of check-point database consists  of following two steps.", "acronym": "ADVP", "ID": "186"}, {"sentence": "NP  UP  UG  NTL  NTP  AP  FP  VP  IP  LP  DP  Noun phr~e ~?~  Digital phrase 14560,.~_=P--_~\"  Digital-classifier .:~.~,~-.p~:  phrase  Phrase expressing =.-\\[-~t~z, 60 ./x~  the period of time  Phrase expressing ~ ~}k ~1~.~_~  the exact time  Adjective phrase ~gk:~Y  ADVP ~:~l:'~:J4~ (~  :~:i~)  Verb phrase -~-~ \\ ] i~   Preposition phrase ~ l~ l /~-   Post-position \" l~r~  phrase  Frame structure ~ ..6..~ ~ ~lJ ~i~ Jl~ .~.,,:i~ q\"  Table 1 Blocks defined in the system  Except PP, LP and DP, each kind of  block is  defined by a set of rules in the form of phrase  structure rule.", "acronym": "ADVP", "ID": "187"}, {"sentence": "2.1.50therphrase types  AjP Adjective phrase (an interesting idea)  AdvP ADVP (you put that nicely)  Quo Quote (\"Indeed, \"she said)  2.2 Grammatical Functions  Below is a list of the FrameNet GFs.", "acronym": "ADVP", "ID": "188"}, {"sentence": "University of Pennsylvania University of Otago We argue in this article that many common ADVP generally taken to signal a discourse relation between syntactically connected units within discourse structure instead work anaphor- ically to contribute relational meaning, with only indirect dependence on discourse structure.", "acronym": "ADVP", "ID": "189"}, {"sentence": "Systems that  ignore this and begin with units that are inevitably  realized as kernel clauses under-utilize the expressive  power of natural language, which can use complex noun  phrases, nominalizations, ADVP, and other  adjuncts to pack information from multiple units into  one clause.", "acronym": "ADVP", "ID": "190"}, {"sentence": "There is  normal ly  no more than two  ADVP before or after  the nominal.", "acronym": "ADVP", "ID": "191"}, {"sentence": "a red large ball\" and the typical ordering  of temporal before spatial ADVP in German.", "acronym": "ADVP", "ID": "192"}, {"sentence": "To identify these NPs as ADVP and  so preserve the intransitive sense of \"run\", we attach attributes  Time or Distance to the entries of nouns like \"time\" and \"mile\"  in the SL lexicon.", "acronym": "ADVP", "ID": "193"}, {"sentence": "The preverbal specifiers are  ha-phrases, bei-phrases, ADVP, degree phrases,  preposition phrases, quantifier phrases, aspect, and modal.", "acronym": "ADVP", "ID": "194"}, {"sentence": "Phrase-level tags Functional tags S Sentence SBJ Subject Q Quotative clause OBJ Object NP Noun phrase CMP Complement VP Verb phrase MOD Modifier VNP Copula phrase AJT Adjunct AP ADVPe CNJ Conjunctive DP Adnoun phrase INT Vocative IP Interjection phrasePRN parenthetical Table 2: Phrase tags used in Sejong treebank.", "acronym": "ADVP", "ID": "195"}, {"sentence": "osition- object phrase  Measure phrase Location phrase   Sentence level  BA sentence BEI sentence SHI sentence  YOU sentence Compound sentence  Table 1:  Chinese check-point taxonomy    Word level  Noun Verb (with Tense) Modal verb  Adjective Adverb Pronoun  Preposition Ambiguous word Plurality  Possessive Comparative & Superlative  degree  Phrase level  Noun phrase Verb phrase Adjective  phrase  ADVPe Preposition phrase   Sentence level  Attribute clause Adverbial clause Noun clause  Hyperbaton   Table 2: English check-point taxonomy  4 Construction of Check-Point Data- base  Given a bilingual corpus with word alignment,  the construction of check-point database consists  of following two steps.", "acronym": "ADVP", "ID": "196"}, {"sentence": "NP  UP  UG  NTL  NTP  AP  FP  VP  IP  LP  DP  Noun phr~e ~?~  Digital phrase 14560,.~_=P--_~\"  Digital-classifier .:~.~,~-.p~:  phrase  Phrase expressing =.-\\[-~t~z, 60 ./x~  the period of time  Phrase expressing ~ ~}k ~1~.~_~  the exact time  Adjective phrase ~gk:~Y  ADVPe ~:~l:'~:J4~ (~  :~:i~)  Verb phrase -~-~ \\ ] i~   Preposition phrase ~ l~ l /~-   Post-position \" l~r~  phrase  Frame structure ~ ..6..~ ~ ~lJ ~i~ Jl~ .~.,,:i~ q\"  Table 1 Blocks defined in the system  Except PP, LP and DP, each kind of  block is  defined by a set of rules in the form of phrase  structure rule.", "acronym": "ADVP", "ID": "197"}, {"sentence": "2.1.50therphrase types  AjP Adjective phrase (an interesting idea)  AdvP ADVPe (you put that nicely)  Quo Quote (\"Indeed, \"she said)  2.2 Grammatical Functions  Below is a list of the FrameNet GFs.", "acronym": "ADVP", "ID": "198"}, {"sentence": "De- claratively subjective clues such as the subjec- tive predicate part of the main clause and subjec- tive sentential ADVPs suggest that the  writer is the source of the opinion.", "acronym": "ADVP", "ID": "199"}, {"sentence": "3.3 Adverb Phrase Chunking When the adverbs appear in succession, they have a great tendency to form an ADVP.", "acronym": "ADVP", "ID": "200"}, {"sentence": "Though an adverb sequence is not always one ADVP, it usually forms one phrase.", "acronym": "ADVP", "ID": "201"}, {"sentence": "5.3.3 Head Words of all Phrases  We consider all phrases or syntactic roles, i.e.,  not only noun and verb phrases but also adjec- tive and ADVPs.", "acronym": "ADVP", "ID": "202"}, {"sentence": "To reduce the number of placeholders, we con- sider the notion of chunk defined in (Abney, 1996), i.e., not recursive kernels of noun, verb, adjective, and ADVPs.", "acronym": "ADVP", "ID": "203"}, {"sentence": "In Proceedings of the International Joint Workshop on BioNLP and its Applications, pages 104?107.", "acronym": "BioNLP", "ID": "204"}, {"sentence": "Settles, B. (2004), Biomedical named entity recognition using conditional random fields and rich feature sets, in Proceedings of the International Joint Workshop on BioNLP and its Applications (NLPBA), 2004, Geneva, Switzerland.", "acronym": "BioNLP", "ID": "205"}, {"sentence": "In N. Collier, P. Ruch, and A. Nazarenko, editors, Proceedings of the International Joint Work- shop on BioNLP and its Applications (JNLPBA), Geneva, Switzerland, pages 70?75, August 28?29.", "acronym": "BioNLP", "ID": "206"}, {"sentence": "In Proceedings of the International Joint Workshop on BioNLP and its Applications (NLPBA), pages 104?107.", "acronym": "BioNLP", "ID": "207"}, {"sentence": "In Proceedings of the International Joint Workshop on 599 BioNLP and its Applications (NLPBA), pages 88?91.", "acronym": "BioNLP", "ID": "208"}, {"sentence": "In Proceedings of the  International Joint Workshop on Natural Language  Processing in BioNLP and its Applications,  pages 70-75.", "acronym": "BioNLP", "ID": "209"}, {"sentence": "In Proceedings of the Symposium for Semantic Mining in BioNLP (SMBM 2010), pages 137?141, Hinxton.", "acronym": "BioNLP", "ID": "210"}, {"sentence": "In Proceedings of Semantic Mining in BioNLP (SMBM 2012), pages 10?17.", "acronym": "BioNLP", "ID": "211"}, {"sentence": "of the International Joint Workshop on Nat- ural Language Processing in BioNLP and its Applica- tions, pages 66?69, Geneva, Switzerland.", "acronym": "BioNLP", "ID": "212"}, {"sentence": "In Proceedings of the Symposium for Semantic Mining in BioNLP (SMBM 2010), pages 84?92, Hinxton.", "acronym": "BioNLP", "ID": "213"}, {"sentence": "Sense id Pattern Synonyms 1 kare(he) ga(nominative) soba(noodles) wo(accusative) kuu (eat) 2 kare (he) ga(nominative) fukugyo(a part-time job) de(accusative) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense VB Classes Id Sense VB Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4 ask {negau, tanomu} 14 accept {uketoru, ukeru, morau, osameru} 5 leave {saru, hanareru} 15 take {uketoru, toru, kaisyakusuru, miru} 6 move {saru, utsuru} 16 lose {ushinau, nakusu} 7 pass {saru, kieru, sugiru", "acronym": "VB", "ID": "214"}, {"sentence": "Major type Minor type  Kanji Reading, Writing, Radical, The  order of writing, Classification  Word knowl- edge  Katakana, How to use Kana, Ap- propriate Noun, To fill blanks for  VB, Adjective, and Adjunct,  Synonym, Antonym, Particle,  Conjunction, Onomatopoeia, Po- lite Expression, Punctuation mark Reading com- prehension  Who, What, When, Where, How,  Why question, Extract specific  phrases, Progress order of a story,  Prose and Verse   Composition Constructing sentence, How to  write composition  Table 1.", "acronym": "VB", "ID": "215"}, {"sentence": "VBless clauses: nouns, adjectives, and adverbs, lexical or derived, functioning as predicates ?", "acronym": "VB", "ID": "216"}, {"sentence": "d1> ga <word2> wo ukeireru / yurusu (forgive) [Concept relation] agent object [Case particle] ga (nominative) wo (accusative) [Sense identifier] 0ee0de; 0f58b4; 0f98ee 0f0157; 30f6b0 0ee0de: the part of a something written that makes reference to a particular matter 0f58b4: a generally-held opinion 0f98ee: the people who citizens of a nation 0f0157: a human being 30f6b0: human Figure 4: Extracted VB frames of ?", "acronym": "VB", "ID": "217"}, {"sentence": "c?2009 ACL and AFNLP Classifying Japanese Polysemous VBs based on Fuzzy C-means Clustering Yoshimi Suzuki Interdisciplinary Graduate School of Medicine and Engineering University of Yamanashi, Japan ysuzuki@yamanashi.ac.jp Fumiyo Fukumoto Interdisciplinary Graduate School of Medicine and Engineering University of Yamanashi, Japan fukumoto@yamanashi.ac.jp Abstract This paper presents a method for classify- ing Japanese polysemous verbs using an alg", "acronym": "VB", "ID": "218"}, {"sentence": "Sense id Pattern Synonyms 1 kare(he) ga(nominative) soba(noodles) wo(accusative) kuu (eat) 2 kare (he) ga(nominative) fukugyo(a part-time job) de(accusative) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense VB Classes Id Sense VB Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4 ask {negau, tanomu} 14 accept {uketoru, ukeru, morau, osameru} 5 leave {saru, hanareru} 15 take {uketoru, toru, kaisyakusuru, miru} 6 move {saru, utsuru} 16 lose {ushinau, nakusu} 7 pas", "acronym": "VB", "ID": "219"}, {"sentence": "Of the over 1000  nouns which had VBs, 712 were not already  on the LDOCE fist augmented by Filtering.", "acronym": "VB", "ID": "220"}, {"sentence": "To compositionally build VBd 567 semantic representations of our EDUs, we (Subba et al, 2006) integrated a robust parser, LCFLEX (Rose?,", "acronym": "VB", "ID": "221"}, {"sentence": "We then perform a simple post processing step: we reas- sign classes to each VBd on the final scores they have received and recalculate their scores.", "acronym": "VB", "ID": "222"}, {"sentence": "347 Computational Linguistics Volume 32, Number 3 with VBs to form deverbal nouns are listed and exemplified in Figure 1 on page 348.", "acronym": "VB", "ID": "223"}, {"sentence": "Then we sent these  rJouns through our morphological analyzer to  extract those with VBs.", "acronym": "VB", "ID": "224"}, {"sentence": "2 Online VB for Polylingual Topic Models Hierarchical generative Bayesian models, such as topic models, have proven to be very effective for modeling document collections and discover- ing underlying latent semantic structures.", "acronym": "VB", "ID": "225"}, {"sentence": "The VBian EM algorithm for incomplete data: with application to scoring graphical model structures.", "acronym": "VB", "ID": "226"}, {"sentence": "2.4 VB Moore (2004) showed that the EM algorithm is par- ticularly susceptible to overfitting in the case of rare words when training IBM Model 1.", "acronym": "VB", "ID": "227"}, {"sentence": "In order to pre- vent overfitting, we use the VB ex- tension of the EM algorithm (Beal, 2003).", "acronym": "VB", "ID": "228"}, {"sentence": "For the Bayesian runs, we compared two infer- ence methods: Gibbs sampling, as described above, and VBian EM (Beal and Ghahra- mani, 2003), both of which are implemented in Carmel.", "acronym": "VB", "ID": "229"}, {"sentence": "They built a social network from the Marvel COM in which characters are the nodes, linked by their co-occurrence in the same book.", "acronym": "COM", "ID": "230"}, {"sentence": "here is meant to refer to all genres, in- cluding literature as well as more popular genres  such as fantasy, thrillers, COM, etc.", "acronym": "COM", "ID": "231"}, {"sentence": "the character is the first gay figure in the official @entity6 -- the movies , television shows , COM and books approved by @entity6 franchise owner @entity22 -- according to @entity24 , editor of \" @entity6 \" books at @entity28 imprint @entity26 .", "acronym": "COM", "ID": "232"}, {"sentence": "For example, when Superman?s prowess was first documented in the COM he did not have x-ray vision.", "acronym": "COM", "ID": "233"}, {"sentence": "Two studies on cognitive strategies used by second language learners (Ka- plan and Lucas, 2001; Lucas, 2004) used a data set of 58 jokes compiled from newspaper COM, 32 of which rely on lexical ambiguity.", "acronym": "COM", "ID": "234"}, {"sentence": "3.2 COM A domain should be complex enough to warrant the use of a QA system.", "acronym": "COM", "ID": "235"}, {"sentence": "COM of System NARA  The complexity of the algorithm is usually measured  by the growth rate of its time and space requirements,  as a function of the size of its input (or the length  of input string) to which the algorithm is applied.", "acronym": "COM", "ID": "236"}, {"sentence": "Reducing the Time COM of the Fuzzy C-means Algorithm, In Trans.", "acronym": "COM", "ID": "237"}, {"sentence": "Cutler, A. (1983) Lexical COM and Sentence  Processing, in G. B. Flores d'Arcais and R.J.  Jarvella, eds.", "acronym": "COM", "ID": "238"}, {"sentence": "In J. Vicedo, P. Martnez- Barco, R. Muoz, and M. Saiz Noeda, editors, Ad- vances in Natural Language Processing, volume 3230 of Lecture Notes in COM Science, pages 82?90.", "acronym": "COM", "ID": "239"}, {"sentence": "SPIRE, number 3772 in Lecture Notes in COM Science, pages 161?166.", "acronym": "COM", "ID": "240"}, {"sentence": "The evaluation was conducted  on 133 paragraphs of annotated COM Sci-  ence text.", "acronym": "COM", "ID": "241"}, {"sentence": "of COM Science,  Brown University,  \\[nge I j th \\[ ec\\] ~cs.", "acronym": "COM", "ID": "242"}, {"sentence": "Given this, we have developed rule-based, machine-learning-based and reSRC- based approaches for estimation of character gen- der, age and salient personality attributes.", "acronym": "SRC", "ID": "243"}, {"sentence": "Given the above possible SRCs of informar  tion, we arrive at the following equation, where  F(p) denotes a function from pronouns to their  antecedents:  F(p) = argmaxP( A(p) = alp, h, l~', t, l, so, d~ A~')  where A(p) is a random variable denoting the  referent of the pronoun p and a is a proposed  antecedent.", "acronym": "SRC", "ID": "244"}, {"sentence": "A hybrid ap- proach is adopted, where pattern-based and statistical methods are used along with utilization of external knowledge SRCs.", "acronym": "SRC", "ID": "245"}, {"sentence": "The utilization of available reSRCs containing associations between person names and gender was followed in (Elson and 41 McKeown, 2010).", "acronym": "SRC", "ID": "246"}, {"sentence": "Our first experiment  shows the relative contribution of each SRC  Of information and demonstrates a uccess rate  of 82.9% for all SRCs combined.", "acronym": "SRC", "ID": "247"}, {"sentence": "PRED SENSE (c6): The lemma plus sense number of the predicate As for the task of SRCn, the features of the predicate word in addition to those of the word under consideration can also be used; we mark features of the predicate with an extra ?", "acronym": "SRC", "ID": "248"}, {"sentence": "Semantic role labeling is achieved us- ing maximum entropy (MaxEnt) model based SRCn and integer linear programming (ILP) based post inference.", "acronym": "SRC", "ID": "249"}, {"sentence": "2 System Description The proposed system sequentially performs syn- tactic dependency parsing, predicate identification, local SRCn, global sequence generation, and roleset information based selec- tion.", "acronym": "SRC", "ID": "250"}, {"sentence": "PRED SENSE (c6): The lemma plus sense number of the predicate As for the task of SRC, the features of the predicate word in addition to those of the word under consideration can also be used; we mark features of the predicate with an extra ?", "acronym": "SRC", "ID": "251"}, {"sentence": "Semantic role labeling is achieved us- ing maximum entropy (MaxEnt) model based SRC and integer linear programming (ILP) based post inference.", "acronym": "SRC", "ID": "252"}, {"sentence": "2 System Description The proposed system sequentially performs syn- tactic dependency parsing, predicate identification, local SRC, global sequence generation, and roleset information based selec- tion.", "acronym": "SRC", "ID": "253"}, {"sentence": "In the medical texts, however, FA turn out to be the major glue for establishing local coherence, while anaphora, pronomi- nal anaphora in particular, play a far less important role than in other text genres.", "acronym": "FA", "ID": "254"}, {"sentence": "This allows us to deal with various forms of pronom- inal, nominal and FA in a uniform way.", "acronym": "FA", "ID": "255"}, {"sentence": "One of such latent relationship is indirect anaphora, FA, or bridging ref- erence, such as the following examples.", "acronym": "FA", "ID": "256"}, {"sentence": "We claim that only sophisticated knowledge representation languages with powerful terminological reasoning capabilities, such as those from the KL-ONE family, are able to deal with the full range of challenges of referentially adequate text understanding, in particular considering nominal and FA.", "acronym": "FA", "ID": "257"}, {"sentence": "In IT texts, (pro)nominal anaphora and FA occur at an almost balanced rate [27].", "acronym": "FA", "ID": "258"}, {"sentence": "But the work on bridging references characterizes this relationship as referential or anaphoric; this can be seen in the various terms under which this phenomenon is discussed: bridging references, indirect anaphora, FA, and partial anaphora.", "acronym": "FA", "ID": "259"}, {"sentence": "169 Labeled Macro F 1 LAS Labeled F 1 (complete task) (syntactic dependencies) (semantic dependencies) WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown johansson 84.86 (1) 85.95 75.95 89.32 (1) 90.13 82.81 80.37 (1) 81.75 69.06 che 82.66 (2) 83.78 73.57 86.75 (5) 87.51 80.73 78.52 (2) 80.00 66.37 ciaramita 82.06 (3) 83.25 72.46 86.60 (11) 87.47 79.67 77.50 (3) 79.00 65.24 zhao 81.44 (4)", "acronym": "LAS", "ID": "260"}, {"sentence": "The big drop for German in pred/5k case 4LAS, with punctuation being taking into account.", "acronym": "LAS", "ID": "261"}, {"sentence": "nces    14 While correctly tokenized sentences yield  results that are not extremely different from those  using gold standard information, and the drop in  accuracy in them can be attributed to the  differences introduced through stemming and  automatic parts of speech as well as the absence of  the linguistic features, incorrectly tokenized  sentences show a completely different picture as  the LAS now plummets to  33.6%, which is 37.96 percentage points below  that on correctly tokenized sentences.", "acronym": "LAS", "ID": "262"}, {"sentence": "Labeled Macro F 1 LAS Labeled F 1 (complete task) (syntactic dependencies) (semantic dependencies) WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown vickrey ? ? ? ? ? ?", "acronym": "LAS", "ID": "263"}, {"sentence": "The best results on Arabic in the CoNLL 2007  shared task were obtained by Hall et al(2007) as  they obtained a LAS of  76.52%, 9.6 percentage points above the highest  score of the 2006 shared task.", "acronym": "LAS", "ID": "264"}, {"sentence": "Ta- ble 2 shows LAS obtained with the three online classifiers.", "acronym": "LAS", "ID": "265"}, {"sentence": "The results of the transition-based model, including the graph-based model shows some larger differ- ences The labeled and unLASs are not statistically significant and we concluded that (1) and (2) do not probably hold.", "acronym": "LAS", "ID": "266"}, {"sentence": "Splitting up the morphology is a neutral operation in terms of labeled and unLASs; however, it is worth noting that our results with the separate test for the completion model is more competitive, providing an improvement of 0.14 UAS.", "acronym": "LAS", "ID": "267"}, {"sentence": "When tested using 10-fold cross validation on the enhanced PTB, the parser achieves a LAS of 89.8, which is lower than the current state- of-the-art for transition-based dependency parsers (to wit, the 91.8 score of Zhang and Nivre 2011, although not directly comparable given that they test exclusively on WSJ Section 23), but with the advantage of providing us with the deep linguistic information from the XLE.", "acronym": "LAS", "ID": "268"}, {"sentence": "A. 1The attachment score only considers whether a word is as- signed the correct head; the LAS in addition requires that it is assigned the correct dependency type; cf.", "acronym": "LAS", "ID": "269"}, {"sentence": "For the sole transition-based parsers trained with the selected features, we obtain for Chinese, Hungar- ian and Russian higher labeled and unLASs.", "acronym": "LAS", "ID": "270"}, {"sentence": "Unlabeled attachment score drops from 84.09% to  83.53% and LAS from 85.68%  to 85.44%.", "acronym": "LAS", "ID": "271"}, {"sentence": "The resulting LAS was non-significantly lower (0.2%) than the score for the marginalised inference with the joint model.", "acronym": "LAS", "ID": "272"}, {"sentence": "The syntactic dependencies for both English and Chinese were obtained using the state-of-the-art Maltparser dependency parser, which achieved 84% and 88% LASs for Chinese and English respectively.", "acronym": "LAS", "ID": "273"}, {"sentence": "Furthermore, we also calculated gen- eral accuracies as micro-averages from the cross- validation sets, for which we used two metrics, namely the LAS ASL and the unLAS ASU , which are both accuracy metrics that compute the percentage of cor- rectly parsed dependencies over all tokens, where the unlabelled metric only requires a match with the correct head, and the labelled metric additionally re- quires the correct dependency relation to be chosen.", "acronym": "LAS", "ID": "274"}, {"sentence": "However, most evaluations of syntactic treebanks use simple ac- curacy measures such as bracket F 1 scores for constituent trees (NEGRA, Brants, 2000; TIGER, Brants and Hansen, 2002; Cat3LB, Civit et al, 2003; The Arabic Treebank, Maamouri et al, 2008) or labelled or unLASs for dependency syntax (PDT, Haji?c, 2004; PCEDT Mikulov?a and ?", "acronym": "LAS", "ID": "275"}, {"sentence": "The Dev-5k and Dev columns report LAS on the development sets.", "acronym": "LAS", "ID": "276"}, {"sentence": "The unLASs of the converted dependencies are shown as the accuracies in Table 5, since most bunsetsu-based dependency parsers out- put only unlabeled structure.", "acronym": "LAS", "ID": "277"}, {"sentence": "We tried to select the graph-based feature templates of the completion model after the selection of the 8 LMP [(LAS + morphology accuracy + part-of-speech accuracy)/3] would have been another alternative.", "acronym": "LAS", "ID": "278"}, {"sentence": "Benefiting from the rich features selected in the tree kernel space, our model achieved the best reported unLAS of 93.72 without using any additional resource.", "acronym": "LAS", "ID": "279"}, {"sentence": "(For the remaining languages, the training data has not been split at all.)5 A dry run at the end of the development phase gave a LAS of 80.46 over the twelve required languages.", "acronym": "LAS", "ID": "280"}, {"sentence": "LA is the LAS and UA is the Unla- belled Attachment Score <training file>?).", "acronym": "LAS", "ID": "281"}, {"sentence": "Feature Train + Dev Set Test Set Chance Baseline 9.1 9.1 Character unigram 33.99 34.70 Character bigram 51.64 49.80 Character trigram 66.43 66.70 RASP POS unigram 43.76 45.10 RASP POS bigram 58.93 61.60 RASP POS trigram 59.39 62.70 Function word unigram 51.38 54.00 Function word bigram 59.73 63.00 Word unigram 74.61 75.50 Word bigram 74.46 76.00 Word trigram 63.60 65.00 TSG Fragments 72.16 72.70 SD 73.78 75.90 Adaptor Grammar POS/FW n-grams 69.76 70.00 Table 1: Classification results for our individual features.", "acronym": "SD", "ID": "282"}, {"sentence": "On the other hand, we obtain the de- pendency relations by the SD  Parser4.", "acronym": "SD", "ID": "283"}, {"sentence": "variant of SD (de Marneffe and Manning, 2008a), in which preposition- and conjunction- words do not appear as nodes in the tree but are instead anno- tated on the dependency label between the content words they connect, e.g. prep with(saw, telescope).", "acronym": "SD", "ID": "284"}, {"sentence": "The performance for word n-grams, TSG frag- ments and SD is very strong and comparable to previously reported research.", "acronym": "SD", "ID": "285"}, {"sentence": "In T.H. King and E.M. Bender, editors, GEAF 2007, SD, CA.", "acronym": "SD", "ID": "286"}, {"sentence": "In order to address this distant association issue, we examined the collapsed-ccprocessed-dependencies output besides the basic-dependenciesout- put of the SD CoreNLP dependency engine (de Marneffe and Manning, 2012).", "acronym": "SD", "ID": "287"}, {"sentence": "were performed using the SD CoreNLP suite of tools (CoreNLP, 2014).", "acronym": "SD", "ID": "288"}, {"sentence": "The most plausible point is calculated by using the potential function with the SD Method on request.", "acronym": "SD", "ID": "289"}, {"sentence": "The  algorithms for obtaining the best translation ma-  trix were shown based on the SD  Method, an algorithm well known in the field of  non-linear programming.", "acronym": "SD", "ID": "290"}, {"sentence": "The results show that the proposed model trained by a global optimizer outperforms models trained by the other local optimizers (SD and BFGS); The penalty in the proposed model works well because the degeneracies in the penalized model seem to decrease, and the computation is noteworthy stable.", "acronym": "SD", "ID": "291"}, {"sentence": "The re-  suiting structures are then scored using some mea-  sure of success that compares these parses to the  correct SDs for the sentences  provided in the training corpus.", "acronym": "SD", "ID": "292"}, {"sentence": "In FUG, not only is  there no formal distinction between categories and  SDs, but even the distinction be-  tween SDs and grammars disappears.", "acronym": "SD", "ID": "293"}, {"sentence": "Thus, in LFG, syntactic ategories and  the SDs known as f-structures are  exactly the same kind of object.", "acronym": "SD", "ID": "294"}, {"sentence": "Data bases as tools for linguistic research  The data bases contain again linguistic objects together  with additional informations (e.g. SDs,  classifications).", "acronym": "SD", "ID": "295"}, {"sentence": "god 0.085 (0.045) news 0.100 (0.044) christmas 0.081 (0.046) way 0.078 (0.044) jesus 0.060 (0.038) obamacare 0.068 (0.041) kenya 0.052 (0.035) white 0.059 (0.037) brave 0.043 (0.035) let 0.058 (0.038) bravo 0.041 (0.035) course 0.046 (0.033) know 0.038 (0.030) huh 0.044 (0.036) dennis 0.038 (0.029) education 0.043 (0.032) ronald 0.036 (0.030) president 0.039 (0.031) Table 5: Average weights (and SDs calculated across samples) for top 10 NNP ?", "acronym": "SD", "ID": "296"}, {"sentence": "Figure 5 presents the SD for con- tent and readability scores: concerning content, automatically generated summaries using back- ground information achieved the highest SD scores (see also figure 6 for a sample story).", "acronym": "SD", "ID": "297"}, {"sentence": "Although One-Class Support Vec- tor Machine (OSVM) (Manevitz and Yousef, 2001) can learn from just positive examples, according to Yu et al (2002) they are prone to underfitting and overfitting when data is scant (which happens in 2The mean, median, SD and histogram of the overlapping distribution are calculated and included as features.", "acronym": "SD", "ID": "298"}, {"sentence": "Due to the variance inherent to the stochastic gradient descent procedure, we repeat the experiment 100 times and report the median performance and SDs (of different SGD runs).", "acronym": "SD", "ID": "299"}, {"sentence": "interval expresses the SD of the temporal intervals between two successive posts.", "acronym": "SD", "ID": "300"}, {"sentence": "0.00  0.50  1.00  1.50  2.00  2.50  3.00  3.50  4.00  4.50  5.00  Simple (News  only)  Background  only  Background +  News  Human  Extractive  Human  Abstractive  Content (avg) Readability (avg)  Content (stdev) Readability (stdev)  Figure 6: Average and SD of the content and readability scores for one news story.", "acronym": "SD", "ID": "301"}, {"sentence": "To adapt our system to the task, we first heuris- tically converted the question into a query fact us- ing the subject and object SD labels (de Marneffe and Manning, 2008).", "acronym": "SD", "ID": "302"}, {"sentence": "Although all systems perform syntactic analy- sis of input texts, there is a fair amount of vari- ety in the applied parsers, which include the parser of Charniak and Johnson (2005) with the biomed- ical domain model of McClosky (2009) and the SD conversion (de Marneffe et al 2006) ?", "acronym": "SD", "ID": "303"}, {"sentence": "To extract events from text, we use the SD Parser (De Marneffe et al, 2006; Socher et al, 2013).", "acronym": "SD", "ID": "304"}, {"sentence": "We find a remarkable number of sim- ilarities between the approaches of the systems, with all four utilizing full parsing and a depen- dency representation of the syntactic analysis, and the three highest-ranking further specifically the phrase structure parser of Charniak and Johnson (2005) with the biomedical domain model of Mc- Closky (2009), converted into SD form using the Stanford tools (de Marneffe et al, 2006).", "acronym": "SD", "ID": "305"}, {"sentence": "The unigram and dependency annotations are derived from the SD Parser (Klein and Manning, 2003).", "acronym": "SD", "ID": "306"}, {"sentence": "The first speech recognition systems  tended to be SD (before using a sys-  tem, a person had first to read a list of words or sen-  tences).", "acronym": "SD", "ID": "307"}, {"sentence": "whether A is \"to the left of\" B in  a scene, is dependent on the orientation of the  speaker/viewer); other expressions are orientation  independent such as \"above\" and \"below\" which  implicitly refer to the downward pull of gravity (al-  though in space verticality is SD).", "acronym": "SD", "ID": "308"}, {"sentence": "Our project uses well developed  SD voice recognit ion equip-  ment with a small enough vocabulary to  achieve usable accuracy rates.", "acronym": "SD", "ID": "309"}, {"sentence": "In this paper, we demonstrate that the attribute set is SD.", "acronym": "SD", "ID": "310"}, {"sentence": "We also  anticipate moving from SD recognition to a  speaker adaptive mode which will require far less training  data for new speakers.", "acronym": "SD", "ID": "311"}, {"sentence": "Using our approach we have made a  prototype of the recognizer for isolated word,  SD ASR system for Hindi.", "acronym": "SD", "ID": "312"}, {"sentence": "MRF  (Markov Random Field) classify the term depend- encies in queries into SD and  full dependence, which respectively corresponds to  ordered and unordered co-occurrence within a pre- define-sized window in documents.", "acronym": "SD", "ID": "313"}, {"sentence": "The model thus reflects a hy- pothesis that eye movements are largely unaffected by semantic content, that eye movements depend on the physical properties and frequency of words, and that there is a SD between fixa- tion times.", "acronym": "SD", "ID": "314"}, {"sentence": "nist.gov/related projects/muc/. 2The ACE program, www.nist.gov/speech/tests/ace/. genre-specific corpus of German newspaper com- mentaries, taken from the daily papers Ma?rkische Allgemeine Zeitung and Tagesspiegel.", "acronym": "ACE", "ID": "315"}, {"sentence": "1 Introduction There has been much recent interest in identifying events, times and their relations along the timeline, from event and time ordering problems in the Temp- Eval shared tasks (Verhagen et al, 2007; Verhagen et al, 2010), to identifying time arguments of event structures in the ACE pro- gram (Linguistic Data Consortium, 2005; Gupta and Ji, 2009), to timestamping event intervals in the Knowledge Base Population shared task (Artiles et al.,", "acronym": "ACE", "ID": "316"}, {"sentence": "2010), and the ACE pro- gram only looked at time arguments for specific types of events, like being born or transferring money.", "acronym": "ACE", "ID": "317"}, {"sentence": "Linguistic Re- sources and Evaluation Techniques for Evaluation of Cross-Document ACE.", "acronym": "ACE", "ID": "318"}, {"sentence": "The ACE 2008 corpus (Linguistic Data Consortium 2008) for relation detection and recognition collects English and Arabic texts from a variety of resources including radio and TV broadcast news, talk shows, newswire articles, Internet news groups, Web logs, and conversational telephone speech.", "acronym": "ACE", "ID": "319"}, {"sentence": "It is worth noting that our classification is more  fine-grained than efforts like the EDT task in  ACE1 program (Mitchell  and Strassel 2002), which distinguishes between  toponyms that are a Facility ?", "acronym": "ACE", "ID": "320"}, {"sentence": "Instead, we will adopt the nomencla- ture of the ACE program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it).", "acronym": "ACE", "ID": "321"}, {"sentence": "From Table 1 we can see that the two more formal corpora, NEWSHOUR and CROSSFIRE, have longer sentences, more sentences per turn, and fewer disfluencies (particularly nonlexicalized filled pauses and FSs) than English CALLHOME and the GROUP MEETINGS.", "acronym": "FS", "ID": "322"}, {"sentence": "The experimenter then tran-  scribed what the subject said, excluding FSs, and  sent he transcription tothe system, which automatically  generated the synthesized response.", "acronym": "FS", "ID": "323"}, {"sentence": "In section 5.3 we shall present three components for identifying most of the major classes of speech disfluencies in the input of the summarization system, such as filled pauses, repetitions, and FSs.", "acronym": "FS", "ID": "324"}, {"sentence": "As such, the utterances exhibit the usual difficulties associated with real language such as agreement errors, FSs, fragments, and constituent dislocations.", "acronym": "FS", "ID": "325"}, {"sentence": "The transcriber eliminated  hesitations, \"urns\" and FSs, but otherwise sim-  ply transmitted a transcription ofwhat the subject said.", "acronym": "FS", "ID": "326"}, {"sentence": "There are repeats, FSs, a lot of pause filling words such as um and uh, etc.", "acronym": "FS", "ID": "327"}, {"sentence": "Following  this, it is hoped to incorporate some simple types of domain-  and discourse-dependent knowledge into the program, in  particular knowledge about expected relations among objects  in a given domain and a simple discourse FS.", "acronym": "FS", "ID": "328"}, {"sentence": "A three-tiered partition of a modal- ity, discourse and domain layer is connected with a dou- ble threaded FS.", "acronym": "FS", "ID": "329"}, {"sentence": "Our theory of FS is related to the task-based theory of (Grosz, 1977).", "acronym": "FS", "ID": "330"}, {"sentence": "Along the lines of (Kirschner and Bernardi, 2007), we aim for a precise definition of FS for IQA questions.", "acronym": "FS", "ID": "331"}, {"sentence": "The pragmatic-semantic level of  therue-rheme and FSs has already been defined for  restricted blocks worlds dialogues (Pignataro 1987) and will be  incorporated into an automatic focus assignment system.", "acronym": "FS", "ID": "332"}, {"sentence": "clicks in the follow- ing 8 categories: Play (Pl), Pause (Pa), SeekFw (Sf), SeekBw (Sb), FS (SSf), ScrollBw (SSb), RatechangeFast (Rf), RatechangeSlow (Rs).", "acronym": "FS", "ID": "333"}, {"sentence": "We assume that it should be possible to condense the comparison into one (more or less simple) rule: the narrower the RC (similarity < relatedness < evocation) and the narrower the data considered (lexical semantic selection rule < any kind of selection rule < random selection) the better the correlation between hu- man judgment and semantic measure11.", "acronym": "RC", "ID": "334"}, {"sentence": "6.b, the CS \"ELeMent\", which  usually has two meanings ( an object concept and  a membership RC), functions as an  object concept.", "acronym": "RC", "ID": "335"}, {"sentence": "This comparison is based on the  838  cross-corRC as it is used in digital  signal processing.", "acronym": "RC", "ID": "336"}, {"sentence": "The non-  restrictive RC has the effect of uniquely  determining two parrots that Anne owns.", "acronym": "RC", "ID": "337"}, {"sentence": "The prototypical exam-  ple used throughout this paper is the non-restrictive  RC.", "acronym": "RC", "ID": "338"}, {"sentence": "Nonrest r ic t ive  modi f iers  Uniqueness, or maxi-  mality, is forced by non-restrictive modification, as  can be the case in RCs and adjective-noun  phrases.", "acronym": "RC", "ID": "339"}, {"sentence": "9We exclude infinitival RCs from these fig-  ures, for example \"I called a plumber TRACE to fix the  sink\" where 'plumber' is co-indexed with the trace sub-  ject of the infinitival.", "acronym": "RC", "ID": "340"}, {"sentence": "This  section describes a probabilistic treatment of extrac-  tion from RCs.", "acronym": "RC", "ID": "341"}, {"sentence": "5.3 Effect of RC All the experiments above considered left context only.", "acronym": "RC", "ID": "342"}, {"sentence": "Left Context Focus RC Combined Class - - - - - a a n b i d ?", "acronym": "RC", "ID": "343"}, {"sentence": "Context Accuracy (%) Left Context Only 91.31 RC Only 88.26 Both Contexts 92.54 Table 3: The effect of using both left and right context.", "acronym": "RC", "ID": "344"}, {"sentence": "\b\u000b\t\u000b\u000b  \u0001 shi ,1Lo \u0002 zhang \u0003 ma \u0004 ying \u0005 jiu \u0006 biao \u0007 shi ,2Lo ,1Co ,2Co ,3Co ,1Ro ,2Ro \b\u000b\t\u000b  \b\u000b\t\u000b  Left Context RC Candidate to be parsed), it will be rejected immediately.", "acronym": "RC", "ID": "345"}, {"sentence": "NUM sg GEND fem DEF - CASE nom SUBJ h i 1 3 7 7 7 7 7 7 7 7 5 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Figure 3: F-structure for example (3) 2.4 Left-RC Rules The left-right context annotation module is based on a tripartite division of local subtrees into a left- hand-side context (LHS) followed by a head (H) followed by a right-hand-side context (RHS).", "acronym": "RC", "ID": "346"}, {"sentence": "Description Feature Trigram + Context x1x2x3x4x5 Trigram x2x3x4 Left Context x1x2 RC x4x5 Center Word x3 Trigram - Center Word x2x4 Left Word + RC x2x4x5 Right Word + Left Context x1x2x3 Type of Trigram: number, punctuation, alphabetic letter and other t(x2)t(x3)t(x4) Table 2: Features employed to measure the sim- ilarity between two vertices, in a given tex- t ?", "acronym": "RC", "ID": "347"}, {"sentence": "The algorithms, including Description Length Gain (DLG) (Kit and Wilks 1999), AV (Feng et al 2004a, 2004b), and Branching Entropy (Tanaka-Ishii 2005; Jin and Tanaka-Ishii 2006), were evaluated on SIGHAN Bakeoff-3 data set (Levow 2006).", "acronym": "AV", "ID": "348"}, {"sentence": "Zhang et al (2013) uses eight types of features such as Mu- tual Information and AV and they extract dynamic statistical features from both an in-domain corpus and an out-of-domain corpus us- ing co-training.", "acronym": "AV", "ID": "349"}, {"sentence": "2.3.2 AV When a string appears under different linguistic environments, it may carry a meaning.", "acronym": "AV", "ID": "350"}, {"sentence": "Haodi Feng, Kang Chen, Xiaotie Deng, Weimin Zheng  AV Criteria for Chinese Word  Extraction, Computational Linguistics March 2004,  Vol.", "acronym": "AV", "ID": "351"}, {"sentence": "We applied a subword- based word segmenter using CRFs and ex- tended the segmenter with OOV words recognized by AV.", "acronym": "AV", "ID": "352"}, {"sentence": "Another common practice is to introduce some statistics-based measures, such 648 as boundary entropy (Jin and Tanaka-Ishii, 2006) and AV (Feng et al 2004), which are commonly used in unsupervised CWS models.", "acronym": "AV", "ID": "353"}, {"sentence": "2) characters, we define the left AV of Llav(s) as the number of distinct characters that precede s in a corpus.", "acronym": "AV", "ID": "354"}, {"sentence": "Similarly, the right AV Rlav(s) is defined as the num- ber of distinct characters that succeed s. We first extract all strings whose length are be- tween 2 and 4 from the unlabeled data, and calculate their AV values.", "acronym": "AV", "ID": "355"}, {"sentence": "This prin- ciple is introduced as the AV criterion for identifying meaningful Chinese words in (Feng et al, 2004).", "acronym": "AV", "ID": "356"}, {"sentence": "Beyond these ideas, Liang (2005) and Sun and Xu (2011) experiment with deriv- ing a large set of statistical features such as mu- tual information and AV from un- labelled data, and add them to supervised dis- criminative training.", "acronym": "AV", "ID": "357"}, {"sentence": "Several pos- sible uses of inflectional sublexicon were also suggested, among which the most widely useful can be the possibility to use all word-forms of a lemma in the web-search engine such as Google, AV etc.", "acronym": "AV", "ID": "358"}, {"sentence": "5 We used eight well-known Web search engines such as Google (http://www.google.com/), Ask Jeeves (http://web.ask.com/), AV (http://www.altavista.com/), LookSmart (http://search.looksmart.com/), Teoma (http://s.teoma.com/), AlltheWeb (http://www.alltheweb.com/), Ly- cos (http://search.lycos.com/), and Yahoo! (", "acronym": "AV", "ID": "359"}, {"sentence": "Thus, the author computes  the Pointwise Mutual Information score between  seed words and new words on the basis of the  number of AV hits returned when querying  the seed word and the word to be classified with  the ?", "acronym": "AV", "ID": "360"}, {"sentence": "Lapata and Keller (2004) achieved improved results on this task by using the database of AV?s search engine as a corpus.", "acronym": "AV", "ID": "361"}, {"sentence": "Related Work: Comparison with Broder?s Sketches Broder?s sketches (Broder 1997), originally introduced for removing duplicates in the AV index, have been applied to a variety of applications (Broder et al 1997; Haveliwala, Gionis, and Indyk 2000; Haveliwala et al 2002).", "acronym": "AV", "ID": "362"}, {"sentence": "A Japanese predicate bunsetsu consists of a main verb followed by a se- quence of AVs and sentence final parti- cles.", "acronym": "AV", "ID": "363"}, {"sentence": "Other constraints require an AV to be modified by a full verb, or prescribe morphosyntactical agreement between a determiner and its regent (the word modified by the determiner).", "acronym": "AV", "ID": "364"}, {"sentence": "There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, AVs used in one lan- guage but not the other (e.g., English He walked and French Il est alle?),", "acronym": "AV", "ID": "365"}, {"sentence": "Through a brief error analysis, we found that main bottle neck for verbal predicate is AV be and have.", "acronym": "AV", "ID": "366"}, {"sentence": "A bunsetsu is a grammat- ical and phonological unit in Japanese, which con- sists of an independent-word such as noun, verb or adverb followed by a sequence of zero or more dependent-words such as AVs, postposi- tional particles or sentence final particles.", "acronym": "AV", "ID": "367"}, {"sentence": "We added the following complementary informa- tion to the tags and named the new tag sets Base or Full and suffix: inf: add inflection information to the POS tag (verbs, adjectives, and AVs) and the phrase tags (Table 2).", "acronym": "AV", "ID": "368"}, {"sentence": "0.00  0.50  1.00  1.50  2.00  2.50  3.00  3.50  4.00  4.50  5.00  Simple (News  only)  Background  only  Background +  News  Human  Extractive  Human  Abstractive  content readability  Figure 4: AV of the content and readability scores for each summary creation method.", "acronym": "AV", "ID": "369"}, {"sentence": "The first is the sentence alignment confidence measure, based on which the best whole sentence alignment is se- 938 # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 939911 43.53 50.51 -3.49 53.14 40.60 6.27 ALF 618179 43.11 50.24 -3.56 51.75 42.05 4.85 Table 7: Improved Arabic-English Newswire Translation with Alignment Link Filtering # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 598721 49.91 39.90 5.00 57.30 30.98 13.16 ALF 383561 48.94 40.00 4.42 55.99 31.92 12.04 Table 8: Improved Arabic-English Web-Blog Translation with Alignment Link Filtering lected among multiple alignments and it obtained 0.8 F-measure improvement over the single best Chinese-English aligner.", "acronym": "AV", "ID": "370"}, {"sentence": "The first is the sentence alignment confidence measure, based on which the best whole sentence alignment is se- 938 # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 939911 43.53 50.51 -3.49 53.14 40.60 6.27 ALF 618179 43.11 50.24 -3.56 51.75 42.05 4.85 Table 7: Improved Arabic-English Newswire Translation with Alignment Link Filtering # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 598721 49.91 39.90 5.00 57.30 30.98 13.16 ALF 383561 48.94 40.00 4.42 55.99 31.92", "acronym": "AV", "ID": "371"}, {"sentence": "The tail documents typically have lower phrase coverage, thus incor- rect phrase translation pairs derived from incorrect 937 # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 934206 60.74 28.05 16.35 69.02 17.83 25.60 ALF 797685 60.33 28.52 15.91 68.31 19.27 24.52 Table 5: Improved Chinese-English Newswire Translation with Alignment Link Filtering # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 934206 62.87 25.08 18.89 66.55 18.80 23.88 ALF 797685 62.30 24.89 18.70 65.97", "acronym": "AV", "ID": "372"}, {"sentence": "289  1  2  3  4  5  6  Clause in Covington's  distance function  \"identical consonants or glides\"  \"identical vowels\"  \"vowel ength difference only\"  \"non-identical vowels\"  \"non-identical consonants\"  \"no similarity\"  Covington's  penalty  10  30  60  100  AV  Hamming  distance  0.0  0.0  1.0  2.2  4.81  8.29  Interpolated  average  distance  0.0  0.0  12.4  27.3  58.1  100.0  Table 2: The clause-by-clause comparison of Covington's distance function (column 3) and a feature-based  distance function (columns 4 and 5).", "acronym": "AV", "ID": "373"}, {"sentence": "s. The tail documents typically have lower phrase coverage, thus incor- rect phrase translation pairs derived from incorrect 937 # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 934206 60.74 28.05 16.35 69.02 17.83 25.60 ALF 797685 60.33 28.52 15.91 68.31 19.27 24.52 Table 5: Improved Chinese-English Newswire Translation with Alignment Link Filtering # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 934206 62.87 25.08 18.89 66.55 18.80 23.88 ALF 797685 62.30 24.89 18.70 65.97 19.25 23.36 Table 6: Improved Chinese-English Web-Blog Translation with Alignment Link Filtering alignment links are more likely to be selected.", "acronym": "AV", "ID": "374"}, {"sentence": "AV of strings with length 3: L3av(c[i:i+2]), L3av(c[i+1:i+3]), R3av(c[i?2:i]), R3av(c[i?3:i?1]); ?", "acronym": "AV", "ID": "375"}, {"sentence": "AV of strings with length 4: L4av(c[i:i+3]), L4av(c[i+1:i+4]), R4av(c[i?3:i]), R4av(c[i?4:i?1]); ?", "acronym": "AV", "ID": "376"}, {"sentence": "c?2008 Association for Computational Linguistics Evaluating a Crosslinguistic Grammar Resource: A CA Study of Wambaya Emily M. Bender University of Washington Department of Linguistics Box 354340 Seattle WA 98195-4340 ebender@u.washington.edu Abstract This paper evaluates the LinGO Grammar Ma- trix, a cross-linguistic resource for the de- velopment of precision broad coverage gram- mars, by applying it to the Australian language Wambaya.", "acronym": "CA", "ID": "377"}, {"sentence": "Grac?a, Joa?o V., Joana P. Pardal, Lu??sa Coheur, and Diamantino CAiro.", "acronym": "CA", "ID": "378"}, {"sentence": "CAs  like this are currently excluded from the MUC  coreference task, which limits itself to relations  between NPs.", "acronym": "CA", "ID": "379"}, {"sentence": "Thus h(M)<harnaony(M),  CA >A. lf M is in list, then harmony(M) must he defined  and set at a wdue _< h(M).", "acronym": "CA", "ID": "380"}, {"sentence": "As expected, the overall accuracy of identify- ing contingency and expansion relations is lower, Task All relations Explicit relations only Comparison 91.28% (76.54%) 97.23% (69.72%) Contingency 84.44% (76.81%) 93.99% (79.73%) Temporal 94.79% (86.54%) 95.4% (79.98%) Expansion 77.51% (55.67%) 97.61% (65.16%) Table 2: Decision tree CA us- ing only the presence of connectives as binary fea- tures.", "acronym": "CA", "ID": "381"}, {"sentence": "On explicit data only, the two-way CA for the four main types of relations is 94% and higher.", "acronym": "CA", "ID": "382"}, {"sentence": "Over explicit data only, the CA for comparison relation versus any other relation is 97.23%, and precision and recall is 0.95 and above.", "acronym": "CA", "ID": "383"}, {"sentence": "The results are reported in Table 4 in terms of average CA.", "acronym": "CA", "ID": "384"}, {"sentence": "In four-way classification, disambiguating be- tween the four main semantic types of discourse relations leads to 74.74% CA.", "acronym": "CA", "ID": "385"}, {"sentence": "ones, we evaluated its CA on a (new) test set of size 300, split evenly between sen- tences of sequence length 24 and sequence length 1.", "acronym": "CA", "ID": "386"}, {"sentence": "The user?s CAion set Acomu consisted of 11 actions such as saying the color of a light (e.g., ?", "acronym": "CA", "ID": "387"}, {"sentence": "This is an important re- sult because past work that has applied POMDPs to dialog systems has employed a single modality (CAions), and have largely had fixed persistent state.", "acronym": "CA", "ID": "388"}, {"sentence": "The model of the user?s CAion assumes that the user provides correct (but possibly incomplete) informa- tion with p = 0.9, and remains silent with p = 0.1.", "acronym": "CA", "ID": "389"}, {"sentence": "In the experiments below, the value of perr is varied to explore how the POMDP policy trades off between the ping action and CAions.", "acronym": "CA", "ID": "390"}, {"sentence": "Second, the user?s action au is decomposed into two components: atsu denotes troubleshooting actions that are directed toward the product, such as turning a modem on or off, entering a user name or just observing the status lights; and acomu denotes CAions to the dialog system such as saying ?", "acronym": "CA", "ID": "391"}, {"sentence": "As perr increases, the policy decreasingly employs the ping diagnostic action in favor of the ask-working-ok CAion until perr = 20%, at which point the ping action is 84 85 86 87 88 89 90 91 92 93 94 0% 5% 10 % 15 % 20 % 25 % 30 % 35 % 40 % 45 % 50 % p err  (ping error rate) Av e ra ge   re tu rn Figure 4: Error rate of the ping action vs. reward gained per dialog.", "acronym": "CA", "ID": "392"}, {"sentence": "For the purpose of evaluation of CA, we developed an approach similar to (Boros et al, 1996) in which computing CA is reduced to com- paring strings representing core contentful concepts.", "acronym": "CA", "ID": "393"}, {"sentence": "The Alpino system achieves a CA of around 90% on common Dutch corpora (Van Noord, 2007).", "acronym": "CA", "ID": "394"}, {"sentence": "4.1 ASR and CA We evaluated overall word, sentence, and con-cept accuracy for all 8,228 spoken utterances to the system, shown in the first row of Table 1.", "acronym": "CA", "ID": "395"}, {"sentence": "Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 29?30, Los Angeles, CA, June 2010.", "acronym": "CA", "ID": "396"}, {"sentence": "Opinion texts \u0001 System by University of Southern CA (Kim and Hovy 2005) 16.", "acronym": "CA", "ID": "397"}, {"sentence": "In Proceedings, Tenth An-  nual IEEE Symposium on Logic in Computer Science,  pages 464-473, San Diego, CA, 26-29 June.", "acronym": "CA", "ID": "398"}, {"sentence": "In Proceedings of the 34th Annual Meeting of the  Association for Computational Linguistics, pages  184-191, CA, USA.,", "acronym": "CA", "ID": "399"}, {"sentence": "An  Effect of Combining CA with  ATNG-based Analysis  The next section shows one application of the  LUTE-E J  parser, which is a machine translation  system.", "acronym": "CA", "ID": "400"}, {"sentence": "A Merit  of Using CA  In two sentences, each having different syntactic  structures, there is a problem involved in identifying  each case by extracting semantic relations between a  predicate  and arguments  (NPs, or NPs  hav ing   prepositional marks).", "acronym": "CA", "ID": "401"}, {"sentence": "Comlmtational  561   Corpus-based NLP    A CA Method Cooperating with ATNG  and Its Application to Machine Translation  Hitoshi IIDA, Kentaro OGURA and Hirosato NOMURA  Musashino Electrical Communication Laboratory, N.T.T.  Musashino-shi, Tokyo, 180, Japan  Abstract  This paper present a new method for parsing  English sentences.", "acronym": "CA", "ID": "402"}, {"sentence": "L.._ CA \\[  *case-frame*  <*agent> J  I <*object> I  __~ STRUCTU~D-CONSTITUZNT-BUFFER I  ~ CA \\[ \\]  Fig.1 Conceptual Diagram of LUTE-EJ Analysis  analysis of i NOUN Phrase  ATNG-based analysis  process  (embedded clause,  noun clause  I. I  2.5.", "acronym": "CA", "ID": "403"}, {"sentence": "General category constraints ensure that the preposition can attach to nouns and verbs, but not, say, to a determiner or to PU.", "acronym": "PU", "ID": "404"}, {"sentence": "Examples 1 and 2 show the representation that would be obtained for two imaginary English sen- 2Clause delimiters are PU marks other than com- mata, relative pronouns and subordinating conjunctions.", "acronym": "PU", "ID": "405"}, {"sentence": "Batista et al (2007) inserted a module for re- covering PU marks, based on maximum entropy models, after the ASR module.", "acronym": "PU", "ID": "406"}, {"sentence": "We propose the following guidelines for segmentation (for a more complete discussion see our other article in this volume): \u0000 tokens do not contain white space; \u0000 tokens either are PU marks or do not contain any PU marks; \u0000 an exception to the previous guideline are certain words containing the hyphen (e.g., mass-media, s-ka = an abbreviation of sp?\u0007ka ?", "acronym": "PU", "ID": "407"}, {"sentence": "n the way c appears in A and B. c' = 0 if c ap-  pears in neither A or B; d = 1 if c appears in both  A and B; d = 2 if c appears in A and not in B;  and d = 3 if c appears not in A but in B. We con-  sider clue expressions from the following grammat-  ical classes: nominals, adjectives, demonstratives,  adverbs, sentence connectives, verbs, sentence-final  particles, topic-marking particles, and PU  marks.", "acronym": "PU", "ID": "408"}, {"sentence": "But since the length of an ZC.Ad/2 f~ string  consisting of a fixed munber of units is l)ounded  each PUable string could be PUed up such  that it exceeds this bound.", "acronym": "PU", "ID": "409"}, {"sentence": "FP-2 When used in external subcutaneous infu- sion PUs for insulin, NovoLog should not be mixed with any other insulins or diluent.", "acronym": "PU", "ID": "410"}, {"sentence": "In How many X questions  (where X is a noun), quantified phrases whose head  noun is also X are ranked above bare numbers or  other quantified phrases: for example, in the query  How many lives were lost in the Lockerbie air crash,  entities such as 270 lives or almost 300 lives would  be ranked above entities such as 200 PUkins or  150.", "acronym": "PU", "ID": "411"}, {"sentence": "A hm, g'aage L is f in i te ly   PUab le  'ill there is a constant c such th, at  for any w C L with, \\['w\\[ > c, there arc a finite  number k and strings uo , . . .", "acronym": "PU", "ID": "412"}, {"sentence": "The system  receives a score of 1, 1/2, 1/3, 1/4, 1/5, or 0, re-  2perhaps less desirably, people would not be recognized  as a synonym of lives in this example: 200 people would be  indistinguishable from 200 PUkins.", "acronym": "PU", "ID": "413"}, {"sentence": "2.3 Pmnpabi l i ty  and Semi l inear i ty   We will first consider the prol)erty of being  finitely PUabh,, its detined in (Oroenink, 1997).", "acronym": "PU", "ID": "414"}, {"sentence": "Hence the number of  units cannot be increased by PUing and all  PUable parts must consist of case markers  solely?", "acronym": "PU", "ID": "415"}, {"sentence": "that occur 40 times or fewer into the low oc-  curring class, disallowing nodes to be split if  they have 50 or fewer datapoints, and pruning  back nodes that give the smallest improvement  in node imPU.", "acronym": "PU", "ID": "416"}, {"sentence": ", goodwill, belong, accommodate, serve, merit, deserve, shine, radiate, glow, beam, disillusion, disenchant, proclaim, laud, glorify, extol, exalt, cheer, consider, purify, enervate, recuperate, amusingly, dearly, dear, affectionately, thoroughly, soundly, well, simply, time, posterboard, fettle, mildness, clemency, successfulness, prosper- ity, wellbeing, well-being, upbeat, wholeness, haleness, PU, pureness, innocence, antithesis, serendipity, superordinate, superior, possible, pleaser, idolizer, idoliser, amoralist Negative tawdry, shoddy, cheapjack, scrimy, unsound, unfit, bad, sorry, sad, pitiful, lamentable, distressing, de- plorable, abject, unfortunate, inauspicious, humbug, trouble, inconvenience, disoblige, bother, smell, stink, reek, twinge, sting, prick, burn, sting, burn,", "acronym": "PU", "ID": "417"}, {"sentence": "Our model achieves a cluster PU score of 90.3% on this dataset com- pared to 89.7% reported in Grenager and Manning.", "acronym": "PU", "ID": "418"}, {"sentence": "Our method increases the PU of the induced role clus- ters by a wide margin over a strong baseline.", "acronym": "PU", "ID": "419"}, {"sentence": "PU: it has been created only as an anal- ysis of English, and has not been compro- mised by publishing constraints or other non- lexicographic goals ?", "acronym": "PU", "ID": "420"}, {"sentence": "We report cluster PU, accuracy, precision, recall, and F1 for our latent variable logistic classifier (LogLV) and a baseline that assigns arguments to clusters accord- ing to their syntactic function (SyntFunc).", "acronym": "PU", "ID": "421"}, {"sentence": "However, in other cases there is no explicit indication of the direction of offset from the TF.", "acronym": "TF", "ID": "422"}, {"sentence": "Consider, for example, ex- pressions like the following: (1) three days ago (2) last Monday (3) in two weeks time Once we know the TF, calculation of the temporal location referred to in each of these cases is straightforward, since the", "acronym": "TF", "ID": "423"}, {"sentence": "In some cases, it is sufficient to determine what is sometimes called the TF, so that the precise location of a relative temporal expression on a timeline can be determined with respect to this ?", "acronym": "TF", "ID": "424"}, {"sentence": "As part of the overall process, they use a heuristic for the interpretation of weekday names: if the day name in a clause is the same as that of the TF, then the TF is used; 4 otherwise, they look for any ?", "acronym": "TF", "ID": "425"}, {"sentence": "Consider, for example, ex- pressions like the following: (1) three days ago (2) last Monday (3) in two weeks time Once we know the TF, calculation of the temporal location referred to in each of these cases is straightforward, since the temporal ex- pressions themselves explicitly indicate what we c ?", "acronym": "TF", "ID": "426"}, {"sentence": "We will not explicitly address the question of deter- mining the TF: although this is clearly a key ingredient, we have found that using the doc- ument creation date performs well for the kinds of documents (typically newswire stories and similar document types) we are working with.", "acronym": "TF", "ID": "427"}, {"sentence": "More so- phisticated strategies for TF tracking would likely be required in other genres.", "acronym": "TF", "ID": "428"}, {"sentence": "They report Ac1 figures of 57.7% for TFy and 65.3% for user frequency, which counts the number of distinct users in a cell using a given term and is intended to offset bias resulting from users who upload a large batch of similar photos at a given location.", "acronym": "TF", "ID": "429"}, {"sentence": "Then, for the English term tE, translated contextual vector cvtrJ (tE) is constructed as below: each English sen- tence sE which contains tE is translated into Japanese sentence strJ , then the TFy vectors5 v(strJ ) of Japanese translation s tr J are 5In the TFy vectores, compound terms are restricted to be up to five words long both for English and Japanese.", "acronym": "TF", "ID": "430"}, {"sentence": "The next module compares the functions of dq against the functions of d. To make this comparison we have divided the module into three sub-modules: (a) Pre-processing: line breaks, tabs and spaces re- moval as well as case folding; (b) Features extrac- tion: character n-grams extraction, weighting based on normalized TFy (tf ); and (c) Compar- ison: a cosine similarity estimation.", "acronym": "TF", "ID": "431"}, {"sentence": "Based on the results of our previous study, this paper further examines the correlation of TFy and the reliability of bilingual term correspondences estimated from bilingual news articles.", "acronym": "TF", "ID": "432"}, {"sentence": "Since we represent word to- kens rather than word types in the cohesion graph, we do not need to model the TFy tf separately, instead we set salience to the log value of the inverse document frequency idf : salience(t i ) = log |D| |{d : t i ?", "acronym": "TF", "ID": "433"}, {"sentence": "Our TFy figure of 65.0% significantly beats theirs, but we found that user frequency actually degraded our dev set results by 5%.", "acronym": "TF", "ID": "434"}, {"sentence": "1143-  1146  Yamamoto, M. and Church, K.W. (1998) Using Suffix  Arrays to Compare TF and  Document Frequency for All Substrings in Corpus.", "acronym": "TF", "ID": "435"}, {"sentence": "Yamamoto and Church TF and Document Frequency for All Substrings  computed over the classes rather than over the substrings, which would be prohibitive.", "acronym": "TF", "ID": "436"}, {"sentence": "As illustrated in the  figure, s\\[i = 16\\] is the first suffix to start with the term \"to_be\" and s~ = 17\\] is the last  Yamamoto and Church TF and Document Frequency for All Substrings  Input  corpus :  \" to_be_or_not_ to_be\"   Position:  Characters:  Initialized  Suffix Array  s\\[O\\] 0  s\\[l\\] 1  s \\ [2\\ ]  2  s \\ [3\\ ]  3  s\\ [13\\]  13  s\\[18\\]  14  s \\[15\\] 15  s \\[16\\] 15  s\\ [17\\]  17  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 I t lo l_ lb le l_ lo l r l_ ln lo l t l_ l t lo l_ lb le l  .~\"  : : : : \"\"{ i i i !", "acronym": "TF", "ID": "437"}, {"sentence": "TF: wij is set to tfij , the num- ber of occurrences of lemi in the context cj .", "acronym": "TF", "ID": "438"}, {"sentence": "Section 2 describes the algorithms  and the code that were used to compute term frequencies and document frequencies  Yamamoto and Church TF and Document Frequency for All Substrings  for all substrings in two large corpora, an English corpus of 50 million words of the  Wall Street Journal, and a Japanese corpus of 216 million characters of the Mainichi  Shimbun.", "acronym": "TF", "ID": "439"}, {"sentence": "3.2 Phrasal TFing je invoque le Re`glement I orderofpointon arise PRP NNNNDTVBP IN IN NPNPNP PP NP PP VP S Figure 2: Phrasal Translation Example Since P alignments often align phrasal transla- Phrasal Filter Off Phrasal Filter On Alignment Type S S \u0000 P P S S \u0000 P P Head Crossings 0.236 4.790 5.284 0.172 2.772 2.492 Modifier Crossings 0.056 0.880 0.988 0.048 0.516 0.362 Phrasal Translations ? ?", "acronym": "TF", "ID": "440"}, {"sentence": "4.2 TFing  The direct translation process assumes that the re- trieved search-result pages of a term exactly contain  snippets from a certain region (e.g. Hong Kong) and  written in the target language (e.g. traditional Chi- nese).", "acronym": "TF", "ID": "441"}, {"sentence": "They report Ac1 figures of 57.7% for TF and 65.3% for user frequency, which counts the number of distinct users in a cell using a given term and is intended to offset bias resulting from users who upload a large batch of similar photos at a given location.", "acronym": "TF", "ID": "442"}, {"sentence": "Then, for the English term tE, translated contextual vector cvtrJ (tE) is constructed as below: each English sen- tence sE which contains tE is translated into Japanese sentence strJ , then the TF vectors5 v(strJ ) of Japanese translation s tr J are 5In the TF vectores, compound terms are restricted to be up to five words long both for English and Japanese.", "acronym": "TF", "ID": "443"}, {"sentence": "The next module compares the functions of dq against the functions of d. To make this comparison we have divided the module into three sub-modules: (a) Pre-processing: line breaks, tabs and spaces re- moval as well as case folding; (b) Features extrac- tion: character n-grams extraction, weighting based on normalized TF (tf ); and (c) Compar- ison: a cosine similarity estimation.", "acronym": "TF", "ID": "444"}, {"sentence": "Based on the results of our previous study, this paper further examines the correlation of TF and the reliability of bilingual term correspondences estimated from bilingual news articles.", "acronym": "TF", "ID": "445"}, {"sentence": "Since we represent word to- kens rather than word types in the cohesion graph, we do not need to model the TF tf separately, instead we set salience to the log value of the inverse document frequency idf : salience(t i ) = log |D| |{d : t i ?", "acronym": "TF", "ID": "446"}, {"sentence": "Our TF figure of 65.0% significantly beats theirs, but we found that user frequency actually degraded our dev set results by 5%.", "acronym": "TF", "ID": "447"}, {"sentence": "The best TF re- sults were high (about 90% F score), but relied on hand-built event extraction systems.", "acronym": "TF", "ID": "448"}, {"sentence": "It has applications in text classification, TF, analysis of product review, analysis of responses to surveys, and mining online discussions.", "acronym": "TF", "ID": "449"}, {"sentence": "The percentage of nonrelevant texts in EJV is so low (approximately 5% in th e training corpus and 10% in the MUC-5 test sets) that a system can almost ignore the TF subtas k without suffering a serious degradation in performance; the system can be optimized in favor of generating tie-ups even when it is not sure there", "acronym": "TF", "ID": "450"}, {"sentence": "levant texts criterion, which figures in two of the dimensions, is based on the view tha t the more a system's performance would suffer as a consequence of ignoring the TF (document detection ) subtask, the harder the task .", "acronym": "TF", "ID": "451"}, {"sentence": "33 The percent nonrelevant texts criterion, which figures in two of the dimensions, is based on the view tha t the more a system's performance would suffer as a consequence of ignoring the TF (document detection ) subtask, the harder the task .", "acronym": "TF", "ID": "452"}, {"sentence": "The percentage of nonrelevant texts in EJV is so low (approximately 5% in th e training corpus and 10% in the MUC-5 test sets) that a system can almost ignore the TF subtas k without suffering a serious degradation in performance; the system can be optimized in favor of generating tie-ups even when it is not sure there is sufficient information in the text .", "acronym": "TF", "ID": "453"}, {"sentence": "This phenomenon is also known as the DOP hypothesis (Bod 1998), and has been confirmed for Tree-DOP on the ATIS, OVIS and WSJ treebanks (see Bod 1993, 1998, 1999, 2000a; Sima'an 1999; Bonnema et al 1997; Hoogweg 2000).", "acronym": "WSJ", "ID": "454"}, {"sentence": "Examples are \"is\" and \"has\", which  appear frequently in WSJ: these  verbs are not \"selective\" enough and the associ-  ated probability is not strong enough to rule out  165  erroneous candidates.", "acronym": "WSJ", "ID": "455"}, {"sentence": "Following this, we  show the results of applying this method to the  21-million-word 1987 WSJ cor-  pus using two different pronoun reference strate-  gies of varying sophistication, and evaluate their  performance using honorifics as reliable gender  indicators.", "acronym": "WSJ", "ID": "456"}, {"sentence": "DOP models have been shown to achieve state-of-the-art parsing performance on benchmarks such as the WSJ corpus (see Bod 2000a).", "acronym": "WSJ", "ID": "457"}, {"sentence": "This program differs  from earlier work in its almost complete lack of  hand-crafting, relying instead on a very small  corpus of Penn WSJ Tree-bank  text (Marcus et al, 1993) that has been marked  with co-reference information.", "acronym": "WSJ", "ID": "458"}, {"sentence": "This result is promising since Tree- DOP has been shown to obtain state-of-the-art performance on the WSJ corpus (see Bod 2000a).", "acronym": "WSJ", "ID": "459"}, {"sentence": "For example, Pan et al (2011)  hand-annotated of  a  portion  of  the  TIMEBANK  corpus that consisted of WSJ  arti- cles.", "acronym": "WSJ", "ID": "460"}, {"sentence": "3 Non-IF Pruning Through Non-Expert Annotations To prune the non-informative features, a tradi- tional approach would be to hire and train anno- tators to label which portion of each training sen- tence is informative or non-informative.", "acronym": "IF", "ID": "461"}, {"sentence": "4 Pruning Non-IFs for Sentiment Classification We conducted an experiment on sentiment classifi- cation in the domain of camera reviews to test the effect of pruning non-informative features based on AMT workers?", "acronym": "IF", "ID": "462"}, {"sentence": "4.4 IFs Next, we study the importance of individual features by measuring their chi-squared statistic with respect to the class variable.", "acronym": "IF", "ID": "463"}, {"sentence": "cted, NUM, solve, cars, pull, kinds, congress impacted C , solve C , cars C , NUM C , pool C , writing C , death C , link C - should, seems, comments should C , comments C V E R I F E X P + owed, consumed, saw, ex- pert, interesting, him, re- acted, refinance owed C , consumed C , expert C , reacted C , happened C , interesting C - impacted, wo impacted C , wo C , concern C , died C Table 5: Most IFs for UNI and UNI CCT 10 Unigrams with the largest weight (magnitude) with respect to each class ( + : positive weight / - : negative weight).", "acronym": "IF", "ID": "464"}, {"sentence": "Table 1: Classification Accuracy All Features IFs 41.7% 45.8% In this experiment, pruning the non-informative features improves the accuracy by more than 4%.", "acronym": "IF", "ID": "465"}, {"sentence": "The other approach is to learn from the IFs-Set derived from the sen- tences with the non-informative portion removed by the AMT workers.", "acronym": "IF", "ID": "466"}, {"sentence": "The probability of any node in the hi- erarchy is the product of the probabilities of that node and all of its ANCs, up to the root.", "acronym": "ANC", "ID": "467"}, {"sentence": "Previously, 600,000 years old  ANCs, called homo hudlabar [sic] in scientific  term, were supposed to be the most ancient  inhabitants of the region.", "acronym": "ANC", "ID": "468"}, {"sentence": "As lexicalization is a bottom-up process, for reading-off dom(am, an), it is sufficient to look at the lowest common ANC (LCA) of both an- chors; if the anchors cannot lexicalize the LCA, they won?t be able to lexicalize the constituents larger than LCA.", "acronym": "ANC", "ID": "469"}, {"sentence": "Types that are used indirectly are either ANC types to types that are used directly, or types that are used as the value of a feature in a constraint in the Matrix core types on a type that is used (directly or indirectly) by the Wambaya-specific portion of the grammar.", "acronym": "ANC", "ID": "470"}, {"sentence": "Some approach  attempts to locate bilingual text within a web  page (Jiang et al, 2009); some others attempt to  collect web pages in different languages and  decide the parallel relationship between the web  pages by means of structural cues, like exist- ence of a common ANC web page, similarity  between URLs, and similarity between the  HTML structures (Chen and Nie, 2000; Resnik                                                    1 This work has been done while the first author was visit- ing Microsoft Research Asia.", "acronym": "ANC", "ID": "471"}, {"sentence": "We then looked  at an ANC tree following the WordNet hypernym  relation.", "acronym": "ANC", "ID": "472"}, {"sentence": "These lan- guage models were estimated from the written por- tion of the ANC Second Re- lease (Ide and Suderman, 2004), which consists of approximately 20 million tokens, using Kneser and Ney (1995) smoothing.", "acronym": "ANC", "ID": "473"}, {"sentence": "Travel Guides was drawn from the Berlitz travel guides data in the Open ANC (Ide and Suder- man, 2004) and includes very verbose sentences 18 4 ?", "acronym": "ANC", "ID": "474"}, {"sentence": "2.2 Test Set The test set consists of 4806 instances of 50 target words: 20 verbs (1901 instances), 20 nouns (1908), and 10 adjectives (997).3 Instances are extracted from the Open ANC, being a mix of both written and spoken contexts of target words.4 Only 542 instances are assigned more than one sense by annotators, thus have graded senses.", "acronym": "ANC", "ID": "475"}, {"sentence": "tute Berkeley, California USA collinb@icsi.berkeley.edu Christiane Fellbaum Princeton University Princeton, New Jersey USA fellbaum@princeton.edu Rebecca Passonneau Columbia University New York, New York USA becky@cs.columbia.edu Abstract The Manually Annotated Sub-Corpus (MASC) project provides data and annota- tions to serve as the base for a community- wide annotation effort of a subset of the ANC.", "acronym": "ANC", "ID": "476"}, {"sentence": "The information in each lexical entry was compiled manually in a data-driven fashion by exploring its use in our corpora of reference, TimeBank and the ANC (Slate and NYTimes fragments).22 De Facto takes as input a document (or a set of them) and returns the factuality profiles of each event.", "acronym": "ANC", "ID": "477"}, {"sentence": "\\[25\\] Sows L, ConcSs : Information  Processing in Mind and Machine, Addison-Wesley,  Reading, Massachusetts, 1984  \\[25a\\] van Eynde, F., The Semantics of Tense and Aspect,  in: Proceedings of EAIA-90, 2rid Advanced School in  Artificial Intelligence.", "acronym": "ConcSs", "ID": "478"}, {"sentence": "Sowa John 1984 ConcSs : information  processing in mind and machine, Addison Wesley,  Reading, MA.", "acronym": "ConcSs", "ID": "479"}, {"sentence": "(Sowa 84)  J. Sowa, ConcSs - Information processing  in mind and machine, Addison Wesley Publishing Com-  pany, Reading, Mass., 1984.", "acronym": "ConcSs", "ID": "480"}, {"sentence": "L. Erlbaum Associates,  1977  Sowa, John F. ConcSs:  Information Processing in Mind and  Machine.", "acronym": "ConcSs", "ID": "481"}, {"sentence": "Sowa John 1984, ConcSs: informa-  tion processing in mind and machine , Addison  Wesley, Reading Mass.  Talmy L. 1985, Lexicalisation patterns: Semantic  structure in lexical forms, Language typology and  syntactic description, 3 , Cambridge University  Press, New York, p. 57-149.", "acronym": "ConcSs", "ID": "482"}, {"sentence": "ConcSs are structured configurations of conceptual units, which  are mental representations of certain aspects of the external world.", "acronym": "ConcSs", "ID": "483"}, {"sentence": "ConcSs are obtained from W through the process called word-to-concept map- ping.", "acronym": "ConcSs", "ID": "484"}, {"sentence": "395 446 412 ASL 18.0 20.3 18.7 Vocabulary Size 213 176 202 199 Running OOVs - 2.6k 44.3% 35.4% 32.1% 34.7% Running OOVs - 2", "acronym": "ASL", "ID": "485"}, {"sentence": "1666 1878 1761 ASL 8.3 10.4 8.8 Vocabulary Size 778 596 603 600 Singletons 618 417 395 Dev+Test Sentences 500 500 Running Words + Punct.", "acronym": "ASL", "ID": "486"}, {"sentence": "4161 4657 4362 ASL 8.3 9.3 8.7 Vocabulary Size 1457 1030 1055 1", "acronym": "ASL", "ID": "487"}, {"sentence": "22227 24808 23308 ASL 8.4 9.5 8.8 Vocabulary Size 4546 2605 2645 2642 Singletons 2728 1253 1211 reduced corpus Sentences 200 200 (200) Running Words + Punct.", "acronym": "ASL", "ID": "488"}, {"sentence": "4161 4657 4362 ASL 8.3 9.3 8.7 Vocabulary Size 1457 1030 1055 1052 Running OOVs - 2.6k 12.1% 5.2% 4.8% Running OOVs - 200 34.5% 27.6% 21.4% OOVs - 2.6k 32.7% 19.5% 19.7% OOVs - 200 76.2% 66.0% 66.8% External Test Sentences 22 22 Running Words + Punct.", "acronym": "ASL", "ID": "489"}, {"sentence": "395 446 412 ASL 18.0 20.3 18.7 Vocabulary Size 213 176 202 199 Running OOVs - 2.6k 44.3% 35.4% 32.1% 34.7% Running OOVs - 200 53.7% 44.6% 43.7% 47.3 % OOVs - 2.6k 61.5% 45.4% 44.0% 44.7% OOVs - 200 74.6% 63.1% 63.9% 64.8% Table 2: Statistics of the Serbian-English short phrases Serbian English Phrases original base forms original no article Entries 351 351 351 351 Running Words + Punct.", "acronym": "ASL", "ID": "490"}, {"sentence": "Number of Sentences 935  ASL 12  Range of Sentence Lengths 7-17  Correct Parse Present 96%  Correct Parse Most Likely 73%  Table 6: Results for Test Set A  P1 P  FRV2 Fr  SD Fr  IANYTI  Ti  JBVVN* :lJ  Table 5: Sample of grammatical  category mappings  Number of Sentences 1105  ASL 12  Range of Sentence Lengths 7-17  Correct Parse Present 95%  Correct Parse Most Likely 75%  Table 7:", "acronym": "ASL", "ID": "491"}, {"sentence": "Number of Sentences 935  ASL 12  Range of Sentence Lengths 7-17  Correct Parse Present 96%  Correct Parse Most Likely 73%  Table 6: Results for Test Set A  P1 P  FRV2 Fr  SD Fr  IANYTI  Ti  JBVVN* :lJ  Table 5: Sample of grammatical  category mappings  Number of Sentences 1105  ASL 12  Range of Sentence Lengths 7-17  Correct Parse Present 95%  Correct Parse Most Likely 75%  Table 7: Results for Test Set B  191  Recall (see above) that the geometric mean of the  number of parses per word, or equivalently the total num-  ber of parses for the entire test set, must be held con-  stant over the course of the grammar's development, to  eliminate trivial so", "acronym": "ASL", "ID": "492"}, {"sentence": "4161 4657 4362 ASL 8.3 9.3 8.7 Vocabulary Size 1457 1030 1055 1052 Running OOVs - 2.6k 12.1% 5.2% 4.8% Running OOVs - 200 34.5% 27.6% 21.4% OOVs - 2.6k 32.7% 19.5% 19.7% OOVs - 200 76.2% 66.0% 66.8% External Test Sentences 22 22 Running W", "acronym": "ASL", "ID": "493"}, {"sentence": "Spatial mapping in compara- tive discourse frames in an ASL  lecture.", "acronym": "ASL", "ID": "494"}, {"sentence": "A Survey and Critique of  ASL Natural Language Genera- tion and Machine Translation Systems.", "acronym": "ASL", "ID": "495"}, {"sentence": "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 761?769, Beijing, August 2010 Controlling Listening-oriented Dialogue using Partially Observable MDP Toyomi Meguro?,", "acronym": "MDP", "ID": "496"}, {"sentence": "Dialogue control algorithm for ambient intelligence based on partially observable MDPes.", "acronym": "MDP", "ID": "497"}, {"sentence": "The number of forward?BA calls for normal HMM is 40k (one for each sentence and EM iteration), for the symmetric model using ?", "acronym": "BA", "ID": "498"}, {"sentence": "\u0003f(x,zc ) (13) Hence the projection step uses the same inference algorithm (forward?BA for HMMs) to compute the gradient, only modifying the local factors using the current setting of ?.", "acronym": "BA", "ID": "499"}, {"sentence": "p (z, x) exp{??\u0005f(x, z)} All these quantities can be computed separately in each model using forward?BA and, furthermore, Z?", "acronym": "BA", "ID": "500"}, {"sentence": "For HMM word alignments, we need to make several calls to forward?BA in order to choose ?.", "acronym": "BA", "ID": "501"}, {"sentence": "We compare five differente models: ADD is the BA model with parameters ?", "acronym": "BA", "ID": "502"}, {"sentence": "References:  I. \"A Theory of Linguistic Models MEANING -- TEXT\", Moscow,  1972 (in RU).", "acronym": "RU", "ID": "503"}, {"sentence": "6 Future  D i rec t ions   The goal of my current research is to combine the  new alignment algorithm with a cognate identifica-  tion procedure, The alignment of cognates is possi-  8For example, stress regularly falls on the initial syllable  in Czech and on the penultimate syllable in Polish, while in  RU itcan fall anywhere in the word.", "acronym": "RU", "ID": "504"}, {"sentence": "Another example:  \"President and wife came to capital\"  (Articles and pronouns ere dropped to reflect RU),  This phrase is processed as  \"President roof-country with xhis wife came to capital Eof-  country\".", "acronym": "RU", "ID": "505"}, {"sentence": "In  the second one, \"v i r tua l  pronoun\" occas -   iona l ly  turned to be a rea l  one; whi le  in  Eng l i sh  i t s  use i s   qu i te  natura l ,  in  RU the use of  posess ive  pronoun would  have an emphatic meaning.", "acronym": "RU", "ID": "506"}, {"sentence": "One admirable standardization effort in the field of Slavic part of speech (POS) tagging has been the Multext-East project (Erjavec, 2001), one of whose aims was to construct mutually compati- ble tagsets for 8 European languages, including 4 Slavic languages (originally Bulgarian, Czech and Slovene, later extended to Croatian); additionally, a Multext-East-style tagset for RU was con- structed at the University of T?bingen (http: //www.sfb441.uni-tuebingen.de/c1/ tagset.html).", "acronym": "RU", "ID": "507"}, {"sentence": "that have D1 as an operand do not produce 2It is also necessary to show that no position set is merged in two different RU, but this easily follows from the fact that hD1?D2(X) = hD1?D2(Y ) if and only if X ?", "acronym": "RU", "ID": "508"}, {"sentence": "The binarization is 2-feasible if all of the involved RU are 2- feasible.", "acronym": "RU", "ID": "509"}, {"sentence": "A binarization of X is a sequence of RU resulting in a new collection with two or fewer position sets.", "acronym": "RU", "ID": "510"}, {"sentence": "as a sequence of RU, where each re- duction is characterized by a pair of position sets (X1, X2) which are merged into X1 ?", "acronym": "RU", "ID": "511"}, {"sentence": "if they merge two position sets differing only by D1, but in this case, one of the RU must merge D1 so it does not produce any reduction in ??.", "acronym": "RU", "ID": "512"}, {"sentence": "6 Evaluation Lexical LSA CAST GUITAR GUITAR Method Substitution Addition RU 0.595 0.527 0.530 0.640 F-score 0.420 0.348 0.347 0.441 Cosine Similarity 0.774 0.726 0.804 0.805 Main Topic Similarity 0.686 0.630 0.643 0.699 Table 3: Evaluation of the GUITAR improvement - summarization ratio: 15%.", "acronym": "RU", "ID": "513"}, {"sentence": "In this paper, we present a comparison of six summarizers as well as a meta-evaluation including eight measures: Precision/Recall, Percent Agree- ment, Kappa, RU, Relevance Correla- tion, and three types of Content-Based measures (cosine, longest common subsequence, and word overlap).", "acronym": "RU", "ID": "514"}, {"sentence": "We found that while all measures tend to rank summarizers in different orders, measures like Kappa, RU, Relevance Correlation and Content-Based each offer significant advantages over the more simplistic methods.", "acronym": "RU", "ID": "515"}, {"sentence": "Evaluation Lexical LSA CAST GUITAR GUITAR Method Substitution Addittion RU 0.645 0.618 0.626 0.678 F-score 0.557 0.522 0.524 0.573 Cosine Similarity 0.", "acronym": "RU", "ID": "516"}, {"sentence": "1 2 3 4 5] is represented as [1/5 2/4 3/4 5 Evaluation Lexical LSA Manual Manual Method Substitution Additition RU 0.595 0.573 0.662 F-score 0.420 0.410 0.489 Cosine Similarity 0.774 0.806 0.823 Main Topic Similarity 0.686 0.682 0.747 Table 1: Evaluation of the manual annotation improvement - summarization ratio: 15%.", "acronym": "RU", "ID": "517"}, {"sentence": "Evaluation Lexical LSA Manual Manual Method Substitution Addition RU 0.645 0.662 0.688 F-score 0.557 0.549 0.583 Cosine Similarity 0.863 0.878 0.886 Main Topic Similarity 0.836 0.829 0.866 Table 2: Evaluation of the manual annotation improvement - summarization ratio: 30%.", "acronym": "RU", "ID": "518"}, {"sentence": "Since the research described  herein we have thought of other influences on  AR and their statistical corre-  lates.", "acronym": "AR", "ID": "519"}, {"sentence": "We incorpo-  rate multiple AR factors into  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.", "acronym": "AR", "ID": "520"}, {"sentence": "Although this success rate  overstates the effect, it is a clear indication that  knowledge of a referent's gender and animatic-  ity is essential to AR.", "acronym": "AR", "ID": "521"}, {"sentence": "The more accurately the topic of a seg-  ment can be identified, the higher the success  rate we expect an AR system  can achieve.", "acronym": "AR", "ID": "522"}, {"sentence": "This indicates  that syntax does play a very important role in  AR.", "acronym": "AR", "ID": "523"}, {"sentence": "There are four possible operations (Right, Left,  SH and Reduce) for the configuration at hand.", "acronym": "SH", "ID": "524"}, {"sentence": "If the parsing operation                                                             1  To estimate the current operation (Left, Right, SH and  Reduce) by SVMs, we need to build 6 classifiers(Left-Right,  Left-SH, Left-Reduce, Right-SH, Right-Reduce and SH- Reduce).", "acronym": "SH", "ID": "525"}, {"sentence": "which may  depend on t, and t has a parent on its left side, the  parser removes t from the stack S.  SH: If there is no dependency between n and t,  and the triple does not satisfy the conditions for  Reduce, then push n onto the stack S.  In this work, we adopt SVMs for estimating the  word dependency attachments.", "acronym": "SH", "ID": "526"}, {"sentence": "from entity to related task: entity is a se- mantic argument of the task SH to an unrelated focus From the analysis of our WoZ data we get cer- tain intuitions about salient focus flow between some preceding dialogue and a FU Q. First of all, we learn that a dialogue context of just one previ- ous user question and one previous system answer generally provides enough information to resolve context-dependent FU Qs.", "acronym": "SH", "ID": "527"}, {"sentence": "For this reason, the perfor- mance of ALICE might be over-estimated in this evaluation; ALICE delivered much better results than SH?s method on this corpus.", "acronym": "SH", "ID": "528"}, {"sentence": "For more general acronym identification, we adapted the method of SH (2003).", "acronym": "SH", "ID": "529"}, {"sentence": "5 Const ra in ts ,  cho ices ,  and  SH   Given the target audience for this software,  the programming langage chosen is as close as  possible to the graphic interface.", "acronym": "SH", "ID": "530"}, {"sentence": "We apply the linear chain CRF (Lafferty et al, 2001), and show results using standard and softmax- margin CRF (SM-CRF) (Gimpel and Smith, 2010), with features consisting of word SH features, neighboring words, previous prediction and pre- fixes/suffixes.", "acronym": "SH", "ID": "531"}, {"sentence": "It is closely connected with the SH, pronunciation or meaning of Chi- nese characters.", "acronym": "SH", "ID": "532"}, {"sentence": "E TERRORIST KILLING OF ATTORNEY GENERAL ROBERTO GARCIA ALVARADO AND ACCUSED THE FMLN OF THE CRIME Preprocessor results : Node 0 : (SALVADORAN PRESIDENT-ELECT ALFREDO CRISTIANI ) Node 1 : CONDEMNED Node 2 : (THE TERRORIST) Node 3 : (KILLING) KILLIN G Node 4 : OF Node 5 : ((ATTORNEY GENERAL) ROBERTO GARCIA ALVARADO ) Node 6 : AND Node 7 : ACCUSE D Node 8 : (THE (FMLN) ) Node 9 : OF Node 10 : (THE CRIME ) Node 11 : The LINK parser LINK is a bottom-up, unification-based chart parser .", "acronym": "FMLN", "ID": "533"}, {"sentence": "33 score : 0 \"TERRORISTS \" :732 score : 1 \"FMLN\" ) (EVENT-LOCATION-OF : ?", "acronym": "FMLN", "ID": "534"}, {"sentence": "Here are the results produced by the LINK parser for sentences 1 and 2 : Sentence 1 : SALVADORAN PRESIDENT-ELECT ALFREDO CRISTIANI CONDEMNED TH E TERRORIST KILLING OF ATTORNEY GENERAL ROBERTO GARCIA ALVARADO AND ACCUSE D THE FMLN OF THE CRIME ((SENTENCE 1) actions ((ACTION-DESC (SEM-REF MURDER) ) (ACTOR (SEM-REF TERRORIST) (ACT-WORD (TERRORIST)) ) (OBJECT (SEM-REF GOVERNMENT-OFFICIAL) (ACT-WORD (ATTORNEY GENERAL) ) (NAME (ROBERTO GARCIA ALVARADO)))) ) Sentence 2 : LEGISLATIVE ASSEMBLY PRESIDENT RICARDO VALDIVIESO AND VIC E PRESIDENT-ELECT FRANCISCO MERINO ALSO DECLARED THAT THE DEA", "acronym": "FMLN", "ID": "535"}, {"sentence": "The initial chart is displayed, with potential noun phrases already grouped to- gether : Next sentence (1) : SALVADORAN PRESIDENT-ELECT ALFREDO CRISTIANI CONDEMNE D THE TERRORIST KILLING OF ATTORNEY GENERAL ROBERTO GARCIA ALVARADO AND ACCUSED THE FMLN OF THE CRIME Preprocessor results : Node 0 : (SALVADORAN PRESIDENT-ELECT ALFREDO CRISTIANI ) Node 1 : CONDEMNED Node 2 : (THE TERRORIST) Node 3 : (KILLING) KILLIN G Node 4 : OF Node 5 : ((ATTORNEY GENERAL) ROBERTO GARCIA ALVARADO ) Node 6 : AND Node 7 : ACCUSE D Node 8 : (THE (FMLN) ) Node 9 : OF Node 10 : (THE CRIME )", "acronym": "FMLN", "ID": "536"}, {"sentence": "This research is part of the Interactive sYstems IYAS (Iyas) project, which is developed by the Arabic Lan- guage Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), Hamad bin Khalifa University (HBKU), part of Qatar Founda- tion in collaboration with MIT-CSAIL.", "acronym": "IYAS", "ID": "537"}, {"sentence": "It is part of the Interactive sYstems IYAS (Iyas) project, which is developed in collaboration with MIT-CSAIL.", "acronym": "IYAS", "ID": "538"}, {"sentence": "Acknowledgments This research is part of the Interactive sYstems IYAS (Iyas) project, conducted by the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the Qatar Foundation.", "acronym": "IYAS", "ID": "539"}, {"sentence": "This research is part of the IYAS (Iyas) project, which is developed by the Arabic Lan- guage Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), Hamad bin Khalifa University (HBKU), part of Qatar Founda- tion in collaboration with MIT-CSAIL.", "acronym": "IYAS", "ID": "540"}, {"sentence": "It is part of the IYAS (Iyas) project, which is developed in collaboration with MIT-CSAIL.", "acronym": "IYAS", "ID": "541"}, {"sentence": "The limited utility of extra parses 1055 PP Clause Diff Mod NP 1-Word NP Corpus F-score Attach Attach Label Attach Attach Co-ord Span Unary aInt.a Other Best 0.022 0.016 0.013 0.011 0.011 0.010 0.009 0.006 0.005 0.021 WSJ 23 92.07 Brown-F 85.91 Brown-G 84.56 Brown-K 84.09 Brown-L 83.95 Brown-M 84.65 Brown-N 85.20 Brown-P 84.09 Brown-R 83.60 G-WBs 84.15 G-Web Email 81.18 Worst 0.040 0.035 0.053 0.020 0.034 0.023 0.046 0.009 0.029 0.073 Table 5: Average number of node errors per word for a range of domains using the Charniak parser with reranking and the self-trained model.", "acronym": "WB", "ID": "542"}, {"sentence": "Length WSJ 23 Newswire 2416 23.5 Brown F Popular 3164 23.4 Brown G Biographies 3279 25.5 Brown K General 3881 17.2 Brown L Mystery 3714 15.7 Brown M Science 881 16.6 Brown N Adventure 4415 16.0 Brown P Romance 3942 17.4 Brown R Humour 967 22.7 G-WBs Blogs 1016 23.6 G-Web Email E-mail 2450 11.9 Table 6: Variation in size and contents of the domains we consider.", "acronym": "WB", "ID": "543"}, {"sentence": "In other  words, the standard must be as easy to follow  as the convention of inserting blanks at  WBs in English text processing.", "acronym": "WB", "ID": "544"}, {"sentence": "To uti- lize the WB information provided by punctu- ations, we extract all strings with length l(2 ?", "acronym": "WB", "ID": "545"}, {"sentence": "This architecture differs from con-  ventional Japanese WBers in that it does  not attempt to simultaneously attack the problems  of identifying segmentation candidates and  choosing the most probable analysis.", "acronym": "WB", "ID": "546"}, {"sentence": "The preced- ing and succeeding strings of punctuations carry ad- ditional WB information, since punctuations should be segmented as a word.", "acronym": "WB", "ID": "547"}, {"sentence": "Since WBs  are not conventionally marked in Chinese text corpora, a  character-based collocation system has the dual  advantages of avoiding pre-proccssing distortion and  directly accessing sub-lexical information.", "acronym": "WB", "ID": "548"}, {"sentence": "4 Word Co-Occurrence Matrix Each word (BF) from the list was sought in the balanced, 300 million segments7 version of the National Corpus of Polish (NKJP).", "acronym": "BF", "ID": "549"}, {"sentence": "The lexical representation of a regular  adjective has an entry in the lexicon as follows:  i 300 buon bueno  where \"buen-\" is the stem and \"bueno\" (good) the  dictionary BF.", "acronym": "BF", "ID": "550"}, {"sentence": "Inflection We introduced tag suffixes for inflec- tion as clues to identify the attachment position of the verb and adjective phrases, because Japanese verbs and adjectives have inflections, which depends 110 (no label) BF cont continuative form attr attributive form neg negative form hyp hypothetical form imp imperative form stem stem Table 2: Inflection tag suffixes on their modifying words and phrases (e.g. noun and verb phrases).", "acronym": "BF", "ID": "551"}, {"sentence": "Each of about 55,000 BFs requires at  le~t one arc in the graph.", "acronym": "BF", "ID": "552"}, {"sentence": "As English is a language with fewer inflections when compared to Romanian, which accommodates for gender and case as a suffix to the BF of a word, the automatic translation into English is closer to a human translation (experiment three).", "acronym": "BF", "ID": "553"}, {"sentence": "All verbs, nouns and prepositions were first reduced to their BFs in order to reduce the parameter space.", "acronym": "BF", "ID": "554"}, {"sentence": "As in SBTDM-wide, in SBTDM-tall the lower BFs occur toward the root of the tree.", "acronym": "BF", "ID": "555"}, {"sentence": "An idea of the lingulstic ov-  erage is given by the equivalent BF, which is  199  D1%-13.1  VEP~B(prop) = NOUN(interr-indir-loc) REFLEX <GOVERNOR,> NOUN(subj)  ;; Features and Agreements  <GOVERNOR> (MOOD ind) (TENSE pres) (NUMBER .x) ....  NOUN-1 ....  I%EFLEX nil  NOUN-2  (NUMBER ..x) ....  DefKS KS-24.13  ;;Composition  TO-HAVE-SOURCE= MOUNT <JOLLY> <HEADER> RIVER  ; ;Meaning  (TO-HAVE-SOURCE ! *", "acronym": "BF", "ID": "556"}, {"sentence": "SBTDM-wide is a shallow tree in which the BF in- creases from the root downward in the sequence 3, 6, 6, 9, 9, 12, 12.", "acronym": "BF", "ID": "557"}, {"sentence": "Clearly, ECFG can generate all sets of deriva- tion trees that GDG can, while CFG cannot (because of the unbounded BF of ECFG and of GDG); ECFG can also generate all sets of deriva- tion trees that CFG can, while GDG cannot (because of the lexicalization requirement).", "acronym": "BF", "ID": "558"}, {"sentence": "6 Conclusion We have presented a generative string-rewriting system, Extended Context-Free Grammar, whose derivation trees are dependency trees with un- bounded BF.", "acronym": "BF", "ID": "559"}, {"sentence": "For a grammar with a decision tree of average  BF 5, a parse tree that uses 20 different pro-  ductions will have a priority of the order (0.2) 2?", "acronym": "BF", "ID": "560"}, {"sentence": "We achieve our best labeled BF-score using 28 signatures with an unknown threshold of five.", "acronym": "BF", "ID": "561"}, {"sentence": "As outlined in Section 6, the treebank guide- lines are somewhat ambiguous as to the appropriate BFor coordinate NPs which consist entirely of proper nouns.", "acronym": "BF", "ID": "562"}, {"sentence": "Using the labeled BFrom the out- put of both parsers causes XLE to always fail when parsing.", "acronym": "BF", "ID": "563"}, {"sentence": "and the Penn Treebank BFor this sen-  tence was:  ( ( (Theb igdog)  a te ) .", "acronym": "BF", "ID": "564"}, {"sentence": "7 Results There is a statistically significant improvement3 in labeled BF-score on Sec.", "acronym": "BF", "ID": "565"}, {"sentence": "Stage 2 processing is then  free to assign to the compound any BFor which it  3The design of this level of Lucy is influenced by Hobbs  (1985), which advocates a level of \"surfaey\" logical form  with predicates close to actual English words and a structure  similar to the syntactic structure of the sentence.", "acronym": "BF", "ID": "566"}, {"sentence": "c?2014 Association for Computational Linguistics CHISPA on the GO A mobile Chinese-Spanish translation service for travelers in trouble Jordi Centelles 1,2 , Marta R. Costa-juss ` a 1,2 and Rafael E. Banchs 2 1 UPC, Barcelona 2 Institute for Infocomm Research, Singapore {visjcs,vismrc,rembanchs}@i2r.a-star.edu.sg Abstract This demo showcases a translation service that allows travelers to have an easy and convenient access to Chinese-Spanish translations via a mo- bile app.", "acronym": "UPC", "ID": "567"}, {"sentence": "o and Jordi Turmo TALP Research Center UPC Barcelona, Spain {esapena, padro, turmo}@lsi.upc.edu Abstract This paper describes the participation of RelaxCor in the Semeval-2010 task number 1: ?", "acronym": "UPC", "ID": "568"}, {"sentence": "c?2015 Association for Computational Linguistics Low-Rank Regularization for Sparse Conjunctive Feature Spaces: An Application to Named Entity Classification Audi Primadhanty UPC primadhanty@cs.upc.edu Xavier Carreras Ariadna Quattoni Xerox Research Centre Europe xavier.carreras@xrce.xerox.com ariadna.quattoni@xrce.xerox.com Abstract Entity classification, like many other important problems in NLP, involves learning classifiers over sparse high- dimensional feature spaces that result from the conjunction of elementary fea- tures of the", "acronym": "UPC", "ID": "569"}, {"sentence": "e A. R. Fonollosa TALP Research Center UPC, Barcelona {marta.ruiz,jose.fonollosa}@upc.edu Abstract Neural Machine Translation (MT) has reached state-of-the-art results.", "acronym": "UPC", "ID": "570"}, {"sentence": "Semiautomatic creation of taxonomies Javier Farreres and Horacio Rodr?guez farreres@lsi.upc.es and horacio@lsi.upc.es Department of Computer Languages and Systems UPC de Catalunya Karina Gibert karina@eio.upc.es Department of Statistics and Operations Research UPC de Catalunya Abstract In this paper we face the automatic con- struction of a lexical taxonomy for the Spanish language using as input a taxon- omy of English (WordNet)1 and a set of bilingual (English/Spanish) resources.", "acronym": "UPC", "ID": "571"}, {"sentence": "160  A Quantitative Method for Machine Translation Evaluation    Jes?s Tom?s  Escola Polit?cnica Superior de  Gandia  UPC de  Val?ncia  jtomas@upv.es  Josep ?", "acronym": "UPC", "ID": "572"}, {"sentence": "ELiRF,  UPC de Val?ncia, Spain  prosso@dsic.upv.es  {fer.callotl,mmontesg,villasen}  @inaoep.mx      Abstract  This paper presents three methods to evaluate  the Semantic Textual Similarity (STS).", "acronym": "UPC", "ID": "573"}, {"sentence": "ngel Mas  Departament d?Idiomes  UPC de  Val?ncia  jamas@idm.upv.es  Francisco Casacuberta  Institut Tecnol?gic  d?Inform?tica  UPC de  Val?ncia  fcn@iti.upv.es    Abstract  Accurate evaluation of machine  translation (MT) is an open problem.", "acronym": "UPC", "ID": "574"}]