[{"sentence": "It has been sug- 878 Change test years BOW sLDA FWD SemTreeFWD CS 2008-2010 0.1015 0.0774 0.1079 0.1426 2011-2012 0.1663 0.1203 0.1664 0.1736 5 years 0.1274 0.0945 0.1313 0.1550 Information Technology 2008-2010 0.0580 0.0585 0.0701 0.0846 2011-2012 0.0894 0.0681 0.1076 0.1273 5 years 0.0705 0.0623 0.0851 0.1017 Telecommunication Services 2008-2010 0.1501 0.1615 0.1497 0.2409 2011-2012 0.2256 0.2084 0.2191 0.4009 5 years 0.1803 0.1803 0.1774 0.30", "label": "Consumer Staples", "ID": "1"}, {"sentence": "ples 2008-2010 0.1015 0.0774 0.1079 0.1426 2011-2012 0.1663 0.1203 0.1664 0.1736 5 years 0.1274 0.0945 0.1313 0.1550 Information Technology 2008-2010 0.0580 0.0585 0.0701 0.0846 2011-2012 0.0894 0.0681 0.1076 0.1273 5 years 0.0705 0.0623 0.0851 0.1017 Telecommunication Services 2008-2010 0.1501 0.1615 0.1497 0.2409 2011-2012 0.2256 0.2084 0.2191 0.4009 5 years 0.1803 0.1803 0.1774 0.3049 Polarity CS 2008-2010 0.0359 0.0383 0.0956 0.1054 2011-2012 0.0938 0.0270 0.1131 0.1285 5 years 0.0590 0.0338 0.1026 0.1147 p-value >>0.1000 0.0918 0.0489 Information Technology 2008-2010 0.0551 0.0332 0.0697 0.0763 2011-2012 0.0591 0.0516 0.0764 0.0857 5 years 0.0567 0.0405 0.0723 0.0801 p-value 0.0626 0.0948 0.0103 Telecommunication Services 2008-2010 0.0402 0.0464 0.0821 0.0745 2011-2012 0", "label": "Consumer Staples", "ID": "2"}, {"sentence": "Regarding coordination, the analysis reveals that different parsers show different biases with re- spect to CS.", "label": "coordination structures", "ID": "3"}, {"sentence": "under-produces the same structures as the first- order model, and that both models have specific problems in dealing with CS, specifically coordination of NPs containing PPs.", "label": "coordination structures", "ID": "4"}, {"sentence": "More generally, experiment 4 suggests that for the notoriously difficult problem of pars- ing CS, a hybrid approach that combines parse selection of n best analyses with pre-bracketed scope in the input results in a con- siderable reduction in error rate compared to each of these methods used in isolation.", "label": "coordination structures", "ID": "5"}, {"sentence": "Another motivation for using t-trees is that we believe that local tree contexts in t-trees carry more information relevant for correct lexical choice, compared to linear contexts in the surface sentence shapes, mainly because of long-distance dependencies and CS.", "label": "coordination structures", "ID": "6"}, {"sentence": "Yet we show that the second-order model under-produces the same structures as the first- order model, and that both models have specific problems in dealing with CS, specifically coordination of NPs containing PPs.", "label": "coordination structures", "ID": "7"}, {"sentence": "The remainder comprises mainly CS (for example, ?", "label": "coordination structures", "ID": "8"}, {"sentence": "From the con- stituent parse we extracted CS into a simplified data structure that captures each conjunction along with its conjuncts.", "label": "coordination structures", "ID": "9"}, {"sentence": "In Proceedings of the International Conference of Young CSs.", "label": "Computer Scientist", "ID": "10"}, {"sentence": "c i e n t i s t s  : \"  NATURAL SCIENTISTS AND MTHEblA'CICIANS  181 CSs  1812 Coquter Systems Anal yszs  Applications engineer  Engineering analyst  Programer engineering and scientific  Systcms engineer  Cb@pUier analp t  Cornput ifig-sys tems analyst  Computer-systems planning  Syst ws ancflyst, data processing  Systems andryst, computer sys tems  Systems engineer- 189,739  1819 CSs, Not Elsewhere Classified  Systems engiheen electronic data proc  Systems analyst 6usiness elect~onic d  Computer application,engineer  ~igital-co*hputer programmer  Electronic data programmer  !", "label": "Computer Scientist", "ID": "11"}, {"sentence": "In the International MultiConference of Engineers and CSs, volume 3, pages 2086?", "label": "Computer Scientist", "ID": "12"}, {"sentence": "Ann E. Robinson is a CS in the  Artificial Intelligence Center at SR I  International.", "label": "Computer Scientist", "ID": "13"}, {"sentence": "with AFIPS, OMB s t a t e d  t h a t  profess ional   or2anizgt isns  and o the r  i n t e r e s t ed  p a r t i e s  were provided an opportunity t o   comment on the  c l a s s i f i c a t i o n   me Manual i s e s t ruc tu red  he i r a r ch i ca l ly  and,.for example, proposes t he   fellowing categor ies  f o r  \"computer s c i e n t i s t s  : \"  NATURAL SCIENTISTS AND MTHEblA'CICIANS  181 CSs  1812 Coquter Systems Anal yszs  Applications engineer  Engineering analyst  Programer engineering and scientific  Systcms engineer  Cb@pUier analp t  Cornput ifig-sys tems analyst  Computer-systems planning  Syst ws ancflyst, data processing  Systems andryst, computer sys tems  Systems engineer- 189,739  1819 CSs, Not Elsewhere Classified  Systems engiheen elec", "label": "Computer Scientist", "ID": "14"}, {"sentence": "Second, it is not clear that vari- ous grammatical categories and their values have the same interpretation in each language; for ex- ample, it is rather surprising that only the Roma- nian tagset explicitly mentions strong and weak pronominal forms, it is not clear whether negative pronouns in Romanian, Slovene, CS and Bul- garian are negative in the same sense of participat- ing in Negative Concord, it is not clear why Roma- nian has negative adverbs while, say, CS lacks them, etc.", "label": "Czech", "ID": "15"}, {"sentence": "values have the same interpretation in each language; for ex- ample, it is rather surprising that only the Roma- nian tagset explicitly mentions strong and weak pronominal forms, it is not clear whether negative pronouns in Romanian, Slovene, CS and Bul- garian are negative in the same sense of participat- ing in Negative Concord, it is not clear why Roma- nian has negative adverbs while, say, CS lacks them, etc.", "label": "Czech", "ID": "16"}, {"sentence": "Such correspondences are not exceptional, e.g., the at least three masculine gen- ders of Polish (Man?czak, 1956; Saloni, 1976) are mapped into the single masculine gender of many other languages, the dual and the plural numbers of some languages (Slovene, CS) are mapped to plural of other languages, etc.", "label": "Czech", "ID": "17"}, {"sentence": "6 Future  D i rec t ions   The goal of my current research is to combine the  new alignment algorithm with a cognate identifica-  tion procedure, The alignment of cognates is possi-  8For example, stress regularly falls on the initial syllable  in CS and on the penultimate syllable in Polish, while in  Russian itcan fall anywhere in the word.", "label": "Czech", "ID": "18"}, {"sentence": "For example, the Multext-East tagset for CS assumes the following parts of speech: noun, verb, adjective, pronoun, adverb, adposi- tion, conjunction, numeral, interjection, resid- ual, abbreviation and particle.", "label": "Czech", "ID": "19"}, {"sentence": "One admirable standardization effort in the field of Slavic part of speech (POS) tagging has been the Multext-East project (Erjavec, 2001), one of whose aims was to construct mutually compati- ble tagsets for 8 European languages, including 4 Slavic languages (originally Bulgarian, CS and Slovene, later extended to Croatian); additionally, a Multext-East-style tagset for Russian was con- structed at the University of T?bingen (http: //www.sfb441.uni-tuebingen.de/c1/ tagset.html).", "label": "Czech", "ID": "20"}, {"sentence": "CSs have used LS- COLIN from a comparability point of view, to  analyze the visual modality in LSF: they studied  torso (Segouat, 2006) and facial (Ch?telat-Pel?,", "label": "Computer scientist", "ID": "21"}, {"sentence": "CSs need semantics for the purpose of natural language  processing.", "label": "Computer scientist", "ID": "22"}, {"sentence": "universal s, the analysis  of  discourse and text, computar technology can bc of help tc the l i n g u i s t ,  and,  in many subfields of computer science automated lnngua~e processing, the  deslgn of human/machme i ~ t e r f ~ c e s ,  the structuring of data bases,  linguistics  has much to offer the ccnnputer scientist, vet up until how, relatively few  such cross contributions have been made CSs have been slow  to discgvei the v d u e  of Ilnguistirs to their w o r ~ ,  the tine has come for  linguists t o  take the initiitivc and to  train themselves (and their students)  to hake use of and contribute to the field of computer science,  Speci?lized traltning in the us& of t he  corrputer with-ln a particular  discipline is not new Students i n  mary soclal sciences nok", "label": "Computer scientist", "ID": "23"}, {"sentence": "CSs will appreciate the cognate problem of extracting information from the web, and the economic riches associated with state-of-the-art text mining technologies.", "label": "Computer scientist", "ID": "24"}, {"sentence": "CSs can be taught methods for automatic text processing, leading to projects on text mining and chatbots.", "label": "Computer scientist", "ID": "25"}, {"sentence": "This convention  somewhat simplifies the statement of the satisfaction  3CSs may have met L KR in another guise.", "label": "Computer scientist", "ID": "26"}, {"sentence": "Introduction Development of large-scale grammars for natural languages is an active area of research in HLT.", "label": "human language technology", "ID": "27"}, {"sentence": "Survey of the state of  the art in HLT.", "label": "human language technology", "ID": "28"}, {"sentence": "1st Internal Conference on  HLT research, 1-5, San Di- ego, CA.", "label": "human language technology", "ID": "29"}, {"sentence": "Promising directions for future  application of HLT to  language tutors include incorporating continuous  speech recognition in a range of target languages  and continuing the development of dialog and  NLP-dnven animated graphics.", "label": "human language technology", "ID": "30"}, {"sentence": "Consequently, phenomena in- herent to natural languages may severely hamper the performance of HLT when applied to small collections.", "label": "human language technology", "ID": "31"}, {"sentence": "This paper describes a system that attempts to cope with semantic variability through the use of state of the art HLT.", "label": "human language technology", "ID": "32"}, {"sentence": "1156   HLT: The 2010 Annual Conference of the North American Chapter of the ACL, pages 939?947, Los Angeles, California, June 2010.", "label": "Human Language Technologies", "ID": "33"}, {"sentence": "In HLT 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Com- panion Volume, Short Papers, NAACL-Short ?", "label": "Human Language Technologies", "ID": "34"}, {"sentence": "In Pro- ceedings of HLT: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 101?109, Boulder, Colorado, June.", "label": "Human Language Technologies", "ID": "35"}, {"sentence": "In Pro- ceedings of HLT: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 172?180.", "label": "Human Language Technologies", "ID": "36"}, {"sentence": "38   HLT: The 2010 Annual Conference of the North American Chapter of the ACL, pages 813?821, Los Angeles, California, June 2010.", "label": "Human Language Technologies", "ID": "37"}, {"sentence": "In Proceed- ings of HLT: The 2009 Annual Conference of the North American Chap- ter of the Association for Computational Linguistics, NAACL ?", "label": "Human Language Technologies", "ID": "38"}, {"sentence": "In Proceedings of the HLT Conference and Conference on Empirical Methods in Natural Language Processing, pages 779?786, Vancouver.", "label": "Human Language Technology", "ID": "39"}, {"sentence": "In James Allan, editor, Proceedings of the 1st International Conference on HLT Research.", "label": "Human Language Technology", "ID": "40"}, {"sentence": "of the HLT / Empirical Meth- ods in Natural Language Processing Conference, pages 411?418.", "label": "Human Language Technology", "ID": "41"}, {"sentence": "of the Second International Confer- ence of HLT Research, pages 138?145, March.", "label": "Human Language Technology", "ID": "42"}, {"sentence": "In Proceedings of the HLT Conference of the NAACL, Main Conference, pages 160?167, June.", "label": "Human Language Technology", "ID": "43"}, {"sentence": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on HLT (NAACL), pages 48?54, Morristown, NJ.", "label": "Human Language Technology", "ID": "44"}, {"sentence": "For com- puting the counts of positive and negative words (Feature 15 and 16) we used the GI database (Stone et al.,", "label": "General Inquirer", "ID": "45"}, {"sentence": "An em- pirical evaluation was conducted on 3596 words ex- tracted from GI (Stone et al, 1966).", "label": "General Inquirer", "ID": "46"}, {"sentence": "The labeled dataset used as a gold standard is GI lexicon (Stone et al, 1966) as in the work by Turney and Littman (2003).", "label": "General Inquirer", "ID": "47"}, {"sentence": "The GI: A Computer Approach to Content Analysis.", "label": "General Inquirer", "ID": "48"}, {"sentence": "Largely missing from popular lexical resources such as WordNet and the GI (Stone et al, 1966) is stylistic information; there are, for instance, no resources which provide com- prehensive information about the formality level of words, which relates to the appropriateness of a word in a given context.", "label": "General Inquirer", "ID": "49"}, {"sentence": "Operative GI  and Radiology reports contain similar proportions  of historical conditions (9% and 6%).", "label": "Gastrointestinal", "ID": "50"}, {"sentence": "Echocardio- grams appear to be most similar to Radiology re- ports and Operative GI reports, which  may be supported by the fact that these reports are  used to document findings from tests conducted  during the current visit.", "label": "Gastrointestinal", "ID": "51"}, {"sentence": "Examples of report sections include Review of Sys- tems (Emergency Department), Findings (Opera- tive GI and Radiology) and  Discharge Diagnosis (Emergency Department and  Discharge Summary).", "label": "Gastrointestinal", "ID": "52"}, {"sentence": "One is that some outcomes do not have any cue word: (5) GI symptoms and headaches have been reported with both montelukast and zafirlukast.", "label": "Gastrointestinal", "ID": "53"}, {"sentence": "4 Methods  4.1 Dataset Generation  We randomly selected seven reports from each of  six genres of clinical reports dictated at the Univer- sity of Pittsburgh Medical Center during 2007  These included Discharge Summaries, Surgical  Pathology, Radiology, Echocardiograms, Opera- tive GI, and Emergency Department  reports.", "label": "Gastrointestinal", "ID": "54"}, {"sentence": "Operative GI and  Radiology reports showed the lowest prevalence of  both temporal expressions and trigger terms.", "label": "Gastrointestinal", "ID": "55"}, {"sentence": "Hybrid language mod- els for OOV word detection in large vocab- ulary conversational speech recognition.", "label": "out of vocabulary", "ID": "56"}, {"sentence": "However, in Equation (1),  we take into account OOV words and  normalization for some word-to-word similarity  metrics that may be larger than 1.", "label": "out of vocabulary", "ID": "57"}, {"sentence": "Effects of OOV words in spoken document retrieval.", "label": "out of vocabulary", "ID": "58"}, {"sentence": "Red words are OOV and all share a common embedding.", "label": "out of vocabulary", "ID": "59"}, {"sentence": "Leveraging a larger French-English model to translate OOV Haitian words, by creating a mapping from Haitian words onto French.", "label": "out of vocabulary", "ID": "60"}, {"sentence": "The perplexity measures did  not include OOV words since our recognition  system does not currently have the capability of detecting  these words.", "label": "out of vocabulary", "ID": "61"}, {"sentence": "c?2013 Association for Computational Linguistics Effect of OOV terms on inferring eligibility criteria for a  retrospective study in Hebrew EHR      Raphael Cohen*  Computer Science Dept.", "label": "Out Of Vocabulary", "ID": "62"}, {"sentence": "Semantic inherent features for OOV Words, be they nouns, verbs, adjec- tives or adverbs, are provided by a fully revised version of WordNet (Fellbaum, 1998) ?", "label": "Out of Vocabulary", "ID": "63"}, {"sentence": "These features rely on language models, MSA and Egyptian morphologi- cal analyzers and a Highly Dialectal Egyptian lex- icon to decide whether each word is MSA, Egyp- tian, Both, or OOV.", "label": "Out of Vocabulary", "ID": "64"}, {"sentence": "Semantic inherent features for OOV words , be they nouns, verbs, adjectives or adverbs, are provided by a fully revised version of WordNet ?", "label": "Out of Vocabulary", "ID": "65"}, {"sentence": "Even in very lim- ited domains and with a small vocabulary speech recognition is never 100% accurate, if only be- cause people may use OoD (Out of Domain) or OoV (OOV) words.", "label": "Out of Vocabulary", "ID": "66"}, {"sentence": "In particular, morphological analysis is a prerequisite in order to better cope with OOV Words(OOW) by means of guessing techniques based on morphological rules; statistical processing ?", "label": "Out of Vocabulary", "ID": "67"}, {"sentence": "OOV Words  One of the main reasons for the relatively high number of NA  answers to Class A utterances was simply vocabulary: Nineteen  of the test utterances (13% of the test set) contained vocabulary  outside Delphi's lexicon.", "label": "Out of Vocabulary", "ID": "68"}, {"sentence": "252  4250  A  +  I  B  +  I  C  +  I  D  +  I  3000 3000 3000 3000  Figure 1: The setup o f thematr ix fo r the  first SVD.", "label": "singular value decomposition", "ID": "69"}, {"sentence": "The mathematical basis for this transformation is SVD5; for the details of the matrix transformations, we refer the reader to the discussion of Turney and Littman (2003).", "label": "singular value decomposition", "ID": "70"}, {"sentence": "Latent Dirichlet al cation and SVD based multi- document summarization.", "label": "singular value decomposition", "ID": "71"}, {"sentence": "Text summarization and SVD.", "label": "singular value decomposition", "ID": "72"}, {"sentence": "e wonder assume feel say mean bet  angeles francisco sox rouge kong diego zone vegas inning layer  Oil   must  through in at over into with from for by across  we you i he she nobody who it everybody there they  might would could cannot will should can may does helps  500 features  500 features  500 features  500 features  A  B  C  D  22,771 words  Figure 2: The setup of the matrix for the second SVD.", "label": "singular value decomposition", "ID": "73"}, {"sentence": "252  4250  A  +  I  B  +  I  C  +  I  D  +  I  3000 3000 3000 3000  Figure 1: The setup o f thematr ix fo r the  first SVDn.", "label": "singular value decompositio", "ID": "74"}, {"sentence": "The mathematical basis for this transformation is SVDn5; for the details of the matrix transformations, we refer the reader to the discussion of Turney and Littman (2003).", "label": "singular value decompositio", "ID": "75"}, {"sentence": "Latent Dirichlet al cation and SVDn based multi- document summarization.", "label": "singular value decompositio", "ID": "76"}, {"sentence": "Text summarization and SVDn.", "label": "singular value decompositio", "ID": "77"}, {"sentence": "e wonder assume feel say mean bet  angeles francisco sox rouge kong diego zone vegas inning layer  Oil   must  through in at over into with from for by across  we you i he she nobody who it everybody there they  might would could cannot will should can may does helps  500 features  500 features  500 features  500 features  A  B  C  D  22,771 words  Figure 2: The setup of the matrix for the second SVDn.", "label": "singular value decompositio", "ID": "78"}, {"sentence": "This result indicates that it is not the words selected for the calculation of the proximities that matter, but the semantic relations in the spaces extracted from the word co-occurrences by the SVD.", "label": "Singular Value Decomposition", "ID": "79"}, {"sentence": "In or- der to reduce the noise and the data sparsity, we apply the SVD algo- rithm by reducing the original vector space into 300 dimensions.", "label": "Singular Value Decomposition", "ID": "80"}, {"sentence": "In general, the algo- rithms that incorporate both statistical feature se- lection and SVD lead to the best results, except for the Hungary data when no stoplist is used.", "label": "Singular Value Decomposition", "ID": "81"}, {"sentence": "The system is based on a com- bination of k-Nearest Neighbor classifiers, with each classifier learning from a distinct set of features: local features (syntactic, col- locations features), topical features (bag-of- words, domain information) and latent fea- tures learned from a reduced space using SVD.", "label": "Singular Value Decomposition", "ID": "82"}, {"sentence": "This frequency table undergoes a SVD that extracts the most important orthogonal dimensions, and, consequently, discards the small sources of variability in term usage.", "label": "Singular Value Decomposition", "ID": "83"}, {"sentence": "1990) is a technique for implicitly capturing the semantic properties of texts, based on the use of SVD to produce a rank- reduced approximation of an original matrix of word and document frequencies.", "label": "Singular Value Decomposition", "ID": "84"}, {"sentence": "J1 J2 J3 %agr \u000b %agr \u000b %agr \u000b J2 0.88 0.59 J3 0.98 0.91 0.90 0.67 GS 0.97 0.89 0.90 0.65 0.98 0.90 Table 2: AG for the unary/binary parameter: inter-judge (J1, J2, J3), and with GS J1 J2 J3 %agr \u000b %agr \u000b %agr \u000b J2 0.83 0.74 J3 0.88 0.80 0.80 0.68 GS 0.93 0.89 0.83 0.74 0.92 0.87 Table 3: AG for the basic/event/object pa- rameter: inter-judge (J1, J2, J3), and with GS As can be seen, the agreement among judges is remarkably high for a lexical semantics task: All but one values of the kappa", "label": "Agreement", "ID": "85"}, {"sentence": "AG: subject and object agreement in per- son and number (and to some extent gender) marked in the clitic cluster, agreement between nouns and adnominal modifiers in case, number and gender ?", "label": "Agreement", "ID": "86"}, {"sentence": "16 Sample Correct Incorrect N/A AG CLIP 519 sets 65.8% 31.7% 2.5% 76.1% CLIP?", "label": "Agreement", "ID": "87"}, {"sentence": "Human AG: After obtaining the human annotation results, we first study human consen- sus on the ordering task.", "label": "Agreement", "ID": "88"}, {"sentence": "J1 J2 J3 %agr \u000b %agr \u000b %agr \u000b J2 0.88 0.59 J3 0.98 0.91 0.90 0.67 GS 0.97 0.89 0.90 0.65 0.98 0.90 Table 2: AG for the unary/binary parameter: inter-judge (J1, J2, J3), and with GS J1 J2 J3 %agr \u000b %agr \u000b %agr \u000b J2 0.83 0.74 J3 0.88 0.80 0.80 0.68 GS 0.93 0.89 0.83 0.74 0.92 0.87 Table 3: AG for the basic/event/object pa- rameter: inter-judge (J1, J2, J3), and with GS As can be seen, the agreement among judges is remarkably high for a lexical semantics task: All but one values of the kappa statistics are above 0.6 (+/-0.13 for a 95% confidence interval).", "label": "Agreement", "ID": "89"}, {"sentence": "Measures: Given the human generated gold stan- dard of partial constraints, we use the follow- ing measures to evaluate the automatically gen- AgreedBy Cluster Constraint Order Constraint 1 37.14% 89.22% 2 46.95% 10.78% 3 15.92% 0.00% Table 7: Human AG on Ordering erated full ordering of aspects: (1) Cluster Pre- cision (prc): for all the aspect pairs placed in the same cluster by human, we calculate the per- centage of them that are also placed together in the system output. (", "label": "Agreement", "ID": "90"}, {"sentence": "c?2009 Association for Computational Linguistics Automatic AG Construction from Human-Human Dialogs  using Clustering Method    Cheongjae Lee, Sangkeun Jung, Kyungduk Kim, Gary Geunbae Lee  Department of Computer Science and Engineering  Pohang University of Science and Technology  Pohang, South Korea  {lcj80,hugman,getta,gblee}@postech.ac.kr         Abstract  Various knowledge sources are used for spo- ken dialog systems such as task model, do-", "label": "Agenda Graph", "ID": "91"}, {"sentence": "3 AG  In this section, we begin with a brief overview of  EBDM framework and agenda graph.", "label": "Agenda Graph", "ID": "92"}, {"sentence": "4 AG In this paper, agenda graph G is simply a way of encoding the domain-specific dialog control to com- plete the task.", "label": "Agenda Graph", "ID": "93"}, {"sentence": "HYPms are only extracted for the events  and their subjects and objects, not for the local  context words.", "label": "Hyperny", "ID": "94"}, {"sentence": "Relation Sample size Validation HYPmy of 419 synsets 44,1% Member of 379 synsets 24,3% Part of 290 synsets 24,8% Table 4: Automatic validation of triples idence on each combination of terms a ?", "label": "Hyperny", "ID": "95"}, {"sentence": "3.3 WordNet HYPms  Events with the same hypernyms may have simi- lar durations.", "label": "Hyperny", "ID": "96"}, {"sentence": "Automatic Construction of a HYPm-labeled Noun Hierarchy from Text.", "label": "Hyperny", "ID": "97"}, {"sentence": "HYPm of Part of Member of Term-based triples 62,591 2,805 5,929 1st Mapped 27,750 1,460 3,962 Same synset 233 5 12 Already present 3,970 40 167 Semi-mapped triples 7,952 262 357 2nd Mapped 88 1 0 Could be inferred 50 0 0 Already present 13 0 0 Synset-based triples 23,572 1,416 3,783 Table 2: Results of triples mapping A small sample of this problem can be observed in Figure 1.", "label": "Hyperny", "ID": "98"}, {"sentence": "Then, for each triple (A R B), the patterns were used to search for ev- 8HYPmy patterns included: [hypo] e?", "label": "Hyperny", "ID": "99"}, {"sentence": "Then, UC HYP the actual information needs of the user by consulting the user model and applying goal analysis.", "label": "hypothesized", "ID": "100"}, {"sentence": "We HYP that coreferencing entities in the same  genre domain can be considered to be harder in terms  of achieving high precision because the consistency of  the contents between documents in the same genre  domain makes it significantly harder to create a unique  model for each entity to aid the task of distinguishing  one entity from another.", "label": "hypothesized", "ID": "101"}, {"sentence": "For each entity mention e in the evaluation set, we  first locate the truth chain TC that contains that mention  (it can be in only one truth chain) and the system?s  HYP chain HC that contains it (again, there can  Chung Heong Gooi and James Allan  Center for Intelligent Information Retrieval  Department of Computer Science  University of Massachusetts  Amherst, MA 01003  {cgooi,allan}@cs.umass.edu  be only one hypothesis chain).", "label": "hypothesized", "ID": "102"}, {"sentence": "Evaluation then  proceeds by comparing the true chains to the system?s  HYP chains.", "label": "hypothesized", "ID": "103"}, {"sentence": "We HYP that  using a highly precise set of patterns along with  precise lexicon should enable a promising IE  performance.", "label": "hypothesized", "ID": "104"}, {"sentence": "by comparing the true chains to the system?s  HYP chains.", "label": "hypothesized", "ID": "105"}, {"sentence": "For each entity mention e in the evaluation set, we  first locate the truth chain TC that contains that mention  (it can be in only one truth chain) and the system?s  HYP chain HC that contains it (again,", "label": "hypothesized", "ID": "106"}, {"sentence": "True validation of the Matrix qua HYP linguistic universals re- quires many more such case studies, but this first test is promising.", "label": "hypothesized", "ID": "107"}, {"sentence": "To recognize that HYP 1 is entailed by the text, a human reader must recognize that ?", "label": "Hypothesis", "ID": "108"}, {"sentence": "2 Classification and HYP As mentioned above, the semantic classification of adjectives is not settled in theoretical linguistics.", "label": "Hypothesis", "ID": "109"}, {"sentence": "Samiian, V. (1991) Prepositions in Persian and  the Neutralization HYP.", "label": "Hypothesis", "ID": "110"}, {"sentence": "in the HYP can match ?", "label": "Hypothesis", "ID": "111"}, {"sentence": "In the example shown in figure 1, this means recognizing that the Text entails HYP 1, while HYP 2 con- tradicts the Text.", "label": "Hypothesis", "ID": "112"}, {"sentence": "Test Set HYP WSJ BROWN Greater Lesser F1 UAS LAS F1 UAS LAS phrase+deps phrase .042 .029 .018 .140 .022 .009 phrase+deps deps ?", "label": "Hypothesis", "ID": "113"}, {"sentence": "To recognize that HYP 2 contradicts the Text, similar steps are required, together with the inference that because the stated purchase pr", "label": "Hypothesis", "ID": "114"}, {"sentence": "We  would like to know, therefore, whether the pat-  tern of pronoun references that we observe for  a given referent is the result of our supposed  \"HYP about pronoun reference\" - that is,  the pronoun reference strategy we have provi-  sionally adopted in order to gather statistics -  or whether the result of some other unidentified  process.", "label": "hypothesis", "ID": "115"}, {"sentence": "The intuition behind this HYP is that if a certain suffix forms basic adjectives, they will behave like ordinary basic adjectives; similarly, if a derived ad- jective has undergone semantic change and as a re- sult has shifted class, it will also behav", "label": "hypothesis", "ID": "116"}, {"sentence": "One of the purposes of the paper is to test whether this HYP is right.", "label": "hypothesis", "ID": "117"}, {"sentence": "1 Introduction The main HYP underlying the tasks in Lex- ical Acquisition is that it is possible to infer lexi- cal properties from distributional evidence, taken as a generalisation of a word?s linguistic behaviour in corpora.", "label": "hypothesis", "ID": "118"}, {"sentence": "The likelihood ratio is adapted  from Dunning (1993, page 66) and uses the raw  frequencies of each pronoun class in the cor-  pus as the null HYP, Pr(gc0i) as well as  Pr(ref E gci) from equation 9.", "label": "hypothesis", "ID": "119"}, {"sentence": "This is based on the HYP that the author ?", "label": "hypothesis", "ID": "120"}, {"sentence": "Our HYP, which will be tested on Sec- tion 4.3, is that syntax is more reliable than mor- phology as a basis for semantic classification.", "label": "hypothesis", "ID": "121"}, {"sentence": "Rather, a more important MOT for a circumscribed domain is the need for clearly defined knowledge sources.", "label": "motivation", "ID": "122"}, {"sentence": "The reasons underlying these results are that the newspaper writing is naturally better planned than speech and that speech transcriptions are affected by the several problems described before (and the original MOT for the work), hence the idea of using them as background information.", "label": "motivation", "ID": "123"}, {"sentence": "The MOT for generalized gap scores arises  from the fact that in diachronic phonology not only  individual segments but also entire morphemes and  syllables are sometimes deleted.", "label": "motivation", "ID": "124"}, {"sentence": "With the MOT of parsing efficiency, much research has been recently devoted to the design of efficient algorithms for rank reduction, in cases in which this can be carried out at no extra increase in the fan-out. (", "label": "motivation", "ID": "125"}, {"sentence": "Their primary MOT is to classify  phonological oppositions rather than to reflect the  phonetic haracteristics of sounds.", "label": "motivation", "ID": "126"}, {"sentence": "The TD  admits that certain instructions may be incom-  patible with the definition of coreference but no  reason is given for these incompatibilities and  no intuitive MOT for the relation IDENT is  offered.", "label": "motivation", "ID": "127"}, {"sentence": "Rather, a more important MOTn for a circumscribed domain is the need for clearly defined knowledge sources.", "label": "motivatio", "ID": "128"}, {"sentence": "The reasons underlying these results are that the newspaper writing is naturally better planned than speech and that speech transcriptions are affected by the several problems described before (and the original MOTn for the work), hence the idea of using them as background information.", "label": "motivatio", "ID": "129"}, {"sentence": "The MOTn for generalized gap scores arises  from the fact that in diachronic phonology not only  individual segments but also entire morphemes and  syllables are sometimes deleted.", "label": "motivatio", "ID": "130"}, {"sentence": "With the MOTn of parsing efficiency, much research has been recently devoted to the design of efficient algorithms for rank reduction, in cases in which this can be carried out at no extra increase in the fan-out. (", "label": "motivatio", "ID": "131"}, {"sentence": "Their primary MOTn is to classify  phonological oppositions rather than to reflect the  phonetic haracteristics of sounds.", "label": "motivatio", "ID": "132"}, {"sentence": "The TD  admits that certain instructions may be incom-  patible with the definition of coreference but no  reason is given for these incompatibilities and  no intuitive MOTn for the relation IDENT is  offered.", "label": "motivatio", "ID": "133"}, {"sentence": "2 MOT PP attachment disambiguation has often been studied as a benchmark test for empirical meth- ods in natural language processing.", "label": "Motivation", "ID": "134"}, {"sentence": "3.3 Experiments 3.3.1 Experiment 1 MOT and Noise Model For our first exper- iment, we assume that the probability of arriving at some word w?", "label": "Motivation", "ID": "135"}, {"sentence": "MOT  We will begin by introducing structured prediction with various NLP examples.", "label": "Motivation", "ID": "136"}, {"sentence": "1 Introduction and MOT Automated verbal irony detection is a challenging problem.", "label": "Motivation", "ID": "137"}, {"sentence": "2 MOT  A trigram language model predicts the next word  based only on two preceding words, blindly dis- carding any other relevant word that may lie three  or more positions to the left.", "label": "Motivation", "ID": "138"}, {"sentence": "0.007246 0.009528 3.3.2 Experiment 2 MOT and Noise Model For our second ex- periment, we hypothesize that there is an inverse re- lationship between unobserved word frequency and random walk path probability.", "label": "Motivation", "ID": "139"}, {"sentence": "NOM Dep 1,811 83.3%??", "label": "Nominative", "ID": "140"}, {"sentence": "Sentence) (per Sentence) NAIST Text Corpus Training 1,751 24,283 664,898 (27.4) 68,602 (2.83) Development 480 4,833 136,585 (28.3) 13,852 (2.87) Test 696 9,284 255,624 (27.5) 26,309 (2.83) Chat Dialog Corpus Training 184 6,960 61,872 (8.9) 7,470 (1.07) Test 101 4,056 38,099 (9.4) 5,333 (1.31) Table 1: Sizes of Corpora Zero- Zero- Exophora Case Corpus # of Arguments Dep Intra Inter exo1 exo2 exog NOM NAIST 68,598 54.5% 17.3% 11.4% 2.0% 0.0% 14.7% Dialogue 7,467 31.8% 7.4% 12.6% 23.9% 5.6% 18.8% Accusative NAIST 27,986 89.2% 6.9% 3.4% 0.0% 0.0% 0.4% Dialogue 1,901 46.6% 12.8% 27.5% 0.8% 0.1% 12.2% Datative NAIST 6,893 84.7% 10.2% 4.3% 0.0% 0.0% 0.8% Dialogue 2,089 37.6% 7.8% 15.0% 2.5% 1.1% 36.1% Table 2: Distribution of Arguments in Training Corpora Table 1 shows the statistics of t", "label": "Nominative", "ID": "141"}, {"sentence": "Special Noun Phrases Candidate Argumentsin Past  Sentences Candidate Argumentsin Current Sentence Candidate Arguments SelectorNOMModel SelectorAccusativeModel SelectorDativeModel exo1exophoric(first person) zero-anaphoric(inter-sentential) Phrase 2 NULLno argument Figure 2: Structure of Argument Identification and Classification assigned to exo1 and exo2.", "label": "Nominative", "ID": "142"}, {"sentence": "lace/temporal/modal  SYNTB5 case/preposition case/preposition  SYNTB6 case/preposition case/preposition  SYNTB7 case/preposition case/preposition  SYNTB8 case/preposition case/preposition  SYNTB9 Gender Separable Prefix  SYNTB 10 Alternate Gender Reilexivity  SYNTC1 .. Inlpersomd Subject only  SYNTC2 Usage of Participle  SYNTC3 .o x  SIJItJECT x x  SOURCE x x  FREQ x x  FREQS x x  STYLE x x  XREF NOM Singular lmqnitive without ~-(e)n\",  without separat}lc prefix  XREF (cont'd) Plural Furm lml}erfect, Past Participle  'I'RI{NN x x  I)ATUM x x  141,F2,F3,F4,1;5 . . . .", "label": "Nominative", "ID": "143"}, {"sentence": "The first set (ANERGaz) pro- posed by (Benajiba and Rosso, 2008), which 80 Feature Feature Values Aspect Verb aspect: Command, Imperfective, Perfective, Not applicable Case Grammatical case: NOM, Accusative, Genitive, Not applicable, Undefined Gender Nominal Gender: Feminine, Masculine, Not applicable Mood Grammatical mood: Indicative, Jussive, Subjunctive, Not applicable, Undefined Number Grammatical number: Singular, Plural, Dual, Not applicable, Undefined Person Person Information: 1st, 2nd, 3rd, Not applicable State Grammatical state: Indefinite, Definite, Construct/Poss/Id", "label": "Nominative", "ID": "144"}, {"sentence": "each part of speech:  Different for each part of speech:  Different for each part of speech:  Different for each part of speech:  Obligatory Coml}lcments  Subject Area  Source: Userid of who coded; origin of entry  Frequency Count  Source of Frequency Cmmt  Level of Style  Stem for 1st Entry:  Secondary Stems:  11yphenation  Date of last Update  5 additional, not yet used fields  WORT Infinitive NOM  CONT # #  CAT N VERB  MORPII1 Declension code Sets of suffixes  MORPtI2 Alternative declension Syntax dud scope of  stems  SYNTA 1 .. Valency  SYNTA2-5 Preposition Preposition  SYNTB 1-4 place/temporal/modal place/temporal/modal  SYNTB5 case/preposition case/preposition  SYNTB6 case/preposition case/preposition  SYNTB7 case/preposition case/preposition  SYNTB8 case/preposition case/pre", "label": "Nominative", "ID": "145"}, {"sentence": "For each term candidate, we gener- ated a canonical form (NOM, singular), a  morphologically normalised form (list of normal- ised words comprising the term candidate) and a  list of nested term candidates (see Table 3 for  examples).", "label": "nominative", "ID": "146"}, {"sentence": "refers to the co- occurrence pattern between a verb and a noun 33 [Sentence pattern] <word1> ga <word2> wo taberu (eat) [Sense relation] agent object [Case particle] ga (NOM) wo (accusative) [Sense identifier] 30f6b0 (human);30f6bf (animal) 30f6bf(animal);30f6ca(plants); 30f6e5(parts of plants); 3f9639(food and drink); 3f963a(feed) Figure 1: An example of a verb ?", "label": "nominative", "ID": "147"}, {"sentence": "Sense id Pattern Synonyms 1 kare(he) ga(NOM) soba(noodles) wo(accusative) kuu (eat) 2 kare (he) ga(NOM) fukugyo(a part-time job) de(accusative) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense Verb Classes Id Sense Verb Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratu", "label": "nominative", "ID": "148"}, {"sentence": "Sense id Pattern Synonyms 1 kare(he) ga(NOM) soba(noodles) wo(accusative) kuu (eat) 2 kare (he) ga(NOM) fukugyo(a part-time job) de(accusative) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense Verb Classes Id Sense Verb Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4 ask {negau, tanomu} 14 accept {uketor", "label": "nominative", "ID": "149"}, {"sentence": "At the end of the ATR proc- ess, terms are converted into their canonical form  (singular, NOM case), which is not neces- sarily identical to the normalised form (the se- quence of the corresponding singular words in  singular, NOM case).", "label": "nominative", "ID": "150"}, {"sentence": "brown, edu  Abst ract   This paper presents an algorithm for identi-  fying proNOM anaphora and two experi-  ments based upon this algorithm.", "label": "nominal", "ID": "151"}, {"sentence": "The idea is similar to that used in  the centering approach (Brennan et al, 1987)  where a continued topic is the highest-ranked  candidate for proNOMization.", "label": "nominal", "ID": "152"}, {"sentence": "8 Conc lus t ion  and  Future  Research   We have presented a statistical method for  proNOM anaphora that achieves an accuracy  of 84.2%.", "label": "nominal", "ID": "153"}, {"sentence": "Two broad approaches for the iden- tification of story characters were followed: (i) named entity recognition, and (ii) identification of character NOMs, e.g., ?", "label": "nominal", "ID": "154"}, {"sentence": "The more fre-  quently an entity is repeated, the more likely it  is to be the topic of the story and thus to be  a candidate for proNOMization.", "label": "nominal", "ID": "155"}, {"sentence": "An  algorithm for proNOM anaphora resolu-  tion.", "label": "nominal", "ID": "156"}, {"sentence": "The asymmetry of these  PRs over sets should be represented in the  semantic representation, i  order to account for the  difference in truth conditions.", "label": "predication", "ID": "157"}, {"sentence": "However, given that Urdu employs produc- tive syntactic complex predicate formation for much of its verbal PR, the verb lexicon for Urdu will be smaller than its English counterpart.", "label": "predication", "ID": "158"}, {"sentence": "b. anjum nE dI saddaf kO [ciTTHI likHnE] c. anjum nE [ciTTHI likHnE] saddaf kO dI The manipulation of PRal structures in the lexicon via lexical rules (as is done for the English passive, for example), is therefore inadequate for complex PR.", "label": "predication", "ID": "159"}, {"sentence": "Examples are correlative clauses (these are an old Indo-European feature which most modern European languages have lost), extensive use of complex PR, and rampant pro-drop.", "label": "predication", "ID": "160"}, {"sentence": "in the sense that the labels of several of the elementary PRs (eps) are not related to any argument position of any other ep.", "label": "predication", "ID": "161"}, {"sentence": "Nev-  ertheless it can still give us guidance on which  candidates are more PR than others.", "label": "probable", "ID": "162"}, {"sentence": "In  other words, the nearer the end of the story a  pronoun occurs, the more PR it is that  its referent has been mentioned several times.", "label": "probable", "ID": "163"}, {"sentence": "2.3 Decoding Alignments are normally predicted using the Viterbi algorithm (which selects the single most PR path through the HMM?s lattice).", "label": "probable", "ID": "164"}, {"sentence": "Q(U k ) shows comparison of the actual val- ues of internal or external edges with its re- spective expectation value under the assump- tion of equally PR links and given data sizes.", "label": "probable", "ID": "165"}, {"sentence": "This is almost cer- tainly a misleading figure, since those two words do not form a plausible verb phrase; it is much more PR that the very strong, in fact id- iomatic, correlation ?", "label": "probable", "ID": "166"}, {"sentence": "It seems PR that the ef- fect of the 10K training corpus can be greatly  augmented by adding sentence pairs that have  been aligned from multiple translations using  the techniques described in, e.g., Barzilay &  McKeown (2001) and Pang et al (2003).", "label": "probable", "ID": "167"}, {"sentence": "is reset, and the same PR is repeated for the rest of the story.", "label": "process", "ID": "168"}, {"sentence": "We  would like to know, therefore, whether the pat-  tern of pronoun references that we observe for  a given referent is the result of our supposed  \"hypothesis about pronoun reference\" - that is,  the pronoun reference strategy we have provi-  sionally adopted in order to gather statistics -  or whether the result of some other unidentified  PR.", "label": "process", "ID": "169"}, {"sentence": "In the testing phase, we used a different subset with 80 adjectives as Gold Standard against which we could compare the clustering results (see Section 3.2 for details on the manual annotation PR).", "label": "process", "ID": "170"}, {"sentence": "In order to address this distant association issue, we examined the collapsed-ccPRed-dependencies output besides the basic-dependenciesout- put of the Stanford CoreNLP dependency engine (de Marneffe and Manning, 2012).", "label": "process", "ID": "171"}, {"sentence": "3.1 Linguistic PrePRing The first step is linguistic pre-PRing of the stories.", "label": "process", "ID": "172"}, {"sentence": "Transferring corefer- ence resolvers with PR.", "label": "posterior regularization", "ID": "173"}, {"sentence": "In most cases this meant four iterations for normal EM training and two iterations using PR.", "label": "posterior regularization", "ID": "174"}, {"sentence": "We see in Figure 10 that for both domains, the models trained using PR perform better than the baseline model trained using EM.", "label": "posterior regularization", "ID": "175"}, {"sentence": "The PR?trained models still performed better, but the differences get smaller after doing the symmetrization.", "label": "posterior regularization", "ID": "176"}, {"sentence": "Cross- lingual discriminative learning of sequence models with PR.", "label": "posterior regularization", "ID": "177"}, {"sentence": "4.3 Rare vs. Common Words One of the main benefits of using the PR constraints described is an alleviation of the garbage collector effect (Brown et al 1993a).", "label": "posterior regularization", "ID": "178"}, {"sentence": "Within Acquilex IP Project, a unification framework  based on typed feature structures \\[4\\] was ddveloped, the  LKB (Lexical Knowledge Base), in order to represent  conceptual units corresponding to lexieal senses, lexical  and PRs, multilingual rclalionships, elc.", "label": "phrasal rule", "ID": "179"}, {"sentence": "Rule Coverage Marcu et al (2006) showed that many useful PRs cannot be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al, 2006).", "label": "phrasal rule", "ID": "180"}, {"sentence": "In our experiments with the publicly available SVM system we used all except paraPRs extracted from bilingual corpora (Cohn and Lap- ata, 2008).", "label": "phrasal rule", "ID": "181"}, {"sentence": "Hierarchical rules were extracted from a subset which has about 35M/41M words5, and the rest of the training data were used to extract PRs as in (Och, 2003; Chiang, 2005).", "label": "phrasal rule", "ID": "182"}, {"sentence": "The original SHOGUN design integrated several different approaches b y combining different knowledge sources, such as syntax, semantics, PRs, and domain knowledge, a t run-time .", "label": "phrasal rule", "ID": "183"}, {"sentence": "For the example, after Skolemizing the existential quantifiers, this contains the ground atoms: {man(A), agent(B,A), drive(B)} KB: The knowledge base is a set of lexical and PRs generated from distributional semantics, along with a similarity score for each rule (section 2.6).", "label": "phrasal rule", "ID": "184"}, {"sentence": "We incorpo-  rate multiple anaphora resolution factors into  a statistical framework - -  specifically the dis-  tance between the PRonoun and the PRoposed  antecedent, gender/number/animaticity of the  PRoposed antecedent, governing head informa-  tion and noun phrase repetition.", "label": "pr", "ID": "185"}, {"sentence": "A Statistical ApPRoach to Anaphora Resolution  Niyu  Ge, John  Hale and Eugene Charn iak   Dept.", "label": "pr", "ID": "186"}, {"sentence": "brown, edu  Abst ract   This paper PResents an algorithm for identi-  fying PRonominal anaphora and two experi-  ments based upon this algorithm.", "label": "pr", "ID": "187"}, {"sentence": "We combine  them into a single PRobability that enables us  to identify the referent.", "label": "pr", "ID": "188"}, {"sentence": "We combine  them into a single PRobability that", "label": "pr", "ID": "189"}, {"sentence": "Annotators were tasked with evaluating three types of output from our Q/A system: (1) the ranked list of passages retrieved by our system?s PR module, (2) the list of passages identified as being CE by the scenario, and (3) the set of answers marked as being CE by the scenario (AnsSet3).", "label": "Passage Retrieval", "ID": "190"}, {"sentence": "c?2013 Association for Computational Linguistics The Answer is at your Fingertips: Improving PR for Web Question Answering with Search Behavior Data Mikhail Ageev?", "label": "Passage Retrieval", "ID": "191"}, {"sentence": "Each document is split by sentences, and for each sentence a QA-SYS PR Score (TextScore) is computed as a linear combination of term frequency score, proximity score, and term coverage score.", "label": "Passage Retrieval", "ID": "192"}, {"sentence": "Biased LexRank: PR Using Random Walks With Question-Based Priors.", "label": "Passage Retrieval", "ID": "193"}, {"sentence": "2.1 PR  The first step is to find passages likely to contain the  answer to the query.", "label": "Passage Retrieval", "ID": "194"}, {"sentence": "We believe KBA TREC 10 (313) TREC 11 (453) + - + - SA + 185 43 254 58 - 24 61 41 100 Table 2: PR Analysis that this is because of compounding errors that occurred during the multiple combination process.", "label": "Passage Retrieval", "ID": "195"}, {"sentence": "Using just the word token, positive preci- sion is slightly higher than for the 10-feature clas- sifier, but PRl is 11.6% lower.", "label": "positive recal", "ID": "196"}, {"sentence": "Figure 1 plots the positive precision of the two methods against their PRl, and figure 2 shows negative precision against negative recall.", "label": "positive recal", "ID": "197"}, {"sentence": "Add the prior polarity, and PRl improves, but at the expense of precision, which is 12.6% lower than for the 10-feature classifier.", "label": "positive recal", "ID": "198"}, {"sentence": "In Figure 1, this leads to a positive precision of 0.72 (and PRl 0.49), which does not improve much by adopting a larger t+, unless one is willing to set t+ at almost 1 at the price of very low posi- tive recall.", "label": "positive recal", "ID": "199"}, {"sentence": "is adopted, where rateTP   is the true positive rate (also called PRl  or sensitivity) and rateTN  is the true negative rate  (also called negative recall or specificity) (Kubat  and Matwin, 1997).", "label": "positive recal", "ID": "200"}, {"sentence": "988 1 Cohesion 5 Relation 9 NodeSpec 2 CohesionTxt 6 RelationTxt 10 NodeSpecTxt 3 CohesionMod 7 RelationMod 11 NodeSpecMod 4 CohesionTxtMod 8 RelationTxtMod 12 NodeSpecTxtMod Table 2: Metric types in Figures 1-3 Figure 1: F Score for Positive Ratings All metrics have a bias towards positive ratings with attendant high PRl values and im- proved f-score for positive polarity assignments.", "label": "positive recal", "ID": "201"}, {"sentence": "Longest matching is the most popular approach to Thai WS (Pooworawan, 1986).", "label": "word segmentation", "ID": "202"}, {"sentence": "6 Conclusion This paper proposes a two-step approach to Thai WS.", "label": "word segmentation", "ID": "203"}, {"sentence": "Studying the characteristics of Thai language, we find that WS possesses ambiguities at both character and syllable levels.", "label": "word segmentation", "ID": "204"}, {"sentence": "The technique we propose is a two-step process to WS.", "label": "word segmentation", "ID": "205"}, {"sentence": "A recognized effective approach to WS is Longest Matching, a method based on dictionary.", "label": "word segmentation", "ID": "206"}, {"sentence": "The experimental results show the syllable segmentation accuracy of more than 96.65% and the overall WS accuracy of 97%.", "label": "word segmentation", "ID": "207"}, {"sentence": "In the 80-sentence corpus under consideration, the  sentence structure is complex and stylized; with an  average of 20 WS.", "label": "words per sentence", "ID": "208"}, {"sentence": "5 10 15 10 20 30 40 50 average sentence length in words numbe r of ali gnmen ts per  senten ce Figure 1: Linear relation between the average number of WS and number of alignments per sentence We use the LL matrix as the similarity matrix for languages including all 6, 660 alignments.", "label": "words per sentence", "ID": "209"}, {"sentence": "# of web pages 12,938,606 # of unique bloggers 60,658 average # of pages/blogger 213.3 # of pages with comments 6,421,577 # of comments 50,560,024 average # of comment/page 7.873 # of words 5,600,597,095 # of all sentences 354,288,529 # of WS (average) 15 # of characters per sentence (average) 77 taken from Google (response to one simple query: ?", "label": "words per sentence", "ID": "210"}, {"sentence": "The average sentence length for Serbian is about 8.5 WS, and for English about 9.5.", "label": "words per sentence", "ID": "211"}, {"sentence": "This consists of 518,080 words (ap- proximately 20 WS, on average) of text annotated with a detailed semantic and syntac- tic tagset.", "label": "words per sentence", "ID": "212"}, {"sentence": "characters per word 5.22 WS 21.17 words per text 8,476 simple words 75.52% sentences per text 400.34 passive voice 15.11% total sentences 13,091 simplified sentences 16,71% Table 5: Statistics from the balanced text sample Figure 2: Clauses per sentence in the sample 4.2 Simplification analysis We manually analysed and annotated all sentences  in  our  samples.", "label": "words per sentence", "ID": "213"}, {"sentence": "Unsupervised Large Vocabulary WS Disambiguation with Graph-based Al- gorithms for Sequence Data Labeling, In Proc.", "label": "Word Sense", "ID": "214"}, {"sentence": "2.3 WS Determination The shared task of CoNLL-2008 for word sense disambiguation task is to determine the sense of an output predicate.", "label": "Word Sense", "ID": "215"}, {"sentence": "c?2013 Association for Computational Linguistics DALE: A WS Disambiguation System for Biomedical Documents Trained using Automatically Labeled Examples Judita Preiss and Mark Stevenson Department of Computer Science, University of Sheffield Regent Court, 211 Portobello Sheffield S1 4DP, United Kingdom j.preiss,m.stevenson@dcs.shef.ac.uk Abstract Automatic interpretation of documents is hampered by the fact that language contains terms which have", "label": "Word Sense", "ID": "216"}, {"sentence": "Finally, the WS Disam- biguation module uses cosine similarity to compare the centroid of each possible CUI of the ambiguous term (retrieved from the Centroid Database) with the ambiguous term?s feature vector (Stevenson et al 2008).", "label": "Word Sense", "ID": "217"}, {"sentence": "Unsupervised Graph- based WS Disambiguation Using Measures of Word Semantic Similarity.", "label": "Word Sense", "ID": "218"}, {"sentence": "WS Disambiguation in the Biomedical Domain.", "label": "Word Sense", "ID": "219"}, {"sentence": "Phrase-level tags Functional tags S Sentence SBJ Subject Q Quotative clause OBJ Object NP Noun phrase CMP Complement VP Verb phrase MOD Modifier VNP Copula phrase AJT Adjunct AP ADVP CNJ Conjunctive DP Adnoun phrase INT Vocative IP Interjection phrasePRN parenthetical Table 2: Phrase tags used in Sejong treebank.", "label": "Adverb phrase", "ID": "220"}, {"sentence": "osition- object phrase  Measure phrase Location phrase   Sentence level  BA sentence BEI sentence SHI sentence  YOU sentence Compound sentence  Table 1:  Chinese check-point taxonomy    Word level  Noun Verb (with Tense) Modal verb  Adjective Adverb Pronoun  Preposition Ambiguous word Plurality  Possessive Comparative & Superlative  degree  Phrase level  Noun phrase Verb phrase Adjective  phrase  ADVP Preposition phrase   Sentence level  Attribute clause Adverbial clause Noun clause  Hyperbaton   Table 2: English check-point taxonomy  4 Construction of Check-Point Data- base  Given a bilingual corpus with word alignment,  the construction of check-point database consists  of following two steps.", "label": "Adverb phrase", "ID": "221"}, {"sentence": "NP  UP  UG  NTL  NTP  AP  FP  VP  IP  LP  DP  Noun phr~e ~?~  Digital phrase 14560,.~_=P--_~\"  Digital-classifier .:~.~,~-.p~:  phrase  Phrase expressing =.-\\[-~t~z, 60 ./x~  the period of time  Phrase expressing ~ ~}k ~1~.~_~  the exact time  Adjective phrase ~gk:~Y  ADVP ~:~l:'~:J4~ (~  :~:i~)  Verb phrase -~-~ \\ ] i~   Preposition phrase ~ l~ l /~-   Post-position \" l~r~  phrase  Frame structure ~ ..6..~ ~ ~lJ ~i~ Jl~ .~.,,:i~ q\"  Table 1 Blocks defined in the system  Except PP, LP and DP, each kind of  block is  defined by a set of rules in the form of phrase  structure rule.", "label": "Adverb phrase", "ID": "222"}, {"sentence": "2.1.50therphrase types  AjP Adjective phrase (an interesting idea)  AdvP ADVP (you put that nicely)  Quo Quote (\"Indeed, \"she said)  2.2 Grammatical Functions  Below is a list of the FrameNet GFs.", "label": "Adverb phrase", "ID": "223"}, {"sentence": "University of Pennsylvania University of Otago We argue in this article that many common ADVP generally taken to signal a discourse relation between syntactically connected units within discourse structure instead work anaphor- ically to contribute relational meaning, with only indirect dependence on discourse structure.", "label": "adverbial phrases", "ID": "224"}, {"sentence": "Systems that  ignore this and begin with units that are inevitably  realized as kernel clauses under-utilize the expressive  power of natural language, which can use complex noun  phrases, nominalizations, ADVP, and other  adjuncts to pack information from multiple units into  one clause.", "label": "adverbial phrases", "ID": "225"}, {"sentence": "There is  normal ly  no more than two  ADVP before or after  the nominal.", "label": "adverbial phrases", "ID": "226"}, {"sentence": "a red large ball\" and the typical ordering  of temporal before spatial ADVP in German.", "label": "adverbial phrases", "ID": "227"}, {"sentence": "To identify these NPs as ADVP and  so preserve the intransitive sense of \"run\", we attach attributes  Time or Distance to the entries of nouns like \"time\" and \"mile\"  in the SL lexicon.", "label": "adverbial phrases", "ID": "228"}, {"sentence": "The preverbal specifiers are  ha-phrases, bei-phrases, ADVP, degree phrases,  preposition phrases, quantifier phrases, aspect, and modal.", "label": "adverbial phrases", "ID": "229"}, {"sentence": "Phrase-level tags Functional tags S Sentence SBJ Subject Q Quotative clause OBJ Object NP Noun phrase CMP Complement VP Verb phrase MOD Modifier VNP Copula phrase AJT Adjunct AP ADVPe CNJ Conjunctive DP Adnoun phrase INT Vocative IP Interjection phrasePRN parenthetical Table 2: Phrase tags used in Sejong treebank.", "label": "Adverb phras", "ID": "230"}, {"sentence": "osition- object phrase  Measure phrase Location phrase   Sentence level  BA sentence BEI sentence SHI sentence  YOU sentence Compound sentence  Table 1:  Chinese check-point taxonomy    Word level  Noun Verb (with Tense) Modal verb  Adjective Adverb Pronoun  Preposition Ambiguous word Plurality  Possessive Comparative & Superlative  degree  Phrase level  Noun phrase Verb phrase Adjective  phrase  ADVPe Preposition phrase   Sentence level  Attribute clause Adverbial clause Noun clause  Hyperbaton   Table 2: English check-point taxonomy  4 Construction of Check-Point Data- base  Given a bilingual corpus with word alignment,  the construction of check-point database consists  of following two steps.", "label": "Adverb phras", "ID": "231"}, {"sentence": "NP  UP  UG  NTL  NTP  AP  FP  VP  IP  LP  DP  Noun phr~e ~?~  Digital phrase 14560,.~_=P--_~\"  Digital-classifier .:~.~,~-.p~:  phrase  Phrase expressing =.-\\[-~t~z, 60 ./x~  the period of time  Phrase expressing ~ ~}k ~1~.~_~  the exact time  Adjective phrase ~gk:~Y  ADVPe ~:~l:'~:J4~ (~  :~:i~)  Verb phrase -~-~ \\ ] i~   Preposition phrase ~ l~ l /~-   Post-position \" l~r~  phrase  Frame structure ~ ..6..~ ~ ~lJ ~i~ Jl~ .~.,,:i~ q\"  Table 1 Blocks defined in the system  Except PP, LP and DP, each kind of  block is  defined by a set of rules in the form of phrase  structure rule.", "label": "Adverb phras", "ID": "232"}, {"sentence": "2.1.50therphrase types  AjP Adjective phrase (an interesting idea)  AdvP ADVPe (you put that nicely)  Quo Quote (\"Indeed, \"she said)  2.2 Grammatical Functions  Below is a list of the FrameNet GFs.", "label": "Adverb phras", "ID": "233"}, {"sentence": "De- claratively subjective clues such as the subjec- tive predicate part of the main clause and subjec- tive sentential ADVPs suggest that the  writer is the source of the opinion.", "label": "adverb phrase", "ID": "234"}, {"sentence": "3.3 Adverb Phrase Chunking When the adverbs appear in succession, they have a great tendency to form an ADVP.", "label": "adverb phrase", "ID": "235"}, {"sentence": "Though an adverb sequence is not always one ADVP, it usually forms one phrase.", "label": "adverb phrase", "ID": "236"}, {"sentence": "ADVP e.g., computer-teishi-no-tame data-hikitsugi-mo-konnandatta. (", "label": "adverb phrase", "ID": "237"}, {"sentence": "5.3.3 Head Words of all Phrases  We consider all phrases or syntactic roles, i.e.,  not only noun and verb phrases but also adjec- tive and ADVPs.", "label": "adverb phrase", "ID": "238"}, {"sentence": "To reduce the number of placeholders, we con- sider the notion of chunk defined in (Abney, 1996), i.e., not recursive kernels of noun, verb, adjective, and ADVPs.", "label": "adverb phrase", "ID": "239"}, {"sentence": "So, the semantic domain  for NMods is \\[C--C\\].", "label": "noun modifier", "ID": "240"}, {"sentence": "This generalizes the features to capture time expressions with prepo- sitions, as NMods, or other constructs.", "label": "noun modifier", "ID": "241"}, {"sentence": "4  Similarly, a NMod, such as black, combines  with a common noun, such as stone, to form a new  common noun - black stone.", "label": "noun modifier", "ID": "242"}, {"sentence": "To capture NMods that act as predi- cates, e.g. ?", "label": "noun modifier", "ID": "243"}, {"sentence": "American Journal of Computational Linguistics, Volume 9, Number 1, January-March 1983 15  Michael G. Main and David B. Benson Denotational Semantics for 'Natural Language Q-A Programs  SYNTACTIC CATEGORY SEMANTIC DOMAIN & BASIC PHRASES  SE (sentence)  CN (common oun)  IV  (intransitive verb)  NG (noun group)  TV (transitive verb)  NM (NMod)  MG (modifying roup)  PP (preposition or participial)  NU (numerals)  S = \\[U-*T\\].", "label": "noun modifier", "ID": "244"}, {"sentence": "In the cate-  gory grammar of section 3, the categories include sen-  tence, intransitive verb, common noun, noun group,  NMod, numeral, and so on.", "label": "noun modifier", "ID": "245"}, {"sentence": "The NP structure is  described as follows,  (R4) <NP> :: = (<NHD >){ < NP>/NOUN}( < NMP >)  / < Gerund-PH > / < To-infmitive~PH > /That < CLAUSE >  154  where NHD(Noun HeaDer) is ~premodification\" and  NMP(NMod Phrase) is \"postmodif ication'.", "label": "Noun Modifier", "ID": "246"}, {"sentence": "As an initial feeling for the coverage of the  NLP for which information is currently acquired,  TEL1 provides semantics for the word categories  Adjective  e.g. an expensive restaurant  NMod  e.g. a graduate student  Noun  e.g. a pub  and the phrase types  Adjective Phrase  e.g. employees responsible for  the planning projects  Noun-Modifier Phrase  e.g. the speech researchers  Prepositional Phrase  e.g. the trails on the Franconia-Region map  Verb Phrase  e.g. employees that report to Brachman  Functional Noun Phrase  e.g. the size of department 11387,  the colleagues o", "label": "Noun Modifier", "ID": "247"}, {"sentence": "In  Type  Total noun phrases  Articles  Left Modifiers of Nouns  Navy  339  27  72  4  \\[ Medical  532  38  Adjectival Modifiers:  Adj  Adj + Adj  Possessive N 138  34  4 0  NMods:  Noun 99 76  N+N 25 4  Verb 7 0  Table I: Left Modifier Statistics  Right Modifiers of Nouns  Type \\[ Navy \\[ Medical  Prepositional Phrases 95 107  Relative Clauses 1 5  Adverb 4 0  Reduced Relative Clauses 7 9  Table 2: Right Modifier Statistics  506  the sublanguage of Navy messages, unmarked verb  modifiers of nouns also occur.", "label": "Noun Modifier", "ID": "248"}, {"sentence": "4.1.4 NMods.", "label": "Noun Modifier", "ID": "249"}, {"sentence": "References  Barker, Ken & Szpakowicz, Stan, \"Semi-Automatic  Recognition of NMod Relationships\",  Proceedings of COLING-ACL '98.", "label": "Noun Modifier", "ID": "250"}, {"sentence": "Global Positioning) ?u\u001c (Long-time Development) Table 1: Semantic Relations between NMod and Verb Nominalization Head.", "label": "Noun Modifier", "ID": "251"}, {"sentence": "2 Overview of our QE submission Each translated sentence is assigned a score between 1 and 5.", "label": "quality estimation", "ID": "252"}, {"sentence": "3 The baseline features The QE shared task organizers pro- vided a baseline system including several interesting features.", "label": "quality estimation", "ID": "253"}, {"sentence": "Our experiments on two machine trans- lation QE datasets show uniform significant accuracy gains from multi-task learning, and consistently out- perform strong baselines.", "label": "quality estimation", "ID": "254"}, {"sentence": "The SDL language weaver systems in the WMT12 QE shared task.", "label": "quality estimation", "ID": "255"}, {"sentence": "In addition to showing empirical performance gains on QE applications, an im- portant contribution of this paper is in introduc- ing Gaussian Processes to the NLP community,1 a technique that has great potential to further per- formance in a wider range of NLP applications.", "label": "quality estimation", "ID": "256"}, {"sentence": "In Section 2, we give an overview of our QE sys- tem.", "label": "quality estimation", "ID": "257"}, {"sentence": "An Investigation on the Effectiveness of Features for Translation QE.", "label": "Quality Estimation", "ID": "258"}, {"sentence": "c?2012 Association for Computational Linguistics LORIA System for the WMT12 QE Shared Task Langlois David langlois@loria.fr Raybaud Sylvain LORIA, Universite?", "label": "Quality Estimation", "ID": "259"}, {"sentence": "2011) and for QE regression in (Cohn and Specia, 2013; Shah et al 2013).", "label": "Quality Estimation", "ID": "260"}, {"sentence": "c?2013 Association for Computational Linguistics Reducing Annotation Effort for QE via Active Learning Daniel Beck and Lucia Specia and Trevor Cohn Department of Computer Science University of Sheffield Sheffield, United Kingdom {debeck1,l.specia,t.cohn}@sheffield.ac.uk Abstract Quality estimation models provide feed- back on the quality of machine translated texts.", "label": "Quality Estimation", "ID": "261"}, {"sentence": "Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 240?251, Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics Coping with the Subjectivity of Human Judgements in MT QE Marco Turchi Matteo Negri Marcello Federico Fondazione Bruno Kessler, FBK-irst Trento , Italy {turchi|negri|federico}@fbk.eu Abstract Supervised approaches to NLP tasks rely on high-quality data annotations, which typically result from expensive manual la- belling procedures.", "label": "Quality Estimation", "ID": "262"}, {"sentence": "3 Experimental Settings 3.1 Datasets We perform experiments using four MT datasets manually annotated for quality: English-Spanish (en-es): 2, 254 sentences translated by Moses (Koehn et al, 2007), as pro- vided by the WMT12 QE shared task (Callison-Burch et al, 2012).", "label": "Quality Estimation", "ID": "263"}, {"sentence": "Major type Minor type  Kanji Reading, Writing, Radical, The  order of writing, Classification  Word knowl- edge  Katakana, How to use Kana, Ap- propriate Noun, To fill blanks for  Verb, Adjective, and Adjunct,  Synonym, Antonym, Particle,  CJion, Onomatopoeia, Po- lite Expression, Punctuation mark Reading com- prehension  Who, What, When, Where, How,  Why question, Extract specific  phrases, Progress order of a story,  Prose and Verse   Composition Constructing sentence, How to  write composition  Table 1.", "label": "Conjunct", "ID": "264"}, {"sentence": "Coordination of more than one constituent are of-  ten classified as CJion Reduction (4), Gap-  ping ( la- lb) and Right Node Raising (5) (Hudson,  1976).", "label": "Conjunct", "ID": "265"}, {"sentence": "A contrario, in the case of CJion Reduc-  tions, wh-sentences as well as cliticization are al-  2The star * marks ungrammatical sentences.", "label": "Conjunct", "ID": "266"}, {"sentence": "CJion reduction, gapping  and right-node raising.", "label": "Conjunct", "ID": "267"}, {"sentence": "s a,e What do A say b,a,f,e Whole story b Part of story - Whole story b Part of story b,f,c 16 c,f 11(18) 0(1, 1) 10 c 8(11) 0(0, 1) 2 b,c,f 1(2) 0(0, 0) 10 a 10(12) 4(9, 9) 4 - 0(5) 0(0, 0) 2 d 1(2) 1(3, 3) 10 f 8(11) 0(0, 0) 10 f 7(12) 3(3, 3) 1 - 0(1) 0(0, 6) 100 - 74(116) 10(22, 34) Why How How long, how often, how large Total Paragraph The others To fill blanks Not have interrogative pronoun CJion Progress order of a story Where 4 3(5) 0(1 When 4 3(5) 0(0 Who Question type 17(26) 1(1, 6)What 22 5(6) 1(4, 4) , 1) , 0)   Table 4.", "label": "Conjunct", "ID": "268"}, {"sentence": "It seems then  that the structure needed for CJion Reduc-  tion is some generalization of the standard structure  used for coordination of constituents.", "label": "Conjunct", "ID": "269"}, {"sentence": "(16) Hits(Jim, t1) = 763,000,000 Hits(Jim, t2) = 757,000,000 Problems with CJion and disjunction The search engines?", "label": "conjunct", "ID": "270"}, {"sentence": "This is a potential problem for us because we have to query for CJions of terms and disjunctions of inflected forms.", "label": "conjunct", "ID": "271"}, {"sentence": "Examples 1 and 2 show the representation that would be obtained for two imaginary English sen- 2Clause delimiters are punctuation marks other than com- mata, relative pronouns and subordinating CJions.", "label": "conjunct", "ID": "272"}, {"sentence": "We specifically extract the character reference CH either from the dependency relation nsubj, which links a speech verb SV with a CH that is the syntactic subject of a clause, or from the dependency relation dobj, which links a SV with a CH that is the direct object of the speech verb, across a CJ (e.g., and).", "label": "conjunct", "ID": "273"}, {"sentence": "Since the nine proposed part of speech types have varying crosslinguistic validity (e.g., not all languages have CJions), it might be better to provide software support for creating the disjunctive types as the need arises, rather than predefining them.", "label": "conjunct", "ID": "274"}, {"sentence": "Coor- dinating elements are commata and coordinating CJions.", "label": "conjunct", "ID": "275"}, {"sentence": "The representation for sentence 1 states that the first element of the 5-gram (-3; third word to the left of the adjective) is empty (because the second element is a phrase boundary marker), that the sec- ond element is a clause delimiter (CJion that), the third one (-1; word preceding the adjective) is a definite determiner, and the fourth one (+1; word following the adjective) is a common noun.", "label": "conjunct", "ID": "276"}, {"sentence": "In Proceedings of the International Joint Workshop on BioNLP and its Applications, pages 104?107.", "label": "Natural Language Processing in Biomedicine", "ID": "277"}, {"sentence": "Settles, B. (2004), Biomedical named entity recognition using conditional random fields and rich feature sets, in Proceedings of the International Joint Workshop on BioNLP and its Applications (NLPBA), 2004, Geneva, Switzerland.", "label": "Natural Language Processing in Biomedicine", "ID": "278"}, {"sentence": "In N. Collier, P. Ruch, and A. Nazarenko, editors, Proceedings of the International Joint Work- shop on BioNLP and its Applications (JNLPBA), Geneva, Switzerland, pages 70?75, August 28?29.", "label": "Natural Language Processing in Biomedicine", "ID": "279"}, {"sentence": "In Proceedings of the International Joint Workshop on BioNLP and its Applications (NLPBA), pages 104?107.", "label": "Natural Language Processing in Biomedicine", "ID": "280"}, {"sentence": "In Proceedings of the International Joint Workshop on 599 BioNLP and its Applications (NLPBA), pages 88?91.", "label": "Natural Language Processing in Biomedicine", "ID": "281"}, {"sentence": "In Proceedings of the ACL-03 Workshop on BioNLP, pages 41?", "label": "Natural Language Processing in Biomedicine", "ID": "282"}, {"sentence": "In Proceedings of the  International Joint Workshop on Natural Language  Processing in BioNLP and its Applications,  pages 70-75.", "label": "Biomedicine", "ID": "283"}, {"sentence": "In Proceedings of the Symposium for Semantic Mining in BioNLP (SMBM 2010), pages 137?141, Hinxton.", "label": "Biomedicine", "ID": "284"}, {"sentence": "In Proceedings of Semantic Mining in BioNLP (SMBM 2012), pages 10?17.", "label": "Biomedicine", "ID": "285"}, {"sentence": "Terminology-driven Literature Min- ing and Knowledge Acquisition in BioNLP.", "label": "Biomedicine", "ID": "286"}, {"sentence": "of the International Joint Workshop on Nat- ural Language Processing in BioNLP and its Applica- tions, pages 66?69, Geneva, Switzerland.", "label": "Biomedicine", "ID": "287"}, {"sentence": "In Proceedings of the Symposium for Semantic Mining in BioNLP (SMBM 2010), pages 84?92, Hinxton.", "label": "Biomedicine", "ID": "288"}, {"sentence": "2.2 IE from homepages From a technology point of view our procedure is a Web Content Mining tool, but it differs from the popular techniques used nowadays.", "label": "Information extraction", "ID": "289"}, {"sentence": "2.3 IE Finally, there exist large bodies of work on infor- mation extraction using models based on Markov and semi-Markov CRFs (Lafferty et al, 2001; Sarawagi and Cohen, 2004), and in particular for the task of named entity recognition (McCallum and Li, 2003).", "label": "Information extraction", "ID": "290"}, {"sentence": "IE techniques may support better understanding of these fundamental systems by identifying and structuring the molecu- lar processes underlying two component signaling.", "label": "Information extraction", "ID": "291"}, {"sentence": "IE over structured data: Question answering with freebase.", "label": "Information extraction", "ID": "292"}, {"sentence": "6 Conclusion and Future Work IE is an important first step in data mining applications.", "label": "Information extraction", "ID": "293"}, {"sentence": "IE from full text scientific articles: where are the keywords?", "label": "Information extraction", "ID": "294"}, {"sentence": "Harabagiu S., Maiorano, S. 2000, Acquisition of  Linguistic Patterns for Knowledge-Based  IE, in Proceedings of LREC- 2000,Athens Greece.", "label": "Information Extraction", "ID": "295"}, {"sentence": "Especially in the very factual text genres  targeted in IE (see the Ap-  pendix of the present paper for an example), few  problems are likely to occur.", "label": "Information Extraction", "ID": "296"}, {"sentence": "Coling 2008: Proceedings of the workshop on Multi-source Multilingual IE and Summarization, pages 33?40 Manchester, August 2008 Mixed-Source Multi-Document Speech-to-Text Summarization Ricardo Ribeiro INESC ID Lisboa/ISCTE/IST Spoken Language Systems Lab Rua Alves Redol, 9 1000-029 Lisboa, Portugal rdmr@l2f.inesc-id.pt David Martins de Matos INESC ID Lisboa/IST Spoken Language Systems Lab Rua Alves Redol, 9 1000-029 Lisboa, Portugal david@l2f.ine", "label": "Information Extraction", "ID": "297"}, {"sentence": "A good characterisation of their semantics can help identify referents in a given (con)text in dialog- based tasks, Question Answering systems, or even advanced IE tasks.", "label": "Information Extraction", "ID": "298"}, {"sentence": "Riloff, E. and Jones, R. 1999, Learning Dictionaries  for IE by Multi-Level  Bootstrapping, In Proceedings AAAI-99  pp.", "label": "Information Extraction", "ID": "299"}, {"sentence": "of the ACL-EACL Workshop on Automatic IE and Building of Lexical Semantic Resources, pages 16?", "label": "Information Extraction", "ID": "300"}, {"sentence": "A corpus should be created that can be  used as a tool for linguists not working on  the MUC IE task  The TD makes it clear that the annotation  task has been simplified in a number of ways.", "label": "information extraction", "ID": "301"}, {"sentence": "There has been significant work recently in the  IE community on a problem known  as Entity Detection and Tracking within the Automatic  Content Extraction (ACE) evaluations (NIST 2003).", "label": "information extraction", "ID": "302"}, {"sentence": "These advantages and the availability of off-the-shelf solvers have led to a  large variety of NLP tasks being formulated within it, including semantic role labeling,  syntactic parsing, co-reference resolution, summarization, transliteration and joint  IE.", "label": "information extraction", "ID": "303"}, {"sentence": "including co-reference  resolution, sentence compression and IE and use these to  explain several of the key advantages the framework offers.", "label": "information extraction", "ID": "304"}, {"sentence": "We will review several successful applications  of these methods in diverse tasks such as IE and textual  entailment. [", "label": "information extraction", "ID": "305"}, {"sentence": "171 Figure 1: Map A of the Molecular Interaction Map compiled by Kohn (1999) 2 Biomedical NLP Full-text articles are becoming increasingly avail- able to NLP researchers, who have begun inves- tigating how specific sections and structures can be mined in various IE tasks.", "label": "information extraction", "ID": "306"}, {"sentence": "All layers apply ReLus (ReLUs) (Nair and Hinton, 2010) and use dropout for regularization (Hinton et al, 2012).", "label": "rectified linear unit", "ID": "307"}, {"sentence": "Elman, 1990), are de- scribed by the following equations: z t = f(W i,z x t +W z,z z t?1 ) o t = g(W z,o z t ) where x t is the vector describing the input at time t; z t is the vector giving the hidden state at time t; o t is the vector giving the predicted output at time t; f and g are element-wise nonlinear func- tions (typically sigmoids, hyperbolic tangent, or ReLus); and W i,z , W z,z , and W z,o are learned matrices describing linear transforma- tions.", "label": "rectified linear unit", "ID": "308"}, {"sentence": "s (z) = 1 1 + exp(?3.75z) and ReLus clipped between 0 and 5 for the unit activations: ?(", "label": "rectified linear unit", "ID": "309"}, {"sentence": "The out- put pt (after a component-wise ReLu (ReLU) nonlinearity (Glorot et al, 2011)) is then used to compute the probability of the parser action at time t as: p(zt | pt) = exp ( g>ztpt + qzt ) ?", "label": "rectified linear unit", "ID": "310"}, {"sentence": "We use a single hidden layer in our model and apply ReLu (ReLU) activation function over the hidden layer outputs.", "label": "rectified linear unit", "ID": "311"}, {"sentence": "A simple way to initialize recurrent networks of ReLus.", "label": "rectified linear unit", "ID": "312"}, {"sentence": "On ReLus for Speech Processing.", "label": "Rectified Linear Unit", "ID": "313"}, {"sentence": "Specifically, we use the ReLu, or ReLU (Krizhevsky et al.,", "label": "Rectified Linear Unit", "ID": "314"}, {"sentence": "We also studied some of the newer activation functions: ReLus (Nair and Hinton, 2010), Maxout networks (Goodfel- low et al.,", "label": "Rectified Linear Unit", "ID": "315"}, {"sentence": "ReLu (ReLU) (Nair and Hin- ton, 2010): max(0, x); ?", "label": "Rectified Linear Unit", "ID": "316"}, {"sentence": "After each affine transformation, a ReLus (ReLU) (Nair and Hinton, 2010) non-linearity is ap- plied.", "label": "Rectified Linear Unit", "ID": "317"}, {"sentence": "ReLus Improve Restricted Boltzmann Machines.", "label": "Rectified Linear Unit", "ID": "318"}, {"sentence": "Sense id Pattern Synonyms 1 kare(he) ga(nominative) soba(noodles) wo(accusative) kuu (eat) 2 kare (he) ga(nominative) fukugyo(a part-time job) de(accusative) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense VB Classes Id Sense VB Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4 ask {negau, tanomu} 14 accept {uketoru, ukeru, morau, osameru} 5 leave {saru, hanareru} 15 take {uketoru, toru, kaisyakusuru, miru} 6 move {saru, utsuru} 16 lose {ushinau, nakusu} 7 pass {saru, kieru, sugiru", "label": "Verb", "ID": "319"}, {"sentence": "Major type Minor type  Kanji Reading, Writing, Radical, The  order of writing, Classification  Word knowl- edge  Katakana, How to use Kana, Ap- propriate Noun, To fill blanks for  VB, Adjective, and Adjunct,  Synonym, Antonym, Particle,  Conjunction, Onomatopoeia, Po- lite Expression, Punctuation mark Reading com- prehension  Who, What, When, Where, How,  Why question, Extract specific  phrases, Progress order of a story,  Prose and Verse   Composition Constructing sentence, How to  write composition  Table 1.", "label": "Verb", "ID": "320"}, {"sentence": "VBless clauses: nouns, adjectives, and adverbs, lexical or derived, functioning as predicates ?", "label": "Verb", "ID": "321"}, {"sentence": "d1> ga <word2> wo ukeireru / yurusu (forgive) [Concept relation] agent object [Case particle] ga (nominative) wo (accusative) [Sense identifier] 0ee0de; 0f58b4; 0f98ee 0f0157; 30f6b0 0ee0de: the part of a something written that makes reference to a particular matter 0f58b4: a generally-held opinion 0f98ee: the people who citizens of a nation 0f0157: a human being 30f6b0: human Figure 4: Extracted VB frames of ?", "label": "Verb", "ID": "322"}, {"sentence": "c?2009 ACL and AFNLP Classifying Japanese Polysemous VBs based on Fuzzy C-means Clustering Yoshimi Suzuki Interdisciplinary Graduate School of Medicine and Engineering University of Yamanashi, Japan ysuzuki@yamanashi.ac.jp Fumiyo Fukumoto Interdisciplinary Graduate School of Medicine and Engineering University of Yamanashi, Japan fukumoto@yamanashi.ac.jp Abstract This paper presents a method for classify- ing Japanese polysemous verbs using an alg", "label": "Verb", "ID": "323"}, {"sentence": "Sense id Pattern Synonyms 1 kare(he) ga(nominative) soba(noodles) wo(accusative) kuu (eat) 2 kare (he) ga(nominative) fukugyo(a part-time job) de(accusative) kurasu (live) Table 2: Examples of test verbs and their polysemic gold standard senses Id Sense VB Classes Id Sense VB Classes 1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru} 2 prey {negau, inoru} 12 persuade {oshieru, satosu} 3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru} 4 ask {negau, tanomu} 14 accept {uketoru, ukeru, morau, osameru} 5 leave {saru, hanareru} 15 take {uketoru, toru, kaisyakusuru, miru} 6 move {saru, utsuru} 16 lose {ushinau, nakusu} 7 pas", "label": "Verb", "ID": "324"}, {"sentence": "Of the over 1000  nouns which had VBs, 712 were not already  on the LDOCE fist augmented by Filtering.", "label": "verb base", "ID": "325"}, {"sentence": "To compositionally build VBd 567 semantic representations of our EDUs, we (Subba et al, 2006) integrated a robust parser, LCFLEX (Rose?,", "label": "verb base", "ID": "326"}, {"sentence": "We then perform a simple post processing step: we reas- sign classes to each VBd on the final scores they have received and recalculate their scores.", "label": "verb base", "ID": "327"}, {"sentence": "347 Computational Linguistics Volume 32, Number 3 with VBs to form deverbal nouns are listed and exemplified in Figure 1 on page 348.", "label": "verb base", "ID": "328"}, {"sentence": "Then we sent these  rJouns through our morphological analyzer to  extract those with VBs.", "label": "verb base", "ID": "329"}, {"sentence": "light VB form root verb meaning  baithanaa ?????", "label": "verb base", "ID": "330"}, {"sentence": "2 Online VB for Polylingual Topic Models Hierarchical generative Bayesian models, such as topic models, have proven to be very effective for modeling document collections and discover- ing underlying latent semantic structures.", "label": "Variational Bayes", "ID": "331"}, {"sentence": "The VBian EM algorithm for incomplete data: with application to scoring graphical model structures.", "label": "Variational Bayes", "ID": "332"}, {"sentence": "2.4 VB Moore (2004) showed that the EM algorithm is par- ticularly susceptible to overfitting in the case of rare words when training IBM Model 1.", "label": "Variational Bayes", "ID": "333"}, {"sentence": "Dynamic Language  Model Adaptation Using VB Inference.", "label": "Variational Bayes", "ID": "334"}, {"sentence": "In order to pre- vent overfitting, we use the VB ex- tension of the EM algorithm (Beal, 2003).", "label": "Variational Bayes", "ID": "335"}, {"sentence": "For the Bayesian runs, we compared two infer- ence methods: Gibbs sampling, as described above, and VBian EM (Beal and Ghahra- mani, 2003), both of which are implemented in Carmel.", "label": "Variational Bayes", "ID": "336"}, {"sentence": "They built a social network from the Marvel COM in which characters are the nodes, linked by their co-occurrence in the same book.", "label": "comics", "ID": "337"}, {"sentence": "here is meant to refer to all genres, in- cluding literature as well as more popular genres  such as fantasy, thrillers, COM, etc.", "label": "comics", "ID": "338"}, {"sentence": "the character is the first gay figure in the official @entity6 -- the movies , television shows , COM and books approved by @entity6 franchise owner @entity22 -- according to @entity24 , editor of \" @entity6 \" books at @entity28 imprint @entity26 .", "label": "comics", "ID": "339"}, {"sentence": "For example, when Superman?s prowess was first documented in the COM he did not have x-ray vision.", "label": "comics", "ID": "340"}, {"sentence": "Two studies on cognitive strategies used by second language learners (Ka- plan and Lucas, 2001; Lucas, 2004) used a data set of 58 jokes compiled from newspaper COM, 32 of which rely on lexical ambiguity.", "label": "comics", "ID": "341"}, {"sentence": "3.2 COM A domain should be complex enough to warrant the use of a QA system.", "label": "Complexity", "ID": "342"}, {"sentence": "The COM of Recognition of Linguistically Adequate Depen- dency Grammars.", "label": "Complexity", "ID": "343"}, {"sentence": "COM of System NARA  The complexity of the algorithm is usually measured  by the growth rate of its time and space requirements,  as a function of the size of its input (or the length  of input string) to which the algorithm is applied.", "label": "Complexity", "ID": "344"}, {"sentence": "1986 Computational COM of Current GPSG  Theory.", "label": "Complexity", "ID": "345"}, {"sentence": "Reducing the Time COM of the Fuzzy C-means Algorithm, In Trans.", "label": "Complexity", "ID": "346"}, {"sentence": "Cutler, A. (1983) Lexical COM and Sentence  Processing, in G. B. Flores d'Arcais and R.J.  Jarvella, eds.", "label": "Complexity", "ID": "347"}, {"sentence": "In J. Vicedo, P. Martnez- Barco, R. Muoz, and M. Saiz Noeda, editors, Ad- vances in Natural Language Processing, volume 3230 of Lecture Notes in COM Science, pages 82?90.", "label": "Computer", "ID": "348"}, {"sentence": "SPIRE, number 3772 in Lecture Notes in COM Science, pages 161?166.", "label": "Computer", "ID": "349"}, {"sentence": "The General Inquirer: A COM Approach to Content Analysis.", "label": "Computer", "ID": "350"}, {"sentence": "COM Speech and Language, 22(2):107?129.", "label": "Computer", "ID": "351"}, {"sentence": "The evaluation was conducted  on 133 paragraphs of annotated COM Sci-  ence text.", "label": "Computer", "ID": "352"}, {"sentence": "of COM Science,  Brown University,  \\[nge I j th \\[ ec\\] ~cs.", "label": "Computer", "ID": "353"}, {"sentence": "Given this, we have developed rule-based, machine-learning-based and reSRC- based approaches for estimation of character gen- der, age and salient personality attributes.", "label": "source", "ID": "354"}, {"sentence": "Given the above possible SRCs of informar  tion, we arrive at the following equation, where  F(p) denotes a function from pronouns to their  antecedents:  F(p) = argmaxP( A(p) = alp, h, l~', t, l, so, d~ A~')  where A(p) is a random variable denoting the  referent of the pronoun p and a is a proposed  antecedent.", "label": "source", "ID": "355"}, {"sentence": "A hybrid ap- proach is adopted, where pattern-based and statistical methods are used along with utilization of external knowledge SRCs.", "label": "source", "ID": "356"}, {"sentence": "The utilization of available reSRCs containing associations between person names and gender was followed in (Elson and 41 McKeown, 2010).", "label": "source", "ID": "357"}, {"sentence": "Our first experiment  shows the relative contribution of each SRC  Of information and demonstrates a uccess rate  of 82.9% for all SRCs combined.", "label": "source", "ID": "358"}, {"sentence": "Generalizing over lexical features: Selectional prefer- ences for SRCn.", "label": "semantic role classificatio", "ID": "359"}, {"sentence": "Generalizing over lexical features: selectional preferences for SRCn.", "label": "semantic role classificatio", "ID": "360"}, {"sentence": "Generalizing over lexical features: Selec- tional preferences for SRCn.", "label": "semantic role classificatio", "ID": "361"}, {"sentence": "PRED SENSE (c6): The lemma plus sense number of the predicate As for the task of SRCn, the features of the predicate word in addition to those of the word under consideration can also be used; we mark features of the predicate with an extra ?", "label": "semantic role classificatio", "ID": "362"}, {"sentence": "Semantic role labeling is achieved us- ing maximum entropy (MaxEnt) model based SRCn and integer linear programming (ILP) based post inference.", "label": "semantic role classificatio", "ID": "363"}, {"sentence": "2 System Description The proposed system sequentially performs syn- tactic dependency parsing, predicate identification, local SRCn, global sequence generation, and roleset information based selec- tion.", "label": "semantic role classificatio", "ID": "364"}, {"sentence": "Generalizing over lexical features: Selectional prefer- ences for SRC.", "label": "semantic role classification", "ID": "365"}, {"sentence": "Generalizing over lexical features: selectional preferences for SRC.", "label": "semantic role classification", "ID": "366"}, {"sentence": "Generalizing over lexical features: Selec- tional preferences for SRC.", "label": "semantic role classification", "ID": "367"}, {"sentence": "PRED SENSE (c6): The lemma plus sense number of the predicate As for the task of SRC, the features of the predicate word in addition to those of the word under consideration can also be used; we mark features of the predicate with an extra ?", "label": "semantic role classification", "ID": "368"}, {"sentence": "Semantic role labeling is achieved us- ing maximum entropy (MaxEnt) model based SRC and integer linear programming (ILP) based post inference.", "label": "semantic role classification", "ID": "369"}, {"sentence": "2 System Description The proposed system sequentially performs syn- tactic dependency parsing, predicate identification, local SRC, global sequence generation, and roleset information based selec- tion.", "label": "semantic role classification", "ID": "370"}, {"sentence": "In the medical texts, however, FA turn out to be the major glue for establishing local coherence, while anaphora, pronomi- nal anaphora in particular, play a far less important role than in other text genres.", "label": "functional anaphora", "ID": "371"}, {"sentence": "This allows us to deal with various forms of pronom- inal, nominal and FA in a uniform way.", "label": "functional anaphora", "ID": "372"}, {"sentence": "One of such latent relationship is indirect anaphora, FA, or bridging ref- erence, such as the following examples.", "label": "functional anaphora", "ID": "373"}, {"sentence": "We claim that only sophisticated knowledge representation languages with powerful terminological reasoning capabilities, such as those from the KL-ONE family, are able to deal with the full range of challenges of referentially adequate text understanding, in particular considering nominal and FA.", "label": "functional anaphora", "ID": "374"}, {"sentence": "In IT texts, (pro)nominal anaphora and FA occur at an almost balanced rate [27].", "label": "functional anaphora", "ID": "375"}, {"sentence": "But the work on bridging references characterizes this relationship as referential or anaphoric; this can be seen in the various terms under which this phenomenon is discussed: bridging references, indirect anaphora, FA, and partial anaphora.", "label": "functional anaphora", "ID": "376"}, {"sentence": "169 Labeled Macro F 1 LAS Labeled F 1 (complete task) (syntactic dependencies) (semantic dependencies) WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown johansson 84.86 (1) 85.95 75.95 89.32 (1) 90.13 82.81 80.37 (1) 81.75 69.06 che 82.66 (2) 83.78 73.57 86.75 (5) 87.51 80.73 78.52 (2) 80.00 66.37 ciaramita 82.06 (3) 83.25 72.46 86.60 (11) 87.47 79.67 77.50 (3) 79.00 65.24 zhao 81.44 (4)", "label": "Labeled Attachment Score", "ID": "377"}, {"sentence": "The big drop for German in pred/5k case 4LAS, with punctuation being taking into account.", "label": "Labeled Attachment Score", "ID": "378"}, {"sentence": "nces    14 While correctly tokenized sentences yield  results that are not extremely different from those  using gold standard information, and the drop in  accuracy in them can be attributed to the  differences introduced through stemming and  automatic parts of speech as well as the absence of  the linguistic features, incorrectly tokenized  sentences show a completely different picture as  the LAS now plummets to  33.6%, which is 37.96 percentage points below  that on correctly tokenized sentences.", "label": "Labeled Attachment Score", "ID": "379"}, {"sentence": "Labeled Macro F 1 LAS Labeled F 1 (complete task) (syntactic dependencies) (semantic dependencies) WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown WSJ+Brown WSJ Brown vickrey ? ? ? ? ? ?", "label": "Labeled Attachment Score", "ID": "380"}, {"sentence": "The best results on Arabic in the CoNLL 2007  shared task were obtained by Hall et al(2007) as  they obtained a LAS of  76.52%, 9.6 percentage points above the highest  score of the 2006 shared task.", "label": "Labeled Attachment Score", "ID": "381"}, {"sentence": "Ta- ble 2 shows LAS obtained with the three online classifiers.", "label": "Labeled Attachment Score", "ID": "382"}, {"sentence": "The results of the transition-based model, including the graph-based model shows some larger differ- ences The labeled and unLASs are not statistically significant and we concluded that (1) and (2) do not probably hold.", "label": "labeled accuracy score", "ID": "383"}, {"sentence": "Splitting up the morphology is a neutral operation in terms of labeled and unLASs; however, it is worth noting that our results with the separate test for the completion model is more competitive, providing an improvement of 0.14 UAS.", "label": "labeled accuracy score", "ID": "384"}, {"sentence": "When tested using 10-fold cross validation on the enhanced PTB, the parser achieves a LAS of 89.8, which is lower than the current state- of-the-art for transition-based dependency parsers (to wit, the 91.8 score of Zhang and Nivre 2011, although not directly comparable given that they test exclusively on WSJ Section 23), but with the advantage of providing us with the deep linguistic information from the XLE.", "label": "labeled accuracy score", "ID": "385"}, {"sentence": "A. 1The attachment score only considers whether a word is as- signed the correct head; the LAS in addition requires that it is assigned the correct dependency type; cf.", "label": "labeled accuracy score", "ID": "386"}, {"sentence": "For the sole transition-based parsers trained with the selected features, we obtain for Chinese, Hungar- ian and Russian higher labeled and unLASs.", "label": "labeled accuracy score", "ID": "387"}, {"sentence": "Unlabeled attachment score drops from 84.09% to  83.53% and LAS from 85.68%  to 85.44%.", "label": "labeled accuracy score", "ID": "388"}, {"sentence": "The resulting LAS was non-significantly lower (0.2%) than the score for the marginalised inference with the joint model.", "label": "labelled attachment score", "ID": "389"}, {"sentence": "The syntactic dependencies for both English and Chinese were obtained using the state-of-the-art Maltparser dependency parser, which achieved 84% and 88% LASs for Chinese and English respectively.", "label": "labelled attachment score", "ID": "390"}, {"sentence": "Furthermore, we also calculated gen- eral accuracies as micro-averages from the cross- validation sets, for which we used two metrics, namely the LAS ASL and the unLAS ASU , which are both accuracy metrics that compute the percentage of cor- rectly parsed dependencies over all tokens, where the unlabelled metric only requires a match with the correct head, and the labelled metric additionally re- quires the correct dependency relation to be chosen.", "label": "labelled attachment score", "ID": "391"}, {"sentence": "curacy differences (e.g. 67.4% against our 65.0% in LAS).", "label": "labelled attachment score", "ID": "392"}, {"sentence": "However, most evaluations of syntactic treebanks use simple ac- curacy measures such as bracket F 1 scores for constituent trees (NEGRA, Brants, 2000; TIGER, Brants and Hansen, 2002; Cat3LB, Civit et al, 2003; The Arabic Treebank, Maamouri et al, 2008) or labelled or unLASs for dependency syntax (PDT, Haji?c, 2004; PCEDT Mikulov?a and ?", "label": "labelled attachment score", "ID": "393"}, {"sentence": "The Dev-5k and Dev columns report LAS on the development sets.", "label": "labeled attachment score", "ID": "394"}, {"sentence": "UnLAS (UAS) ignoring punctuation is used to evaluate parsing quality.", "label": "labeled attachment score", "ID": "395"}, {"sentence": "The LAS varies from 91.65 to 65.68 but is abov", "label": "labeled attachment score", "ID": "396"}, {"sentence": "The unLASs of the converted dependencies are shown as the accuracies in Table 5, since most bunsetsu-based dependency parsers out- put only unlabeled structure.", "label": "labeled attachment score", "ID": "397"}, {"sentence": "We tried to select the graph-based feature templates of the completion model after the selection of the 8 LMP [(LAS + morphology accuracy + part-of-speech accuracy)/3] would have been another alternative.", "label": "labeled attachment score", "ID": "398"}, {"sentence": "Benefiting from the rich features selected in the tree kernel space, our model achieved the best reported unLAS of 93.72 without using any additional resource.", "label": "labeled attachment score", "ID": "399"}, {"sentence": "(For the remaining languages, the training data has not been split at all.)5 A dry run at the end of the development phase gave a LAS of 80.46 over the twelve required languages.", "label": "labeled attachment score", "ID": "400"}, {"sentence": "LA is the LAS and UA is the Unla- belled Attachment Score <training file>?).", "label": "Labelled Attachment Score", "ID": "401"}, {"sentence": "A Unified Morpho-Syntactic Scheme of SD.", "label": "Stanford Dependencies", "ID": "402"}, {"sentence": "Feature Train + Dev Set Test Set Chance Baseline 9.1 9.1 Character unigram 33.99 34.70 Character bigram 51.64 49.80 Character trigram 66.43 66.70 RASP POS unigram 43.76 45.10 RASP POS bigram 58.93 61.60 RASP POS trigram 59.39 62.70 Function word unigram 51.38 54.00 Function word bigram 59.73 63.00 Word unigram 74.61 75.50 Word bigram 74.46 76.00 Word trigram 63.60 65.00 TSG Fragments 72.16 72.70 SD 73.78 75.90 Adaptor Grammar POS/FW n-grams 69.76 70.00 Table 1: Classification results for our individual features.", "label": "Stanford Dependencies", "ID": "403"}, {"sentence": "On the other hand, we obtain the de- pendency relations by the SD  Parser4.", "label": "Stanford Dependencies", "ID": "404"}, {"sentence": "variant of SD (de Marneffe and Manning, 2008a), in which preposition- and conjunction- words do not appear as nodes in the tree but are instead anno- tated on the dependency label between the content words they connect, e.g. prep with(saw, telescope).", "label": "Stanford Dependencies", "ID": "405"}, {"sentence": "The performance for word n-grams, TSG frag- ments and SD is very strong and comparable to previously reported research.", "label": "Stanford Dependencies", "ID": "406"}, {"sentence": "Parsing to SD: Trade-offs between speed and accuracy.", "label": "Stanford Dependencies", "ID": "407"}, {"sentence": "In T.H. King and E.M. Bender, editors, GEAF 2007, SD, CA.", "label": "Stanford", "ID": "408"}, {"sentence": "SD CoreNLP tool.", "label": "Stanford", "ID": "409"}, {"sentence": "In order to address this distant association issue, we examined the collapsed-ccprocessed-dependencies output besides the basic-dependenciesout- put of the SD CoreNLP dependency engine (de Marneffe and Manning, 2012).", "label": "Stanford", "ID": "410"}, {"sentence": "were performed using the SD CoreNLP suite of tools (CoreNLP, 2014).", "label": "Stanford", "ID": "411"}, {"sentence": "SD typed dependencies manual.", "label": "Stanford", "ID": "412"}, {"sentence": "The most plausible point is calculated by using the potential function with the SD Method on request.", "label": "Steepest Descent", "ID": "413"}, {"sentence": "The  algorithms for obtaining the best translation ma-  trix were shown based on the SD  Method, an algorithm well known in the field of  non-linear programming.", "label": "Steepest Descent", "ID": "414"}, {"sentence": "The results show that the proposed model trained by a global optimizer outperforms models trained by the other local optimizers (SD and BFGS); The penalty in the proposed model works well because the degeneracies in the penalized model seem to decrease, and the computation is noteworthy stable.", "label": "Steepest Descent", "ID": "415"}, {"sentence": "Characterizing SDs pro- duced by various grammatical formalisms.", "label": "structural description", "ID": "416"}, {"sentence": "The re-  suiting structures are then scored using some mea-  sure of success that compares these parses to the  correct SDs for the sentences  provided in the training corpus.", "label": "structural description", "ID": "417"}, {"sentence": "In FUG, not only is  there no formal distinction between categories and  SDs, but even the distinction be-  tween SDs and grammars disappears.", "label": "structural description", "ID": "418"}, {"sentence": "Thus, in LFG, syntactic ategories and  the SDs known as f-structures are  exactly the same kind of object.", "label": "structural description", "ID": "419"}, {"sentence": "Data bases as tools for linguistic research  The data bases contain again linguistic objects together  with additional informations (e.g. SDs,  classifications).", "label": "structural description", "ID": "420"}, {"sentence": "god 0.085 (0.045) news 0.100 (0.044) christmas 0.081 (0.046) way 0.078 (0.044) jesus 0.060 (0.038) obamacare 0.068 (0.041) kenya 0.052 (0.035) white 0.059 (0.037) brave 0.043 (0.035) let 0.058 (0.038) bravo 0.041 (0.035) course 0.046 (0.033) know 0.038 (0.030) huh 0.044 (0.036) dennis 0.038 (0.029) education 0.043 (0.032) ronald 0.036 (0.030) president 0.039 (0.031) Table 5: Average weights (and SDs calculated across samples) for top 10 NNP ?", "label": "standard deviation", "ID": "421"}, {"sentence": "Figure 5 presents the SD for con- tent and readability scores: concerning content, automatically generated summaries using back- ground information achieved the highest SD scores (see also figure 6 for a sample story).", "label": "standard deviation", "ID": "422"}, {"sentence": "Although One-Class Support Vec- tor Machine (OSVM) (Manevitz and Yousef, 2001) can learn from just positive examples, according to Yu et al (2002) they are prone to underfitting and overfitting when data is scant (which happens in 2The mean, median, SD and histogram of the overlapping distribution are calculated and included as features.", "label": "standard deviation", "ID": "423"}, {"sentence": "Due to the variance inherent to the stochastic gradient descent procedure, we repeat the experiment 100 times and report the median performance and SDs (of different SGD runs).", "label": "standard deviation", "ID": "424"}, {"sentence": "interval expresses the SD of the temporal intervals between two successive posts.", "label": "standard deviation", "ID": "425"}, {"sentence": "0.00  0.50  1.00  1.50  2.00  2.50  3.00  3.50  4.00  4.50  5.00  Simple (News  only)  Background  only  Background +  News  Human  Extractive  Human  Abstractive  Content (avg) Readability (avg)  Content (stdev) Readability (stdev)  Figure 6: Average and SD of the content and readability scores for one news story.", "label": "standard deviation", "ID": "426"}, {"sentence": "To adapt our system to the task, we first heuris- tically converted the question into a query fact us- ing the subject and object SD labels (de Marneffe and Manning, 2008).", "label": "Stanford Dependency", "ID": "427"}, {"sentence": "POS, Chunking and Dependency  Relations:The SD parser 14  for  English.", "label": "Stanford Dependency", "ID": "428"}, {"sentence": "Although all systems perform syntactic analy- sis of input texts, there is a fair amount of vari- ety in the applied parsers, which include the parser of Charniak and Johnson (2005) with the biomed- ical domain model of McClosky (2009) and the SD conversion (de Marneffe et al 2006) ?", "label": "Stanford Dependency", "ID": "429"}, {"sentence": "To extract events from text, we use the SD Parser (De Marneffe et al, 2006; Socher et al, 2013).", "label": "Stanford Dependency", "ID": "430"}, {"sentence": "We find a remarkable number of sim- ilarities between the approaches of the systems, with all four utilizing full parsing and a depen- dency representation of the syntactic analysis, and the three highest-ranking further specifically the phrase structure parser of Charniak and Johnson (2005) with the biomedical domain model of Mc- Closky (2009), converted into SD form using the Stanford tools (de Marneffe et al, 2006).", "label": "Stanford Dependency", "ID": "431"}, {"sentence": "The unigram and dependency annotations are derived from the SD Parser (Klein and Manning, 2003).", "label": "Stanford Dependency", "ID": "432"}, {"sentence": "The first speech recognition systems  tended to be SD (before using a sys-  tem, a person had first to read a list of words or sen-  tences).", "label": "speaker dependent", "ID": "433"}, {"sentence": "whether A is \"to the left of\" B in  a scene, is dependent on the orientation of the  speaker/viewer); other expressions are orientation  independent such as \"above\" and \"below\" which  implicitly refer to the downward pull of gravity (al-  though in space verticality is SD).", "label": "speaker dependent", "ID": "434"}, {"sentence": "Our project uses well developed  SD voice recognit ion equip-  ment with a small enough vocabulary to  achieve usable accuracy rates.", "label": "speaker dependent", "ID": "435"}, {"sentence": "In this paper, we demonstrate that the attribute set is SD.", "label": "speaker dependent", "ID": "436"}, {"sentence": "We also  anticipate moving from SD recognition to a  speaker adaptive mode which will require far less training  data for new speakers.", "label": "speaker dependent", "ID": "437"}, {"sentence": "Using our approach we have made a  prototype of the recognizer for isolated word,  SD ASR system for Hindi.", "label": "speaker dependent", "ID": "438"}, {"sentence": "MRF  (Markov Random Field) classify the term depend- encies in queries into SD and  full dependence, which respectively corresponds to  ordered and unordered co-occurrence within a pre- define-sized window in documents.", "label": "sequential dependence", "ID": "439"}, {"sentence": "The model thus reflects a hy- pothesis that eye movements are largely unaffected by semantic content, that eye movements depend on the physical properties and frequency of words, and that there is a SD between fixa- tion times.", "label": "sequential dependence", "ID": "440"}, {"sentence": "nist.gov/related projects/muc/. 2The ACE program, www.nist.gov/speech/tests/ace/. genre-specific corpus of German newspaper com- mentaries, taken from the daily papers Ma?rkische Allgemeine Zeitung and Tagesspiegel.", "label": "Automated Content Extraction", "ID": "441"}, {"sentence": "1 Introduction There has been much recent interest in identifying events, times and their relations along the timeline, from event and time ordering problems in the Temp- Eval shared tasks (Verhagen et al, 2007; Verhagen et al, 2010), to identifying time arguments of event structures in the ACE pro- gram (Linguistic Data Consortium, 2005; Gupta and Ji, 2009), to timestamping event intervals in the Knowledge Base Population shared task (Artiles et al.,", "label": "Automated Content Extraction", "ID": "442"}, {"sentence": "2010), and the ACE pro- gram only looked at time arguments for specific types of events, like being born or transferring money.", "label": "Automated Content Extraction", "ID": "443"}, {"sentence": "Linguistic Re- sources and Evaluation Techniques for Evaluation of Cross-Document ACE.", "label": "Automatic Content Extraction", "ID": "444"}, {"sentence": "The ACE 2008 corpus (Linguistic Data Consortium 2008) for relation detection and recognition collects English and Arabic texts from a variety of resources including radio and TV broadcast news, talk shows, newswire articles, Internet news groups, Web logs, and conversational telephone speech.", "label": "Automatic Content Extraction", "ID": "445"}, {"sentence": "It is worth noting that our classification is more  fine-grained than efforts like the EDT task in  ACE1 program (Mitchell  and Strassel 2002), which distinguishes between  toponyms that are a Facility ?", "label": "Automatic Content Extraction", "ID": "446"}, {"sentence": "In Proceed- ings of the NIST 2007 ACE Workshop.", "label": "Automatic Content Extraction", "ID": "447"}, {"sentence": "Instead, we will adopt the nomencla- ture of the ACE program (NIST, 2004): we will call the instances of textual references to objects/abstractions mentions, which can be either named (e.g. John Mayor), nominal (the president) or pronominal (she, it).", "label": "Automatic Content Extraction", "ID": "448"}, {"sentence": "From Table 1 we can see that the two more formal corpora, NEWSHOUR and CROSSFIRE, have longer sentences, more sentences per turn, and fewer disfluencies (particularly nonlexicalized filled pauses and FSs) than English CALLHOME and the GROUP MEETINGS.", "label": "false start", "ID": "449"}, {"sentence": "The experimenter then tran-  scribed what the subject said, excluding FSs, and  sent he transcription tothe system, which automatically  generated the synthesized response.", "label": "false start", "ID": "450"}, {"sentence": "In section 5.3 we shall present three components for identifying most of the major classes of speech disfluencies in the input of the summarization system, such as filled pauses, repetitions, and FSs.", "label": "false start", "ID": "451"}, {"sentence": "As such, the utterances exhibit the usual difficulties associated with real language such as agreement errors, FSs, fragments, and constituent dislocations.", "label": "false start", "ID": "452"}, {"sentence": "The transcriber eliminated  hesitations, \"urns\" and FSs, but otherwise sim-  ply transmitted a transcription ofwhat the subject said.", "label": "false start", "ID": "453"}, {"sentence": "There are repeats, FSs, a lot of pause filling words such as um and uh, etc.", "label": "false start", "ID": "454"}, {"sentence": "FS for sentence clas- sification in evidence-based medicine.", "label": "Feature stacking", "ID": "455"}, {"sentence": "Following  this, it is hoped to incorporate some simple types of domain-  and discourse-dependent knowledge into the program, in  particular knowledge about expected relations among objects  in a given domain and a simple discourse FS.", "label": "focus structure", "ID": "456"}, {"sentence": "A three-tiered partition of a modal- ity, discourse and domain layer is connected with a dou- ble threaded FS.", "label": "focus structure", "ID": "457"}, {"sentence": "Our theory of FS is related to the task-based theory of (Grosz, 1977).", "label": "focus structure", "ID": "458"}, {"sentence": "Along the lines of (Kirschner and Bernardi, 2007), we aim for a precise definition of FS for IQA questions.", "label": "focus structure", "ID": "459"}, {"sentence": "The pragmatic-semantic level of  therue-rheme and FSs has already been defined for  restricted blocks worlds dialogues (Pignataro 1987) and will be  incorporated into an automatic focus assignment system.", "label": "focus structure", "ID": "460"}, {"sentence": "using the coherence relations to build a  FS; and  4.", "label": "focus structure", "ID": "461"}, {"sentence": "clicks in the follow- ing 8 categories: Play (Pl), Pause (Pa), SeekFw (Sf), SeekBw (Sb), FS (SSf), ScrollBw (SSb), RatechangeFast (Rf), RatechangeSlow (Rs).", "label": "ScrollFw", "ID": "462"}, {"sentence": "We assume that it should be possible to condense the comparison into one (more or less simple) rule: the narrower the RC (similarity < relatedness < evocation) and the narrower the data considered (lexical semantic selection rule < any kind of selection rule < random selection) the better the correlation between hu- man judgment and semantic measure11.", "label": "relation concept", "ID": "463"}, {"sentence": "6.b, the CS \"ELeMent\", which  usually has two meanings ( an object concept and  a membership RC), functions as an  object concept.", "label": "relation concept", "ID": "464"}, {"sentence": "This comparison is based on the  838  cross-corRC as it is used in digital  signal processing.", "label": "relation concept", "ID": "465"}, {"sentence": "The non-  restrictive RC has the effect of uniquely  determining two parrots that Anne owns.", "label": "relative clause", "ID": "466"}, {"sentence": "English RC constructions.", "label": "relative clause", "ID": "467"}, {"sentence": "The prototypical exam-  ple used throughout this paper is the non-restrictive  RC.", "label": "relative clause", "ID": "468"}, {"sentence": "Nonrest r ic t ive  modi f iers  Uniqueness, or maxi-  mality, is forced by non-restrictive modification, as  can be the case in RCs and adjective-noun  phrases.", "label": "relative clause", "ID": "469"}, {"sentence": "9We exclude infinitival RCs from these fig-  ures, for example \"I called a plumber TRACE to fix the  sink\" where 'plumber' is co-indexed with the trace sub-  ject of the infinitival.", "label": "relative clause", "ID": "470"}, {"sentence": "This  section describes a probabilistic treatment of extrac-  tion from RCs.", "label": "relative clause", "ID": "471"}, {"sentence": "5.3 Effect of RC All the experiments above considered left context only.", "label": "Right Context", "ID": "472"}, {"sentence": "Left Context Focus RC Combined Class - - - - - a a n b i d ?", "label": "Right Context", "ID": "473"}, {"sentence": "Context Accuracy (%) Left Context Only 91.31 RC Only 88.26 Both Contexts 92.54 Table 3: The effect of using both left and right context.", "label": "Right Context", "ID": "474"}, {"sentence": "\b\u000b\t\u000b\u000b  \u0001 shi ,1Lo \u0002 zhang \u0003 ma \u0004 ying \u0005 jiu \u0006 biao \u0007 shi ,2Lo ,1Co ,2Co ,3Co ,1Ro ,2Ro \b\u000b\t\u000b  \b\u000b\t\u000b  Left Context RC Candidate to be parsed), it will be rejected immediately.", "label": "Right Context", "ID": "475"}, {"sentence": "NUM sg GEND fem DEF - CASE nom SUBJ h i 1 3 7 7 7 7 7 7 7 7 5 3 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 5 Figure 3: F-structure for example (3) 2.4 Left-RC Rules The left-right context annotation module is based on a tripartite division of local subtrees into a left- hand-side context (LHS) followed by a head (H) followed by a right-hand-side context (RHS).", "label": "Right Context", "ID": "476"}, {"sentence": "Description Feature Trigram + Context x1x2x3x4x5 Trigram x2x3x4 Left Context x1x2 RC x4x5 Center Word x3 Trigram - Center Word x2x4 Left Word + RC x2x4x5 Right Word + Left Context x1x2x3 Type of Trigram: number, punctuation, alphabetic letter and other t(x2)t(x3)t(x4) Table 2: Features employed to measure the sim- ilarity between two vertices, in a given tex- t ?", "label": "Right Context", "ID": "477"}, {"sentence": "Work on PG: Barzilay and McKeown (2001) extract paraphrases from a mono- lingual parallel corpus, containing multiple transla- tions of the same source.", "label": "paraphrase generation", "ID": "478"}, {"sentence": "The rest of this paper is organized as follows: We describe distributional PG in Section 2, antonym discovery in Section 3, and paraphrase-augmented SMT in Section 4.", "label": "paraphrase generation", "ID": "479"}, {"sentence": "Improved statistical machine trans- lation using monolingual text and a shallow lexical re- source for hybrid phrasal PG.", "label": "paraphrase generation", "ID": "480"}, {"sentence": "Chevelu et al (2009) introduce a new PG tool based on Monte- Carlo sampling.", "label": "paraphrase generation", "ID": "481"}, {"sentence": "7 Related Work This paper brings together several sub-areas: SMT, PG, distributional seman- tic distance measures, and antonym-related work.", "label": "paraphrase generation", "ID": "482"}, {"sentence": "Application-driven statistical PG.", "label": "paraphrase generation", "ID": "483"}, {"sentence": "3 Paraphrasing exercise answers 3.1 PG and pre-ranking Given a sentence, and our Simple Wiktionary para- phrases (about 20,650 extracted paraphrases), we can generate sentential paraphrases by simple syn- tactic pattern matching ?", "label": "Paraphrase generation", "ID": "484"}, {"sentence": "PG is useful in applications where it is needed to choose between different forms to keep the most fit.", "label": "Paraphrase generation", "ID": "485"}, {"sentence": "3 PG system Given a phrase, the proposed system generates its syntactic variants in the following four steps: 1.", "label": "Paraphrase generation", "ID": "486"}, {"sentence": "3.1 System description PG tools based on SMT meth- ods need a language model and a paraphrase table.", "label": "Paraphrase generation", "ID": "487"}, {"sentence": "PG, on the other hand, has  been an area of active research and the related  work has been thoroughly surveyed in  Androutsopoulos and Malakasiotis (2010) as well  as in Madnani and Dorr (2010).", "label": "Paraphrase generation", "ID": "488"}, {"sentence": "PG and information retrieval from stored text.", "label": "Paraphrase generation", "ID": "489"}, {"sentence": "The algorithms, including Description Length Gain (DLG) (Kit and Wilks 1999), AV (Feng et al 2004a, 2004b), and Branching Entropy (Tanaka-Ishii 2005; Jin and Tanaka-Ishii 2006), were evaluated on SIGHAN Bakeoff-3 data set (Levow 2006).", "label": "Accessor Variety", "ID": "490"}, {"sentence": "Zhang et al (2013) uses eight types of features such as Mu- tual Information and AV and they extract dynamic statistical features from both an in-domain corpus and an out-of-domain corpus us- ing co-training.", "label": "Accessor Variety", "ID": "491"}, {"sentence": "2.3.2 AV When a string appears under different linguistic environments, it may carry a meaning.", "label": "Accessor Variety", "ID": "492"}, {"sentence": "AV Criteriafor Chinese Word Extraction.", "label": "Accessor Variety", "ID": "493"}, {"sentence": "Haodi Feng, Kang Chen, Xiaotie Deng, Weimin Zheng  AV Criteria for Chinese Word  Extraction, Computational Linguistics March 2004,  Vol.", "label": "Accessor Variety", "ID": "494"}, {"sentence": "We applied a subword- based word segmenter using CRFs and ex- tended the segmenter with OOV words recognized by AV.", "label": "Accessor Variety", "ID": "495"}, {"sentence": "Another common practice is to introduce some statistics-based measures, such 648 as boundary entropy (Jin and Tanaka-Ishii, 2006) and AV (Feng et al 2004), which are commonly used in unsupervised CWS models.", "label": "accessor variety", "ID": "496"}, {"sentence": "Unsupervised segmentation of Chinese corpus using AV.", "label": "accessor variety", "ID": "497"}, {"sentence": "2) characters, we define the left AV of Llav(s) as the number of distinct characters that precede s in a corpus.", "label": "accessor variety", "ID": "498"}, {"sentence": "AV? (", "label": "accessor variety", "ID": "499"}, {"sentence": "Similarly, the right AV Rlav(s) is defined as the num- ber of distinct characters that succeed s. We first extract all strings whose length are be- tween 2 and 4 from the unlabeled data, and calculate their AV values.", "label": "accessor variety", "ID": "500"}, {"sentence": "This prin- ciple is introduced as the AV criterion for identifying meaningful Chinese words in (Feng et al, 2004).", "label": "accessor variety", "ID": "501"}, {"sentence": "Beyond these ideas, Liang (2005) and Sun and Xu (2011) experiment with deriv- ing a large set of statistical features such as mu- tual information and AV from un- labelled data, and add them to supervised dis- criminative training.", "label": "accessor variety", "ID": "502"}, {"sentence": "hU As search engines, we used AV (http://www.", "label": "AltaVista", "ID": "503"}, {"sentence": "Several pos- sible uses of inflectional sublexicon were also suggested, among which the most widely useful can be the possibility to use all word-forms of a lemma in the web-search engine such as Google, AV etc.", "label": "AltaVista", "ID": "504"}, {"sentence": "5 We used eight well-known Web search engines such as Google (http://www.google.com/), Ask Jeeves (http://web.ask.com/), AV (http://www.altavista.com/), LookSmart (http://search.looksmart.com/), Teoma (http://s.teoma.com/), AlltheWeb (http://www.alltheweb.com/), Ly- cos (http://search.lycos.com/), and Yahoo! (", "label": "AltaVista", "ID": "505"}, {"sentence": "Thus, the author computes  the Pointwise Mutual Information score between  seed words and new words on the basis of the  number of AV hits returned when querying  the seed word and the word to be classified with  the ?", "label": "AltaVista", "ID": "506"}, {"sentence": "Lapata and Keller (2004) achieved improved results on this task by using the database of AV?s search engine as a corpus.", "label": "AltaVista", "ID": "507"}, {"sentence": "Related Work: Comparison with Broder?s Sketches Broder?s sketches (Broder 1997), originally introduced for removing duplicates in the AV index, have been applied to a variety of applications (Broder et al 1997; Haveliwala, Gionis, and Indyk 2000; Haveliwala et al 2002).", "label": "AltaVista", "ID": "508"}, {"sentence": "A Japanese predicate bunsetsu consists of a main verb followed by a se- quence of AVs and sentence final parti- cles.", "label": "auxiliary verb", "ID": "509"}, {"sentence": "Other constraints require an AV to be modified by a full verb, or prescribe morphosyntactical agreement between a determiner and its regent (the word modified by the determiner).", "label": "auxiliary verb", "ID": "510"}, {"sentence": "There are many reasons why a simple word-to-word (1-to-1) correspondence is not possible for every sentence pair: for instance, AVs used in one lan- guage but not the other (e.g., English He walked and French Il est alle?),", "label": "auxiliary verb", "ID": "511"}, {"sentence": "Through a brief error analysis, we found that main bottle neck for verbal predicate is AV be and have.", "label": "auxiliary verb", "ID": "512"}, {"sentence": "A bunsetsu is a grammat- ical and phonological unit in Japanese, which con- sists of an independent-word such as noun, verb or adverb followed by a sequence of zero or more dependent-words such as AVs, postposi- tional particles or sentence final particles.", "label": "auxiliary verb", "ID": "513"}, {"sentence": "We added the following complementary informa- tion to the tags and named the new tag sets Base or Full and suffix: inf: add inflection information to the POS tag (verbs, adjectives, and AVs) and the phrase tags (Table 2).", "label": "auxiliary verb", "ID": "514"}, {"sentence": "0.00  0.50  1.00  1.50  2.00  2.50  3.00  3.50  4.00  4.50  5.00  Simple (News  only)  Background  only  Background +  News  Human  Extractive  Human  Abstractive  content readability  Figure 4: AV of the content and readability scores for each summary creation method.", "label": "Average", "ID": "515"}, {"sentence": "The first is the sentence alignment confidence measure, based on which the best whole sentence alignment is se- 938 # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 939911 43.53 50.51 -3.49 53.14 40.60 6.27 ALF 618179 43.11 50.24 -3.56 51.75 42.05 4.85 Table 7: Improved Arabic-English Newswire Translation with Alignment Link Filtering # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 598721 49.91 39.90 5.00 57.30 30.98 13.16 ALF 383561 48.94 40.00 4.42 55.99 31.92 12.04 Table 8: Improved Arabic-English Web-Blog Translation with Alignment Link Filtering lected among multiple alignments and it obtained 0.8 F-measure improvement over the single best Chinese-English aligner.", "label": "Average", "ID": "516"}, {"sentence": "The first is the sentence alignment confidence measure, based on which the best whole sentence alignment is se- 938 # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 939911 43.53 50.51 -3.49 53.14 40.60 6.27 ALF 618179 43.11 50.24 -3.56 51.75 42.05 4.85 Table 7: Improved Arabic-English Newswire Translation with Alignment Link Filtering # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 598721 49.91 39.90 5.00 57.30 30.98 13.16 ALF 383561 48.94 40.00 4.42 55.99 31.92", "label": "Average", "ID": "517"}, {"sentence": "The tail documents typically have lower phrase coverage, thus incor- rect phrase translation pairs derived from incorrect 937 # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 934206 60.74 28.05 16.35 69.02 17.83 25.60 ALF 797685 60.33 28.52 15.91 68.31 19.27 24.52 Table 5: Improved Chinese-English Newswire Translation with Alignment Link Filtering # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 934206 62.87 25.08 18.89 66.55 18.80 23.88 ALF 797685 62.30 24.89 18.70 65.97", "label": "Average", "ID": "518"}, {"sentence": "289  1  2  3  4  5  6  Clause in Covington's  distance function  \"identical consonants or glides\"  \"identical vowels\"  \"vowel ength difference only\"  \"non-identical vowels\"  \"non-identical consonants\"  \"no similarity\"  Covington's  penalty  10  30  60  100  AV  Hamming  distance  0.0  0.0  1.0  2.2  4.81  8.29  Interpolated  average  distance  0.0  0.0  12.4  27.3  58.1  100.0  Table 2: The clause-by-clause comparison of Covington's distance function (column 3) and a feature-based  distance function (columns 4 and 5).", "label": "Average", "ID": "519"}, {"sentence": "s. The tail documents typically have lower phrase coverage, thus incor- rect phrase translation pairs derived from incorrect 937 # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 934206 60.74 28.05 16.35 69.02 17.83 25.60 ALF 797685 60.33 28.52 15.91 68.31 19.27 24.52 Table 5: Improved Chinese-English Newswire Translation with Alignment Link Filtering # phrase pairs AV Tail TER BLEU (TER-BLEU)/2 TER BLEU (TER-BLEU)/2 Baseline 934206 62.87 25.08 18.89 66.55 18.80 23.88 ALF 797685 62.30 24.89 18.70 65.97 19.25 23.36 Table 6: Improved Chinese-English Web-Blog Translation with Alignment Link Filtering alignment links are more likely to be selected.", "label": "Average", "ID": "520"}, {"sentence": "AV criteria for Chi- nese word extraction.", "label": "Accessor variety", "ID": "521"}, {"sentence": "AV criteria for chinese word extraction.", "label": "Accessor variety", "ID": "522"}, {"sentence": "AV criteria for Chinese word extraction.", "label": "Accessor variety", "ID": "523"}, {"sentence": "AV of strings with length 3: L3av(c[i:i+2]), L3av(c[i+1:i+3]), R3av(c[i?2:i]), R3av(c[i?3:i?1]); ?", "label": "Accessor variety", "ID": "524"}, {"sentence": "AV of strings with length 2: L2av(c[i:i+1]), L2av(c[i+1:i+2]), R2av(c[i?1:i]), R2av(c[i?2:i?1]).", "label": "Accessor variety", "ID": "525"}, {"sentence": "AV of strings with length 4: L4av(c[i:i+3]), L4av(c[i+1:i+4]), R4av(c[i?3:i]), R4av(c[i?4:i?1]); ?", "label": "Accessor variety", "ID": "526"}, {"sentence": "c?2008 Association for Computational Linguistics Evaluating a Crosslinguistic Grammar Resource: A CA Study of Wambaya Emily M. Bender University of Washington Department of Linguistics Box 354340 Seattle WA 98195-4340 ebender@u.washington.edu Abstract This paper evaluates the LinGO Grammar Ma- trix, a cross-linguistic resource for the de- velopment of precision broad coverage gram- mars, by applying it to the Australian language Wambaya.", "label": "Case", "ID": "527"}, {"sentence": "Grac?a, Joa?o V., Joana P. Pardal, Lu??sa Coheur, and Diamantino CAiro.", "label": "Case", "ID": "528"}, {"sentence": "CA: case assignment by verbs to dependents ?", "label": "Case", "ID": "529"}, {"sentence": "CA .", "label": "Case", "ID": "530"}, {"sentence": "CAs  like this are currently excluded from the MUC  coreference task, which limits itself to relations  between NPs.", "label": "Case", "ID": "531"}, {"sentence": "Thus h(M)<harnaony(M),  CA >A. lf M is in list, then harmony(M) must he defined  and set at a wdue _< h(M).", "label": "Case", "ID": "532"}, {"sentence": "As expected, the overall accuracy of identify- ing contingency and expansion relations is lower, Task All relations Explicit relations only Comparison 91.28% (76.54%) 97.23% (69.72%) Contingency 84.44% (76.81%) 93.99% (79.73%) Temporal 94.79% (86.54%) 95.4% (79.98%) Expansion 77.51% (55.67%) 97.61% (65.16%) Table 2: Decision tree CA us- ing only the presence of connectives as binary fea- tures.", "label": "classification accuracy", "ID": "533"}, {"sentence": "On explicit data only, the two-way CA for the four main types of relations is 94% and higher.", "label": "classification accuracy", "ID": "534"}, {"sentence": "Over explicit data only, the CA for comparison relation versus any other relation is 97.23%, and precision and recall is 0.95 and above.", "label": "classification accuracy", "ID": "535"}, {"sentence": "The results are reported in Table 4 in terms of average CA.", "label": "classification accuracy", "ID": "536"}, {"sentence": "In four-way classification, disambiguating be- tween the four main semantic types of discourse relations leads to 74.74% CA.", "label": "classification accuracy", "ID": "537"}, {"sentence": "ones, we evaluated its CA on a (new) test set of size 300, split evenly between sen- tences of sequence length 24 and sequence length 1.", "label": "classification accuracy", "ID": "538"}, {"sentence": "The user?s CAion set Acomu consisted of 11 actions such as saying the color of a light (e.g., ?", "label": "communicative act", "ID": "539"}, {"sentence": "This is an important re- sult because past work that has applied POMDPs to dialog systems has employed a single modality (CAions), and have largely had fixed persistent state.", "label": "communicative act", "ID": "540"}, {"sentence": "The model of the user?s CAion assumes that the user provides correct (but possibly incomplete) informa- tion with p = 0.9, and remains silent with p = 0.1.", "label": "communicative act", "ID": "541"}, {"sentence": "In the experiments below, the value of perr is varied to explore how the POMDP policy trades off between the ping action and CAions.", "label": "communicative act", "ID": "542"}, {"sentence": "Second, the user?s action au is decomposed into two components: atsu denotes troubleshooting actions that are directed toward the product, such as turning a modem on or off, entering a user name or just observing the status lights; and acomu denotes CAions to the dialog system such as saying ?", "label": "communicative act", "ID": "543"}, {"sentence": "As perr increases, the policy decreasingly employs the ping diagnostic action in favor of the ask-working-ok CAion until perr = 20%, at which point the ping action is 84 85 86 87 88 89 90 91 92 93 94 0% 5% 10 % 15 % 20 % 25 % 30 % 35 % 40 % 45 % 50 % p err  (ping error rate) Av e ra ge   re tu rn Figure 4: Error rate of the ping action vs. reward gained per dialog.", "label": "communicative act", "ID": "544"}, {"sentence": "Towards understanding spontaneous  speech: word accuracy vs. CA.\"", "label": "concept accuracy", "ID": "545"}, {"sentence": "For the purpose of evaluation of CA, we developed an approach similar to (Boros et al, 1996) in which computing CA is reduced to com- paring strings representing core contentful concepts.", "label": "concept accuracy", "ID": "546"}, {"sentence": "The Alpino system achieves a CA of around 90% on common Dutch corpora (Van Noord, 2007).", "label": "concept accuracy", "ID": "547"}, {"sentence": "4.1 ASR and CA We evaluated overall word, sentence, and con-cept accuracy for all 8,228 spoken utterances to the system, shown in the first row of Table 1.", "label": "concept accuracy", "ID": "548"}, {"sentence": "Towards understanding spon- taneous speech: Word accuracy vs. CA.", "label": "concept accuracy", "ID": "549"}, {"sentence": "The example above yields the following meaning representation for CA.", "label": "concept accuracy", "ID": "550"}, {"sentence": "Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 29?30, Los Angeles, CA, June 2010.", "label": "California", "ID": "551"}, {"sentence": "University of CA, Los Angeles.", "label": "California", "ID": "552"}, {"sentence": "Opinion texts \u0001 System by University of Southern CA (Kim and Hovy 2005) 16.", "label": "California", "ID": "553"}, {"sentence": "In Proceedings, Tenth An-  nual IEEE Symposium on Logic in Computer Science,  pages 464-473, San Diego, CA, 26-29 June.", "label": "California", "ID": "554"}, {"sentence": "In Proceedings of the 34th Annual Meeting of the  Association for Computational Linguistics, pages  184-191, CA, USA.,", "label": "California", "ID": "555"}, {"sentence": "CA State  University, Fresno.", "label": "California", "ID": "556"}, {"sentence": "CA ! \\]", "label": "Case Analysis", "ID": "557"}, {"sentence": "An  Effect of Combining CA with  ATNG-based Analysis  The next section shows one application of the  LUTE-E J  parser, which is a machine translation  system.", "label": "Case Analysis", "ID": "558"}, {"sentence": "A Merit  of Using CA  In two sentences, each having different syntactic  structures, there is a problem involved in identifying  each case by extracting semantic relations between a  predicate  and arguments  (NPs, or NPs  hav ing   prepositional marks).", "label": "Case Analysis", "ID": "559"}, {"sentence": "Comlmtational  561   Corpus-based NLP    A CA Method Cooperating with ATNG  and Its Application to Machine Translation  Hitoshi IIDA, Kentaro OGURA and Hirosato NOMURA  Musashino Electrical Communication Laboratory, N.T.T.  Musashino-shi, Tokyo, 180, Japan  Abstract  This paper present a new method for parsing  English sentences.", "label": "Case Analysis", "ID": "560"}, {"sentence": "L.._ CA \\[  *case-frame*  <*agent> J  I <*object> I  __~ STRUCTU~D-CONSTITUZNT-BUFFER I  ~ CA \\[ \\]  Fig.1 Conceptual Diagram of LUTE-EJ Analysis  analysis of i NOUN Phrase  ATNG-based analysis  process  (embedded clause,  noun clause  I. I  2.5.", "label": "Case Analysis", "ID": "561"}, {"sentence": "General category constraints ensure that the preposition can attach to nouns and verbs, but not, say, to a determiner or to PU.", "label": "punctuation", "ID": "562"}, {"sentence": "Examples 1 and 2 show the representation that would be obtained for two imaginary English sen- 2Clause delimiters are PU marks other than com- mata, relative pronouns and subordinating conjunctions.", "label": "punctuation", "ID": "563"}, {"sentence": "Batista et al (2007) inserted a module for re- covering PU marks, based on maximum entropy models, after the ASR module.", "label": "punctuation", "ID": "564"}, {"sentence": "We propose the following guidelines for segmentation (for a more complete discussion see our other article in this volume): \u0000 tokens do not contain white space; \u0000 tokens either are PU marks or do not contain any PU marks; \u0000 an exception to the previous guideline are certain words containing the hyphen (e.g., mass-media, s-ka = an abbreviation of sp?\u0007ka ?", "label": "punctuation", "ID": "565"}, {"sentence": "n the way c appears in A and B. c' = 0 if c ap-  pears in neither A or B; d = 1 if c appears in both  A and B; d = 2 if c appears in A and not in B;  and d = 3 if c appears not in A but in B. We con-  sider clue expressions from the following grammat-  ical classes: nominals, adjectives, demonstratives,  adverbs, sentence connectives, verbs, sentence-final  particles, topic-marking particles, and PU  marks.", "label": "punctuation", "ID": "566"}, {"sentence": "But since the length of an ZC.Ad/2 f~ string  consisting of a fixed munber of units is l)ounded  each PUable string could be PUed up such  that it exceeds this bound.", "label": "pump", "ID": "567"}, {"sentence": "FP-2 When used in external subcutaneous infu- sion PUs for insulin, NovoLog should not be mixed with any other insulins or diluent.", "label": "pump", "ID": "568"}, {"sentence": "In How many X questions  (where X is a noun), quantified phrases whose head  noun is also X are ranked above bare numbers or  other quantified phrases: for example, in the query  How many lives were lost in the Lockerbie air crash,  entities such as 270 lives or almost 300 lives would  be ranked above entities such as 200 PUkins or  150.", "label": "pump", "ID": "569"}, {"sentence": "A hm, g'aage L is f in i te ly   PUab le  'ill there is a constant c such th, at  for any w C L with, \\['w\\[ > c, there arc a finite  number k and strings uo , . . .", "label": "pump", "ID": "570"}, {"sentence": "The system  receives a score of 1, 1/2, 1/3, 1/4, 1/5, or 0, re-  2perhaps less desirably, people would not be recognized  as a synonym of lives in this example: 200 people would be  indistinguishable from 200 PUkins.", "label": "pump", "ID": "571"}, {"sentence": "2.3 Pmnpabi l i ty  and Semi l inear i ty   We will first consider the prol)erty of being  finitely PUabh,, its detined in (Oroenink, 1997).", "label": "pump", "ID": "572"}, {"sentence": "Hence the number of  units cannot be increased by PUing and all  PUable parts must consist of case markers  solely?", "label": "pump", "ID": "573"}, {"sentence": "that occur 40 times or fewer into the low oc-  curring class, disallowing nodes to be split if  they have 50 or fewer datapoints, and pruning  back nodes that give the smallest improvement  in node imPU.", "label": "purity", "ID": "574"}, {"sentence": ", goodwill, belong, accommodate, serve, merit, deserve, shine, radiate, glow, beam, disillusion, disenchant, proclaim, laud, glorify, extol, exalt, cheer, consider, purify, enervate, recuperate, amusingly, dearly, dear, affectionately, thoroughly, soundly, well, simply, time, posterboard, fettle, mildness, clemency, successfulness, prosper- ity, wellbeing, well-being, upbeat, wholeness, haleness, PU, pureness, innocence, antithesis, serendipity, superordinate, superior, possible, pleaser, idolizer, idoliser, amoralist Negative tawdry, shoddy, cheapjack, scrimy, unsound, unfit, bad, sorry, sad, pitiful, lamentable, distressing, de- plorable, abject, unfortunate, inauspicious, humbug, trouble, inconvenience, disoblige, bother, smell, stink, reek, twinge, sting, prick, burn, sting, burn,", "label": "purity", "ID": "575"}, {"sentence": "Our model achieves a cluster PU score of 90.3% on this dataset com- pared to 89.7% reported in Grenager and Manning.", "label": "purity", "ID": "576"}, {"sentence": "Our method increases the PU of the induced role clus- ters by a wide margin over a strong baseline.", "label": "purity", "ID": "577"}, {"sentence": "PU: it has been created only as an anal- ysis of English, and has not been compro- mised by publishing constraints or other non- lexicographic goals ?", "label": "purity", "ID": "578"}, {"sentence": "We report cluster PU, accuracy, precision, recall, and F1 for our latent variable logistic classifier (LogLV) and a baseline that assigns arguments to clusters accord- ing to their syntactic function (SyntFunc).", "label": "purity", "ID": "579"}, {"sentence": "However, in other cases there is no explicit indication of the direction of offset from the TF.", "label": "temporal focus", "ID": "580"}, {"sentence": "Consider, for example, ex- pressions like the following: (1) three days ago (2) last Monday (3) in two weeks time Once we know the TF, calculation of the temporal location referred to in each of these cases is straightforward, since the", "label": "temporal focus", "ID": "581"}, {"sentence": "In some cases, it is sufficient to determine what is sometimes called the TF, so that the precise location of a relative temporal expression on a timeline can be determined with respect to this ?", "label": "temporal focus", "ID": "582"}, {"sentence": "As part of the overall process, they use a heuristic for the interpretation of weekday names: if the day name in a clause is the same as that of the TF, then the TF is used; 4 otherwise, they look for any ?", "label": "temporal focus", "ID": "583"}, {"sentence": "Consider, for example, ex- pressions like the following: (1) three days ago (2) last Monday (3) in two weeks time Once we know the TF, calculation of the temporal location referred to in each of these cases is straightforward, since the temporal ex- pressions themselves explicitly indicate what we c ?", "label": "temporal focus", "ID": "584"}, {"sentence": "We will not explicitly address the question of deter- mining the TF: although this is clearly a key ingredient, we have found that using the doc- ument creation date performs well for the kinds of documents (typically newswire stories and similar document types) we are working with.", "label": "temporal focus", "ID": "585"}, {"sentence": "More so- phisticated strategies for TF tracking would likely be required in other genres.", "label": "temporal focus", "ID": "586"}, {"sentence": "They report Ac1 figures of 57.7% for TFy and 65.3% for user frequency, which counts the number of distinct users in a cell using a given term and is intended to offset bias resulting from users who upload a large batch of similar photos at a given location.", "label": "term frequenc", "ID": "587"}, {"sentence": "Then, for the English term tE, translated contextual vector cvtrJ (tE) is constructed as below: each English sen- tence sE which contains tE is translated into Japanese sentence strJ , then the TFy vectors5 v(strJ ) of Japanese translation s tr J are 5In the TFy vectores, compound terms are restricted to be up to five words long both for English and Japanese.", "label": "term frequenc", "ID": "588"}, {"sentence": "The next module compares the functions of dq against the functions of d. To make this comparison we have divided the module into three sub-modules: (a) Pre-processing: line breaks, tabs and spaces re- moval as well as case folding; (b) Features extrac- tion: character n-grams extraction, weighting based on normalized TFy (tf ); and (c) Compar- ison: a cosine similarity estimation.", "label": "term frequenc", "ID": "589"}, {"sentence": "Based on the results of our previous study, this paper further examines the correlation of TFy and the reliability of bilingual term correspondences estimated from bilingual news articles.", "label": "term frequenc", "ID": "590"}, {"sentence": "Since we represent word to- kens rather than word types in the cohesion graph, we do not need to model the TFy tf separately, instead we set salience to the log value of the inverse document frequency idf : salience(t i ) = log |D| |{d : t i ?", "label": "term frequenc", "ID": "591"}, {"sentence": "Our TFy figure of 65.0% significantly beats theirs, but we found that user frequency actually degraded our dev set results by 5%.", "label": "term frequenc", "ID": "592"}, {"sentence": "1143-  1146  Yamamoto, M. and Church, K.W. (1998) Using Suffix  Arrays to Compare TF and  Document Frequency for All Substrings in Corpus.", "label": "Term Frequency", "ID": "593"}, {"sentence": "Yamamoto and Church TF and Document Frequency for All Substrings  computed over the classes rather than over the substrings, which would be prohibitive.", "label": "Term Frequency", "ID": "594"}, {"sentence": "Baseline system (Simple TF) 2.", "label": "Term Frequency", "ID": "595"}, {"sentence": "As illustrated in the  figure, s\\[i = 16\\] is the first suffix to start with the term \"to_be\" and s~ = 17\\] is the last  Yamamoto and Church TF and Document Frequency for All Substrings  Input  corpus :  \" to_be_or_not_ to_be\"   Position:  Characters:  Initialized  Suffix Array  s\\[O\\] 0  s\\[l\\] 1  s \\ [2\\ ]  2  s \\ [3\\ ]  3  s\\ [13\\]  13  s\\[18\\]  14  s \\[15\\] 15  s \\[16\\] 15  s\\ [17\\]  17  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 I t lo l_ lb le l_ lo l r l_ ln lo l t l_ l t lo l_ lb le l  .~\"  : : : : \"\"{ i i i !", "label": "Term Frequency", "ID": "596"}, {"sentence": "TF: wij is set to tfij , the num- ber of occurrences of lemi in the context cj .", "label": "Term Frequency", "ID": "597"}, {"sentence": "Section 2 describes the algorithms  and the code that were used to compute term frequencies and document frequencies  Yamamoto and Church TF and Document Frequency for All Substrings  for all substrings in two large corpora, an English corpus of 50 million words of the  Wall Street Journal, and a Japanese corpus of 216 million characters of the Mainichi  Shimbun.", "label": "Term Frequency", "ID": "598"}, {"sentence": "3.2 Phrasal TFing je invoque le Re`glement I orderofpointon arise PRP NNNNDTVBP IN IN NPNPNP PP NP PP VP S Figure 2: Phrasal Translation Example Since P alignments often align phrasal transla- Phrasal Filter Off Phrasal Filter On Alignment Type S S \u0000 P P S S \u0000 P P Head Crossings 0.236 4.790 5.284 0.172 2.772 2.492 Modifier Crossings 0.056 0.880 0.988 0.048 0.516 0.362 Phrasal Translations ? ?", "label": "Translation Filter", "ID": "599"}, {"sentence": "4.2 TFing  The direct translation process assumes that the re- trieved search-result pages of a term exactly contain  snippets from a certain region (e.g. Hong Kong) and  written in the target language (e.g. traditional Chi- nese).", "label": "Translation Filter", "ID": "600"}, {"sentence": "They report Ac1 figures of 57.7% for TF and 65.3% for user frequency, which counts the number of distinct users in a cell using a given term and is intended to offset bias resulting from users who upload a large batch of similar photos at a given location.", "label": "term frequency", "ID": "601"}, {"sentence": "Then, for the English term tE, translated contextual vector cvtrJ (tE) is constructed as below: each English sen- tence sE which contains tE is translated into Japanese sentence strJ , then the TF vectors5 v(strJ ) of Japanese translation s tr J are 5In the TF vectores, compound terms are restricted to be up to five words long both for English and Japanese.", "label": "term frequency", "ID": "602"}, {"sentence": "The next module compares the functions of dq against the functions of d. To make this comparison we have divided the module into three sub-modules: (a) Pre-processing: line breaks, tabs and spaces re- moval as well as case folding; (b) Features extrac- tion: character n-grams extraction, weighting based on normalized TF (tf ); and (c) Compar- ison: a cosine similarity estimation.", "label": "term frequency", "ID": "603"}, {"sentence": "Based on the results of our previous study, this paper further examines the correlation of TF and the reliability of bilingual term correspondences estimated from bilingual news articles.", "label": "term frequency", "ID": "604"}, {"sentence": "Since we represent word to- kens rather than word types in the cohesion graph, we do not need to model the TF tf separately, instead we set salience to the log value of the inverse document frequency idf : salience(t i ) = log |D| |{d : t i ?", "label": "term frequency", "ID": "605"}, {"sentence": "Our TF figure of 65.0% significantly beats theirs, but we found that user frequency actually degraded our dev set results by 5%.", "label": "term frequency", "ID": "606"}, {"sentence": "TF?", "label": "text filtering", "ID": "607"}, {"sentence": "The best TF re- sults were high (about 90% F score), but relied on hand-built event extraction systems.", "label": "text filtering", "ID": "608"}, {"sentence": "It has applications in text classification, TF, analysis of product review, analysis of responses to surveys, and mining online discussions.", "label": "text filtering", "ID": "609"}, {"sentence": "The percentage of nonrelevant texts in EJV is so low (approximately 5% in th e training corpus and 10% in the MUC-5 test sets) that a system can almost ignore the TF subtas k without suffering a serious degradation in performance; the system can be optimized in favor of generating tie-ups even when it is not sure there", "label": "text filtering", "ID": "610"}, {"sentence": "levant texts criterion, which figures in two of the dimensions, is based on the view tha t the more a system's performance would suffer as a consequence of ignoring the TF (document detection ) subtask, the harder the task .", "label": "text filtering", "ID": "611"}, {"sentence": "33 The percent nonrelevant texts criterion, which figures in two of the dimensions, is based on the view tha t the more a system's performance would suffer as a consequence of ignoring the TF (document detection ) subtask, the harder the task .", "label": "text filtering", "ID": "612"}, {"sentence": "Boosting and rocchio applied to TF.", "label": "text filtering", "ID": "613"}, {"sentence": "The percentage of nonrelevant texts in EJV is so low (approximately 5% in th e training corpus and 10% in the MUC-5 test sets) that a system can almost ignore the TF subtas k without suffering a serious degradation in performance; the system can be optimized in favor of generating tie-ups even when it is not sure there is sufficient information in the text .", "label": "text filtering", "ID": "614"}, {"sentence": "Hirst G. (1981) \"Discourse Oriented Anaphoral  Resolution in Natural LU:  A Review.\"", "label": "Language Understanding", "ID": "615"}, {"sentence": "Sabah G~rard 1997, The fundamental role of  pragmatics in Natural LU  and its implications for modular, cognitively mo-  tivated architectures, Studies in Computational  Pragmatics: Abduction, Belief, and Context, Uni-  versity College Press, to appear, London.", "label": "Language Understanding", "ID": "616"}, {"sentence": "44  Domain Dependent Natural LU  Klaus Heje Munch  Dept.", "label": "Language Understanding", "ID": "617"}, {"sentence": "Small, S. 1980 Word Expert Parsing: a Theory of Distributed Word-  Based Natural LU.", "label": "Language Understanding", "ID": "618"}, {"sentence": "Ch er-based LU  K. of  K.  atural Language Processing, 2004,    We develop a system to solve questions of sec- ond grade language tests.", "label": "Language Understanding", "ID": "619"}, {"sentence": "But there is no clear process for identifying potential tasks (other than consensus by a sufficient num- ber of researchers), nor for quantifying their po- tential contribution to existing NLP tasks, let alne to Natural LU.", "label": "Language Understanding", "ID": "620"}, {"sentence": "to the player, a good model needs to have some level of LU, while a naive model that memorizes all unique action texts in the original game will do poorly.", "label": "language understanding", "ID": "621"}, {"sentence": "This indicates that the DRRN has some generalization ability and gains a useful level of LU in the game scenario.", "label": "language understanding", "ID": "622"}, {"sentence": "Conceptual dependency: A the- ory of natural LU.", "label": "language understanding", "ID": "623"}, {"sentence": "Having imt)lemented our approach in a sys-  tem of LU which already  deals with a wide variety of referring expres-  sions, we have demonstrated its practicality.", "label": "language understanding", "ID": "624"}, {"sentence": "The system core is the Dialogue Manager, which processes the information coming from the different input modality agents by means of a natural LU module and provides output in the appropriate modality.", "label": "language understanding", "ID": "625"}, {"sentence": "Nevertheless, they are key to various NLP applications, including those benefiting from deep natural LU (e.g., textual inference (Bobrow et al, 2007)), generation of well- formed output (e.g., natural language weather alert systems (Lareau and Wanner, 2007)) or both (as in machine translation (Oepen et al, 2007)).", "label": "language understanding", "ID": "626"}, {"sentence": "This phenomenon is also known as the DOP hypothesis (Bod 1998), and has been confirmed for Tree-DOP on the ATIS, OVIS and WSJ treebanks (see Bod 1993, 1998, 1999, 2000a; Sima'an 1999; Bonnema et al 1997; Hoogweg 2000).", "label": "Wall Street Journal", "ID": "627"}, {"sentence": "Examples are \"is\" and \"has\", which  appear frequently in WSJ: these  verbs are not \"selective\" enough and the associ-  ated probability is not strong enough to rule out  165  erroneous candidates.", "label": "Wall Street Journal", "ID": "628"}, {"sentence": "Following this, we  show the results of applying this method to the  21-million-word 1987 WSJ cor-  pus using two different pronoun reference strate-  gies of varying sophistication, and evaluate their  performance using honorifics as reliable gender  indicators.", "label": "Wall Street Journal", "ID": "629"}, {"sentence": "DOP models have been shown to achieve state-of-the-art parsing performance on benchmarks such as the WSJ corpus (see Bod 2000a).", "label": "Wall Street Journal", "ID": "630"}, {"sentence": "This program differs  from earlier work in its almost complete lack of  hand-crafting, relying instead on a very small  corpus of Penn WSJ Tree-bank  text (Marcus et al, 1993) that has been marked  with co-reference information.", "label": "Wall Street Journal", "ID": "631"}, {"sentence": "This result is promising since Tree- DOP has been shown to obtain state-of-the-art performance on the WSJ corpus (see Bod 2000a).", "label": "Wall Street Journal", "ID": "632"}, {"sentence": "For example, Pan et al (2011)  hand-annotated of  a  portion  of  the  TIMEBANK  corpus that consisted of WSJ  arti- cles.", "label": "Wall Street  Journal", "ID": "633"}, {"sentence": "3 Non-IF Pruning Through Non-Expert Annotations To prune the non-informative features, a tradi- tional approach would be to hire and train anno- tators to label which portion of each training sen- tence is informative or non-informative.", "label": "Informative Feature", "ID": "634"}, {"sentence": "4 Pruning Non-IFs for Sentiment Classification We conducted an experiment on sentiment classifi- cation in the domain of camera reviews to test the effect of pruning non-informative features based on AMT workers?", "label": "Informative Feature", "ID": "635"}, {"sentence": "4.4 IFs Next, we study the importance of individual features by measuring their chi-squared statistic with respect to the class variable.", "label": "Informative Feature", "ID": "636"}, {"sentence": "cted, NUM, solve, cars, pull, kinds, congress impacted C , solve C , cars C , NUM C , pool C , writing C , death C , link C - should, seems, comments should C , comments C V E R I F E X P + owed, consumed, saw, ex- pert, interesting, him, re- acted, refinance owed C , consumed C , expert C , reacted C , happened C , interesting C - impacted, wo impacted C , wo C , concern C , died C Table 5: Most IFs for UNI and UNI CCT 10 Unigrams with the largest weight (magnitude) with respect to each class ( + : positive weight / - : negative weight).", "label": "Informative Feature", "ID": "637"}, {"sentence": "Table 1: Classification Accuracy All Features IFs 41.7% 45.8% In this experiment, pruning the non-informative features improves the accuracy by more than 4%.", "label": "Informative Feature", "ID": "638"}, {"sentence": "The other approach is to learn from the IFs-Set derived from the sen- tences with the non-informative portion removed by the AMT workers.", "label": "Informative-Feature", "ID": "639"}, {"sentence": "The probability of any node in the hi- erarchy is the product of the probabilities of that node and all of its ANCs, up to the root.", "label": "ancestor", "ID": "640"}, {"sentence": "Previously, 600,000 years old  ANCs, called homo hudlabar [sic] in scientific  term, were supposed to be the most ancient  inhabitants of the region.", "label": "ancestor", "ID": "641"}, {"sentence": "As lexicalization is a bottom-up process, for reading-off dom(am, an), it is sufficient to look at the lowest common ANC (LCA) of both an- chors; if the anchors cannot lexicalize the LCA, they won?t be able to lexicalize the constituents larger than LCA.", "label": "ancestor", "ID": "642"}, {"sentence": "Types that are used indirectly are either ANC types to types that are used directly, or types that are used as the value of a feature in a constraint in the Matrix core types on a type that is used (directly or indirectly) by the Wambaya-specific portion of the grammar.", "label": "ancestor", "ID": "643"}, {"sentence": "Some approach  attempts to locate bilingual text within a web  page (Jiang et al, 2009); some others attempt to  collect web pages in different languages and  decide the parallel relationship between the web  pages by means of structural cues, like exist- ence of a common ANC web page, similarity  between URLs, and similarity between the  HTML structures (Chen and Nie, 2000; Resnik                                                    1 This work has been done while the first author was visit- ing Microsoft Research Asia.", "label": "ancestor", "ID": "644"}, {"sentence": "We then looked  at an ANC tree following the WordNet hypernym  relation.", "label": "ancestor", "ID": "645"}, {"sentence": "These lan- guage models were estimated from the written por- tion of the ANC Second Re- lease (Ide and Suderman, 2004), which consists of approximately 20 million tokens, using Kneser and Ney (1995) smoothing.", "label": "American National Corpus", "ID": "646"}, {"sentence": "Travel Guides was drawn from the Berlitz travel guides data in the Open ANC (Ide and Suder- man, 2004) and includes very verbose sentences 18 4 ?", "label": "American National Corpus", "ID": "647"}, {"sentence": "2.2 Test Set The test set consists of 4806 instances of 50 target words: 20 verbs (1901 instances), 20 nouns (1908), and 10 adjectives (997).3 Instances are extracted from the Open ANC, being a mix of both written and spoken contexts of target words.4 Only 542 instances are assigned more than one sense by annotators, thus have graded senses.", "label": "American National Corpus", "ID": "648"}, {"sentence": "tute Berkeley, California USA collinb@icsi.berkeley.edu Christiane Fellbaum Princeton University Princeton, New Jersey USA fellbaum@princeton.edu Rebecca Passonneau Columbia University New York, New York USA becky@cs.columbia.edu Abstract The Manually Annotated Sub-Corpus (MASC) project provides data and annota- tions to serve as the base for a community- wide annotation effort of a subset of the ANC.", "label": "American National Corpus", "ID": "649"}, {"sentence": "The information in each lexical entry was compiled manually in a data-driven fashion by exploring its use in our corpora of reference, TimeBank and the ANC (Slate and NYTimes fragments).22 De Facto takes as input a document (or a set of them) and returns the factuality profiles of each event.", "label": "American National Corpus", "ID": "650"}, {"sentence": "The ANC: A standardized resource of American English.", "label": "American National Corpus", "ID": "651"}, {"sentence": "\\[25\\] Sows L, ConcSs : Information  Processing in Mind and Machine, Addison-Wesley,  Reading, Massachusetts, 1984  \\[25a\\] van Eynde, F., The Semantics of Tense and Aspect,  in: Proceedings of EAIA-90, 2rid Advanced School in  Artificial Intelligence.", "label": "Conceptual structures", "ID": "652"}, {"sentence": "Sowa John 1984 ConcSs : information  processing in mind and machine, Addison Wesley,  Reading, MA.", "label": "Conceptual structures", "ID": "653"}, {"sentence": "ConcSs: Informa-  processing in man and machine.", "label": "Conceptual structures", "ID": "654"}, {"sentence": "(Sowa 84)  J. Sowa, ConcSs - Information processing  in mind and machine, Addison Wesley Publishing Com-  pany, Reading, Mass., 1984.", "label": "Conceptual structures", "ID": "655"}, {"sentence": "L. Erlbaum Associates,  1977  Sowa, John F. ConcSs:  Information Processing in Mind and  Machine.", "label": "Conceptual structures", "ID": "656"}, {"sentence": "Sowa John 1984, ConcSs: informa-  tion processing in mind and machine , Addison  Wesley, Reading Mass.  Talmy L. 1985, Lexicalisation patterns: Semantic  structure in lexical forms, Language typology and  syntactic description, 3 , Cambridge University  Press, New York, p. 57-149.", "label": "Conceptual structures", "ID": "657"}, {"sentence": "ConcSs are structured configurations of conceptual units, which  are mental representations of certain aspects of the external world.", "label": "Conceptual representations", "ID": "658"}, {"sentence": "ConcSs are obtained from W through the process called word-to-concept map- ping.", "label": "Conceptual representations", "ID": "659"}, {"sentence": "395 446 412 ASL 18.0 20.3 18.7 Vocabulary Size 213 176 202 199 Running OOVs - 2.6k 44.3% 35.4% 32.1% 34.7% Running OOVs - 2", "label": "Average Sentence Length", "ID": "660"}, {"sentence": "1666 1878 1761 ASL 8.3 10.4 8.8 Vocabulary Size 778 596 603 600 Singletons 618 417 395 Dev+Test Sentences 500 500 Running Words + Punct.", "label": "Average Sentence Length", "ID": "661"}, {"sentence": "4161 4657 4362 ASL 8.3 9.3 8.7 Vocabulary Size 1457 1030 1055 1", "label": "Average Sentence Length", "ID": "662"}, {"sentence": "22227 24808 23308 ASL 8.4 9.5 8.8 Vocabulary Size 4546 2605 2645 2642 Singletons 2728 1253 1211 reduced corpus Sentences 200 200 (200) Running Words + Punct.", "label": "Average Sentence Length", "ID": "663"}, {"sentence": "4161 4657 4362 ASL 8.3 9.3 8.7 Vocabulary Size 1457 1030 1055 1052 Running OOVs - 2.6k 12.1% 5.2% 4.8% Running OOVs - 200 34.5% 27.6% 21.4% OOVs - 2.6k 32.7% 19.5% 19.7% OOVs - 200 76.2% 66.0% 66.8% External Test Sentences 22 22 Running Words + Punct.", "label": "Average Sentence Length", "ID": "664"}, {"sentence": "395 446 412 ASL 18.0 20.3 18.7 Vocabulary Size 213 176 202 199 Running OOVs - 2.6k 44.3% 35.4% 32.1% 34.7% Running OOVs - 200 53.7% 44.6% 43.7% 47.3 % OOVs - 2.6k 61.5% 45.4% 44.0% 44.7% OOVs - 200 74.6% 63.1% 63.9% 64.8% Table 2: Statistics of the Serbian-English short phrases Serbian English Phrases original base forms original no article Entries 351 351 351 351 Running Words + Punct.", "label": "Average Sentence Length", "ID": "665"}, {"sentence": "Number of Sentences 935  ASL 12  Range of Sentence Lengths 7-17  Correct Parse Present 96%  Correct Parse Most Likely 73%  Table 6: Results for Test Set A  P1 P  FRV2 Fr  SD Fr  IANYTI  Ti  JBVVN* :lJ  Table 5: Sample of grammatical  category mappings  Number of Sentences 1105  ASL 12  Range of Sentence Lengths 7-17  Correct Parse Present 95%  Correct Parse Most Likely 75%  Table 7:", "label": "Average Sentence Length", "ID": "666"}, {"sentence": "Number of Sentences 935  ASL 12  Range of Sentence Lengths 7-17  Correct Parse Present 96%  Correct Parse Most Likely 73%  Table 6: Results for Test Set A  P1 P  FRV2 Fr  SD Fr  IANYTI  Ti  JBVVN* :lJ  Table 5: Sample of grammatical  category mappings  Number of Sentences 1105  ASL 12  Range of Sentence Lengths 7-17  Correct Parse Present 95%  Correct Parse Most Likely 75%  Table 7: Results for Test Set B  191  Recall (see above) that the geometric mean of the  number of parses per word, or equivalently the total num-  ber of parses for the entire test set, must be held con-  stant over the course of the grammar's development, to  eliminate trivial so", "label": "Average Sentence Length", "ID": "667"}, {"sentence": "4161 4657 4362 ASL 8.3 9.3 8.7 Vocabulary Size 1457 1030 1055 1052 Running OOVs - 2.6k 12.1% 5.2% 4.8% Running OOVs - 200 34.5% 27.6% 21.4% OOVs - 2.6k 32.7% 19.5% 19.7% OOVs - 200 76.2% 66.0% 66.8% External Test Sentences 22 22 Running W", "label": "Average Sentence Length", "ID": "668"}, {"sentence": "Narrative/story  structure, pausing and ASL.", "label": "American Sign Language", "ID": "669"}, {"sentence": "Some arguments for syn-  tactic patterning in ASL.", "label": "American Sign Language", "ID": "670"}, {"sentence": "Invariant characteristics of some mor-  phological processes inASL.", "label": "American Sign Language", "ID": "671"}, {"sentence": "Interaction of Morpholo-  gy and Syntax in ASL.", "label": "American Sign Language", "ID": "672"}, {"sentence": "Spatial mapping in compara- tive discourse frames in an ASL  lecture.", "label": "American Sign Language", "ID": "673"}, {"sentence": "A Survey and Critique of  ASL Natural Language Genera- tion and Machine Translation Systems.", "label": "American Sign Language", "ID": "674"}, {"sentence": "A Machine Translation System  from English to ASL.", "label": "American Sign Language", "ID": "675"}, {"sentence": "Partially Ob-servable MDP for Spoken Dialog Systems.", "label": "Markov Decision Processes", "ID": "676"}, {"sentence": "Partially Observable MDP for Spoken Dialog Sys- tems.", "label": "Markov Decision Processes", "ID": "677"}, {"sentence": "Controlling Listening-oriented Dialogue using Partially Observ- able MDP.", "label": "Markov Decision Processes", "ID": "678"}, {"sentence": "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 761?769, Beijing, August 2010 Controlling Listening-oriented Dialogue using Partially Observable MDP Toyomi Meguro?,", "label": "Markov Decision Processes", "ID": "679"}, {"sentence": "Reinforcement Learning for Fac- tored MDP.", "label": "Markov Decision Processes", "ID": "680"}, {"sentence": "Dialogue control algorithm for ambient intelligence based on partially observable MDPes.", "label": "markov decision process", "ID": "681"}, {"sentence": "Learning to control listening-oriented dialogue using partially observable MDPes.", "label": "markov decision process", "ID": "682"}, {"sentence": "Using MDP for learning dialogue strategies.", "label": "markov decision process", "ID": "683"}, {"sentence": "Par- tially observable MDPes for spo- ken dialog systems.", "label": "markov decision process", "ID": "684"}, {"sentence": "Fac- tored partially observable MDPes for dialogue management.", "label": "markov decision process", "ID": "685"}, {"sentence": "Partially ob- servable MDPes for spoken dialog systems.", "label": "markov decision process", "ID": "686"}, {"sentence": "The forward and BA syllable-level longest matching are performed.", "label": "backward", "ID": "687"}, {"sentence": "The number of forward?BA calls for normal HMM is 40k (one for each sentence and EM iteration), for the symmetric model using ?", "label": "backward", "ID": "688"}, {"sentence": "t, which is accomplished by the forward-BA algorithm for HMMs.", "label": "backward", "ID": "689"}, {"sentence": "\u0003f(x,zc ) (13) Hence the projection step uses the same inference algorithm (forward?BA for HMMs) to compute the gradient, only modifying the local factors using the current setting of ?.", "label": "backward", "ID": "690"}, {"sentence": "p (z, x) exp{??\u0005f(x, z)} All these quantities can be computed separately in each model using forward?BA and, furthermore, Z?", "label": "backward", "ID": "691"}, {"sentence": "according to the BA", "label": "backward", "ID": "692"}, {"sentence": "For HMM word alignments, we need to make several calls to forward?BA in order to choose ?.", "label": "backward", "ID": "693"}, {"sentence": "We compare five differente models: ADD is the BA model with parameters ?", "label": "Basic Additive", "ID": "694"}, {"sentence": "fo~ns?1~.  The expex~ment~  used RU as 1spur language.", "label": "Russian", "ID": "695"}, {"sentence": "References:  I. \"A Theory of Linguistic Models MEANING -- TEXT\", Moscow,  1972 (in RU).", "label": "Russian", "ID": "696"}, {"sentence": "6 Future  D i rec t ions   The goal of my current research is to combine the  new alignment algorithm with a cognate identifica-  tion procedure, The alignment of cognates is possi-  8For example, stress regularly falls on the initial syllable  in Czech and on the penultimate syllable in Polish, while in  RU itcan fall anywhere in the word.", "label": "Russian", "ID": "697"}, {"sentence": "Another example:  \"President and wife came to capital\"  (Articles and pronouns ere dropped to reflect RU),  This phrase is processed as  \"President roof-country with xhis wife came to capital Eof-  country\".", "label": "Russian", "ID": "698"}, {"sentence": "In  the second one, \"v i r tua l  pronoun\" occas -   iona l ly  turned to be a rea l  one; whi le  in  Eng l i sh  i t s  use i s   qu i te  natura l ,  in  RU the use of  posess ive  pronoun would  have an emphatic meaning.", "label": "Russian", "ID": "699"}, {"sentence": "One admirable standardization effort in the field of Slavic part of speech (POS) tagging has been the Multext-East project (Erjavec, 2001), one of whose aims was to construct mutually compati- ble tagsets for 8 European languages, including 4 Slavic languages (originally Bulgarian, Czech and Slovene, later extended to Croatian); additionally, a Multext-East-style tagset for RU was con- structed at the University of T?bingen (http: //www.sfb441.uni-tuebingen.de/c1/ tagset.html).", "label": "Russian", "ID": "700"}, {"sentence": "that have D1 as an operand do not produce 2It is also necessary to show that no position set is merged in two different RU, but this easily follows from the fact that hD1?D2(X) = hD1?D2(Y ) if and only if X ?", "label": "reductions", "ID": "701"}, {"sentence": "The binarization is 2-feasible if all of the involved RU are 2- feasible.", "label": "reductions", "ID": "702"}, {"sentence": "A binarization of X is a sequence of RU resulting in a new collection with two or fewer position sets.", "label": "reductions", "ID": "703"}, {"sentence": "as a sequence of RU, where each re- duction is characterized by a pair of position sets (X1, X2) which are merged into X1 ?", "label": "reductions", "ID": "704"}, {"sentence": "can only produce conflicting RU in ??", "label": "reductions", "ID": "705"}, {"sentence": "if they merge two position sets differing only by D1, but in this case, one of the RU must merge D1 so it does not produce any reduction in ??.", "label": "reductions", "ID": "706"}, {"sentence": "Thus, two RU in ?", "label": "reductions", "ID": "707"}, {"sentence": "We consider the RU in ?", "label": "reductions", "ID": "708"}, {"sentence": "RU? (", "label": "Relative Utility", "ID": "709"}, {"sentence": "6 Evaluation Lexical LSA CAST GUITAR GUITAR Method Substitution Addition RU 0.595 0.527 0.530 0.640 F-score 0.420 0.348 0.347 0.441 Cosine Similarity 0.774 0.726 0.804 0.805 Main Topic Similarity 0.686 0.630 0.643 0.699 Table 3: Evaluation of the GUITAR improvement - summarization ratio: 15%.", "label": "Relative Utility", "ID": "710"}, {"sentence": "Figures 6 and 7 summarize the results obtained through RU.", "label": "Relative Utility", "ID": "711"}, {"sentence": "In this paper, we present a comparison of six summarizers as well as a meta-evaluation including eight measures: Precision/Recall, Percent Agree- ment, Kappa, RU, Relevance Correla- tion, and three types of Content-Based measures (cosine, longest common subsequence, and word overlap).", "label": "Relative Utility", "ID": "712"}, {"sentence": "We found that while all measures tend to rank summarizers in different orders, measures like Kappa, RU, Relevance Correlation and Content-Based each offer significant advantages over the more simplistic methods.", "label": "Relative Utility", "ID": "713"}, {"sentence": "Evaluation Lexical LSA CAST GUITAR GUITAR Method Substitution Addittion RU 0.645 0.618 0.626 0.678 F-score 0.557 0.522 0.524 0.573 Cosine Similarity 0.", "label": "Relative Utility", "ID": "714"}, {"sentence": "1 2 3 4 5] is represented as [1/5 2/4 3/4 5 Evaluation Lexical LSA Manual Manual Method Substitution Additition RU 0.595 0.573 0.662 F-score 0.420 0.410 0.489 Cosine Similarity 0.774 0.806 0.823 Main Topic Similarity 0.686 0.682 0.747 Table 1: Evaluation of the manual annotation improvement - summarization ratio: 15%.", "label": "Relative Utility", "ID": "715"}, {"sentence": "Evaluation Lexical LSA Manual Manual Method Substitution Addition RU 0.645 0.662 0.688 F-score 0.557 0.549 0.583 Cosine Similarity 0.863 0.878 0.886 Main Topic Similarity 0.836 0.829 0.866 Table 2: Evaluation of the manual annotation improvement - summarization ratio: 30%.", "label": "Relative Utility", "ID": "716"}, {"sentence": "Since the research described  herein we have thought of other influences on  AR and their statistical corre-  lates.", "label": "anaphora resolution", "ID": "717"}, {"sentence": "We incorpo-  rate multiple AR factors into  a statistical framework - -  specifically the dis-  tance between the pronoun and the proposed  antecedent, gender/number/animaticity of the  proposed antecedent, governing head informa-  tion and noun phrase repetition.", "label": "anaphora resolution", "ID": "718"}, {"sentence": "Although this success rate  overstates the effect, it is a clear indication that  knowledge of a referent's gender and animatic-  ity is essential to AR.", "label": "anaphora resolution", "ID": "719"}, {"sentence": "The more accurately the topic of a seg-  ment can be identified, the higher the success  rate we expect an AR system  can achieve.", "label": "anaphora resolution", "ID": "720"}, {"sentence": "Mitkov (1997) does a detailed study  on factors in AR.)", "label": "anaphora resolution", "ID": "721"}, {"sentence": "This indicates  that syntax does play a very important role in  AR.", "label": "anaphora resolution", "ID": "722"}, {"sentence": "Proceedings of the Seventh Global Wordnet Conference, chapter Java Libraries for Accessing the PWN: Comparison and Evaluation, pages 78?85.", "label": "Princeton Wordnet", "ID": "723"}, {"sentence": "et al 2004a)  with new literals based on bilingual corpora evidence  and to check the interlingual alignment of our wordnet  against the PWN.", "label": "Princeton Wordnet", "ID": "724"}, {"sentence": "PWN 2.13 (Fellbaum, 1998) was used as the sense inventory.", "label": "Princeton Wordnet", "ID": "725"}, {"sentence": "PWN (Fellbaum et al 2006) is consulted to identify synonyms and other related words commonly used in co-references.", "label": "Princeton Wordnet", "ID": "726"}, {"sentence": "Cornetto combines  two resources with different semantic organisa- tions: the Dutch Wordnet (DWN) which has, like  the PWN, a synset organization and  the Dutch Reference Lexicon (RBN) which is or- ganised in form-meaning composites or lexical  units.", "label": "Princeton Wordnet", "ID": "727"}, {"sentence": "Open Multilingual Wordnet (OMW) project 3 links WordNet style structured resources, in up to 150 languages, to the PWN of English 4 .", "label": "Princeton Wordnet", "ID": "728"}, {"sentence": "Both of them are manually created thesaurus, for Brazil- ian Portuguese and European Portuguese respec- tively, modelled after PWN (Fell- baum, 1998) and thus containing synsets.", "label": "Princeton WordNet", "ID": "729"}, {"sentence": "Some au- thors (de Melo and Weikum, 2008) propose trans- lating PWN to wordnets in other lan- guages, but if this might be suitable for several ap- plications, a problem arises because different lan- guages represent different socio-cultural realities, do not cover exactly the same part of the lexicon and, even where they seem to be common, several concepts are lexicalised differently (Hirst, 2004).", "label": "Princeton WordNet", "ID": "730"}, {"sentence": "Knowledge about words and their meanings is typically structured in lex- ical ontologies, such as PWN (Fell- baum, 1998), but this kind of resources is most of the times handcrafted, which implies much time- consuming human effort.", "label": "Princeton WordNet", "ID": "731"}, {"sentence": "However, while for English most re- searchers use PWN as a gold stan- dard, for other languages it is difficult to find suitable and freely available consensual resources.", "label": "Princeton WordNet", "ID": "732"}, {"sentence": "10 PWN (Fellbaum, 1998) is the most representative lexico-semantic resource for English and also the most accepted model of a lexical ontology.", "label": "Princeton WordNet", "ID": "733"}, {"sentence": "Such approach is applied in a paper by Harabagiu et al(2001), where the PWN (Fellbaum, 1998) serves as an ontology to determine foci types.", "label": "Princeton WordNet", "ID": "734"}, {"sentence": "There are four possible operations (Right, Left,  SH and Reduce) for the configuration at hand.", "label": "Shift", "ID": "735"}, {"sentence": "If the parsing operation                                                             1  To estimate the current operation (Left, Right, SH and  Reduce) by SVMs, we need to build 6 classifiers(Left-Right,  Left-SH, Left-Reduce, Right-SH, Right-Reduce and SH- Reduce).", "label": "Shift", "ID": "736"}, {"sentence": "SH: Push NEXT onto the stack.", "label": "Shift", "ID": "737"}, {"sentence": "which may  depend on t, and t has a parent on its left side, the  parser removes t from the stack S.  SH: If there is no dependency between n and t,  and the triple does not satisfy the conditions for  Reduce, then push n onto the stack S.  In this work, we adopt SVMs for estimating the  word dependency attachments.", "label": "Shift", "ID": "738"}, {"sentence": "from entity to related task: entity is a se- mantic argument of the task SH to an unrelated focus From the analysis of our WoZ data we get cer- tain intuitions about salient focus flow between some preceding dialogue and a FU Q. First of all, we learn that a dialogue context of just one previ- ous user question and one previous system answer generally provides enough information to resolve context-dependent FU Qs.", "label": "Shift", "ID": "739"}, {"sentence": "SH-reduce CCG parsing.", "label": "Shift", "ID": "740"}, {"sentence": "For this reason, the perfor- mance of ALICE might be over-estimated in this evaluation; ALICE delivered much better results than SH?s method on this corpus.", "label": "Schwartz & Hearst", "ID": "741"}, {"sentence": "For more general acronym identification, we adapted the method of SH (2003).", "label": "Schwartz & Hearst", "ID": "742"}, {"sentence": "mouth) because of the similar SH and ?????(", "label": "shape", "ID": "743"}, {"sentence": "5 Const ra in ts ,  cho ices ,  and  SH   Given the target audience for this software,  the programming langage chosen is as close as  possible to the graphic interface.", "label": "shape", "ID": "744"}, {"sentence": "by the alignments because of the analogical SH.", "label": "shape", "ID": "745"}, {"sentence": "We apply the linear chain CRF (Lafferty et al, 2001), and show results using standard and softmax- margin CRF (SM-CRF) (Gimpel and Smith, 2010), with features consisting of word SH features, neighboring words, previous prediction and pre- fixes/suffixes.", "label": "shape", "ID": "746"}, {"sentence": "It is closely connected with the SH, pronunciation or meaning of Chi- nese characters.", "label": "shape", "ID": "747"}, {"sentence": "The SH of ???", "label": "shape", "ID": "748"}, {"sentence": "E TERRORIST KILLING OF ATTORNEY GENERAL ROBERTO GARCIA ALVARADO AND ACCUSED THE FMLN OF THE CRIME Preprocessor results : Node 0 : (SALVADORAN PRESIDENT-ELECT ALFREDO CRISTIANI ) Node 1 : CONDEMNED Node 2 : (THE TERRORIST) Node 3 : (KILLING) KILLIN G Node 4 : OF Node 5 : ((ATTORNEY GENERAL) ROBERTO GARCIA ALVARADO ) Node 6 : AND Node 7 : ACCUSE D Node 8 : (THE (FMLN) ) Node 9 : OF Node 10 : (THE CRIME ) Node 11 : The LINK parser LINK is a bottom-up, unification-based chart parser .", "label": "FARABUNDO MARTI NATIONAL LIBERATION FRONT", "ID": "749"}, {"sentence": "PERP : ORG CONF CLAIMED OR ADMITTED : \"FMLN \"12.", "label": "FARABUNDO MARTI NATIONAL LIBERATION FRONT", "ID": "750"}, {"sentence": "33 score : 0 \"TERRORISTS \" :732 score : 1 \"FMLN\" ) (EVENT-LOCATION-OF : ?", "label": "FARABUNDO MARTI NATIONAL LIBERATION FRONT", "ID": "751"}, {"sentence": "PERP : ORG ID \"FMLN \"11 .", "label": "FARABUNDO MARTI NATIONAL LIBERATION FRONT", "ID": "752"}, {"sentence": "Here are the results produced by the LINK parser for sentences 1 and 2 : Sentence 1 : SALVADORAN PRESIDENT-ELECT ALFREDO CRISTIANI CONDEMNED TH E TERRORIST KILLING OF ATTORNEY GENERAL ROBERTO GARCIA ALVARADO AND ACCUSE D THE FMLN OF THE CRIME ((SENTENCE 1) actions ((ACTION-DESC (SEM-REF MURDER) ) (ACTOR (SEM-REF TERRORIST) (ACT-WORD (TERRORIST)) ) (OBJECT (SEM-REF GOVERNMENT-OFFICIAL) (ACT-WORD (ATTORNEY GENERAL) ) (NAME (ROBERTO GARCIA ALVARADO)))) ) Sentence 2 : LEGISLATIVE ASSEMBLY PRESIDENT RICARDO VALDIVIESO AND VIC E PRESIDENT-ELECT FRANCISCO MERINO ALSO DECLARED THAT THE DEA", "label": "FARABUNDO MARTI NATIONAL LIBERATION FRONT", "ID": "753"}, {"sentence": "The initial chart is displayed, with potential noun phrases already grouped to- gether : Next sentence (1) : SALVADORAN PRESIDENT-ELECT ALFREDO CRISTIANI CONDEMNE D THE TERRORIST KILLING OF ATTORNEY GENERAL ROBERTO GARCIA ALVARADO AND ACCUSED THE FMLN OF THE CRIME Preprocessor results : Node 0 : (SALVADORAN PRESIDENT-ELECT ALFREDO CRISTIANI ) Node 1 : CONDEMNED Node 2 : (THE TERRORIST) Node 3 : (KILLING) KILLIN G Node 4 : OF Node 5 : ((ATTORNEY GENERAL) ROBERTO GARCIA ALVARADO ) Node 6 : AND Node 7 : ACCUSE D Node 8 : (THE (FMLN) ) Node 9 : OF Node 10 : (THE CRIME )", "label": "FARABUNDO MARTI NATIONAL LIBERATION FRONT", "ID": "754"}, {"sentence": "PERP : ORGANIZATION ID \"THE FMLN FRONT \" 11 .", "label": "FARABUIDO MARTI NATIONAL LIBERATION", "ID": "755"}, {"sentence": "PERP: ORGANIZATION ID \"THE FMLN FRONT \" 11 .", "label": "FARABUIDO MARTI NATIONAL LIBERATION", "ID": "756"}, {"sentence": "4 Hierarchical Chinese Restaurant Processes We describe a generative procedure analogous to the CRP of Section 2 for drawing words from the hierarchical Pitman- Yor language model with all Gu?s marginalized out.", "label": "Chinese restaurant process", "ID": "757"}, {"sentence": "Hierarchical topic models and the nested CRP.", "label": "Chinese restaurant process", "ID": "758"}, {"sentence": "12) where the counts are obtained from the seating ar- rangement Su in the CRP corresponding to Gu.", "label": "Chinese restaurant process", "ID": "759"}, {"sentence": "The hierarchical CRP is equivalent to the hierarchical Pitman-Yor language model insofar as the distribution induced on words drawn from them are exactly equal.", "label": "Chinese restaurant process", "ID": "760"}, {"sentence": "Variational inference for the nested CRP.", "label": "Chinese restaurant process", "ID": "761"}, {"sentence": "6) Pseudo-code for drawing words using the hier- archical CRP is given as a recursive function DrawWord(u), while pseudo- code for computing the probability that the next word drawn from Gu will be w is given in WordProb(u,w).", "label": "Chinese restaurant process", "ID": "762"}, {"sentence": "We can either use cross validation to determine the optimal number of topics or employ the infinite topic models, such as Hierarchical Dirichlet Process (HDP) (Teh et al, 2006) and nested CRP (Blei et al, 2010), to automatically adjust the number of topics during training.", "label": "Chinese Restaurant Process", "ID": "763"}, {"sentence": "i=1 p c (e i ) (1) where the translation distributions p t are assumed to have symmetric Dirichlet priors and the source token distribution p c a CRP prior.", "label": "Chinese Restaurant Process", "ID": "764"}, {"sentence": "It would be interesting to extend MVM to model hier- archy explicitly, and compare against baselines such as Brown clustering (Brown et al, 1992), the nested CRP (Blei et al, 2003) and the hierarchical Pachinko Allocation Model (Mimno et al, 2007).", "label": "Chinese Restaurant Process", "ID": "765"}, {"sentence": "In the CRP metaphor, there is one restaurant for each source word s, the s restaurant has ty(s) tables and total |s, ?", "label": "Chinese Restaurant Process", "ID": "766"}, {"sentence": "SBT priors use absolute discounting and learned backoff distributions for 774 smoothing sparse observation counts, rather than the fixed additive discounting utilized in Dirichlet and CRP models.", "label": "Chinese Restaurant Process", "ID": "767"}, {"sentence": "This research is part of the Interactive sYstems IYAS (Iyas) project, which is developed by the Arabic Lan- guage Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), Hamad bin Khalifa University (HBKU), part of Qatar Founda- tion in collaboration with MIT-CSAIL.", "label": "for Answer Search", "ID": "768"}, {"sentence": "It is part of the Interactive sYstems IYAS (Iyas) project, which is developed in collaboration with MIT-CSAIL.", "label": "for Answer Search", "ID": "769"}, {"sentence": "It is part of the Interactive sYstems IYAS (Iyas) project.", "label": "for Answer Search", "ID": "770"}, {"sentence": "Acknowledgments This research is part of the Interactive sYstems IYAS (Iyas) project, conducted by the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the Qatar Foundation.", "label": "for Answer Search", "ID": "771"}, {"sentence": "This research is part of the IYAS (Iyas) project, which is developed by the Arabic Lan- guage Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), Hamad bin Khalifa University (HBKU), part of Qatar Founda- tion in collaboration with MIT-CSAIL.", "label": "Interactive sYstems for Answer Search", "ID": "772"}, {"sentence": "It is part of the IYAS (Iyas) project, which is developed in collaboration with MIT-CSAIL.", "label": "Interactive sYstems for Answer Search", "ID": "773"}, {"sentence": "Emotion Classification Using  WB Corpora.", "label": "Web Blog", "ID": "774"}, {"sentence": "The limited utility of extra parses 1055 PP Clause Diff Mod NP 1-Word NP Corpus F-score Attach Attach Label Attach Attach Co-ord Span Unary aInt.a Other Best 0.022 0.016 0.013 0.011 0.011 0.010 0.009 0.006 0.005 0.021 WSJ 23 92.07 Brown-F 85.91 Brown-G 84.56 Brown-K 84.09 Brown-L 83.95 Brown-M 84.65 Brown-N 85.20 Brown-P 84.09 Brown-R 83.60 G-WBs 84.15 G-Web Email 81.18 Worst 0.040 0.035 0.053 0.020 0.034 0.023 0.046 0.009 0.029 0.073 Table 5: Average number of node errors per word for a range of domains using the Charniak parser with reranking and the self-trained model.", "label": "Web Blog", "ID": "775"}, {"sentence": "Length WSJ 23 Newswire 2416 23.5 Brown F Popular 3164 23.4 Brown G Biographies 3279 25.5 Brown K General 3881 17.2 Brown L Mystery 3714 15.7 Brown M Science 881 16.6 Brown N Adventure 4415 16.0 Brown P Romance 3942 17.4 Brown R Humour 967 22.7 G-WBs Blogs 1016 23.6 G-Web Email E-mail 2450 11.9 Table 6: Variation in size and contents of the domains we consider.", "label": "Web Blog", "ID": "776"}, {"sentence": "Emotion  Classification Using WB Corpora.", "label": "Web Blog", "ID": "777"}, {"sentence": "Emo- tion Classification from WB Corpora,  IEEE/WIC/ACM, 275-278.", "label": "Web Blog", "ID": "778"}, {"sentence": "In other  words, the standard must be as easy to follow  as the convention of inserting blanks at  WBs in English text processing.", "label": "wordbreak", "ID": "779"}, {"sentence": "To uti- lize the WB information provided by punctu- ations, we extract all strings with length l(2 ?", "label": "wordbreak", "ID": "780"}, {"sentence": "This architecture differs from con-  ventional Japanese WBers in that it does  not attempt to simultaneously attack the problems  of identifying segmentation candidates and  choosing the most probable analysis.", "label": "wordbreak", "ID": "781"}, {"sentence": "Rethinking Chinese word segmentation: tokenization, character classification, or WB identification.", "label": "wordbreak", "ID": "782"}, {"sentence": "The preced- ing and succeeding strings of punctuations carry ad- ditional WB information, since punctuations should be segmented as a word.", "label": "wordbreak", "ID": "783"}, {"sentence": "Since WBs  are not conventionally marked in Chinese text corpora, a  character-based collocation system has the dual  advantages of avoiding pre-proccssing distortion and  directly accessing sub-lexical information.", "label": "wordbreak", "ID": "784"}, {"sentence": "4 Word Co-Occurrence Matrix Each word (BF) from the list was sought in the balanced, 300 million segments7 version of the National Corpus of Polish (NKJP).", "label": "base form", "ID": "785"}, {"sentence": "The lexical representation of a regular  adjective has an entry in the lexicon as follows:  i 300 buon bueno  where \"buen-\" is the stem and \"bueno\" (good) the  dictionary BF.", "label": "base form", "ID": "786"}, {"sentence": "Inflection We introduced tag suffixes for inflec- tion as clues to identify the attachment position of the verb and adjective phrases, because Japanese verbs and adjectives have inflections, which depends 110 (no label) BF cont continuative form attr attributive form neg negative form hyp hypothetical form imp imperative form stem stem Table 2: Inflection tag suffixes on their modifying words and phrases (e.g. noun and verb phrases).", "label": "base form", "ID": "787"}, {"sentence": "Each of about 55,000 BFs requires at  le~t one arc in the graph.", "label": "base form", "ID": "788"}, {"sentence": "As English is a language with fewer inflections when compared to Romanian, which accommodates for gender and case as a suffix to the BF of a word, the automatic translation into English is closer to a human translation (experiment three).", "label": "base form", "ID": "789"}, {"sentence": "All verbs, nouns and prepositions were first reduced to their BFs in order to reduce the parameter space.", "label": "base form", "ID": "790"}, {"sentence": "As in SBTDM-wide, in SBTDM-tall the lower BFs occur toward the root of the tree.", "label": "branching factor", "ID": "791"}, {"sentence": "An idea of the lingulstic ov-  erage is given by the equivalent BF, which is  199  D1%-13.1  VEP~B(prop) = NOUN(interr-indir-loc) REFLEX <GOVERNOR,> NOUN(subj)  ;; Features and Agreements  <GOVERNOR> (MOOD ind) (TENSE pres) (NUMBER .x) ....  NOUN-1 ....  I%EFLEX nil  NOUN-2  (NUMBER ..x) ....  DefKS KS-24.13  ;;Composition  TO-HAVE-SOURCE= MOUNT <JOLLY> <HEADER> RIVER  ; ;Meaning  (TO-HAVE-SOURCE ! *", "label": "branching factor", "ID": "792"}, {"sentence": "SBTDM-wide is a shallow tree in which the BF in- creases from the root downward in the sequence 3, 6, 6, 9, 9, 12, 12.", "label": "branching factor", "ID": "793"}, {"sentence": "Clearly, ECFG can generate all sets of deriva- tion trees that GDG can, while CFG cannot (because of the unbounded BF of ECFG and of GDG); ECFG can also generate all sets of deriva- tion trees that CFG can, while GDG cannot (because of the lexicalization requirement).", "label": "branching factor", "ID": "794"}, {"sentence": "6 Conclusion We have presented a generative string-rewriting system, Extended Context-Free Grammar, whose derivation trees are dependency trees with un- bounded BF.", "label": "branching factor", "ID": "795"}, {"sentence": "For a grammar with a decision tree of average  BF 5, a parse tree that uses 20 different pro-  ductions will have a priority of the order (0.2) 2?", "label": "branching factor", "ID": "796"}, {"sentence": "We achieve our best labeled BF-score using 28 signatures with an unknown threshold of five.", "label": "bracketing f", "ID": "797"}, {"sentence": "As outlined in Section 6, the treebank guide- lines are somewhat ambiguous as to the appropriate BFor coordinate NPs which consist entirely of proper nouns.", "label": "bracketing f", "ID": "798"}, {"sentence": "Using the labeled BFrom the out- put of both parsers causes XLE to always fail when parsing.", "label": "bracketing f", "ID": "799"}, {"sentence": "and the Penn Treebank BFor this sen-  tence was:  ( ( (Theb igdog)  a te ) .", "label": "bracketing f", "ID": "800"}, {"sentence": "7 Results There is a statistically significant improvement3 in labeled BF-score on Sec.", "label": "bracketing f", "ID": "801"}, {"sentence": "Stage 2 processing is then  free to assign to the compound any BFor which it  3The design of this level of Lucy is influenced by Hobbs  (1985), which advocates a level of \"surfaey\" logical form  with predicates close to actual English words and a structure  similar to the syntactic structure of the sentence.", "label": "bracketing f", "ID": "802"}, {"sentence": "UPC, Barcelona, Spain.", "label": "Universitat Polit`ecnica de Catalunya", "ID": "803"}, {"sentence": "c?2014 Association for Computational Linguistics CHISPA on the GO A mobile Chinese-Spanish translation service for travelers in trouble Jordi Centelles 1,2 , Marta R. Costa-juss ` a 1,2 and Rafael E. Banchs 2 1 UPC, Barcelona 2 Institute for Infocomm Research, Singapore {visjcs,vismrc,rembanchs}@i2r.a-star.edu.sg Abstract This demo showcases a translation service that allows travelers to have an easy and convenient access to Chinese-Spanish translations via a mo- bile app.", "label": "Universitat Polit`ecnica de Catalunya", "ID": "804"}, {"sentence": "o and Jordi Turmo TALP Research Center UPC Barcelona, Spain {esapena, padro, turmo}@lsi.upc.edu Abstract This paper describes the participation of RelaxCor in the Semeval-2010 task number 1: ?", "label": "Universitat Polit`ecnica de Catalunya", "ID": "805"}, {"sentence": "c?2015 Association for Computational Linguistics Low-Rank Regularization for Sparse Conjunctive Feature Spaces: An Application to Named Entity Classification Audi Primadhanty UPC primadhanty@cs.upc.edu Xavier Carreras Ariadna Quattoni Xerox Research Centre Europe xavier.carreras@xrce.xerox.com ariadna.quattoni@xrce.xerox.com Abstract Entity classification, like many other important problems in NLP, involves learning classifiers over sparse high- dimensional feature spaces that result from the conjunction of elementary fea- tures of the", "label": "Universitat Polit`ecnica de Catalunya", "ID": "806"}, {"sentence": "e A. R. Fonollosa TALP Research Center UPC, Barcelona {marta.ruiz,jose.fonollosa}@upc.edu Abstract Neural Machine Translation (MT) has reached state-of-the-art results.", "label": "Universitat Polit`ecnica de Catalunya", "ID": "807"}, {"sentence": "Semiautomatic creation of taxonomies Javier Farreres and Horacio Rodr?guez farreres@lsi.upc.es and horacio@lsi.upc.es Department of Computer Languages and Systems UPC de Catalunya Karina Gibert karina@eio.upc.es Department of Statistics and Operations Research UPC de Catalunya Abstract In this paper we face the automatic con- struction of a lexical taxonomy for the Spanish language using as input a taxon- omy of English (WordNet)1 and a set of bilingual (English/Spanish) resources.", "label": "Universitat Polit?cnica", "ID": "808"}, {"sentence": "160  A Quantitative Method for Machine Translation Evaluation    Jes?s Tom?s  Escola Polit?cnica Superior de  Gandia  UPC de  Val?ncia  jtomas@upv.es  Josep ?", "label": "Universitat Polit?cnica", "ID": "809"}, {"sentence": "Institut Tecnol?gic d?Inform?tica, UPC de Val?ncia Cam?", "label": "Universitat Polit?cnica", "ID": "810"}, {"sentence": "ELiRF,  UPC de Val?ncia, Spain  prosso@dsic.upv.es  {fer.callotl,mmontesg,villasen}  @inaoep.mx      Abstract  This paper presents three methods to evaluate  the Semantic Textual Similarity (STS).", "label": "Universitat Polit?cnica", "ID": "811"}, {"sentence": "ngel Mas  Departament d?Idiomes  UPC de  Val?ncia  jamas@idm.upv.es  Francisco Casacuberta  Institut Tecnol?gic  d?Inform?tica  UPC de  Val?ncia  fcn@iti.upv.es    Abstract  Accurate evaluation of machine  translation (MT) is an open problem.", "label": "Universitat Polit?cnica", "ID": "812"}]